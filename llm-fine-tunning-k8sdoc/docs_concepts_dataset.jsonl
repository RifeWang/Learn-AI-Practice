{"en": "The Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your {{< glossary_tooltip text=\"cluster\" term_id=\"cluster\" length=\"all\" >}}, and helps you obtain a deeper understanding of how Kubernetes works.", "zh": "概念部分帮助你了解 Kubernetes 系统的各个部分以及\nKubernetes 用来表示{{<glossary_tooltip text=\"集群\" term_id=\"cluster\" length=\"all\" >}}的抽象概念，\n并帮助你更深入地理解 Kubernetes 是如何工作的。"}
{"en": "This document highlights and consolidates configuration best practices that are introduced\nthroughout the user guide, Getting Started documentation, and examples.", "zh": "本文档重点介绍并整合了整个用户指南、入门文档和示例中介绍的配置最佳实践。"}
{"en": "This is a living document. If you think of something that is not on this list but might be useful\nto others, please don't hesitate to file an issue or submit a PR.", "zh": "这是一份不断改进的文件。\n如果你认为某些内容缺失但可能对其他人有用，请不要犹豫，提交 Issue 或提交 PR。"}
{"en": "## General Configuration Tips", "zh": "## 一般配置提示  {#general-configuration-tips}"}
{"en": "- When defining configurations, specify the latest stable API version.", "zh": "- 定义配置时，请指定最新的稳定 API 版本。"}
{"en": "- Configuration files should be stored in version control before being pushed to the cluster. This\n  allows you to quickly roll back a configuration change if necessary. It also aids cluster\n  re-creation and restoration.", "zh": "- 在推送到集群之前，配置文件应存储在版本控制中。\n  这允许你在必要时快速回滚配置更改。\n  它还有助于集群重新创建和恢复。"}
{"en": "- Write your configuration files using YAML rather than JSON. Though these formats can be used\n  interchangeably in almost all scenarios, YAML tends to be more user-friendly.", "zh": "- 使用 YAML 而不是 JSON 编写配置文件。虽然这些格式几乎可以在所有场景中互换使用，但 YAML 往往更加用户友好。"}
{"en": "- Group related objects into a single file whenever it makes sense. One file is often easier to\n  manage than several. See the\n  [guestbook-all-in-one.yaml](https://github.com/kubernetes/examples/tree/master/guestbook/all-in-one/guestbook-all-in-one.yaml)\n  file as an example of this syntax.", "zh": "- 只要有意义，就将相关对象分组到一个文件中。一个文件通常比几个文件更容易管理。\n  请参阅 [guestbook-all-in-one.yaml](https://github.com/kubernetes/examples/tree/master/guestbook/all-in-one/guestbook-all-in-one.yaml)\n  文件作为此语法的示例。"}
{"en": "- Note also that many `kubectl` commands can be called on a directory. For example, you can call\n  `kubectl apply` on a directory of config files.", "zh": "- 另请注意，可以在目录上调用许多 `kubectl` 命令。\n  例如，你可以在配置文件的目录中调用 `kubectl apply`。"}
{"en": "- Don't specify default values unnecessarily: simple, minimal configuration will make errors less likely.", "zh": "- 除非必要，否则不指定默认值：简单的最小配置会降低错误的可能性。"}
{"en": "- Put object descriptions in annotations, to allow better introspection.", "zh": "- 将对象描述放在注释中，以便更好地进行内省。\n\n{{< note >}}"}
{"en": "There is a breaking change introduced in the [YAML 1.2](https://yaml.org/spec/1.2.0/#id2602744)\nboolean values specification with respect to [YAML 1.1](https://yaml.org/spec/1.1/#id864510).\nThis is a known [issue](https://github.com/kubernetes/kubernetes/issues/34146) in Kubernetes.\nYAML 1.2 only recognizes **true** and **false** as valid booleans, while YAML 1.1 also accepts\n**yes**, **no**, **on**, and  **off** as booleans. However, Kubernetes uses YAML\n[parsers](https://github.com/kubernetes/kubernetes/issues/34146#issuecomment-252692024) that are\nmostly compatible with YAML 1.1, which means that using **yes** or **no** instead of **true** or\n**false** in a YAML manifest may cause unexpected errors or behaviors. To avoid this issue, it is\nrecommended to always use **true** or **false** for boolean values in YAML manifests, and to quote\nany strings that may be confused with booleans, such as **\"yes\"** or **\"no\"**.", "zh": "相较于 [YAML 1.1](https://yaml.org/spec/1.1/#id864510)，\n[YAML 1.2](https://yaml.org/spec/1.2.0/#id2602744) 在布尔值规范中引入了一个破坏性的变更。\n这是 Kubernetes 中的一个已知[问题](https://github.com/kubernetes/kubernetes/issues/34146)。\nYAML 1.2 仅识别 **true** 和 **false** 作为有效的布尔值，而 YAML 1.1 还可以接受 \n**yes**、**no**、**on** 和 **off** 作为布尔值。\n然而，Kubernetes 正在使用的 YAML [解析器](https://github.com/kubernetes/kubernetes/issues/34146#issuecomment-252692024)\n与 YAML 1.1 基本兼容，\n这意味着在 YAML 清单中使用 **yes** 或 **no** 而不是 **true** 或 **false** 可能会导致意外的错误或行为。\n为避免此类问题，建议在 YAML 清单中始终使用 **true** 或 **false** 作为布尔值，\n并对任何可能与布尔值混淆的字符串进行引号标记，例如 **\"yes\"** 或 **\"no\"**。"}
{"en": "Besides booleans, there are additional specifications changes between YAML versions. Please refer\nto the [YAML Specification Changes](https://spec.yaml.io/main/spec/1.2.2/ext/changes) documentation\nfor a comprehensive list.", "zh": "除了布尔值之外，YAML 版本之间还存在其他的规范变化。\n请参考 [YAML 规范变更](https://spec.yaml.io/main/spec/1.2.2/ext/changes)文档来获取完整列表。\n{{< /note >}}"}
{"en": "## \"Naked\" Pods versus ReplicaSets, Deployments, and Jobs {#naked-pods-vs-replicasets-deployments-and-jobs}", "zh": "## “独立的“ Pod 与 ReplicaSet、Deployment 和 Job {#naked-pods-vs-replicasets-deployments-and-jobs}"}
{"en": "- Don't use naked Pods (that is, Pods not bound to a [ReplicaSet](/docs/concepts/workloads/controllers/replicaset/) or\n  [Deployment](/docs/concepts/workloads/controllers/deployment/)) if you can avoid it. Naked Pods\n  will not be rescheduled in the event of a node failure.\n\n  A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is\n  always available, and specifies a strategy to replace Pods (such as\n  [RollingUpdate](/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment)), is\n  almost always preferable to creating Pods directly, except for some explicit\n  [`restartPolicy: Never`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) scenarios.\n  A [Job](/docs/concepts/workloads/controllers/job/) may also be appropriate.", "zh": "- 如果可能，不要使用独立的 Pod（即，未绑定到\n  [ReplicaSet](/zh-cn/docs/concepts/workloads/controllers/replicaset/) 或\n  [Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/) 的 Pod）。\n  如果节点发生故障，将不会重新调度这些独立的 Pod。\n\n  Deployment 既可以创建一个 ReplicaSet 来确保预期个数的 Pod 始终可用，也可以指定替换 Pod 的策略（例如\n  [RollingUpdate](/zh-cn/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment)）。\n  除了一些显式的 [`restartPolicy: Never`](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\n  场景外，Deployment 通常比直接创建 Pod 要好得多。\n  [Job](/zh-cn/docs/concepts/workloads/controllers/job/) 也可能是合适的选择。"}
{"en": "## Services", "zh": "## 服务   {#services}"}
{"en": "- Create a [Service](/docs/concepts/services-networking/service/) before its corresponding backend\n  workloads (Deployments or ReplicaSets), and before any workloads that need to access it.\n  When Kubernetes starts a container, it provides environment variables pointing to all the Services\n  which were running when the container was started. For example, if a Service named `foo` exists,\n  all containers will get the following variables in their initial environment:", "zh": "- 在创建相应的后端工作负载（Deployment 或 ReplicaSet），以及在需要访问它的任何工作负载之前创建\n  [服务](/zh-cn/docs/concepts/services-networking/service/)。\n  当 Kubernetes 启动容器时，它提供指向启动容器时正在运行的所有服务的环境变量。\n  例如，如果存在名为 `foo` 的服务，则所有容器将在其初始环境中获得以下变量。\n\n  ```shell\n  FOO_SERVICE_HOST=<the host the Service is running on>\n  FOO_SERVICE_PORT=<the port the Service is running on>\n  ```"}
{"en": "*This does imply an ordering requirement* - any `Service` that a `Pod` wants to access must be\n  created before the `Pod` itself, or else the environment variables will not be populated.\n  DNS does not have this restriction.", "zh": "**这确实意味着在顺序上的要求** - 必须在 `Pod` 本身被创建之前创建 `Pod` 想要访问的任何 `Service`，\n  否则将环境变量不会生效。DNS 没有此限制。"}
{"en": "- An optional (though strongly recommended) [cluster add-on](/docs/concepts/cluster-administration/addons/)\n  is a DNS server. The DNS server watches the Kubernetes API for new `Services` and creates a set\n  of DNS records for each. If DNS has been enabled throughout the cluster then all `Pods` should be\n  able to do name resolution of `Services` automatically.", "zh": "- 一个可选（尽管强烈推荐）的[集群插件](/zh-cn/docs/concepts/cluster-administration/addons/)\n  是 DNS 服务器。DNS 服务器为新的 `Services` 监视 Kubernetes API，并为每个创建一组 DNS 记录。\n  如果在整个集群中启用了 DNS，则所有 `Pod` 应该能够自动对 `Services` 进行名称解析。"}
{"en": "- Don't specify a `hostPort` for a Pod unless it is absolutely necessary. When you bind a Pod to a\n  `hostPort`, it limits the number of places the Pod can be scheduled, because each <`hostIP`,\n  `hostPort`, `protocol`> combination must be unique. If you don't specify the `hostIP` and\n  `protocol` explicitly, Kubernetes will use `0.0.0.0` as the default `hostIP` and `TCP` as the\n  default `protocol`.\n\n  If you only need access to the port for debugging purposes, you can use the\n  [apiserver proxy](/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls)\n  or [`kubectl port-forward`](/docs/tasks/access-application-cluster/port-forward-access-application-cluster/).\n\n  If you explicitly need to expose a Pod's port on the node, consider using a\n  [NodePort](/docs/concepts/services-networking/service/#type-nodeport) Service before resorting to\n  `hostPort`.", "zh": "- 不要为 Pod 指定 `hostPort`，除非非常有必要这样做。\n  当你为 Pod 绑定了 `hostPort`，那么能够运行该 Pod 的节点就有限了，因为每个 `<hostIP, hostPort, protocol>` 组合必须是唯一的。\n  如果你没有明确指定 `hostIP` 和 `protocol`，\n  Kubernetes 将使用 `0.0.0.0` 作为默认的 `hostIP`，使用 `TCP` 作为默认的 `protocol`。\n\n  如果你只需要访问端口以进行调试，则可以使用\n  [apiserver proxy](/zh-cn/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls)\n  或\n  [`kubectl port-forward`](/zh-cn/docs/tasks/access-application-cluster/port-forward-access-application-cluster/)。\n\n  如果你明确需要在节点上公开 Pod 的端口，请在使用 `hostPort` 之前考虑使用\n  [NodePort](/zh-cn/docs/concepts/services-networking/service/#type-nodeport) 服务。"}
{"en": "- Avoid using `hostNetwork`, for the same reasons as `hostPort`.", "zh": "- 避免使用 `hostNetwork`，原因与 `hostPort` 相同。"}
{"en": "- Use [headless Services](/docs/concepts/services-networking/service/#headless-services)\n  (which have a `ClusterIP` of `None`) for service discovery when you don't need `kube-proxy`\n  load balancing.", "zh": "- 当你不需要 `kube-proxy` 负载均衡时，\n  使用[无头服务](/zh-cn/docs/concepts/services-networking/service/#headless-services)\n  （`ClusterIP` 被设置为 `None`）进行服务发现。"}
{"en": "## Using Labels", "zh": "## 使用标签   {#using-labels}"}
{"en": "- Define and use [labels](/docs/concepts/overview/working-with-objects/labels/) that identify\n  __semantic attributes__ of your application or Deployment, such as `{ app.kubernetes.io/name:\n  MyApp, tier: frontend, phase: test, deployment: v3 }`. You can use these labels to select the\n  appropriate Pods for other resources; for example, a Service that selects all `tier: frontend`\n  Pods, or all `phase: test` components of `app.kubernetes.io/name: MyApp`.\n  See the [guestbook](https://github.com/kubernetes/examples/tree/master/guestbook/) app\n  for examples of this approach.\n\n  A Service can be made to span multiple Deployments by omitting release-specific labels from its\n  selector. When you need to update a running service without downtime, use a\n  [Deployment](/docs/concepts/workloads/controllers/deployment/).\n\n  A desired state of an object is described by a Deployment, and if changes to that spec are\n  _applied_, the deployment controller changes the actual state to the desired state at a controlled\n  rate.", "zh": "- 定义并使用[标签](/zh-cn/docs/concepts/overview/working-with-objects/labels/)来识别应用程序\n  或 Deployment 的**语义属性**，例如 `{ app.kubernetes.io/name: MyApp, tier: frontend, phase: test, deployment: v3 }`。\n  你可以使用这些标签为其他资源选择合适的 Pod；\n  例如，一个选择所有 `tier: frontend` Pod 的服务，或者 `app.kubernetes.io/name: MyApp` 的所有 `phase: test` 组件。\n  有关此方法的示例，请参阅 [guestbook](https://github.com/kubernetes/examples/tree/master/guestbook/) 。\n\n  通过从选择器中省略特定发行版的标签，可以使服务跨越多个 Deployment。\n  当你需要不停机的情况下更新正在运行的服务，可以使用 [Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/)。\n\n  Deployment 描述了对象的期望状态，并且如果对该规约的更改被成功应用，则 Deployment\n  控制器以受控速率将实际状态改变为期望状态。"}
{"en": "- Use the [Kubernetes common labels](/docs/concepts/overview/working-with-objects/common-labels/)\n  for common use cases. These standardized labels enrich the metadata in a way that allows tools,\n  including `kubectl` and [dashboard](/docs/tasks/access-application-cluster/web-ui-dashboard), to\n  work in an interoperable way.", "zh": "- 对于常见场景，应使用 [Kubernetes 通用标签](/zh-cn/docs/concepts/overview/working-with-objects/common-labels/)。\n  这些标准化的标签丰富了对象的元数据，使得包括 `kubectl` 和\n  [仪表板（Dashboard）](/zh-cn/docs/tasks/access-application-cluster/web-ui-dashboard)\n  这些工具能够以可互操作的方式工作。"}
{"en": "- You can manipulate labels for debugging. Because Kubernetes controllers (such as ReplicaSet) and\n  Services match to Pods using selector labels, removing the relevant labels from a Pod will stop\n  it from being considered by a controller or from being served traffic by a Service. If you remove\n  the labels of an existing Pod, its controller will create a new Pod to take its place. This is a\n  useful way to debug a previously \"live\" Pod in a \"quarantine\" environment. To interactively remove\n  or add labels, use [`kubectl label`](/docs/reference/generated/kubectl/kubectl-commands#label).", "zh": "- 你可以操纵标签进行调试。\n  由于 Kubernetes 控制器（例如 ReplicaSet）和服务使用选择器标签来匹配 Pod，\n  从 Pod 中删除相关标签将阻止其被控制器考虑或由服务提供服务流量。\n  如果删除现有 Pod 的标签，其控制器将创建一个新的 Pod 来取代它。\n  这是在“隔离“环境中调试先前“活跃“的 Pod 的有用方法。\n  要以交互方式删除或添加标签，请使用 [`kubectl label`](/docs/reference/generated/kubectl/kubectl-commands#label)。"}
{"en": "## Using kubectl", "zh": "## 使用 kubectl   {#using-kubectl}"}
{"en": "- Use `kubectl apply -f <directory>`. This looks for Kubernetes configuration in all `.yaml`,\n  `.yml`, and `.json` files in `<directory>` and passes it to `apply`.", "zh": "- 使用 `kubectl apply -f <目录>`。\n  它在 `<目录>` 中的所有 `.yaml`、`.yml` 和 `.json` 文件中查找 Kubernetes 配置，并将其传递给 `apply`。"}
{"en": "- Use label selectors for `get` and `delete` operations instead of specific object names. See the\n  sections on [label selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors)\n  and [using labels effectively](/docs/concepts/overview/working-with-objects/labels/#using-labels-effectively).", "zh": "- 使用标签选择器进行 `get` 和 `delete` 操作，而不是特定的对象名称。\n- 请参阅[标签选择器](/zh-cn/docs/concepts/overview/working-with-objects/labels/#label-selectors)和\n  [有效使用标签](/zh-cn/docs/concepts/overview/working-with-objects/labels/#using-labels-effectively)部分。"}
{"en": "- Use `kubectl create deployment` and `kubectl expose` to quickly create single-container\n  Deployments and Services.\n  See [Use a Service to Access an Application in a Cluster](/docs/tasks/access-application-cluster/service-access-application-cluster/)\n  for an example.", "zh": "- 使用 `kubectl create deployment` 和 `kubectl expose` 来快速创建单容器 Deployment 和 Service。\n  有关示例，请参阅[使用服务访问集群中的应用程序](/zh-cn/docs/tasks/access-application-cluster/service-access-application-cluster/)。"}
{"en": "overview", "zh": "{{< glossary_definition term_id=\"configmap\" length=\"all\" >}}\n\n{{< caution >}}"}
{"en": "ConfigMap does not provide secrecy or encryption.\nIf the data you want to store are confidential, use a\n{{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}} rather than a ConfigMap,\nor use additional (third party) tools to keep your data private.", "zh": "ConfigMap 并不提供保密或者加密功能。\n如果你想存储的数据是机密的，请使用 {{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}，\n或者使用其他第三方工具来保证你的数据的私密性，而不是用 ConfigMap。\n{{< /caution >}}"}
{"en": "## Motivation\n\nUse a ConfigMap for setting configuration data separately from application code.\n\nFor example, imagine that you are developing an application that you can run on your\nown computer (for development) and in the cloud (to handle real traffic).\nYou write the code to look in an environment variable named `DATABASE_HOST`.\nLocally, you set that variable to `localhost`. In the cloud, you set it to\nrefer to a Kubernetes {{< glossary_tooltip text=\"Service\" term_id=\"service\" >}}\nthat exposes the database component to your cluster.\nThis lets you fetch a container image running in the cloud and\ndebug the exact same code locally if needed.", "zh": "## 动机   {#motivation}\n\n使用 ConfigMap 来将你的配置数据和应用程序代码分开。\n\n比如，假设你正在开发一个应用，它可以在你自己的电脑上（用于开发）和在云上\n（用于实际流量）运行。\n你的代码里有一段是用于查看环境变量 `DATABASE_HOST`，在本地运行时，\n你将这个变量设置为 `localhost`，在云上，你将其设置为引用 Kubernetes 集群中的\n公开数据库组件的 {{< glossary_tooltip text=\"服务\" term_id=\"service\" >}}。\n\n这让你可以获取在云中运行的容器镜像，并且如果有需要的话，在本地调试完全相同的代码。\n\n{{< note >}}"}
{"en": "A ConfigMap is not designed to hold large chunks of data. The data stored in a\nConfigMap cannot exceed 1 MiB. If you need to store settings that are\nlarger than this limit, you may want to consider mounting a volume or use a\nseparate database or file service.", "zh": "ConfigMap 在设计上不是用来保存大量数据的。在 ConfigMap 中保存的数据不可超过\n1 MiB。如果你需要保存超出此尺寸限制的数据，你可能希望考虑挂载存储卷\n或者使用独立的数据库或者文件服务。\n{{< /note >}}"}
{"en": "## ConfigMap object\n\nA ConfigMap is an {{< glossary_tooltip text=\"API object\" term_id=\"object\" >}}\nthat lets you store configuration for other objects to use. Unlike most\nKubernetes objects that have a `spec`, a ConfigMap has `data` and `binaryData`\nfields. These fields accept key-value pairs as their values.  Both the `data`\nfield and the `binaryData` are optional. The `data` field is designed to\ncontain UTF-8 strings while the `binaryData` field is designed to\ncontain binary data as base64-encoded strings.\n\nThe name of a ConfigMap must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).", "zh": "## ConfigMap 对象\n\nConfigMap 是一个让你可以存储其他对象所需要使用的配置的 {{< glossary_tooltip text=\"API 对象\" term_id=\"object\" >}}。\n和其他 Kubernetes 对象都有一个 `spec` 不同的是，ConfigMap 使用 `data` 和\n`binaryData` 字段。这些字段能够接收键-值对作为其取值。`data` 和 `binaryData`\n字段都是可选的。`data` 字段设计用来保存 UTF-8 字符串，而 `binaryData`\n则被设计用来保存二进制数据作为 base64 编码的字串。\n\nConfigMap 的名字必须是一个合法的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。"}
{"en": "Each key under the `data` or the `binaryData` field must consist of\nalphanumeric characters, `-`, `_` or `.`. The keys stored in `data` must not\noverlap with the keys in the `binaryData` field.\n\nStarting from v1.19, you can add an `immutable` field to a ConfigMap\ndefinition to create an [immutable ConfigMap](#configmap-immutable).", "zh": "`data` 或 `binaryData` 字段下面的每个键的名称都必须由字母数字字符或者\n`-`、`_` 或 `.` 组成。在 `data` 下保存的键名不可以与在 `binaryData`\n下出现的键名有重叠。\n\n从 v1.19 开始，你可以添加一个 `immutable` 字段到 ConfigMap 定义中，\n创建[不可变更的 ConfigMap](#configmap-immutable)。"}
{"en": "## ConfigMaps and Pods\n\nYou can write a Pod `spec` that refers to a ConfigMap and configures the container(s)\nin that Pod based on the data in the ConfigMap. The Pod and the ConfigMap must be in\nthe same {{< glossary_tooltip text=\"namespace\" term_id=\"namespace\" >}}.", "zh": "## ConfigMap 和 Pod   {#configmaps-and-pods}\n\n你可以写一个引用 ConfigMap 的 Pod 的 `spec`，并根据 ConfigMap 中的数据在该\nPod 中配置容器。这个 Pod 和 ConfigMap 必须要在同一个\n{{< glossary_tooltip text=\"名字空间\" term_id=\"namespace\" >}} 中。\n\n{{< note >}}"}
{"en": "The `spec` of a {{< glossary_tooltip text=\"static Pod\" term_id=\"static-pod\" >}} cannot refer to a ConfigMap\nor any other API objects.", "zh": "{{< glossary_tooltip text=\"静态 Pod\" term_id=\"static-pod\" >}} 中的 `spec`\n字段不能引用 ConfigMap 或任何其他 API 对象。\n{{< /note >}}"}
{"en": "Here's an example ConfigMap that has some keys with single values,\nand other keys where the value looks like a fragment of a configuration\nformat.", "zh": "这是一个 ConfigMap 的示例，它的一些键只有一个值，其他键的值看起来像是\n配置的片段格式。\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-demo\ndata:\n  # 类属性键；每一个键都映射到一个简单的值\n  player_initial_lives: \"3\"\n  ui_properties_file_name: \"user-interface.properties\"\n\n  # 类文件键\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5\n  user-interface.properties: |\n    color.good=purple\n    color.bad=yellow\n    allow.textmode=true\n```"}
{"en": "There are four different ways that you can use a ConfigMap to configure\na container inside a Pod:\n\n1. Inside a container command and args\n1. Environment variables for a container\n1. Add a file in read-only volume, for the application to read\n1. Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap\n\nThese different methods lend themselves to different ways of modeling\nthe data being consumed.\nFor the first three methods, the\n{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} uses the data from\nthe ConfigMap when it launches container(s) for a Pod.", "zh": "你可以使用四种方式来使用 ConfigMap 配置 Pod 中的容器：\n\n1. 在容器命令和参数内\n1. 容器的环境变量\n1. 在只读卷里面添加一个文件，让应用来读取\n1. 编写代码在 Pod 中运行，使用 Kubernetes API 来读取 ConfigMap\n\n这些不同的方法适用于不同的数据使用方式。\n对前三个方法，{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}}\n使用 ConfigMap 中的数据在 Pod 中启动容器。"}
{"en": "The fourth method means you have to write code to read the ConfigMap and its data.\nHowever, because you're using the Kubernetes API directly, your application can\nsubscribe to get updates whenever the ConfigMap changes, and react\nwhen that happens. By accessing the Kubernetes API directly, this\ntechnique also lets you access a ConfigMap in a different namespace.\n\nHere's an example Pod that uses values from `game-demo` to configure a Pod:", "zh": "第四种方法意味着你必须编写代码才能读取 ConfigMap 和它的数据。然而，\n由于你是直接使用 Kubernetes API，因此只要 ConfigMap 发生更改，\n你的应用就能够通过订阅来获取更新，并且在这样的情况发生的时候做出反应。\n通过直接进入 Kubernetes API，这个技术也可以让你能够获取到不同的名字空间里的 ConfigMap。\n\n下面是一个 Pod 的示例，它通过使用 `game-demo` 中的值来配置一个 Pod：\n\n{{% code_sample file=\"configmap/configure-pod.yaml\" %}}"}
{"en": "A ConfigMap doesn't differentiate between single line property values and\nmulti-line file-like values.\nWhat matters how Pods and other objects consume those values.\n\nFor this example, defining a volume and mounting it inside the `demo`\ncontainer as `/config` creates two files,\n`/config/game.properties` and `/config/user-interface.properties`,\neven though there are four keys in the ConfigMap. This is because the Pod\ndefinition specifies an `items` array in the `volumes` section.\nIf you omit the `items` array entirely, every key  in the ConfigMap becomes\na file with the same name as the key, and you get 4 files.", "zh": "ConfigMap 不会区分单行属性值和多行类似文件的值，重要的是 Pods\n和其他对象如何使用这些值。\n\n上面的例子定义了一个卷并将它作为 `/config` 文件夹挂载到 `demo` 容器内，\n创建两个文件，`/config/game.properties` 和\n`/config/user-interface.properties`，\n尽管 ConfigMap 中包含了四个键。\n这是因为 Pod 定义中在 `volumes` 节指定了一个 `items` 数组。\n如果你完全忽略 `items` 数组，则 ConfigMap 中的每个键都会变成一个与该键同名的文件，\n因此你会得到四个文件。"}
{"en": "## Using ConfigMaps\n\nConfigMaps can be mounted as data volumes. ConfigMaps can also be used by other\nparts of the system, without being directly exposed to the Pod. For example,\nConfigMaps can hold data that other parts of the system should use for configuration.", "zh": "## 使用 ConfigMap   {#using-configmaps}\n\nConfigMap 可以作为数据卷挂载。ConfigMap 也可被系统的其他组件使用，\n而不一定直接暴露给 Pod。例如，ConfigMap 可以保存系统中其他组件要使用的配置数据。"}
{"en": "The most common way to use ConfigMaps is to configure settings for\ncontainers running in a Pod in the same namespace. You can also use a\nConfigMap separately.\n\nFor example, you\nmight encounter {{< glossary_tooltip text=\"addons\" term_id=\"addons\" >}}\nor {{< glossary_tooltip text=\"operators\" term_id=\"operator-pattern\" >}} that\nadjust their behavior based on a ConfigMap.", "zh": "ConfigMap 最常见的用法是为同一命名空间里某 Pod 中运行的容器执行配置。\n你也可以单独使用 ConfigMap。\n\n比如，你可能会遇到基于 ConfigMap 来调整其行为的\n{{< glossary_tooltip text=\"插件\" term_id=\"addons\" >}} 或者\n{{< glossary_tooltip text=\"operator\" term_id=\"operator-pattern\" >}}。"}
{"en": "### Using ConfigMaps as files from a Pod\n\nTo consume a ConfigMap in a volume in a Pod:", "zh": "### 在 Pod 中将 ConfigMap 当做文件使用\n\n要在一个 Pod 的存储卷中使用 ConfigMap:"}
{"en": "1. Create a ConfigMap or use an existing one. Multiple Pods can reference the\n   same ConfigMap.\n1. Modify your Pod definition to add a volume under `.spec.volumes[]`. Name\n   the volume anything, and have a `.spec.volumes[].configMap.name` field set\n   to reference your ConfigMap object.\n1. Add a `.spec.containers[].volumeMounts[]` to each container that needs the\n   ConfigMap. Specify `.spec.containers[].volumeMounts[].readOnly = true` and\n   `.spec.containers[].volumeMounts[].mountPath` to an unused directory name\n   where you would like the ConfigMap to appear.\n1. Modify your image or command line so that the program looks for files in\n   that directory. Each key in the ConfigMap `data` map becomes the filename\n   under `mountPath`.", "zh": "1. 创建一个 ConfigMap 对象或者使用现有的 ConfigMap 对象。多个 Pod 可以引用同一个\n   ConfigMap。\n1. 修改 Pod 定义，在 `spec.volumes[]` 下添加一个卷。\n   为该卷设置任意名称，之后将 `spec.volumes[].configMap.name` 字段设置为对你的\n   ConfigMap 对象的引用。\n1. 为每个需要该 ConfigMap 的容器添加一个 `.spec.containers[].volumeMounts[]`。\n   设置 `.spec.containers[].volumeMounts[].readOnly=true` 并将\n   `.spec.containers[].volumeMounts[].mountPath` 设置为一个未使用的目录名，\n   ConfigMap 的内容将出现在该目录中。\n1. 更改你的镜像或者命令行，以便程序能够从该目录中查找文件。ConfigMap 中的每个\n   `data` 键会变成 `mountPath` 下面的一个文件名。"}
{"en": "This is an example of a Pod that mounts a ConfigMap in a volume:", "zh": "下面是一个将 ConfigMap 以卷的形式进行挂载的 Pod 示例：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - name: mypod\n    image: redis\n    volumeMounts:\n    - name: foo\n      mountPath: \"/etc/foo\"\n      readOnly: true\n  volumes:\n  - name: foo\n    configMap:\n      name: myconfigmap\n```"}
{"en": "Each ConfigMap you want to use needs to be referred to in `.spec.volumes`.\n\nIf there are multiple containers in the Pod, then each container needs its\nown `volumeMounts` block, but only one `.spec.volumes` is needed per ConfigMap.", "zh": "你希望使用的每个 ConfigMap 都需要在 `spec.volumes` 中被引用到。\n\n如果 Pod 中有多个容器，则每个容器都需要自己的 `volumeMounts` 块，但针对每个\nConfigMap，你只需要设置一个 `spec.volumes` 块。"}
{"en": "#### Mounted ConfigMaps are updated automatically\n\nWhen a ConfigMap currently consumed in a volume is updated, projected keys are eventually updated as well.\nThe kubelet checks whether the mounted ConfigMap is fresh on every periodic sync.\nHowever, the kubelet uses its local cache for getting the current value of the ConfigMap.\nThe type of the cache is configurable using the `configMapAndSecretChangeDetectionStrategy` field in\nthe [KubeletConfiguration struct](/docs/reference/config-api/kubelet-config.v1beta1/).", "zh": "#### 被挂载的 ConfigMap 内容会被自动更新\n\n当卷中使用的 ConfigMap 被更新时，所投射的键最终也会被更新。\nkubelet 组件会在每次周期性同步时检查所挂载的 ConfigMap 是否为最新。\n不过，kubelet 使用的是其本地的高速缓存来获得 ConfigMap 的当前值。\n高速缓存的类型可以通过\n[KubeletConfiguration 结构](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/).\n的 `configMapAndSecretChangeDetectionStrategy` 字段来配置。"}
{"en": "A ConfigMap can be either propagated by watch (default), ttl-based, or by redirecting\nall requests directly to the API server.\nAs a result, the total delay from the moment when the ConfigMap is updated to the moment\nwhen new keys are projected to the Pod can be as long as the kubelet sync period + cache\npropagation delay, where the cache propagation delay depends on the chosen cache type\n(it equals to watch propagation delay, ttl of cache, or zero correspondingly).", "zh": "ConfigMap 既可以通过 watch 操作实现内容传播（默认形式），也可实现基于 TTL\n的缓存，还可以直接经过所有请求重定向到 API 服务器。\n因此，从 ConfigMap 被更新的那一刻算起，到新的主键被投射到 Pod 中去，\n这一时间跨度可能与 kubelet 的同步周期加上高速缓存的传播延迟相等。\n这里的传播延迟取决于所选的高速缓存类型\n（分别对应 watch 操作的传播延迟、高速缓存的 TTL 时长或者 0）。"}
{"en": "ConfigMaps consumed as environment variables are not updated automatically and require a pod restart.", "zh": "以环境变量方式使用的 ConfigMap 数据不会被自动更新。\n更新这些数据需要重新启动 Pod。\n\n{{< note >}}"}
{"en": "A container using a ConfigMap as a [subPath](/docs/concepts/storage/volumes#using-subpath) volume mount will not receive ConfigMap updates.", "zh": "使用 ConfigMap 作为 [subPath](/zh-cn/docs/concepts/storage/volumes#using-subpath)\n卷挂载的容器将不会收到 ConfigMap 的更新。\n{{< /note >}}"}
{"en": "### Using Configmaps as environment variables\n\nTo use a Configmap in an {{< glossary_tooltip text=\"environment variable\" term_id=\"container-env-variables\" >}}\nin a Pod:", "zh": "### 使用 Configmap 作为环境变量  {#using-configmaps-as-environment-variables}\n\n使用 Configmap 在 Pod 中设置{{< glossary_tooltip text=\"环境变量\" term_id=\"container-env-variables\" >}}："}
{"en": "1. For each container in your Pod specification, add an environment variable\n   for each Configmap key that you want to use to the\n   `env[].valueFrom.configMapKeyRef` field.\n1. Modify your image and/or command line so that the program looks for values\n   in the specified environment variables.", "zh": "1. 对于 Pod 规约中的每个容器，为要使用的每个 ConfigMap 键添加一个环境变量到\n   `env[].valueFrom.configMapKeyRef` 字段。\n2. 修改你的镜像和/或命令行，以便程序查找指定环境变量中的值。"}
{"en": "This is an example of defining a ConfigMap as a pod environment variable:\n\nThe following ConfigMap (myconfigmap.yaml) stores two properties: username and access_level:", "zh": "下面是一个将 ConfigMap 定义为 Pod 环境变量的示例：\n\n以下 ConfigMap (myconfigmap.yaml) 存储两个属性：username 和 access_level：\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: myconfigmap\ndata:\n  username: k8s-admin\n  access_level: \"1\"\n```"}
{"en": "The following command will create the ConfigMap object:", "zh": "以下命令将创建 ConfigMap 对象：\n\n```shell\nkubectl apply -f myconfigmap.yaml\n```"}
{"en": "The following Pod consumes the content of the ConfigMap as environment variables:", "zh": "以下 Pod 将 ConfigMap 的内容用作环境变量：\n\n{{% code_sample file=\"configmap/env-configmap.yaml\" %}}"}
{"en": "The `envFrom` field instructs Kubernetes to create environment variables from the sources nested within it.\nThe inner `configMapRef` refers to a ConfigMap by its name and selects all its key-value pairs.\nAdd the Pod to your cluster, then retrieve its logs to see the output from the printenv command.\nThis should confirm that the two key-value pairs from the ConfigMap have been set as environment variables:", "zh": "`envFrom` 字段指示 Kubernetes 使用其中嵌套的源创建环境变量。\n内部的 `configMapRef` 通过 ConfigMap 的名称引用之，并选择其所有键值对。\n将 Pod 添加到你的集群中，然后检索其日志以查看 printenv 命令的输出。\n此操作可确认来自 ConfigMap 的两个键值对已被设置为环境变量：\n\n```shell\nkubectl apply -f env-configmap.yaml\n```\n\n```shell\nkubectl logs pod/ env-configmap\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```console\n...\nusername: \"k8s-admin\"\naccess_level: \"1\"\n...\n```"}
{"en": "Sometimes a Pod won't require access to all the values in a ConfigMap.\nFor example, you could have another Pod which only uses the username value from the ConfigMap.\nFor this use case, you can use the `env.valueFrom` syntax instead, which lets you select individual keys in\na ConfigMap. The name of the environment variable can also be different from the key within the ConfigMap.\nFor example:", "zh": "有时 Pod 不需要访问 ConfigMap 中的所有值。\n例如，你可以有另一个 Pod 只使用 ConfigMap 中的 username 值。\n在这种使用场景中，你可以转为使用 `env.valueFrom` 语法，这样可以让你选择 ConfigMap 中的单个键。\n环境变量的名称也可以不同于 ConfigMap 中的键。例如：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: env-configmap\nspec:\n  containers:\n  - name: envars-test-container\n    image: nginx\n    env:\n    - name: CONFIGMAP_USERNAME\n      valueFrom:\n        configMapKeyRef:\n          name: myconfigmap\n          key: username\n```"}
{"en": "In the Pod created from this manifest, you will see that the environment variable\n`CONFIGMAP_USERNAME` is set to the value of the `username` value from the ConfigMap.\nOther keys from the ConfigMap data are not copied into the environment.", "zh": "在从此清单创建的 Pod 中，你将看到环境变量 `CONFIGMAP_USERNAME` 被设置为 ConfigMap 中 `username` 的取值。\n来自 ConfigMap 数据中的其他键不会被复制到环境中。"}
{"en": "It's important to note that the range of characters allowed for environment\nvariable names in pods is [restricted](/docs/tasks/inject-data-application/define-environment-variable-container/#using-environment-variables-inside-of-your-config).\nIf any keys do not meet the rules, those keys are not made available to your container, though\nthe Pod is allowed to start.", "zh": "需要注意的是，Pod 中环境变量名称允许的字符范围是[有限的](/zh-cn/docs/tasks/inject-data-application/define-environment-variable-container/#using-environment-variables-inside-of-your-config)。\n如果某些变量名称不满足这些规则，则即使 Pod 可以被启动，你的容器也无法访问这些环境变量。"}
{"en": "## Immutable ConfigMaps {#configmap-immutable}", "zh": "## 不可变更的 ConfigMap     {#configmap-immutable}\n\n{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}"}
{"en": "The Kubernetes feature _Immutable Secrets and ConfigMaps_ provides an option to set\nindividual Secrets and ConfigMaps as immutable. For clusters that extensively use ConfigMaps\n(at least tens of thousands of unique ConfigMap to Pod mounts), preventing changes to their\ndata has the following advantages:", "zh": "Kubernetes 特性 **Immutable Secret 和 ConfigMap** 提供了一种将各个\nSecret 和 ConfigMap 设置为不可变更的选项。对于大量使用 ConfigMap 的集群\n（至少有数万个各不相同的 ConfigMap 给 Pod 挂载）而言，禁止更改\nConfigMap 的数据有以下好处："}
{"en": "- protects you from accidental (or unwanted) updates that could cause applications outages\n- improves performance of your cluster by significantly reducing load on kube-apiserver, by\n  closing watches for ConfigMaps marked as immutable.", "zh": "- 保护应用，使之免受意外（不想要的）更新所带来的负面影响。\n- 通过大幅降低对 kube-apiserver 的压力提升集群性能，\n  这是因为系统会关闭对已标记为不可变更的 ConfigMap 的监视操作。"}
{"en": "You can create an immutable ConfigMap by setting the `immutable` field to `true`.\nFor example:", "zh": "你可以通过将 `immutable` 字段设置为 `true` 创建不可变更的 ConfigMap。\n例如：\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  ...\ndata:\n  ...\nimmutable: true\n```"}
{"en": "Once a ConfigMap is marked as immutable, it is _not_ possible to revert this change\nnor to mutate the contents of the `data` or the `binaryData` field. You can\nonly delete and recreate the ConfigMap. Because existing Pods maintain a mount point\nto the deleted ConfigMap, it is recommended to recreate these pods.", "zh": "一旦某 ConfigMap 被标记为不可变更，则 _无法_ 逆转这一变化，，也无法更改\n`data` 或 `binaryData` 字段的内容。你只能删除并重建 ConfigMap。\n因为现有的 Pod 会维护一个已被删除的 ConfigMap 的挂载点，建议重新创建这些 Pods。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about [Secrets](/docs/concepts/configuration/secret/).\n* Read [Configure a Pod to Use a ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/).\n* Read about [changing a ConfigMap (or any other Kubernetes object)](/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/)\n* Read [The Twelve-Factor App](https://12factor.net/) to understand the motivation for\n  separating code from configuration.", "zh": "* 阅读 [Secret](/zh-cn/docs/concepts/configuration/secret/)。\n* 阅读[配置 Pod 使用 ConfigMap](/zh-cn/docs/tasks/configure-pod-container/configure-pod-configmap/)。\n* 阅读[修改 ConfigMap（或任何其他 Kubernetes 对象）](/zh-cn/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/)。\n* 阅读 [Twelve-Factor 应用](https://12factor.net/zh_cn/)来了解将代码和配置分开的动机。"}
{"en": "A Secret is an object that contains a small amount of sensitive data such as\na password, a token, or a key. Such information might otherwise be put in a\n{{< glossary_tooltip term_id=\"pod\" >}} specification or in a\n{{< glossary_tooltip text=\"container image\" term_id=\"image\" >}}. Using a\nSecret means that you don't need to include confidential data in your\napplication code.", "zh": "Secret 是一种包含少量敏感信息例如密码、令牌或密钥的对象。\n这样的信息可能会被放在 {{< glossary_tooltip term_id=\"pod\" >}} 规约中或者镜像中。\n使用 Secret 意味着你不需要在应用程序代码中包含机密数据。"}
{"en": "Because Secrets can be created independently of the Pods that use them, there\nis less risk of the Secret (and its data) being exposed during the workflow of\ncreating, viewing, and editing Pods. Kubernetes, and applications that run in\nyour cluster, can also take additional precautions with Secrets, such as avoiding\nwriting sensitive data to nonvolatile storage.\n\nSecrets are similar to {{< glossary_tooltip text=\"ConfigMaps\" term_id=\"configmap\" >}}\nbut are specifically intended to hold confidential data.", "zh": "由于创建 Secret 可以独立于使用它们的 Pod，\n因此在创建、查看和编辑 Pod 的工作流程中暴露 Secret（及其数据）的风险较小。\nKubernetes 和在集群中运行的应用程序也可以对 Secret 采取额外的预防措施，\n例如避免将敏感数据写入非易失性存储。\n\nSecret 类似于 {{<glossary_tooltip text=\"ConfigMap\" term_id=\"configmap\" >}}\n但专门用于保存机密数据。\n\n{{< caution >}}"}
{"en": "Kubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store\n(etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd.\nAdditionally, anyone who is authorized to create a Pod in a namespace can use that access to read\nany Secret in that namespace; this includes indirect access such as the ability to create a\nDeployment.\n\nIn order to safely use Secrets, take at least the following steps:\n\n1. [Enable Encryption at Rest](/docs/tasks/administer-cluster/encrypt-data/) for Secrets.\n1. [Enable or configure RBAC rules](/docs/reference/access-authn-authz/authorization/) with\n   least-privilege access to Secrets.\n1. Restrict Secret access to specific containers.\n1. [Consider using external Secret store providers](https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver).", "zh": "默认情况下，Kubernetes Secret 未加密地存储在 API 服务器的底层数据存储（etcd）中。\n任何拥有 API 访问权限的人都可以检索或修改 Secret，任何有权访问 etcd 的人也可以。\n此外，任何有权限在命名空间中创建 Pod 的人都可以使用该访问权限读取该命名空间中的任何 Secret；\n这包括间接访问，例如创建 Deployment 的能力。\n\n为了安全地使用 Secret，请至少执行以下步骤：\n\n1. 为 Secret [启用静态加密](/zh-cn/docs/tasks/administer-cluster/encrypt-data/)。\n1. 以最小特权访问 Secret 并[启用或配置 RBAC 规则](/zh-cn/docs/reference/access-authn-authz/authorization/)。\n1. 限制 Secret 对特定容器的访问。\n1. [考虑使用外部 Secret 存储驱动](https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver)。"}
{"en": "For more guidelines to manage and improve the security of your Secrets, refer to\n[Good practices for Kubernetes Secrets](/docs/concepts/security/secrets-good-practices).", "zh": "有关管理和提升 Secret 安全性的指南，请参阅\n[Kubernetes Secret 良好实践](/zh-cn/docs/concepts/security/secrets-good-practices)。\n{{< /caution >}}"}
{"en": "See [Information security for Secrets](#information-security-for-secrets) for more details.", "zh": "参见 [Secret 的信息安全](#information-security-for-secrets)了解详情。"}
{"en": "## Uses for Secrets\n\nYou can use Secrets for purposes such as the following:\n- [Set environment variables for a container](/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data).\n- [Provide credentials such as SSH keys or passwords to Pods](/docs/tasks/inject-data-application/distribute-credentials-secure/#provide-prod-test-creds).\n- [Allow the kubelet to pull container images from private registries](/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nThe Kubernetes control plane also uses Secrets; for example,\n[bootstrap token Secrets](#bootstrap-token-secrets) are a mechanism to\nhelp automate node registration.", "zh": "## Secret 的使用 {#uses-for-secrets}\n\n你可以将 Secret 用于以下场景：\n\n- [设置容器的环境变量](/zh-cn/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data)。\n- [向 Pod 提供 SSH 密钥或密码等凭据](/zh-cn/docs/tasks/inject-data-application/distribute-credentials-secure/#provide-prod-test-creds)。\n- [允许 kubelet 从私有镜像仓库中拉取镜像](/zh-cn/docs/tasks/configure-pod-container/pull-image-private-registry/)。\n\nKubernetes 控制面也使用 Secret；\n例如，[引导令牌 Secret](#bootstrap-token-secrets)\n是一种帮助自动化节点注册的机制。"}
{"en": "### Use case: dotfiles in a secret volume\n\nYou can make your data \"hidden\" by defining a key that begins with a dot.\nThis key represents a dotfile or \"hidden\" file. For example, when the following Secret\nis mounted into a volume, `secret-volume`, the volume will contain a single file,\ncalled `.secret-file`, and the `dotfile-test-container` will have this file\npresent at the path `/etc/secret-volume/.secret-file`.", "zh": "### 使用场景：在 Secret 卷中带句点的文件 {#use-case-dotfiles-in-a-secret-volume}\n\n通过定义以句点（`.`）开头的主键，你可以“隐藏”你的数据。\n这些主键代表的是以句点开头的文件或“隐藏”文件。\n例如，当以下 Secret 被挂载到 `secret-volume` 卷上时，该卷中会包含一个名为\n`.secret-file` 的文件，并且容器 `dotfile-test-container`\n中此文件位于路径 `/etc/secret-volume/.secret-file` 处。\n\n{{< note >}}"}
{"en": "Files beginning with dot characters are hidden from the output of  `ls -l`;\nyou must use `ls -la` to see them when listing directory contents.", "zh": "以句点开头的文件会在 `ls -l` 的输出中被隐藏起来；\n列举目录内容时你必须使用 `ls -la` 才能看到它们。\n{{< /note >}}\n\n{{% code language=\"yaml\" file=\"secret/dotfile-secret.yaml\" %}}"}
{"en": "### Use case: Secret visible to one container in a Pod\n\nConsider a program that needs to handle HTTP requests, do some complex business\nlogic, and then sign some messages with an HMAC. Because it has complex\napplication logic, there might be an unnoticed remote file reading exploit in\nthe server, which could expose the private key to an attacker.", "zh": "### 使用场景：仅对 Pod 中一个容器可见的 Secret {#use-case-secret-visible-to-one-container-in-a-pod}\n\n考虑一个需要处理 HTTP 请求，执行某些复杂的业务逻辑，之后使用 HMAC\n来对某些消息进行签名的程序。因为这一程序的应用逻辑很复杂，\n其中可能包含未被注意到的远程服务器文件读取漏洞，\n这种漏洞可能会把私钥暴露给攻击者。"}
{"en": "This could be divided into two processes in two containers: a frontend container\nwhich handles user interaction and business logic, but which cannot see the\nprivate key; and a signer container that can see the private key, and responds\nto simple signing requests from the frontend (for example, over localhost networking).", "zh": "这一程序可以分隔成两个容器中的两个进程：前端容器要处理用户交互和业务逻辑，\n但无法看到私钥；签名容器可以看到私钥，并对来自前端的简单签名请求作出响应\n（例如，通过本地主机网络）。"}
{"en": "With this partitioned approach, an attacker now has to trick the application\nserver into doing something rather arbitrary, which may be harder than getting\nit to read a file.", "zh": "采用这种划分的方法，攻击者现在必须欺骗应用服务器来做一些其他操作，\n而这些操作可能要比读取一个文件要复杂很多。"}
{"en": "### Alternatives to Secrets\n\nRather than using a Secret to protect confidential data, you can pick from alternatives.\n\nHere are some of your options:", "zh": "### Secret 的替代方案  {#alternatives-to-secrets}\n\n除了使用 Secret 来保护机密数据，你也可以选择一些替代方案。\n\n下面是一些选项："}
{"en": "- If your cloud-native component needs to authenticate to another application that you\n  know is running within the same Kubernetes cluster, you can use a\n  [ServiceAccount](/docs/reference/access-authn-authz/authentication/#service-account-tokens)\n  and its tokens to identify your client.\n- There are third-party tools that you can run, either within or outside your cluster,\n  that provide sensitive data. For example, a service that Pods access over HTTPS,\n  that reveals a Secret if the client correctly authenticates (for example, with a ServiceAccount\n  token).", "zh": "- 如果你的云原生组件需要执行身份认证来访问你所知道的、在同一 Kubernetes 集群中运行的另一个应用，\n  你可以使用 [ServiceAccount](/zh-cn/docs/reference/access-authn-authz/authentication/#service-account-tokens)\n  及其令牌来标识你的客户端身份。\n- 你可以运行的第三方工具也有很多，这些工具可以运行在集群内或集群外，提供机密数据管理。\n  例如，这一工具可能是 Pod 通过 HTTPS 访问的一个服务，该服务在客户端能够正确地通过身份认证\n  （例如，通过 ServiceAccount 令牌）时，提供机密数据内容。"}
{"en": "- For authentication, you can implement a custom signer for X.509 certificates, and use\n  [CertificateSigningRequests](/docs/reference/access-authn-authz/certificate-signing-requests/)\n  to let that custom signer issue certificates to Pods that need them.\n- You can use a [device plugin](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)\n  to expose node-local encryption hardware to a specific Pod. For example, you can schedule\n  trusted Pods onto nodes that provide a Trusted Platform Module, configured out-of-band.", "zh": "- 就身份认证而言，你可以为 X.509 证书实现一个定制的签名者，并使用\n  [CertificateSigningRequest](/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests/)\n  来让该签名者为需要证书的 Pod 发放证书。\n- 你可以使用一个[设备插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)\n  来将节点本地的加密硬件暴露给特定的 Pod。例如，你可以将可信任的 Pod\n  调度到提供可信平台模块（Trusted Platform Module，TPM）的节点上。\n  这类节点是另行配置的。"}
{"en": "You can also combine two or more of those options, including the option to use Secret objects themselves.\n\nFor example: implement (or deploy) an {{< glossary_tooltip text=\"operator\" term_id=\"operator-pattern\" >}}\nthat fetches short-lived session tokens from an external service, and then creates Secrets based\non those short-lived session tokens. Pods running in your cluster can make use of the session tokens,\nand operator ensures they are valid. This separation means that you can run Pods that are unaware of\nthe exact mechanisms for issuing and refreshing those session tokens.", "zh": "你还可以将如上选项的两种或多种进行组合，包括直接使用 Secret 对象本身也是一种选项。\n\n例如：实现（或部署）一个 {{< glossary_tooltip text=\"operator\" term_id=\"operator-pattern\" >}}，\n从外部服务取回生命期很短的会话令牌，之后基于这些生命期很短的会话令牌来创建 Secret。\n运行在集群中的 Pod 可以使用这些会话令牌，而 Operator 则确保这些令牌是合法的。\n这种责权分离意味着你可以运行那些不了解会话令牌如何发放与刷新的确切机制的 Pod。"}
{"en": "## Types of Secret {#secret-types}\n\nWhen creating a Secret, you can specify its type using the `type` field of\nthe [Secret](/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/)\nresource, or certain equivalent `kubectl` command line flags (if available).\nThe Secret type is used to facilitate programmatic handling of the Secret data.\n\nKubernetes provides several built-in types for some common usage scenarios.\nThese types vary in terms of the validations performed and the constraints\nKubernetes imposes on them.", "zh": "## Secret 的类型  {#secret-types}\n\n创建 Secret 时，你可以使用 [Secret](/zh-cn/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/)\n资源的 `type` 字段，或者与其等价的 `kubectl` 命令行参数（如果有的话）为其设置类型。\nSecret 类型有助于对 Secret 数据进行编程处理。\n\nKubernetes 提供若干种内置的类型，用于一些常见的使用场景。\n针对这些类型，Kubernetes 所执行的合法性检查操作以及对其所实施的限制各不相同。"}
{"en": "| Built-in Type | Usage |\n|--------------|-------|\n| `Opaque`     |  arbitrary user-defined data |\n| `kubernetes.io/service-account-token` | ServiceAccount token |\n| `kubernetes.io/dockercfg` | serialized `~/.dockercfg` file |\n| `kubernetes.io/dockerconfigjson` | serialized `~/.docker/config.json` file |\n| `kubernetes.io/basic-auth` | credentials for basic authentication |\n| `kubernetes.io/ssh-auth` | credentials for SSH authentication |\n| `kubernetes.io/tls` | data for a TLS client or server |\n| `bootstrap.kubernetes.io/token` | bootstrap token data |", "zh": "| 内置类型     | 用法  |\n|--------------|-------|\n| `Opaque`     | 用户定义的任意数据 |\n| `kubernetes.io/service-account-token` | 服务账号令牌 |\n| `kubernetes.io/dockercfg` | `~/.dockercfg` 文件的序列化形式 |\n| `kubernetes.io/dockerconfigjson` | `~/.docker/config.json` 文件的序列化形式 |\n| `kubernetes.io/basic-auth` | 用于基本身份认证的凭据 |\n| `kubernetes.io/ssh-auth` | 用于 SSH 身份认证的凭据 |\n| `kubernetes.io/tls` | 用于 TLS 客户端或者服务器端的数据 |\n| `bootstrap.kubernetes.io/token` | 启动引导令牌数据 |"}
{"en": "You can define and use your own Secret type by assigning a non-empty string as the\n`type` value for a Secret object (an empty string is treated as an `Opaque` type).", "zh": "通过为 Secret 对象的 `type` 字段设置一个非空的字符串值，你也可以定义并使用自己\nSecret 类型（如果 `type` 值为空字符串，则被视为 `Opaque` 类型）。"}
{"en": "Kubernetes doesn't impose any constraints on the type name. However, if you\nare using one of the built-in types, you must meet all the requirements defined\nfor that type.", "zh": "Kubernetes 并不对类型的名称作任何限制。不过，如果你要使用内置类型之一，\n则你必须满足为该类型所定义的所有要求。"}
{"en": "If you are defining a type of Secret that's for public use, follow the convention\nand structure the Secret type to have your domain name before the name, separated\nby a `/`. For example: `cloud-hosting.example.net/cloud-api-credentials`.", "zh": "如果你要定义一种公开使用的 Secret 类型，请遵守 Secret 类型的约定和结构，\n在类型名前面添加域名，并用 `/` 隔开。\n例如：`cloud-hosting.example.net/cloud-api-credentials`。"}
{"en": "### Opaque Secrets\n\n`Opaque` is the default Secret type if you don't explicitly specify a type in\na Secret manifest. When you create a Secret using `kubectl`, you must use the\n`generic` subcommand to indicate an `Opaque` Secret type. For example, the\nfollowing command creates an empty Secret of type `Opaque`:", "zh": "### Opaque Secret\n\n当你未在 Secret 清单中显式指定类型时，默认的 Secret 类型是 `Opaque`。\n当你使用 `kubectl` 来创建一个 Secret 时，你必须使用 `generic`\n子命令来标明要创建的是一个 `Opaque` 类型的 Secret。\n例如，下面的命令会创建一个空的 `Opaque` 类型的 Secret：\n\n```shell\nkubectl create secret generic empty-secret\nkubectl get secret empty-secret\n```"}
{"en": "The output looks like:", "zh": "输出类似于：\n\n```\nNAME           TYPE     DATA   AGE\nempty-secret   Opaque   0      2m6s\n```"}
{"en": "The `DATA` column shows the number of data items stored in the Secret.\nIn this case, `0` means you have created an empty Secret.", "zh": "`DATA` 列显示 Secret 中保存的数据条目个数。\n在这个例子中，`0` 意味着你刚刚创建了一个空的 Secret。"}
{"en": "### ServiceAccount token Secrets\n\nA `kubernetes.io/service-account-token` type of Secret is used to store a\ntoken credential that identifies a\n{{< glossary_tooltip text=\"ServiceAccount\" term_id=\"service-account\" >}}. This\nis a legacy mechanism that provides long-lived ServiceAccount credentials to\nPods.", "zh": "### ServiceAccount 令牌 Secret  {#service-account-token-secrets}\n\n类型为 `kubernetes.io/service-account-token` 的 Secret 用来存放标识某\n{{< glossary_tooltip text=\"ServiceAccount\" term_id=\"service-account\" >}} 的令牌凭据。\n这是为 Pod 提供长期有效 ServiceAccount 凭据的传统机制。"}
{"en": "In Kubernetes v1.22 and later, the recommended approach is to obtain a\nshort-lived, automatically rotating ServiceAccount token by using the\n[`TokenRequest`](/docs/reference/kubernetes-api/authentication-resources/token-request-v1/)\nAPI instead. You can get these short-lived tokens using the following methods:", "zh": "在 Kubernetes v1.22 及更高版本中，推荐的方法是通过使用\n[`TokenRequest`](/zh-cn/docs/reference/kubernetes-api/authentication-resources/token-request-v1/) API\n来获取短期自动轮换的 ServiceAccount 令牌。你可以使用以下方法获取这些短期令牌："}
{"en": "* Call the `TokenRequest` API either directly or by using an API client like\n  `kubectl`. For example, you can use the\n  [`kubectl create token`](/docs/reference/generated/kubectl/kubectl-commands#-em-token-em-)\n  command.\n* Request a mounted token in a\n  [projected volume](/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume)\n  in your Pod manifest. Kubernetes creates the token and mounts it in the Pod.\n  The token is automatically invalidated when the Pod that it's mounted in is\n  deleted. For details, see\n  [Launch a Pod using service account token projection](/docs/tasks/configure-pod-container/configure-service-account/#launch-a-pod-using-service-account-token-projection).", "zh": "- 直接调用 `TokenRequest` API，或者使用像 `kubectl` 这样的 API 客户端。\n  例如，你可以使用\n  [`kubectl create token`](/docs/reference/generated/kubectl/kubectl-commands#-em-token-em-) 命令。\n- 在 Pod 清单中请求使用[投射卷](/zh-cn/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume)挂载的令牌。\n  Kubernetes 会创建令牌并将其挂载到 Pod 中。\n  当挂载令牌的 Pod 被删除时，此令牌会自动失效。\n  更多细节参阅[启动使用服务账号令牌投射的 Pod](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#launch-a-pod-using-service-account-token-projection)。\n\n{{< note >}}"}
{"en": "You should only create a ServiceAccount token Secret\nif you can't use the `TokenRequest` API to obtain a token,\nand the security exposure of persisting a non-expiring token credential\nin a readable API object is acceptable to you. For instructions, see\n[Manually create a long-lived API token for a ServiceAccount](/docs/tasks/configure-pod-container/configure-service-account/#manually-create-an-api-token-for-a-serviceaccount).", "zh": "只有在你无法使用 `TokenRequest` API 来获取令牌，\n并且你能够接受因为将永不过期的令牌凭据写入到可读取的 API 对象而带来的安全风险时，\n才应该创建 ServiceAccount 令牌 Secret。\n更多细节参阅[为 ServiceAccount 手动创建长期有效的 API 令牌](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#manually-create-an-api-token-for-a-serviceaccount)。\n{{< /note >}}"}
{"en": "When using this Secret type, you need to ensure that the\n`kubernetes.io/service-account.name` annotation is set to an existing\nServiceAccount name. If you are creating both the ServiceAccount and\nthe Secret objects, you should create the ServiceAccount object first.", "zh": "使用这种 Secret 类型时，你需要确保对象的注解 `kubernetes.io/service-account-name`\n被设置为某个已有的 ServiceAccount 名称。\n如果你同时创建 ServiceAccount 和 Secret 对象，应该先创建 ServiceAccount 对象。"}
{"en": "After the Secret is created, a Kubernetes {{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}\nfills in some other fields such as the `kubernetes.io/service-account.uid` annotation, and the\n`token` key in the `data` field, which is populated with an authentication token.\n\nThe following example configuration declares a ServiceAccount token Secret:", "zh": "当 Secret 对象被创建之后，某个 Kubernetes\n{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}会填写\nSecret 的其它字段，例如 `kubernetes.io/service-account.uid` 注解和\n`data` 字段中的 `token` 键值（该键包含一个身份认证令牌）。\n\n下面的配置实例声明了一个 ServiceAccount 令牌 Secret：\n\n{{% code language=\"yaml\" file=\"secret/serviceaccount-token-secret.yaml\" %}}"}
{"en": "After creating the Secret, wait for Kubernetes to populate the `token` key in the `data` field.", "zh": "创建了 Secret 之后，等待 Kubernetes 在 `data` 字段中填充 `token` 主键。"}
{"en": "See the [ServiceAccount](/docs/concepts/security/service-accounts/)\ndocumentation for more information on how ServiceAccounts work.\nYou can also check the `automountServiceAccountToken` field and the\n`serviceAccountName` field of the\n[`Pod`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#pod-v1-core)\nfor information on referencing ServiceAccount credentials from within Pods.", "zh": "参考 [ServiceAccount](/zh-cn/docs/concepts/security/service-accounts/)\n文档了解 ServiceAccount 的工作原理。你也可以查看\n[`Pod`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#pod-v1-core)\n资源中的 `automountServiceAccountToken` 和 `serviceAccountName` 字段文档，\n进一步了解从 Pod 中引用 ServiceAccount 凭据。"}
{"en": "### Docker config Secrets\n\nIf you are creating a Secret to store credentials for accessing a container image registry,\nyou must use one of the following `type` values for that Secret:", "zh": "### Docker 配置 Secret  {#docker-config-secrets}\n\n如果你要创建 Secret 用来存放用于访问容器镜像仓库的凭据，则必须选用以下 `type`\n值之一来创建 Secret："}
{"en": "- `kubernetes.io/dockercfg`: store a serialized `~/.dockercfg` which is the\n  legacy format for configuring Docker command line. The Secret\n  `data` field contains a `.dockercfg` key whose value is the content of a\n  base64 encoded `~/.dockercfg` file.\n- `kubernetes.io/dockerconfigjson`: store a serialized JSON that follows the\n  same format rules as the `~/.docker/config.json` file, which is a new format\n  for `~/.dockercfg`. The Secret `data` field must contain a\n  `.dockerconfigjson` key for which the value is the content of a base64\n  encoded `~/.docker/config.json` file.", "zh": "- `kubernetes.io/dockercfg`：存放 `~/.dockercfg` 文件的序列化形式，它是配置 Docker\n  命令行的一种老旧形式。Secret 的 `data` 字段包含名为 `.dockercfg` 的主键，\n  其值是用 base64 编码的某 `~/.dockercfg` 文件的内容。\n- `kubernetes.io/dockerconfigjson`：存放 JSON 数据的序列化形式，\n  该 JSON 也遵从 `~/.docker/config.json` 文件的格式规则，而后者是 `~/.dockercfg`\n  的新版本格式。使用此 Secret 类型时，Secret 对象的 `data` 字段必须包含\n  `.dockerconfigjson` 键，其键值为 base64 编码的字符串包含 `~/.docker/config.json`\n  文件的内容。"}
{"en": "Below is an example for a `kubernetes.io/dockercfg` type of Secret:", "zh": "下面是一个 `kubernetes.io/dockercfg` 类型 Secret 的示例：\n\n{{% code language=\"yaml\" file=\"secret/dockercfg-secret.yaml\" %}}\n\n{{< note >}}"}
{"en": "If you do not want to perform the base64 encoding, you can choose to use the\n`stringData` field instead.", "zh": "如果你不希望执行 base64 编码转换，可以使用 `stringData` 字段代替。\n{{< /note >}}"}
{"en": "When you create Docker config Secrets using a manifest, the API\nserver checks whether the expected key exists in the `data` field, and\nit verifies if the value provided can be parsed as a valid JSON. The API\nserver doesn't validate if the JSON actually is a Docker config file.\n\nYou can also use `kubectl` to create a Secret for accessing a container\nregistry, such as when you don't have a Docker configuration file:", "zh": "当你使用清单文件通过 Docker 配置来创建 Secret 时，API 服务器会检查 `data` 字段中是否存在所期望的主键，\n并且验证其中所提供的键值是否是合法的 JSON 数据。\n不过，API 服务器不会检查 JSON 数据本身是否是一个合法的 Docker 配置文件内容。\n\n你还可以使用 `kubectl` 创建一个 Secret 来访问容器仓库时，\n当你没有 Docker 配置文件时你可以这样做：\n\n```shell\nkubectl create secret docker-registry secret-tiger-docker \\\n  --docker-email=tiger@acme.example \\\n  --docker-username=tiger \\\n  --docker-password=pass1234 \\\n  --docker-server=my-registry.example:5000\n```"}
{"en": "This command creates a Secret of type `kubernetes.io/dockerconfigjson`.\n\nRetrieve the `.data.dockerconfigjson` field from that new Secret and decode the\ndata:", "zh": "此命令创建一个类型为 `kubernetes.io/dockerconfigjson` 的 Secret。\n\n从这个新的 Secret 中获取 `.data.dockerconfigjson` 字段并执行数据解码：\n\n```shell\nkubectl get secret secret-tiger-docker -o jsonpath='{.data.*}' | base64 -d\n```"}
{"en": "The output is equivalent to the following JSON document (which is also a valid\nDocker configuration file):", "zh": "输出等价于以下 JSON 文档（这也是一个有效的 Docker 配置文件）：\n\n```json\n{\n  \"auths\": {\n    \"my-registry.example:5000\": {\n      \"username\": \"tiger\",\n      \"password\": \"pass1234\",\n      \"email\": \"tiger@acme.example\",\n      \"auth\": \"dGlnZXI6cGFzczEyMzQ=\"\n    }\n  }\n}\n```\n\n{{< caution >}}"}
{"en": "The `auth` value there is base64 encoded; it is obscured but not secret.\nAnyone who can read that Secret can learn the registry access bearer token.\n\nIt is suggested to use [credential providers](/docs/tasks/administer-cluster/kubelet-credential-provider/) to dynamically and securely provide pull secrets on-demand.", "zh": "`auths` 值是 base64 编码的，其内容被屏蔽但未被加密。\n任何能够读取该 Secret 的人都可以了解镜像库的访问令牌。\n\n建议使用[凭据提供程序](/zh-cn/docs/tasks/administer-cluster/kubelet-credential-provider/)来动态、\n安全地按需提供拉取 Secret。\n{{< /caution >}}"}
{"en": "### Basic authentication Secret\n\nThe `kubernetes.io/basic-auth` type is provided for storing credentials needed\nfor basic authentication. When using this Secret type, the `data` field of the\nSecret must contain one of the following two keys:\n\n- `username`: the user name for authentication\n- `password`: the password or token for authentication", "zh": "### 基本身份认证 Secret  {#basic-authentication-secret}\n\n`kubernetes.io/basic-auth` 类型用来存放用于基本身份认证所需的凭据信息。\n使用这种 Secret 类型时，Secret 的 `data` 字段必须包含以下两个键之一：\n\n- `username`: 用于身份认证的用户名；\n- `password`: 用于身份认证的密码或令牌。"}
{"en": "Both values for the above two keys are base64 encoded strings. You can\nalternatively provide the clear text content using the `stringData` field in the\nSecret manifest.\n\nThe following manifest is an example of a basic authentication Secret:", "zh": "以上两个键的键值都是 base64 编码的字符串。\n当然你也可以在 Secret 清单中的使用 `stringData` 字段来提供明文形式的内容。\n\n以下清单是基本身份验证 Secret 的示例：\n\n{{% code language=\"yaml\" file=\"secret/basicauth-secret.yaml\" %}}\n\n{{< note >}}"}
{"en": "The `stringData` field for a Secret does not work well with server-side apply.", "zh": "Secret 的 `stringData` 字段不能很好地与服务器端应用配合使用。\n{{< /note >}}"}
{"en": "The basic authentication Secret type is provided only for convenience.\nYou can create an `Opaque` type for credentials used for basic authentication.\nHowever, using the defined and public Secret type (`kubernetes.io/basic-auth`) helps other\npeople to understand the purpose of your Secret, and sets a convention for what key names\nto expect.", "zh": "提供基本身份认证类型的 Secret 仅仅是出于方便性考虑。\n你也可以使用 `Opaque` 类型来保存用于基本身份认证的凭据。\n不过，使用预定义的、公开的 Secret 类型（`kubernetes.io/basic-auth`）\n有助于帮助其他用户理解 Secret 的目的，并且对其中存在的主键形成一种约定。"}
{"en": "### SSH authentication Secrets\n\nThe builtin type `kubernetes.io/ssh-auth` is provided for storing data used in\nSSH authentication. When using this Secret type, you will have to specify a\n`ssh-privatekey` key-value pair in the `data` (or `stringData`) field\nas the SSH credential to use.\n\nThe following manifest is an example of a Secret used for SSH public/private\nkey authentication:", "zh": "### SSH 身份认证 Secret {#ssh-authentication-secrets}\n\nKubernetes 所提供的内置类型 `kubernetes.io/ssh-auth` 用来存放 SSH 身份认证中所需要的凭据。\n使用这种 Secret 类型时，你就必须在其 `data` （或 `stringData`）\n字段中提供一个 `ssh-privatekey` 键值对，作为要使用的 SSH 凭据。\n\n下面的清单是一个 SSH 公钥/私钥身份认证的 Secret 示例：\n\n{{% code language=\"yaml\" file=\"secret/ssh-auth-secret.yaml\" %}}"}
{"en": "The SSH authentication Secret type is provided only for convenience.\nYou can create an `Opaque` type for credentials used for SSH authentication.\nHowever, using the defined and public Secret type (`kubernetes.io/tls`) helps other\npeople to understand the purpose of your Secret, and sets a convention for what key names\nto expect.\nThe Kubernetes API verifies that the required keys are set for a Secret of this type.", "zh": "提供 SSH 身份认证类型的 Secret 仅仅是出于方便性考虑。\n你可以使用 `Opaque` 类型来保存用于 SSH 身份认证的凭据。\n不过，使用预定义的、公开的 Secret 类型（`kubernetes.io/tls`）\n有助于其他人理解你的 Secret 的用途，也可以就其中包含的主键名形成约定。\nKubernetes API 会验证这种类型的 Secret 中是否设定了所需的主键。\n\n{{< caution >}}"}
{"en": "SSH private keys do not establish trusted communication between an SSH client and\nhost server on their own. A secondary means of establishing trust is needed to\nmitigate \"man in the middle\" attacks, such as a `known_hosts` file added to a ConfigMap.", "zh": "SSH 私钥自身无法建立 SSH 客户端与服务器端之间的可信连接。\n需要其它方式来建立这种信任关系，以缓解“中间人（Man In The Middle）”\n攻击，例如向 ConfigMap 中添加一个 `known_hosts` 文件。\n{{< /caution >}}"}
{"en": "### TLS Secrets\n\nThe `kubernetes.io/tls` Secret type is for storing\na certificate and its associated key that are typically used for TLS.\n\nOne common use for TLS Secrets is to configure encryption in transit for\nan [Ingress](/docs/concepts/services-networking/ingress/), but you can also use it\nwith other resources or directly in your workload.\nWhen using this type of Secret, the `tls.key` and the `tls.crt` key must be provided\nin the `data` (or `stringData`) field of the Secret configuration, although the API\nserver doesn't actually validate the values for each key.\n\nAs an alternative to using `stringData`, you can use the `data` field to provide\nthe base64 encoded certificate and private key. For details, see\n[Constraints on Secret names and data](#restriction-names-data).\n\nThe following YAML contains an example config for a TLS Secret:", "zh": "### TLS Secret\n\n`kubernetes.io/tls` Secret 类型用来存放 TLS 场合通常要使用的证书及其相关密钥。\n\nTLS Secret 的一种典型用法是为 [Ingress](/zh-cn/docs/concepts/services-networking/ingress/)\n资源配置传输过程中的数据加密，不过也可以用于其他资源或者直接在负载中使用。\n当使用此类型的 Secret 时，Secret 配置中的 `data` （或 `stringData`）字段必须包含\n`tls.key` 和 `tls.crt` 主键，尽管 API 服务器实际上并不会对每个键的取值作进一步的合法性检查。\n\n作为使用 `stringData` 的替代方法，你可以使用 `data` 字段来指定 base64 编码的证书和私钥。\n有关详细信息，请参阅 [Secret 名称和数据的限制](#restriction-names-data)。\n\n下面的 YAML 包含一个 TLS Secret 的配置示例：\n\n{{% code language=\"yaml\" file=\"secret/tls-auth-secret.yaml\" %}}"}
{"en": "The TLS Secret type is provided only for convenience.\nYou can create an `Opaque` type for credentials used for TLS authentication.\nHowever, using the defined and public Secret type (`kubernetes.io/tls`)\nhelps ensure the consistency of Secret format in your project. The API server\nverifies if the required keys are set for a Secret of this type.\n\nTo create a TLS Secret using `kubectl`, use the `tls` subcommand:", "zh": "提供 TLS 类型的 Secret 仅仅是出于方便性考虑。\n你可以创建 `Opaque` 类型的 Secret 来保存用于 TLS 身份认证的凭据。\n不过，使用已定义和公开的 Secret 类型（`kubernetes.io/tls`）有助于确保你自己项目中的 Secret 格式的一致性。\nAPI 服务器会验证这种类型的 Secret 是否设定了所需的主键。\n\n要使用 `kubectl` 创建 TLS Secret，你可以使用 `tls` 子命令：\n\n```shell\nkubectl create secret tls my-tls-secret \\\n  --cert=path/to/cert/file \\\n  --key=path/to/key/file\n```"}
{"en": "The public/private key pair must exist before hand. The public key certificate for `--cert` must be .PEM encoded\nand must match the given private key for `--key`.", "zh": "公钥/私钥对必须事先存在，`--cert` 的公钥证书必须采用 .PEM 编码，\n并且必须与 `--key` 的给定私钥匹配。"}
{"en": "### Bootstrap token Secrets\n\nThe `bootstrap.kubernetes.io/token` Secret type is for\ntokens used during the node bootstrap process. It stores tokens used to sign\nwell-known ConfigMaps.", "zh": "### 启动引导令牌 Secret  {#bootstrap-token-secrets}\n\n`bootstrap.kubernetes.io/token` Secret 类型针对的是节点启动引导过程所用的令牌。\n其中包含用来为周知的 ConfigMap 签名的令牌。"}
{"en": "A bootstrap token Secret is usually created in the `kube-system` namespace and\nnamed in the form `bootstrap-token-<token-id>` where `<token-id>` is a 6 character\nstring of the token ID.\n\nAs a Kubernetes manifest, a bootstrap token Secret might look like the\nfollowing:", "zh": "启动引导令牌 Secret 通常创建于 `kube-system` 名字空间内，并以\n`bootstrap-token-<令牌 ID>` 的形式命名；\n其中 `<令牌 ID>` 是一个由 6 个字符组成的字符串，用作令牌的标识。\n\n以 Kubernetes 清单文件的形式，某启动引导令牌 Secret 可能看起来像下面这样：\n\n{{% code language=\"yaml\" file=\"secret/bootstrap-token-secret-base64.yaml\" %}}"}
{"en": "A bootstrap token Secret has the following keys specified under `data`:\n\n- `token-id`: A random 6 character string as the token identifier. Required.\n- `token-secret`: A random 16 character string as the actual token Secret. Required.\n- `description`: A human-readable string that describes what the token is\n  used for. Optional.\n- `expiration`: An absolute UTC time using [RFC3339](https://datatracker.ietf.org/doc/html/rfc3339) specifying when the token\n  should be expired. Optional.\n- `usage-bootstrap-<usage>`: A boolean flag indicating additional usage for\n  the bootstrap token.\n- `auth-extra-groups`: A comma-separated list of group names that will be\n  authenticated as in addition to the `system:bootstrappers` group.", "zh": "启动引导令牌类型的 Secret 会在 `data` 字段中包含如下主键：\n\n- `token-id`：由 6 个随机字符组成的字符串，作为令牌的标识符。必需。\n- `token-secret`：由 16 个随机字符组成的字符串，包含实际的令牌机密。必需。\n- `description`：供用户阅读的字符串，描述令牌的用途。可选。\n- `expiration`：一个使用 [RFC3339](https://datatracker.ietf.org/doc/html/rfc3339)\n  来编码的 UTC 绝对时间，给出令牌要过期的时间。可选。\n- `usage-bootstrap-<usage>`：布尔类型的标志，用来标明启动引导令牌的其他用途。\n- `auth-extra-groups`：用逗号分隔的组名列表，身份认证时除被认证为\n  `system:bootstrappers` 组之外，还会被添加到所列的用户组中。"}
{"en": "You can alternatively provide the values in the `stringData` field of the Secret\nwithout base64 encoding them:\n\n{{% code language=\"yaml\" file=\"secret/bootstrap-token-secret-literal.yaml\" %}}", "zh": "你也可以在 Secret 的 `stringData` 字段中提供值，而无需对其进行 base64 编码：\n\n{{% code language=\"yaml\" file=\"secret/bootstrap-token-secret-literal.yaml\" %}}\n\n{{< note >}}"}
{"en": "The `stringData` field for a Secret does not work well with server-side apply.", "zh": "Secret 的 `stringData` 字段不能很好地与服务器端应用配合使用。\n{{< /note >}}"}
{"en": "## Working with Secrets\n\n### Creating a Secret\n\nThere are several options to create a Secret:\n\n- [Use `kubectl`](/docs/tasks/configmap-secret/managing-secret-using-kubectl/)\n- [Use a configuration file](/docs/tasks/configmap-secret/managing-secret-using-config-file/)\n- [Use the Kustomize tool](/docs/tasks/configmap-secret/managing-secret-using-kustomize/)", "zh": "## 使用 Secret  {#working-with-secrets}\n\n### 创建 Secret  {#creating-a-secret}\n\n创建 Secret 有以下几种可选方式：\n\n- [使用 `kubectl`](/zh-cn/docs/tasks/configmap-secret/managing-secret-using-kubectl/)\n- [使用配置文件](/zh-cn/docs/tasks/configmap-secret/managing-secret-using-config-file/)\n- [使用 Kustomize 工具](/zh-cn/docs/tasks/configmap-secret/managing-secret-using-kustomize/)"}
{"en": "#### Constraints on Secret names and data {#restriction-names-data}\n\nThe name of a Secret object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).", "zh": "#### 对 Secret 名称与数据的约束 {#restriction-names-data}\n\nSecret 对象的名称必须是合法的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。"}
{"en": "You can specify the `data` and/or the `stringData` field when creating a\nconfiguration file for a Secret. The `data` and the `stringData` fields are optional.\nThe values for all keys in the `data` field have to be base64-encoded strings.\nIf the conversion to base64 string is not desirable, you can choose to specify\nthe `stringData` field instead, which accepts arbitrary strings as values.\n\nThe keys of `data` and `stringData` must consist of alphanumeric characters,\n`-`, `_` or `.`. All key-value pairs in the `stringData` field are internally\nmerged into the `data` field. If a key appears in both the `data` and the\n`stringData` field, the value specified in the `stringData` field takes\nprecedence.", "zh": "在为创建 Secret 编写配置文件时，你可以设置 `data` 与/或 `stringData` 字段。\n`data` 和 `stringData` 字段都是可选的。`data` 字段中所有键值都必须是 base64\n编码的字符串。如果不希望执行这种 base64 字符串的转换操作，你可以选择设置\n`stringData` 字段，其中可以使用任何字符串作为其取值。\n\n`data` 和 `stringData` 中的键名只能包含字母、数字、`-`、`_` 或 `.` 字符。\n`stringData` 字段中的所有键值对都会在内部被合并到 `data` 字段中。\n如果某个主键同时出现在 `data` 和 `stringData` 字段中，`stringData`\n所指定的键值具有高优先级。"}
{"en": "#### Size limit {#restriction-data-size}\n\nIndividual Secrets are limited to 1MiB in size. This is to discourage creation\nof very large Secrets that could exhaust the API server and kubelet memory.\nHowever, creation of many smaller Secrets could also exhaust memory. You can\nuse a [resource quota](/docs/concepts/policy/resource-quotas/) to limit the\nnumber of Secrets (or other resources) in a namespace.", "zh": "#### 尺寸限制   {#restriction-data-size}\n\n每个 Secret 的尺寸最多为 1MiB。施加这一限制是为了避免用户创建非常大的 Secret，\n进而导致 API 服务器和 kubelet 内存耗尽。不过创建很多小的 Secret 也可能耗尽内存。\n你可以使用[资源配额](/zh-cn/docs/concepts/policy/resource-quotas/)来约束每个名字空间中\nSecret（或其他资源）的个数。"}
{"en": "### Editing a Secret\n\nYou can edit an existing Secret unless it is [immutable](#secret-immutable). To\nedit a Secret, use one of the following methods:", "zh": "### 编辑 Secret    {#editing-a-secret}\n\n你可以编辑一个已有的 Secret，除非它是[不可变更的](#secret-immutable)。\n要编辑一个 Secret，可使用以下方法之一："}
{"en": "- [Use `kubectl`](/docs/tasks/configmap-secret/managing-secret-using-kubectl/#edit-secret)\n- [Use a configuration file](/docs/tasks/configmap-secret/managing-secret-using-config-file/#edit-secret)", "zh": "- [使用 `kubectl`](/zh-cn/docs/tasks/configmap-secret/managing-secret-using-kubectl/#edit-secret)\n- [使用配置文件](/zh-cn/docs/tasks/configmap-secret/managing-secret-using-config-file/#edit-secret)"}
{"en": "You can also edit the data in a Secret using the [Kustomize tool](/docs/tasks/configmap-secret/managing-secret-using-kustomize/#edit-secret). However, this\nmethod creates a new `Secret` object with the edited data.\n\nDepending on how you created the Secret, as well as how the Secret is used in\nyour Pods, updates to existing `Secret` objects are propagated automatically to\nPods that use the data. For more information, refer to [Using Secrets as files from a Pod](#using-secrets-as-files-from-a-pod) section.", "zh": "你也可以使用\n[Kustomize 工具](/zh-cn/docs/tasks/configmap-secret/managing-secret-using-kustomize/#edit-secret)编辑数据。\n然而这种方法会用编辑过的数据创建新的 `Secret` 对象。\n\n根据你创建 Secret 的方式以及该 Secret 在 Pod 中被使用的方式，对已有 `Secret`\n对象的更新将自动扩散到使用此数据的 Pod。有关更多信息，\n请参阅[在 Pod 以文件形式使用 Secret](#using-secrets-as-files-from-a-pod)。"}
{"en": "### Using a Secret\n\nSecrets can be mounted as data volumes or exposed as\n{{< glossary_tooltip text=\"environment variables\" term_id=\"container-env-variables\" >}}\nto be used by a container in a Pod. Secrets can also be used by other parts of the\nsystem, without being directly exposed to the Pod. For example, Secrets can hold\ncredentials that other parts of the system should use to interact with external\nsystems on your behalf.", "zh": "### 使用 Secret    {#using-a-secret}\n\nSecret 可以以数据卷的形式挂载，也可以作为{{< glossary_tooltip text=\"环境变量\" term_id=\"container-env-variables\" >}}\n暴露给 Pod 中的容器使用。Secret 也可用于系统中的其他部分，而不是一定要直接暴露给 Pod。\n例如，Secret 也可以包含系统中其他部分在替你与外部系统交互时要使用的凭证数据。"}
{"en": "Secret volume sources are validated to ensure that the specified object\nreference actually points to an object of type Secret. Therefore, a Secret\nneeds to be created before any Pods that depend on it.\n\nIf the Secret cannot be fetched (perhaps because it does not exist, or\ndue to a temporary lack of connection to the API server) the kubelet\nperiodically retries running that Pod. The kubelet also reports an Event\nfor that Pod, including details of the problem fetching the Secret.", "zh": "Kubernetes 会检查 Secret 的卷数据源，确保所指定的对象引用确实指向类型为 Secret\n的对象。因此，如果 Pod 依赖于某 Secret，该 Secret 必须先于 Pod 被创建。\n\n如果 Secret 内容无法取回（可能因为 Secret 尚不存在或者临时性地出现 API\n服务器网络连接问题），kubelet 会周期性地重试 Pod 运行操作。kubelet 也会为该 Pod\n报告 Event 事件，给出读取 Secret 时遇到的问题细节。"}
{"en": "#### Optional Secrets {#restriction-secret-must-exist}", "zh": "#### 可选的 Secret   {#restriction-secret-must-exist}"}
{"en": "When you reference a Secret in a Pod, you can mark the Secret as _optional_,\nsuch as in the following example. If an optional Secret doesn't exist,\nKubernetes ignores it.", "zh": "当你在 Pod 中引用 Secret 时，你可以将该 Secret 标记为**可选**，就像下面例子中所展示的那样。\n如果可选的 Secret 不存在，Kubernetes 将忽略它。\n\n{{% code language=\"yaml\" file=\"secret/optional-secret.yaml\" %}}"}
{"en": "By default, Secrets are required. None of a Pod's containers will start until\nall non-optional Secrets are available.", "zh": "默认情况下，Secret 是必需的。在所有非可选的 Secret 都可用之前，Pod 的所有容器都不会启动。"}
{"en": "If a Pod references a specific key in a non-optional Secret and that Secret\ndoes exist, but is missing the named key, the Pod fails during startup.", "zh": "如果 Pod 引用了非可选 Secret 中的特定键，并且该 Secret 确实存在，但缺少所指定的键，\n则 Pod 在启动期间会失败。"}
{"en": "### Using Secrets as files from a Pod {#using-secrets-as-files-from-a-pod}\n\nIf you want to access data from a Secret in a Pod, one way to do that is to\nhave Kubernetes make the value of that Secret be available as a file inside\nthe filesystem of one or more of the Pod's containers.", "zh": "### 在 Pod 以文件形式使用 Secret   {#using-secrets-as-files-from-a-pod}\n\n如果你要在 Pod 中访问来自 Secret 的数据，一种方式是让 Kubernetes 将该 Secret 的值以\n文件的形式呈现，该文件存在于 Pod 中一个或多个容器内的文件系统内。"}
{"en": "For instructions, refer to\n[Create a Pod that has access to the secret data through a Volume](/docs/tasks/inject-data-application/distribute-credentials-secure/#create-a-pod-that-has-access-to-the-secret-data-through-a-volume).", "zh": "相关的指示说明，\n可以参阅[创建一个可以通过卷访问 Secret 数据的 Pod](/zh-cn/docs/tasks/inject-data-application/distribute-credentials-secure/#create-a-pod-that-has-access-to-the-secret-data-through-a-volume)。"}
{"en": "When a volume contains data from a Secret, and that Secret is updated, Kubernetes tracks\nthis and updates the data in the volume, using an eventually-consistent approach.", "zh": "当卷中包含来自 Secret 的数据，而对应的 Secret 被更新，Kubernetes\n会跟踪到这一操作并更新卷中的数据。更新的方式是保证最终一致性。\n\n{{< note >}}"}
{"en": "A container using a Secret as a\n[subPath](/docs/concepts/storage/volumes#using-subpath) volume mount does not receive\nautomated Secret updates.", "zh": "对于以 [subPath](/zh-cn/docs/concepts/storage/volumes#using-subpath) 形式挂载 Secret 卷的容器而言，\n它们无法收到自动的 Secret 更新。\n{{< /note >}}"}
{"en": "The kubelet keeps a cache of the current keys and values for the Secrets that are used in\nvolumes for pods on that node.\nYou can configure the way that the kubelet detects changes from the cached values. The\n`configMapAndSecretChangeDetectionStrategy` field in the\n[kubelet configuration](/docs/reference/config-api/kubelet-config.v1beta1/) controls\nwhich strategy the kubelet uses. The default strategy is `Watch`.", "zh": "Kubelet 组件会维护一个缓存，在其中保存节点上 Pod 卷中使用的 Secret 的当前主键和取值。\n你可以配置 kubelet 如何检测所缓存数值的变化。\n[kubelet 配置](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/)中的\n`configMapAndSecretChangeDetectionStrategy` 字段控制 kubelet 所采用的策略。\n默认的策略是 `Watch`。"}
{"en": "Updates to Secrets can be either propagated by an API watch mechanism (the default), based on\na cache with a defined time-to-live, or polled from the cluster API server on each kubelet\nsynchronisation loop.", "zh": "对 Secret 的更新操作既可以通过 API 的 watch 机制（默认）来传播，\n基于设置了生命期的缓存获取，也可以通过 kubelet 的同步回路来从集群的 API\n服务器上轮询获取。"}
{"en": "As a result, the total delay from the moment when the Secret is updated to the moment\nwhen new keys are projected to the Pod can be as long as the kubelet sync period + cache\npropagation delay, where the cache propagation delay depends on the chosen cache type\n(following the same order listed in the previous paragraph, these are:\nwatch propagation delay, the configured cache TTL, or zero for direct polling).", "zh": "因此，从 Secret 被更新到新的主键被投射到 Pod 中，中间存在一个延迟。\n这一延迟的上限是 kubelet 的同步周期加上缓存的传播延迟，\n其中缓存的传播延迟取决于所选择的缓存类型。\n对应上一段中提到的几种传播机制，延迟时长为 watch 的传播延迟、所配置的缓存 TTL\n或者对于直接轮询而言是零。"}
{"en": "### Using Secrets as environment variables\n\nTo use a Secret in an {{< glossary_tooltip text=\"environment variable\" term_id=\"container-env-variables\" >}}\nin a Pod:", "zh": "### 以环境变量的方式使用 Secret   {#using-secrets-as-environment-variables}\n\n如果需要在 Pod\n中以{{< glossary_tooltip text=\"环境变量\" term_id=\"container-env-variables\" >}}的形式使用 Secret："}
{"en": "1. For each container in your Pod specification, add an environment variable\n   for each Secret key that you want to use to the\n   `env[].valueFrom.secretKeyRef` field.\n1. Modify your image and/or command line so that the program looks for values\n   in the specified environment variables.", "zh": "1. 对于 Pod 规约中的每个容器，针对你要使用的每个 Secret 键，将对应的环境变量添加到\n   `env[].valueFrom.secretKeyRef` 中。\n1. 更改你的镜像或命令行，以便程序能够从指定的环境变量找到所需要的值。"}
{"en": "For instructions, refer to\n[Define container environment variables using Secret data](/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data).", "zh": "相关的指示说明，\n可以参阅[使用 Secret 数据定义容器变量](/zh-cn/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data)。"}
{"en": "It's important to note that the range of characters allowed for environment variable\nnames in pods is [restricted](/docs/tasks/inject-data-application/define-environment-variable-container/#using-environment-variables-inside-of-your-config).\nIf any keys do not meet the rules, those keys are not made available to your container, though\nthe Pod is allowed to start.", "zh": "需要注意的是，Pod 中环境变量名称允许的字符范围是[有限的](/zh-cn/docs/tasks/inject-data-application/define-environment-variable-container/#using-environment-variables-inside-of-your-config)。\n如果某些变量名称不满足这些规则，则即使 Pod 是可以启动的，你的容器也无法访问这些变量。"}
{"en": "### Container image pull Secrets {#using-imagepullsecrets}\n\nIf you want to fetch container images from a private repository, you need a way for\nthe kubelet on each node to authenticate to that repository. You can configure\n_image pull Secrets_ to make this possible. These secrets are configured at the Pod\nlevel.", "zh": "### 容器镜像拉取 Secret  {#using-imagepullsecrets}\n\n如果你尝试从私有仓库拉取容器镜像，你需要一种方式让每个节点上的 kubelet\n能够完成与镜像库的身份认证。你可以配置**镜像拉取 Secret** 来实现这点。\nSecret 是在 Pod 层面来配置的。"}
{"en": "#### Using imagePullSecrets\n\nThe `imagePullSecrets` field is a list of references to Secrets in the same namespace.\nYou can use an `imagePullSecrets` to pass a Secret that contains a Docker (or other) image registry\npassword to the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod.\nSee the [PodSpec API](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core)\nfor more information about the `imagePullSecrets` field.", "zh": "#### 使用 imagePullSecrets {#using-imagepullsecrets-1}\n\n`imagePullSecrets` 字段是一个列表，包含对同一名字空间中 Secret 的引用。\n你可以使用 `imagePullSecrets` 将包含 Docker（或其他）镜像仓库密码的 Secret\n传递给 kubelet。kubelet 使用此信息来替 Pod 拉取私有镜像。\n参阅 [PodSpec API](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec)\n进一步了解 `imagePullSecrets` 字段。"}
{"en": "##### Manually specifying an imagePullSecret\n\nYou can learn how to specify `imagePullSecrets` from the\n[container images](/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod)\ndocumentation.", "zh": "##### 手动设定 imagePullSecret {#manually-specifying-an-imagepullsecret}\n\n你可以通过阅读[容器镜像](/zh-cn/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod)\n文档了解如何设置 `imagePullSecrets`。"}
{"en": "##### Arranging for imagePullSecrets to be automatically attached\n\nYou can manually create `imagePullSecrets`, and reference these from a ServiceAccount. Any Pods\ncreated with that ServiceAccount or created with that ServiceAccount by default, will get their\n`imagePullSecrets` field set to that of the service account.\nSee [Add ImagePullSecrets to a service account](/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account)\nfor a detailed explanation of that process.", "zh": "##### 设置 imagePullSecrets 为自动挂载 {#arranging-for-imagepullsecrets-to-be-automatically-attached}\n\n你可以手动创建 `imagePullSecret`，并在一个 ServiceAccount 中引用它。\n对使用该 ServiceAccount 创建的所有 Pod，或者默认使用该 ServiceAccount 创建的 Pod\n而言，其 `imagePullSecrets` 字段都会设置为该服务账号。\n请阅读[向服务账号添加 ImagePullSecrets](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account)\n来详细了解这一过程。"}
{"en": "### Using Secrets with static Pods {#restriction-static-pod}\n\nYou cannot use ConfigMaps or Secrets with {{< glossary_tooltip text=\"static Pods\" term_id=\"static-pod\" >}}.", "zh": "### 在静态 Pod 中使用 Secret    {#restriction-static-pod}\n\n你不可以在{{< glossary_tooltip text=\"静态 Pod\" term_id=\"static-pod\" >}}\n中使用 ConfigMap 或 Secret。"}
{"en": "## Use cases\n\n### Use case: As container environment variables {#use-case-as-container-environment-variables}\n\nYou can create a Secret and use it to\n[set environment variables for a container](/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data).", "zh": "## 使用场景  {#use-case}\n\n### 使用场景：作为容器环境变量 {#use-case-as-container-environment-variables}\n\n你可以创建 Secret\n并使用它[为容器设置环境变量](/zh-cn/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data)。"}
{"en": "### Use case: Pod with SSH keys\n\nCreate a Secret containing some SSH keys:", "zh": "### 使用场景：带 SSH 密钥的 Pod {#use-case-pod-with-ssh-keys}\n\n创建包含一些 SSH 密钥的 Secret：\n\n```shell\nkubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/path/to/.ssh/id_rsa --from-file=ssh-publickey=/path/to/.ssh/id_rsa.pub\n```"}
{"en": "The output is similar to:", "zh": "输出类似于：\n\n```\nsecret \"ssh-key-secret\" created\n```"}
{"en": "You can also create a `kustomization.yaml` with a `secretGenerator` field containing ssh keys.", "zh": "你也可以创建一个 `kustomization.yaml` 文件，在其 `secretGenerator`\n字段中包含 SSH 密钥。\n\n{{< caution >}}"}
{"en": "Think carefully before sending your own SSH keys: other users of the cluster may have access\nto the Secret.", "zh": "在提供你自己的 SSH 密钥之前要仔细思考：集群的其他用户可能有权访问该 Secret。"}
{"en": "You could instead create an SSH private key representing a service identity that you want to be\naccessible to all the users with whom you share the Kubernetes cluster, and that you can revoke\nif the credentials are compromised.", "zh": "你也可以创建一个 SSH 私钥，代表一个你希望与你共享 Kubernetes 集群的其他用户分享的服务标识。\n当凭据信息被泄露时，你可以收回该访问权限。\n{{< /caution >}}"}
{"en": "Now you can create a Pod which references the secret with the SSH key and\nconsumes it in a volume:", "zh": "现在你可以创建一个 Pod，在其中访问包含 SSH 密钥的 Secret，并通过卷的方式来使用它：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-test-pod\n  labels:\n    name: secret-test\nspec:\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: ssh-key-secret\n  containers:\n  - name: ssh-test-container\n    image: mySshImage\n    volumeMounts:\n    - name: secret-volume\n      readOnly: true\n      mountPath: \"/etc/secret-volume\"\n```"}
{"en": "When the container's command runs, the pieces of the key will be available in:", "zh": "容器命令执行时，秘钥的数据可以在下面的位置访问到：\n\n```\n/etc/secret-volume/ssh-publickey\n/etc/secret-volume/ssh-privatekey\n```"}
{"en": "The container is then free to use the secret data to establish an SSH connection.", "zh": "容器就可以随便使用 Secret 数据来建立 SSH 连接。"}
{"en": "### Use case: Pods with prod / test credentials\n\nThis example illustrates a Pod which consumes a secret containing production credentials and\nanother Pod which consumes a secret with test environment credentials.\n\nYou can create a `kustomization.yaml` with a `secretGenerator` field or run\n`kubectl create secret`.", "zh": "### 使用场景：带有生产、测试环境凭据的 Pod {#use-case-pods-with-prod-test-credentials}\n\n这一示例所展示的一个 Pod 会使用包含生产环境凭据的 Secret，另一个 Pod\n使用包含测试环境凭据的 Secret。\n\n你可以创建一个带有 `secretGenerator` 字段的 `kustomization.yaml` 文件或者运行\n`kubectl create secret` 来创建 Secret。\n\n```shell\nkubectl create secret generic prod-db-secret --from-literal=username=produser --from-literal=password=Y4nys7f11\n```"}
{"en": "The output is similar to:", "zh": "输出类似于：\n\n```\nsecret \"prod-db-secret\" created\n```"}
{"en": "You can also create a secret for test environment credentials.", "zh": "你也可以创建一个包含测试环境凭据的 Secret：\n\n```shell\nkubectl create secret generic test-db-secret --from-literal=username=testuser --from-literal=password=iluvtests\n```"}
{"en": "The output is similar to:", "zh": "输出类似于：\n\n```\nsecret \"test-db-secret\" created\n```\n\n{{< note >}}"}
{"en": "Special characters such as `$`, `\\`, `*`, `=`, and `!` will be interpreted by your\n[shell](https://en.wikipedia.org/wiki/Shell_(computing)) and require escaping.", "zh": "特殊字符（例如 `$`、`\\`、`*`、`=` 和 `!`）会被你的\n[Shell](https://zh.wikipedia.org/wiki/%E6%AE%BC%E5%B1%A4) 解释，因此需要转义。"}
{"en": "In most shells, the easiest way to escape the password is to surround it with single quotes (`'`).\nFor example, if your actual password is `S!B\\*d$zDsb=`, you should execute the command this way:", "zh": "在大多数 Shell 中，对密码进行转义的最简单方式是用单引号（`'`）将其括起来。\n例如，如果你的实际密码是 `S!B\\*d$zDsb`，则应通过以下方式执行命令：\n\n```shell\nkubectl create secret generic dev-db-secret --from-literal=username=devuser --from-literal=password='S!B\\*d$zDsb='\n```"}
{"en": "You do not need to escape special characters in passwords from files (`--from-file`).", "zh": "你无需对文件中的密码（`--from-file`）中的特殊字符进行转义。\n{{< /note >}}"}
{"en": "Now make the Pods:", "zh": "现在生成 Pod：\n\n```shell\ncat <<EOF > pod.yaml\napiVersion: v1\nkind: List\nitems:\n- kind: Pod\n  apiVersion: v1\n  metadata:\n    name: prod-db-client-pod\n    labels:\n      name: prod-db-client\n  spec:\n    volumes:\n    - name: secret-volume\n      secret:\n        secretName: prod-db-secret\n    containers:\n    - name: db-client-container\n      image: myClientImage\n      volumeMounts:\n      - name: secret-volume\n        readOnly: true\n        mountPath: \"/etc/secret-volume\"\n- kind: Pod\n  apiVersion: v1\n  metadata:\n    name: test-db-client-pod\n    labels:\n      name: test-db-client\n  spec:\n    volumes:\n    - name: secret-volume\n      secret:\n        secretName: test-db-secret\n    containers:\n    - name: db-client-container\n      image: myClientImage\n      volumeMounts:\n      - name: secret-volume\n        readOnly: true\n        mountPath: \"/etc/secret-volume\"\nEOF\n```"}
{"en": "Add the pods to the same `kustomization.yaml`:", "zh": "将 Pod 添加到同一 `kustomization.yaml` 文件中：\n\n```shell\ncat <<EOF >> kustomization.yaml\nresources:\n- pod.yaml\nEOF\n```"}
{"en": "Apply all those objects on the API server by running:", "zh": "通过下面的命令在 API 服务器上应用所有这些对象：\n\n```shell\nkubectl apply -k .\n```"}
{"en": "Both containers will have the following files present on their filesystems with the values\nfor each container's environment:", "zh": "两个文件都会在其文件系统中出现下面的文件，文件中内容是各个容器的环境值：\n\n```\n/etc/secret-volume/username\n/etc/secret-volume/password\n```"}
{"en": "Note how the specs for the two Pods differ only in one field; this facilitates\ncreating Pods with different capabilities from a common Pod template.", "zh": "注意这两个 Pod 的规约中只有一个字段不同。\n这便于基于相同的 Pod 模板生成具有不同能力的 Pod。"}
{"en": "You could further simplify the base Pod specification by using two service accounts:\n\n1. `prod-user` with the `prod-db-secret`\n1. `test-user` with the `test-db-secret`\n\nThe Pod specification is shortened to:", "zh": "你可以通过使用两个服务账号来进一步简化这一基本的 Pod 规约：\n\n1. `prod-user` 服务账号使用 `prod-db-secret`\n1. `test-user` 服务账号使用 `test-db-secret`\n\nPod 规约简化为：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-db-client-pod\n  labels:\n    name: prod-db-client\nspec:\n  serviceAccount: prod-db-client\n  containers:\n  - name: db-client-container\n    image: myClientImage\n```"}
{"en": "## Immutable Secrets {#secret-immutable}", "zh": "## 不可更改的 Secret {#secret-immutable}\n\n{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}"}
{"en": "Kubernetes lets you mark specific Secrets (and ConfigMaps) as _immutable_.\nPreventing changes to the data of an existing Secret has the following benefits:\n\n- protects you from accidental (or unwanted) updates that could cause applications outages\n- (for clusters that extensively use Secrets - at least tens of thousands of unique Secret\n  to Pod mounts), switching to immutable Secrets improves the performance of your cluster\n  by significantly reducing load on kube-apiserver. The kubelet does not need to maintain\n  a [watch] on any Secrets that are marked as immutable.", "zh": "Kubernetes 允许你将特定的 Secret（和 ConfigMap）标记为 **不可更改（Immutable）**。\n禁止更改现有 Secret 的数据有下列好处：\n\n- 防止意外（或非预期的）更新导致应用程序中断\n- （对于大量使用 Secret 的集群而言，至少数万个不同的 Secret 供 Pod 挂载），\n  通过将 Secret 标记为不可变，可以极大降低 kube-apiserver 的负载，提升集群性能。\n  kubelet 不需要监视那些被标记为不可更改的 Secret。"}
{"en": "### Marking a Secret as immutable {#secret-immutable-create}\n\nYou can create an immutable Secret by setting the `immutable` field to `true`. For example,", "zh": "### 将 Secret 标记为不可更改   {#secret-immutable-create}\n\n你可以通过将 Secret 的 `immutable` 字段设置为 `true` 创建不可更改的 Secret。\n例如：\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  ...\ndata:\n  ...\nimmutable: true\n```"}
{"en": "You can also update any existing mutable Secret to make it immutable.", "zh": "你也可以更改现有的 Secret，令其不可更改。\n\n{{< note >}}"}
{"en": "Once a Secret or ConfigMap is marked as immutable, it is _not_ possible to revert this change\nnor to mutate the contents of the `data` field. You can only delete and recreate the Secret.\nExisting Pods maintain a mount point to the deleted Secret - it is recommended to recreate\nthese pods.", "zh": "一旦一个 Secret 或 ConfigMap 被标记为不可更改，撤销此操作或者更改 `data`\n字段的内容都是**不**可能的。\n只能删除并重新创建这个 Secret。现有的 Pod 将维持对已删除 Secret 的挂载点 --\n建议重新创建这些 Pod。\n{{< /note >}}"}
{"en": "## Information security for Secrets\n\nAlthough ConfigMap and Secret work similarly, Kubernetes applies some additional\nprotection for Secret objects.", "zh": "## Secret 的信息安全问题 {#information-security-for-secrets}\n\n尽管 ConfigMap 和 Secret 的工作方式类似，但 Kubernetes 对 Secret 有一些额外的保护。"}
{"en": "Secrets often hold values that span a spectrum of importance, many of which can\ncause escalations within Kubernetes (e.g. service account tokens) and to\nexternal systems. Even if an individual app can reason about the power of the\nSecrets it expects to interact with, other apps within the same namespace can\nrender those assumptions invalid.", "zh": "Secret 通常保存重要性各异的数值，其中很多都可能会导致 Kubernetes 中\n（例如，服务账号令牌）或对外部系统的特权提升。\n即使某些个别应用能够推导它期望使用的 Secret 的能力，\n同一名字空间中的其他应用可能会让这种假定不成立。"}
{"en": "A Secret is only sent to a node if a Pod on that node requires it.\nFor mounting Secrets into Pods, the kubelet stores a copy of the data into a `tmpfs`\nso that the confidential data is not written to durable storage.\nOnce the Pod that depends on the Secret is deleted, the kubelet deletes its local copy\nof the confidential data from the Secret.", "zh": "只有当某个节点上的 Pod 需要某 Secret 时，对应的 Secret 才会被发送到该节点上。\n如果将 Secret 挂载到 Pod 中，kubelet 会将数据的副本保存在在 `tmpfs` 中，\n这样机密的数据不会被写入到持久性存储中。\n一旦依赖于该 Secret 的 Pod 被删除，kubelet 会删除来自于该 Secret 的机密数据的本地副本。"}
{"en": "There may be several containers in a Pod. By default, containers you define\nonly have access to the default ServiceAccount and its related Secret.\nYou must explicitly define environment variables or map a volume into a\ncontainer in order to provide access to any other Secret.", "zh": "同一个 Pod 中可能包含多个容器。默认情况下，你所定义的容器只能访问默认 ServiceAccount\n及其相关 Secret。你必须显式地定义环境变量或者将卷映射到容器中，才能为容器提供对其他\nSecret 的访问。"}
{"en": "There may be Secrets for several Pods on the same node. However, only the\nSecrets that a Pod requests are potentially visible within its containers.\nTherefore, one Pod does not have access to the Secrets of another Pod.", "zh": "针对同一节点上的多个 Pod 可能有多个 Secret。不过，只有某个 Pod 所请求的 Secret\n才有可能对 Pod 中的容器可见。因此，一个 Pod 不会获得访问其他 Pod 的 Secret 的权限。"}
{"en": "### Configure least-privilege access to Secrets\n\nTo enhance the security measures around Secrets, Kubernetes provides a mechanism: you can\nannotate a ServiceAccount as `kubernetes.io/enforce-mountable-secrets: \"true\"`.\n\nFor more information, you can refer to the [documentation about this annotation](/docs/concepts/security/service-accounts/#enforce-mountable-secrets).", "zh": "### 配置 Secret 资源的最小特权访问\n\n为了加强对 Secret 的安全措施，Kubernetes 提供了一种机制：\n你可以为 ServiceAccount 添加 `kubernetes.io/enforce-mountable-secrets: \"true\"` 注解。\n\n想了解更多信息，你可以参考[此注解的文档](/zh-cn/docs/concepts/security/service-accounts/#enforce-mountable-secrets)。\n\n{{< warning >}}"}
{"en": "Any containers that run with `privileged: true` on a node can access all\nSecrets used on that node.", "zh": "在一个节点上以 `privileged: true` 运行的所有容器可以访问该节点上使用的所有 Secret。\n{{< /warning >}}\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- For guidelines to manage and improve the security of your Secrets, refer to\n  [Good practices for Kubernetes Secrets](/docs/concepts/security/secrets-good-practices).\n- Learn how to [manage Secrets using `kubectl`](/docs/tasks/configmap-secret/managing-secret-using-kubectl/)\n- Learn how to [manage Secrets using config file](/docs/tasks/configmap-secret/managing-secret-using-config-file/)\n- Learn how to [manage Secrets using kustomize](/docs/tasks/configmap-secret/managing-secret-using-kustomize/)\n- Read the [API reference](/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/) for `Secret`", "zh": "- 有关管理和提升 Secret 安全性的指南，请参阅 [Kubernetes Secret 良好实践](/zh-cn/docs/concepts/security/secrets-good-practices)\n- 学习如何[使用 `kubectl` 管理 Secret](/zh-cn/docs/tasks/configmap-secret/managing-secret-using-kubectl/)\n- 学习如何[使用配置文件管理 Secret](/zh-cn/docs/tasks/configmap-secret/managing-secret-using-config-file/)\n- 学习如何[使用 kustomize 管理 Secret](/zh-cn/docs/tasks/configmap-secret/managing-secret-using-kustomize/)\n- 阅读 [API 参考](/zh-cn/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/)了解 `Secret`"}
{"en": "When you specify a {{< glossary_tooltip term_id=\"pod\" >}}, you can optionally specify how much of each resource a \n{{< glossary_tooltip text=\"container\" term_id=\"container\" >}} needs. The most common resources to specify are CPU and memory \n(RAM); there are others.\n\nWhen you specify the resource _request_ for containers in a Pod, the\n{{< glossary_tooltip text=\"kube-scheduler\" term_id=\"kube-scheduler\" >}} uses this information to decide which node to place the Pod on. \nWhen you specify a resource _limit_ for a container, the {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} enforces those \nlimits so that the running container is not allowed to use more of that resource \nthan the limit you set. The kubelet also reserves at least the _request_ amount of \nthat system resource specifically for that container to use.", "zh": "当你定义 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 时可以选择性地为每个\n{{< glossary_tooltip text=\"容器\" term_id=\"container\" >}}设定所需要的资源数量。\n最常见的可设定资源是 CPU 和内存（RAM）大小；此外还有其他类型的资源。\n\n当你为 Pod 中的 Container 指定了资源 **request（请求）** 时，\n{{< glossary_tooltip text=\"kube-scheduler\" term_id=\"kube-scheduler\" >}}\n就利用该信息决定将 Pod 调度到哪个节点上。\n当你为 Container 指定了资源 **limit（限制）** 时，{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}}\n就可以确保运行的容器不会使用超出所设限制的资源。\nkubelet 还会为容器预留所 **request（请求）** 数量的系统资源，供其使用。"}
{"en": "## Requests and limits\n\nIf the node where a Pod is running has enough of a resource available, it's possible (and\nallowed) for a container to use more resource than its `request` for that resource specifies.\nHowever, a container is not allowed to use more than its resource `limit`.\n\nFor example, if you set a `memory` request of 256 MiB for a container, and that container is in\na Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use\nmore RAM.", "zh": "## 请求和限制  {#requests-and-limits}\n\n如果 Pod 运行所在的节点具有足够的可用资源，容器可能（且可以）使用超出对应资源\n`request` 属性所设置的资源量。不过，容器不可以使用超出其资源 `limit`\n属性所设置的资源量。\n\n例如，如果你将容器的 `memory` 的请求量设置为 256 MiB，而该容器所处的 Pod\n被调度到一个具有 8 GiB 内存的节点上，并且该节点上没有其他 Pod\n运行，那么该容器就可以尝试使用更多的内存。"}
{"en": "If you set a `memory` limit of 4GiB for that container, the kubelet (and\n{{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}) enforce the limit.\nThe runtime prevents the container from using more than the configured resource limit. For example:\nwhen a process in the container tries to consume more than the allowed amount of memory,\nthe system kernel terminates the process that attempted the allocation, with an out of memory\n(OOM) error.\n\nLimits can be implemented either reactively (the system intervenes once it sees a violation)\nor by enforcement (the system prevents the container from ever exceeding the limit). Different\nruntimes can have different ways to implement the same restrictions.", "zh": "如果你将某容器的 `memory` 限制设置为 4 GiB，kubelet\n（和{{< glossary_tooltip text=\"容器运行时\" term_id=\"container-runtime\" >}}）就会确保该限制生效。\n容器运行时会禁止容器使用超出所设置资源限制的资源。\n例如：当容器中进程尝试使用超出所允许内存量的资源时，系统内核会将尝试申请内存的进程终止，\n并引发内存不足（OOM）错误。\n\n限制可以以被动方式来实现（系统会在发现违例时进行干预），或者通过强制生效的方式实现\n（系统会避免容器用量超出限制）。不同的容器运行时采用不同方式来实现相同的限制。\n\n{{< note >}}"}
{"en": "If you specify a limit for a resource, but do not specify any request, and no admission-time\nmechanism has applied a default request for that resource, then Kubernetes copies the limit\nyou specified and uses it as the requested value for the resource.", "zh": "如果你为某个资源指定了限制，但不指定请求，\n并且没有应用准入时机制为该资源设置默认请求，\n然后 Kubernetes 复制你所指定的限制值，将其用作资源的请求值。\n{{< /note >}}"}
{"en": "## Resource types\n\n*CPU* and *memory* are each a *resource type*. A resource type has a base unit.\nCPU represents compute processing and is specified in units of [Kubernetes CPUs](#meaning-of-cpu).\nMemory is specified in units of bytes.\nFor Linux workloads, you can specify _huge page_ resources.\nHuge pages are a Linux-specific feature where the node kernel allocates blocks of memory\nthat are much larger than the default page size.\n\nFor example, on a system where the default page size is 4KiB, you could specify a limit,\n`hugepages-2Mi: 80Mi`. If the container tries allocating over 40 2MiB huge pages (a\ntotal of 80 MiB), that allocation fails.", "zh": "## 资源类型  {#resource-types}\n\n**CPU** 和 **内存** 都是 **资源类型**。每种资源类型具有其基本单位。\nCPU 表达的是计算处理能力，其单位是 [Kubernetes CPU](#meaning-of-cpu)。\n内存的单位是字节。\n对于 Linux 负载，则可以指定巨页（Huge Page）资源。\n巨页是 Linux 特有的功能，节点内核在其中分配的内存块比默认页大小大得多。\n\n例如，在默认页面大小为 4KiB 的系统上，你可以指定限制 `hugepages-2Mi: 80Mi`。\n如果容器尝试分配 40 个 2MiB 大小的巨页（总共 80 MiB ），则分配请求会失败。\n\n{{< note >}}"}
{"en": "You cannot overcommit `hugepages-*` resources.\nThis is different from the `memory` and `cpu` resources.", "zh": "你不能过量使用 `hugepages- *` 资源。\n这与 `memory` 和 `cpu` 资源不同。\n{{< /note >}}"}
{"en": "CPU and memory are collectively referred to as *compute resources*, or *resources*. Compute\nresources are measurable quantities that can be requested, allocated, and\nconsumed. They are distinct from\n[API resources](/docs/concepts/overview/kubernetes-api/). API resources, such as Pods and\n[Services](/docs/concepts/services-networking/service/) are objects that can be read and modified\nthrough the Kubernetes API server.", "zh": "CPU 和内存统称为 **计算资源**，或简称为 **资源**。\n计算资源的数量是可测量的，可以被请求、被分配、被消耗。\n它们与 [API 资源](/zh-cn/docs/concepts/overview/kubernetes-api/)不同。\nAPI 资源（如 Pod 和 [Service](/zh-cn/docs/concepts/services-networking/service/)）是可通过\nKubernetes API 服务器读取和修改的对象。"}
{"en": "## Resource requests and limits of Pod and container\n\nFor each container, you can specify resource limits and requests,\nincluding the following:", "zh": "## Pod 和 容器的资源请求和限制  {#resource-requests-and-limits-of-pod-and-container}\n\n针对每个容器，你都可以指定其资源限制和请求，包括如下选项：\n\n* `spec.containers[].resources.limits.cpu`\n* `spec.containers[].resources.limits.memory`\n* `spec.containers[].resources.limits.hugepages-<size>`\n* `spec.containers[].resources.requests.cpu`\n* `spec.containers[].resources.requests.memory`\n* `spec.containers[].resources.requests.hugepages-<size>`"}
{"en": "Although you can only specify requests and limits for individual containers,\nit is also useful to think about the overall resource requests and limits for\na Pod.\nFor a particular resource, a *Pod resource request/limit* is the sum of the\nresource requests/limits of that type for each container in the Pod.", "zh": "尽管你只能逐个容器地指定请求和限制值，考虑 Pod 的总体资源请求和限制也是有用的。\n对特定资源而言，**Pod 的资源请求/限制** 是 Pod 中各容器对该类型资源的请求/限制的总和。"}
{"en": "## Resource units in Kubernetes\n\n### CPU resource units {#meaning-of-cpu}\n\nLimits and requests for CPU resources are measured in *cpu* units.\nIn Kubernetes, 1 CPU unit is equivalent to **1 physical CPU core**,\nor **1 virtual core**, depending on whether the node is a physical host\nor a virtual machine running inside a physical machine.", "zh": "## Kubernetes 中的资源单位  {#resource-units-in-kubernetes}\n\n### CPU 资源单位    {#meaning-of-cpu}\n\nCPU 资源的限制和请求以 “cpu” 为单位。\n在 Kubernetes 中，一个 CPU 等于 **1 个物理 CPU 核** 或者 **1 个虚拟核**，\n取决于节点是一台物理主机还是运行在某物理主机上的虚拟机。"}
{"en": "Fractional requests are allowed. When you define a container with\n`spec.containers[].resources.requests.cpu` set to `0.5`, you are requesting half\nas much CPU time compared to if you asked for `1.0` CPU.\nFor CPU resource units, the [quantity](/docs/reference/kubernetes-api/common-definitions/quantity/) expression `0.1` is equivalent to the\nexpression `100m`, which can be read as \"one hundred millicpu\". Some people say\n\"one hundred millicores\", and this is understood to mean the same thing.", "zh": "你也可以表达带小数 CPU 的请求。\n当你定义一个容器，将其 `spec.containers[].resources.requests.cpu` 设置为 0.5 时，\n你所请求的 CPU 是你请求 `1.0` CPU 时的一半。\n对于 CPU 资源单位，[数量](/zh-cn/docs/reference/kubernetes-api/common-definitions/quantity/)\n表达式 `0.1` 等价于表达式 `100m`，可以看作 “100 millicpu”。\n有些人说成是“一百毫核”，其实说的是同样的事情。"}
{"en": "CPU resource is always specified as an absolute amount of resource, never as a relative amount. For example,\n`500m` CPU represents the roughly same amount of computing power whether that container\nruns on a single-core, dual-core, or 48-core machine.", "zh": "CPU 资源总是设置为资源的绝对数量而非相对数量值。\n例如，无论容器运行在单核、双核或者 48-核的机器上，`500m` CPU 表示的是大约相同的计算能力。\n\n{{< note >}}"}
{"en": "Kubernetes doesn't allow you to specify CPU resources with a precision finer than\n`1m` or `0.001` CPU. To avoid accidentally using an invalid CPU quantity, it's useful to specify CPU units using the milliCPU form \ninstead of the decimal form when using less than 1 CPU unit. \n\nFor example, you have a Pod that uses `5m` or `0.005` CPU and would like to decrease\nits CPU resources. By using the decimal form, it's harder to spot that `0.0005` CPU\nis an invalid value, while by using the milliCPU form, it's easier to spot that\n`0.5m` is an invalid value.", "zh": "Kubernetes 不允许设置精度小于 `1m` 或 `0.001` 的 CPU 资源。\n为了避免意外使用无效的 CPU 数量，当使用少于 1 个 CPU 单元时，使用\nmilliCPU 形式而不是十进制形式指定 CPU 单元非常有用。\n\n例如，你有一个使用 `5m` 或 `0.005` 核 CPU 的 Pod，并且希望减少其 CPU 资源。\n通过使用十进制形式，更难发现 `0.0005` CPU 是无效值，而通过使用 milliCPU 形式，\n更容易发现 `0.5m` 是无效值。\n{{< /note >}}"}
{"en": "### Memory resource units {#meaning-of-memory}\n\nLimits and requests for `memory` are measured in bytes. You can express memory as\na plain integer or as a fixed-point number using one of these\n[quantity](/docs/reference/kubernetes-api/common-definitions/quantity/) suffixes:\nE, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,\nMi, Ki. For example, the following represent roughly the same value:", "zh": "## 内存资源单位      {#meaning-of-memory}\n\n`memory` 的限制和请求以字节为单位。\n你可以使用普通的整数，或者带有以下\n[数量](/zh-cn/docs/reference/kubernetes-api/common-definitions/quantity/)后缀\n的定点数字来表示内存：E、P、T、G、M、k。\n你也可以使用对应的 2 的幂数：Ei、Pi、Ti、Gi、Mi、Ki。\n例如，以下表达式所代表的是大致相同的值：\n\n```shell\n128974848、129e6、129M、128974848000m、123Mi\n```"}
{"en": "Pay attention to the case of the suffixes. If you request `400m` of memory, this is a request\nfor 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (`400Mi`)\nor 400 megabytes (`400M`).", "zh": "请注意后缀的大小写。如果你请求 `400m` 临时存储，实际上所请求的是 0.4 字节。\n如果有人这样设定资源请求或限制，可能他的实际想法是申请 400Mi 字节（`400Mi`）\n或者 400M 字节。"}
{"en": "## Container resources example {#example-1}\n\nThe following Pod has two containers. Both containers are defined with a request for\n0.25 CPU\nand 64MiB (2<sup>26</sup> bytes) of memory. Each container has a limit of 0.5\nCPU and 128MiB of memory. You can say the Pod has a request of 0.5 CPU and 128\nMiB of memory, and a limit of 1 CPU and 256MiB of memory.", "zh": "## 容器资源示例     {#example-1}\n\n以下 Pod 有两个容器。每个容器的请求为 0.25 CPU 和 64MiB（2<sup>26</sup> 字节）内存，\n每个容器的资源限制为 0.5 CPU 和 128MiB 内存。\n你可以认为该 Pod 的资源请求为 0.5 CPU 和 128 MiB 内存，资源限制为 1 CPU 和 256MiB 内存。\n\n```yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: frontend\nspec:\n  containers:\n  - name: app\n    image: images.my-company.example/app:v4\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n  - name: log-aggregator\n    image: images.my-company.example/log-aggregator:v6\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n```"}
{"en": "## How Pods with resource requests are scheduled\n\nWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to\nrun on. Each node has a maximum capacity for each of the resource types: the\namount of CPU and memory it can provide for Pods. The scheduler ensures that,\nfor each resource type, the sum of the resource requests of the scheduled\ncontainers is less than the capacity of the node.\nNote that although actual memory\nor CPU resource usage on nodes is very low, the scheduler still refuses to place\na Pod on a node if the capacity check fails. This protects against a resource\nshortage on a node when resource usage later increases, for example, during a\ndaily peak in request rate.", "zh": "## 带资源请求的 Pod 如何调度  {#how-pods-with-resource-limits-are-run}\n\n当你创建一个 Pod 时，Kubernetes 调度程序将为 Pod 选择一个节点。\n每个节点对每种资源类型都有一个容量上限：可为 Pod 提供的 CPU 和内存量。\n调度程序确保对于每种资源类型，所调度的容器的资源请求的总和小于节点的容量。\n请注意，尽管节点上的实际内存或 CPU 资源使用量非常低，如果容量检查失败，\n调度程序仍会拒绝在该节点上放置 Pod。\n当稍后节点上资源用量增加，例如到达请求率的每日峰值区间时，节点上也不会出现资源不足的问题。"}
{"en": "## How Kubernetes applies resource requests and limits {#how-pods-with-resource-limits-are-run}\n\nWhen the kubelet starts a container as part of a Pod, the kubelet passes that container's\nrequests and limits for memory and CPU to the container runtime.\n\nOn Linux, the container runtime typically configures\nkernel {{< glossary_tooltip text=\"cgroups\" term_id=\"cgroup\" >}} that apply and enforce the\nlimits you defined.", "zh": "## Kubernetes 应用资源请求与限制的方式 {#how-pods-with-resource-limits-are-run}\n\n当 kubelet 将容器作为 Pod 的一部分启动时，它会将容器的 CPU 和内存请求与限制信息传递给容器运行时。\n\n在 Linux 系统上，容器运行时通常会配置内核\n{{< glossary_tooltip text=\"CGroups\" term_id=\"cgroup\" >}}，负责应用并实施所定义的请求。"}
{"en": "- The CPU limit defines a hard ceiling on how much CPU time that the container can use.\n  During each scheduling interval (time slice), the Linux kernel checks to see if this\n  limit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution.", "zh": "- CPU 限制定义的是容器可使用的 CPU 时间的硬性上限。\n  在每个调度周期（时间片）期间，Linux 内核检查是否已经超出该限制；\n  内核会在允许该 cgroup 恢复执行之前会等待。"}
{"en": "- The CPU request typically defines a weighting. If several different containers (cgroups)\n  want to run on a contended system, workloads with larger CPU requests are allocated more\n  CPU time than workloads with small requests.", "zh": "- CPU 请求值定义的是一个权重值。如果若干不同的容器（CGroups）需要在一个共享的系统上竞争运行，\n  CPU 请求值大的负载会获得比请求值小的负载更多的 CPU 时间。"}
{"en": "- The memory request is mainly used during (Kubernetes) Pod scheduling. On a node that uses\n  cgroups v2, the container runtime might use the memory request as a hint to set\n  `memory.min` and `memory.low`.", "zh": "- 内存请求值主要用于（Kubernetes）Pod 调度期间。在一个启用了 CGroup v2 的节点上，\n  容器运行时可能会使用内存请求值作为设置 `memory.min` 和 `memory.low` 的提示值。"}
{"en": "- The memory limit defines a memory limit for that cgroup. If the container tries to\n  allocate more memory than this limit, the Linux kernel out-of-memory subsystem activates\n  and, typically, intervenes by stopping one of the processes in the container that tried\n  to allocate memory. If that process is the container's PID 1, and the container is marked\n  as restartable, Kubernetes restarts the container.", "zh": "- 内存限制定义的是 cgroup 的内存限制。如果容器尝试分配的内存量超出限制，\n  则 Linux 内核的内存不足处理子系统会被激活，并停止尝试分配内存的容器中的某个进程。\n  如果该进程在容器中 PID 为 1，而容器被标记为可重新启动，则 Kubernetes\n  会重新启动该容器。"}
{"en": "- The memory limit for the Pod or container can also apply to pages in memory backed\n  volumes, such as an `emptyDir`. The kubelet tracks `tmpfs` emptyDir volumes as container\n  memory use, rather than as local ephemeral storage.　When using memory backed `emptyDir`,\n  be sure to check the notes [below](#memory-backed-emptydir).", "zh": "- Pod 或容器的内存限制也适用于通过内存作为介质的卷，例如 `emptyDir` 卷。\n  kubelet 会跟踪 `tmpfs` 形式的 emptyDir 卷用量，将其作为容器的内存用量，\n  而不是临时存储用量。当使用内存作为介质的 `emptyDir` 时，\n  请务必查看[下面](#memory-backed-emptydir)的注意事项。"}
{"en": "If a container exceeds its memory request and the node that it runs on becomes short of\nmemory overall, it is likely that the Pod the container belongs to will be\n{{< glossary_tooltip text=\"evicted\" term_id=\"eviction\" >}}.\n\nA container might or might not be allowed to exceed its CPU limit for extended periods of time.\nHowever, container runtimes don't terminate Pods or containers for excessive CPU usage.\n\nTo determine whether a container cannot be scheduled or is being killed due to resource limits,\nsee the [Troubleshooting](#troubleshooting) section.", "zh": "如果某容器内存用量超过其内存请求值并且所在节点内存不足时，容器所处的 Pod\n可能被{{< glossary_tooltip text=\"逐出\" term_id=\"eviction\" >}}。\n\n每个容器可能被允许也可能不被允许使用超过其 CPU 限制的处理时间。\n但是，容器运行时不会由于 CPU 使用率过高而杀死 Pod 或容器。\n\n要确定某容器是否会由于资源限制而无法调度或被杀死，请参阅[疑难解答](#troubleshooting)节。"}
{"en": "### Monitoring compute & memory resource usage\n\nThe kubelet reports the resource usage of a Pod as part of the Pod\n[`status`](/docs/concepts/overview/working-with-objects/#object-spec-and-status).\n\nIf optional [tools for monitoring](/docs/tasks/debug/debug-cluster/resource-usage-monitoring/)\nare available in your cluster, then Pod resource usage can be retrieved either\nfrom the [Metrics API](/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-api)\ndirectly or from your monitoring tools.", "zh": "### 监控计算和内存资源用量  {#monitoring-compute-memory-resource-usage}\n\nkubelet 会将 Pod 的资源使用情况作为 Pod\n[`status`](/zh-cn/docs/concepts/overview/working-with-objects/#object-spec-and-status)\n的一部分来报告的。\n\n如果为集群配置了可选的[监控工具](/zh-cn/docs/tasks/debug/debug-cluster/resource-usage-monitoring/)，\n则可以直接从[指标 API](/zh-cn/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-api)\n或者监控工具获得 Pod 的资源使用情况。"}
{"en": "### Considerations for memory backed `emptyDir` volumes {#memory-backed-emptydir}", "zh": "### 使用内存作为介质的 `emptyDir` 卷的注意事项 {#memory-backed-emptydir}\n\n{{< caution >}}"}
{"en": "If you do not specify a `sizeLimit` for an `emptyDir` volume, that volume may\nconsume up to that pod's memory limit (`Pod.spec.containers[].resources.limits.memory`).\nIf you do not set a memory limit, the pod has no upper bound on memory consumption,\nand can consume all available memory on the node.  Kubernetes schedules pods based\non resource requests (`Pod.spec.containers[].resources.requests`) and will not\nconsider memory usage above the request when deciding if another pod can fit on\na given node.  This can result in a denial of service and cause the OS to do\nout-of-memory (OOM) handling.  It is possible to create any number of `emptyDir`s\nthat could potentially consume all available memory on the node, making OOM\nmore likely.", "zh": "如果你没有为 `emptyDir` 卷指定 `sizeLimit`，该卷就会消耗 Pod 的内存，\n卷的用量上限为 Pod 的内存限制（`Pod.spec.containers[].resources.limits.memory`）。\n如果你没有设置内存限制，Pod 的内存消耗将没有上限，并且可能会用掉节点上的所有可用内存。\nKubernetes 基于资源请求（`Pod.spec.containers[].resources.requests`）调度 Pod，\n并且在决定另一个 Pod 是否适合调度到某个给定的节点上时，不会考虑超出请求的内存用量。\n这可能导致拒绝服务，并使操作系统出现需要处理内存不足（OOM）的情况。\n用户可以创建任意数量的 `emptyDir`，可能会消耗节点上的所有可用内存，使得 OOM 更有可能发生。\n{{< /caution >}}"}
{"en": "From the perspective of memory management, there are some similarities between\nwhen a process uses memory as a work area and when using memory-backed\n`emptyDir`. But when using memory as a volume like memory-backed `emptyDir`,\nthere are additional points below that you should be careful of.", "zh": "从内存管理的角度来看，进程使用内存作为工作区与使用内存作为 `emptyDir` 的介质有一些相似之处。\n但当将内存用作存储卷（例如内存为介质的 `emptyDir` 卷）时，你需要额外注意以下几点："}
{"en": "* Files stored on a memory-backed volume are almost entirely managed by the\n  user application.  Unlike when used as a work area for a process, you can not\n  rely on things like language-level garbage collection.\n* The purpose of writing files to a volume is to save data or pass it between\n  applications.  Neither Kubernetes nor the OS may automatically delete files\n  from a volume, so memory used by those files can not be reclaimed when the\n  system or the pod are under memory pressure.\n* A memory-backed `emptyDir` is useful because of its performance, but memory\n  is generally much smaller in size and much higher in cost than other storage\n  media, such as disks or SSDs.  Using large amounts of memory for `emptyDir`\n  volumes may affect the normal operation of your pod or of the whole node,\n  so should be used carefully.", "zh": "* 存储在内存为介质的卷上的文件几乎完全由用户应用所管理。\n  与用作进程工作区的用法不同，你无法依赖语言级别垃圾回收这类机制。\n* 将文件写入某个卷的目的是保存数据或在应用之间传递数据。\n  Kubernetes 或操作系统都不会自动从卷中删除文件，\n  因此当系统或 Pod 面临内存压力时，将无法回收这些文件所使用的内存。\n* 以内存为介质的 `emptyDir` 因性能较好而很有用，但内存通常比其他存储介质（如磁盘或 SSD）小得多且成本更高。\n  为 `emptyDir` 卷使用大量内存可能会影响 Pod 或整个节点的正常运行，因此你应谨慎使用。"}
{"en": "If you are administering a cluster or namespace, you can also set\n[ResourceQuota](/docs/concepts/policy/resource-quotas/) that limits memory use;\nyou may also want to define a [LimitRange](/docs/concepts/policy/limit-range/)\nfor additional enforcement.\nIf you specify a `spec.containers[].resources.limits.memory` for each Pod,\nthen the maximum size of an `emptyDir` volume will be the pod's memory limit.", "zh": "如果你在管理集群或命名空间，还可以设置限制内存使用的 [ResourceQuota](/zh-cn/docs/concepts/policy/resource-quotas/)；\n你可能还希望定义一个 [LimitRange](/zh-cn/docs/concepts/policy/limit-range/) 以施加额外的限制。如果为每个 Pod\n指定 `spec.containers[].resources.limits.memory`，那么 `emptyDir` 卷的最大尺寸将是该 Pod 的内存限制。"}
{"en": "As an alternative, a cluster administrator can enforce size limits for\n`emptyDir` volumes in new Pods using a policy mechanism such as\n[ValidationAdmissionPolicy](/docs/reference/access-authn-authz/validating-admission-policy).", "zh": "作为一种替代方案，集群管理员可以使用诸如\n[ValidationAdmissionPolicy](/zh-cn/docs/reference/access-authn-authz/validating-admission-policy)\n之类的策略机制来强制对新 Pod 的 `emptyDir` 卷进行大小限制。"}
{"en": "## Local ephemeral storage\n\nNodes have local ephemeral storage, backed by\nlocally-attached writeable devices or, sometimes, by RAM.\n\"Ephemeral\" means that there is no long-term guarantee about durability.\n\nPods use ephemeral local storage for scratch space, caching, and for logs.\nThe kubelet can provide scratch space to Pods using local ephemeral storage to\nmount [`emptyDir`](/docs/concepts/storage/volumes/#emptydir)\n {{< glossary_tooltip term_id=\"volume\" text=\"volumes\" >}} into containers.", "zh": "## 本地临时存储   {#local-ephemeral-storage}"}
{"en": "feature gate LocalStorageCapacityIsolation", "zh": "{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}\n\n节点通常还可以具有本地的临时性存储，由本地挂接的可写入设备或者有时也用 RAM\n来提供支持。“临时（Ephemeral）”意味着对所存储的数据不提供长期可用性的保证。\n\nPods 通常可以使用临时性本地存储来实现缓冲区、保存日志等功能。\nkubelet 可以为使用本地临时存储的 Pods 提供这种存储空间，允许后者使用\n[`emptyDir`](/zh-cn/docs/concepts/storage/volumes/#emptydir)\n类型的{{< glossary_tooltip term_id=\"volume\" text=\"卷\" >}}将其挂载到容器中。"}
{"en": "The kubelet also uses this kind of storage to hold\n[node-level container logs](/docs/concepts/cluster-administration/logging/#logging-at-the-node-level),\ncontainer images, and the writable layers of running containers.", "zh": "kubelet 也使用此类存储来保存[节点层面的容器日志](/zh-cn/docs/concepts/cluster-administration/logging/#logging-at-the-node-level)、\n容器镜像文件以及运行中容器的可写入层。\n\n{{< caution >}}"}
{"en": "If a node fails, the data in its ephemeral storage can be lost.\nYour applications cannot expect any performance SLAs (disk IOPS for example)\nfrom local ephemeral storage.", "zh": "如果节点失效，存储在临时性存储中的数据会丢失。\n你的应用不能对本地临时性存储的性能 SLA（例如磁盘 IOPS）作任何假定。\n{{< /caution >}}\n\n{{< note >}}"}
{"en": "To make the resource quota work on ephemeral-storage, two things need to be done:\n\n* An admin sets the resource quota for ephemeral-storage in a namespace.\n* A user needs to specify limits for the ephemeral-storage resource in the Pod spec.\n\nIf the user doesn't specify the ephemeral-storage resource limit in the Pod spec,\nthe resource quota is not enforced on ephemeral-storage.", "zh": "为了使临时性存储的资源配额生效，需要完成以下两个步骤：\n\n* 管理员在命名空间中设置临时性存储的资源配额。\n* 用户需要在 Pod spec 中指定临时性存储资源的限制。\n\n如果用户在 Pod spec 中未指定临时性存储资源的限制，\n则临时性存储的资源配额不会生效。\n{{< /note >}}"}
{"en": "Kubernetes lets you track, reserve and limit the amount\nof ephemeral local storage a Pod can consume.", "zh": "Kubernetes 允许你跟踪、预留和限制 Pod\n可消耗的临时性本地存储数量。"}
{"en": "### Configurations for local ephemeral storage\n\nKubernetes supports two ways to configure local ephemeral storage on a node:\n\nIn this configuration, you place all different kinds of ephemeral local data\n(`emptyDir` volumes, writeable layers, container images, logs) into one filesystem.\nThe most effective way to configure the kubelet means dedicating this filesystem\nto Kubernetes (kubelet) data.\n\nThe kubelet also writes\n[node-level container logs](/docs/concepts/cluster-administration/logging/#logging-at-the-node-level)\nand treats these similarly to ephemeral local storage.", "zh": "### 本地临时性存储的配置  {##configurations-for-local-ephemeral-storage}\n\nKubernetes 有两种方式支持节点上配置本地临时性存储：\n\n{{< tabs name=\"local_storage_configurations\" >}}\n{{% tab name=\"单一文件系统\" %}}\n采用这种配置时，你会把所有类型的临时性本地数据（包括 `emptyDir`\n卷、可写入容器层、容器镜像、日志等）放到同一个文件系统中。\n作为最有效的 kubelet 配置方式，这意味着该文件系统是专门提供给 Kubernetes\n（kubelet）来保存数据的。\n\nkubelet 也会生成[节点层面的容器日志](/zh-cn/docs/concepts/cluster-administration/logging/#logging-at-the-node-level)，\n并按临时性本地存储的方式对待之。"}
{"en": "The kubelet writes logs to files inside its configured log directory (`/var/log`\nby default); and has a base directory for other locally stored data\n(`/var/lib/kubelet` by default).\n\nTypically, both `/var/lib/kubelet` and `/var/log` are on the system root filesystem,\nand the kubelet is designed with that layout in mind.\n\nYour node can have as many other filesystems, not used for Kubernetes,\nas you like.", "zh": "kubelet 会将日志写入到所配置的日志目录（默认为 `/var/log`）下的文件中；\n还会针对其他本地存储的数据使用同一个基础目录（默认为 `/var/lib/kubelet`）。\n\n通常，`/var/lib/kubelet` 和 `/var/log` 都是在系统的根文件系统中。kubelet\n的设计也考虑到这一点。\n\n你的集群节点当然可以包含其他的、并非用于 Kubernetes 的很多文件系统。\n{{% /tab %}}\n\n{{% tab name=\"双文件系统\" %}}"}
{"en": "You have a filesystem on the node that you're using for ephemeral data that\ncomes from running Pods: logs, and `emptyDir` volumes. You can use this filesystem\nfor other data (for example: system logs not related to Kubernetes); it can even\nbe the root filesystem.\n\nThe kubelet also writes\n[node-level container logs](/docs/concepts/cluster-administration/logging/#logging-at-the-node-level)\ninto the first filesystem, and treats these similarly to ephemeral local storage.", "zh": "你使用节点上的某个文件系统来保存运行 Pod 时产生的临时性数据：日志和\n`emptyDir` 卷等。你可以使用这个文件系统来保存其他数据（例如：与 Kubernetes\n无关的其他系统日志）；这个文件系统还可以是根文件系统。\n\nkubelet 也将[节点层面的容器日志](/zh-cn/docs/concepts/cluster-administration/logging/#logging-at-the-node-level)\n写入到第一个文件系统中，并按临时性本地存储的方式对待之。"}
{"en": "You also use a separate filesystem, backed by a different logical storage device.\nIn this configuration, the directory where you tell the kubelet to place\ncontainer image layers and writeable layers is on this second filesystem.\n\nThe first filesystem does not hold any image layers or writeable layers.\n\nYour node can have as many other filesystems, not used for Kubernetes,\nas you like.", "zh": "同时你使用另一个由不同逻辑存储设备支持的文件系统。在这种配置下，你会告诉\nkubelet 将容器镜像层和可写层保存到这第二个文件系统上的某个目录中。\n\n第一个文件系统中不包含任何镜像层和可写层数据。\n\n当然，你的集群节点上还可以有很多其他与 Kubernetes 没有关联的文件系统。\n{{% /tab %}}\n{{< /tabs >}}"}
{"en": "The kubelet can measure how much local storage it is using. It does this provided\nthat you have set up the node using one of the supported configurations for local\nephemeral storage.\n\nIf you have a different configuration, then the kubelet does not apply resource\nlimits for ephemeral local storage.", "zh": "kubelet 能够度量其本地存储的用量。\n实现度量机制的前提是你已使用本地临时存储所支持的配置之一对节点进行配置。\n\n如果你的节点配置不同于以上预期，kubelet 就无法对临时性本地存储实施资源限制。\n\n{{< note >}}"}
{"en": "The kubelet tracks `tmpfs` emptyDir volumes as container memory use, rather\nthan as local ephemeral storage.", "zh": "kubelet 会将 `tmpfs` emptyDir 卷的用量当作容器内存用量，而不是本地临时性存储来统计。\n{{< /note >}}\n\n{{< note >}}"}
{"en": "The kubelet will only track the root filesystem for ephemeral storage. OS layouts that mount a separate disk to `/var/lib/kubelet` or `/var/lib/containers` will not report ephemeral storage correctly.", "zh": "kubelet 将仅跟踪临时存储的根文件系统。\n挂载一个单独磁盘到 `/var/lib/kubelet` 或 `/var/lib/containers` 的操作系统布局将不会正确地报告临时存储。\n{{< /note >}}"}
{"en": "### Setting requests and limits for local ephemeral storage\n\nYou can specify `ephemeral-storage` for managing local ephemeral storage. Each\ncontainer of a Pod can specify either or both of the following:\n\n* `spec.containers[].resources.limits.ephemeral-storage`\n* `spec.containers[].resources.requests.ephemeral-storage`", "zh": "### 为本地临时性存储设置请求和限制  {#setting-requests-and-limits-for-local-ephemeral-storage}\n\n你可以指定 `ephemeral-storage` 来管理本地临时性存储。\nPod 中的每个容器可以设置以下属性：\n\n* `spec.containers[].resources.limits.ephemeral-storage`\n* `spec.containers[].resources.requests.ephemeral-storage`"}
{"en": "Limits and requests for `ephemeral-storage` are measured in byte quantities.\nYou can express storage as a plain integer or as a fixed-point number using one of these suffixes:\nE, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,\nMi, Ki. For example, the following quantities all represent roughly the same value:", "zh": "`ephemeral-storage` 的请求和限制是按量纲计量的。\n你可以使用一般整数或者定点数字加上下面的后缀来表达存储量：E、P、T、G、M、k。\n你也可以使用对应的 2 的幂级数来表达：Ei、Pi、Ti、Gi、Mi、Ki。\n例如，下面的表达式所表达的大致是同一个值：\n\n- `128974848`\n- `129e6`\n- `129M`\n- `123Mi`"}
{"en": "Pay attention to the case of the suffixes. If you request `400m` of ephemeral-storage, this is a request\nfor 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (`400Mi`)\nor 400 megabytes (`400M`).", "zh": "请注意后缀的大小写。如果你请求 `400m` 临时存储，实际上所请求的是 0.4 字节。\n如果有人这样设定资源请求或限制，可能他的实际想法是申请 400Mi 字节（`400Mi`）\n或者 400M 字节。"}
{"en": "In the following example, the Pod has two containers. Each container has a request of\n2GiB of local ephemeral storage. Each container has a limit of 4GiB of local ephemeral\nstorage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and\na limit of 8GiB of local ephemeral storage. 500Mi of that limit could be\nconsumed by the `emptyDir` volume.", "zh": "在下面的例子中，Pod 包含两个容器。每个容器请求 2 GiB 大小的本地临时性存储。\n每个容器都设置了 4 GiB 作为其本地临时性存储的限制。\n因此，整个 Pod 的本地临时性存储请求是 4 GiB，且其本地临时性存储的限制为 8 GiB。\n该限制值中有 500Mi 可供 `emptyDir` 卷使用。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: frontend\nspec:\n  containers:\n  - name: app\n    image: images.my-company.example/app:v4\n    resources:\n      requests:\n        ephemeral-storage: \"2Gi\"\n      limits:\n        ephemeral-storage: \"4Gi\"\n    volumeMounts:\n    - name: ephemeral\n      mountPath: \"/tmp\"\n  - name: log-aggregator\n    image: images.my-company.example/log-aggregator:v6\n    resources:\n      requests:\n        ephemeral-storage: \"2Gi\"\n      limits:\n        ephemeral-storage: \"4Gi\"\n    volumeMounts:\n    - name: ephemeral\n      mountPath: \"/tmp\"\n  volumes:\n    - name: ephemeral\n      emptyDir:\n        sizeLimit: 500Mi\n```"}
{"en": "### How Pods with ephemeral-storage requests are scheduled\n\nWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to\nrun on. Each node has a maximum amount of local ephemeral storage it can provide for Pods.\nFor more information, see\n[Node Allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable).\n\nThe scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node.", "zh": "### 带临时性存储的 Pods 的调度行为  {#how-pods-with-ephemeral-storage-requests-are-scheduled}\n\n当你创建一个 Pod 时，Kubernetes 调度器会为 Pod 选择一个节点来运行之。\n每个节点都有一个本地临时性存储的上限，是其可提供给 Pod 使用的总量。\n欲了解更多信息，\n可参考[节点可分配资源](/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)节。\n\n调度器会确保所调度的容器的资源请求总和不会超出节点的资源容量。"}
{"en": "### Ephemeral storage consumption management {#resource-emphemeralstorage-consumption}\n\nIf the kubelet is managing local ephemeral storage as a resource, then the\nkubelet measures storage use in:\n\n- `emptyDir` volumes, except _tmpfs_ `emptyDir` volumes\n- directories holding node-level logs\n- writeable container layers\n\nIf a Pod is using more ephemeral storage than you allow it to, the kubelet\nsets an eviction signal that triggers Pod eviction.", "zh": "### 临时性存储消耗的管理 {#resource-emphemeralstorage-consumption}\n\n如果 kubelet 将本地临时性存储作为资源来管理，则 kubelet 会度量以下各处的存储用量：\n\n- `emptyDir` 卷，除了 **tmpfs** `emptyDir` 卷\n- 保存节点层面日志的目录\n- 可写入的容器镜像层\n\n如果某 Pod 的临时存储用量超出了你所允许的范围，kubelet\n会向其发出逐出（eviction）信号，触发该 Pod 被逐出所在节点。"}
{"en": "For container-level isolation, if a container's writable layer and log\nusage exceeds its storage limit, the kubelet marks the Pod for eviction.\n\nFor pod-level isolation the kubelet works out an overall Pod storage limit by\nsumming the limits for the containers in that Pod. In this case, if the sum of\nthe local ephemeral storage usage from all containers and also the Pod's `emptyDir`\nvolumes exceeds the overall Pod storage limit, then the kubelet also marks the Pod\nfor eviction.", "zh": "就容器层面的隔离而言，如果某容器的可写入镜像层和日志用量超出其存储限制，\nkubelet 也会将所在的 Pod 标记为逐出候选。\n\n就 Pod 层面的隔离而言，kubelet 会将 Pod 中所有容器的限制相加，得到 Pod\n存储限制的总值。如果所有容器的本地临时性存储用量总和加上 Pod 的 `emptyDir`\n卷的用量超出 Pod 存储限制，kubelet 也会将该 Pod 标记为逐出候选。\n\n{{< caution >}}"}
{"en": "If the kubelet is not measuring local ephemeral storage, then a Pod\nthat exceeds its local storage limit will not be evicted for breaching\nlocal storage resource limits.\n\nHowever, if the filesystem space for writeable container layers, node-level logs,\nor `emptyDir` volumes falls low, the node\n{{< glossary_tooltip text=\"taints\" term_id=\"taint\" >}} itself as short on local storage\nand this taint triggers eviction for any Pods that don't specifically tolerate the taint.\n\nSee the supported [configurations](#configurations-for-local-ephemeral-storage)\nfor ephemeral local storage.", "zh": "如果 kubelet 没有度量本地临时性存储的用量，即使 Pod\n的本地存储用量超出其限制也不会被逐出。\n\n不过，如果用于可写入容器镜像层、节点层面日志或者 `emptyDir` 卷的文件系统中可用空间太少，\n节点会为自身设置本地存储不足的{{< glossary_tooltip text=\"污点\" term_id=\"taint\" >}}标签。\n这一污点会触发对那些无法容忍该污点的 Pod 的逐出操作。\n\n关于临时性本地存储的配置信息，请参考[这里](#configurations-for-local-ephemeral-storage)\n{{< /caution >}}"}
{"en": "The kubelet supports different ways to measure Pod storage use:", "zh": "kubelet 支持使用不同方式来度量 Pod 的存储用量：\n\n{{< tabs name=\"resource-emphemeralstorage-measurement\" >}}\n{{% tab name=\"周期性扫描\" %}}"}
{"en": "The kubelet performs regular, scheduled checks that scan each\n`emptyDir` volume, container log directory, and writeable container layer.\n\nThe scan measures how much space is used.", "zh": "kubelet 按预定周期执行扫描操作，检查 `emptyDir` 卷、容器日志目录以及可写入容器镜像层。\n\n这一扫描会度量存储空间用量。\n\n{{< note >}}"}
{"en": "In this mode, the kubelet does not track open file descriptors\nfor deleted files.\n\nIf you (or a container) create a file inside an `emptyDir` volume,\nsomething then opens that file, and you delete the file while it is\nstill open, then the inode for the deleted file stays until you close\nthat file but the kubelet does not categorize the space as in use.", "zh": "在这种模式下，kubelet 并不检查已删除文件所对应的、仍处于打开状态的文件描述符。\n\n如果你（或者容器）在 `emptyDir` 卷中创建了一个文件，\n写入一些内容之后再次打开该文件并执行了删除操作，所删除文件对应的 inode 仍然存在，\n直到你关闭该文件为止。kubelet 不会将该文件所占用的空间视为已使用空间。\n{{< /note >}}\n\n{{% /tab %}}\n\n{{% tab name=\"文件系统项目配额\" %}}\n\n{{< feature-state feature_gate_name=\"LocalStorageCapacityIsolationFSQuotaMonitoring\" >}}"}
{"en": "Project quotas are an operating-system level feature for managing\nstorage use on filesystems. With Kubernetes, you can enable project\nquotas for monitoring storage use. Make sure that the filesystem\nbacking the `emptyDir` volumes, on the node, provides project quota support.\nFor example, XFS and ext4fs offer project quotas.", "zh": "项目配额（Project Quota）是一个操作系统层的功能特性，用来管理文件系统中的存储用量。\n在 Kubernetes 中，你可以启用项目配额以监视存储用量。\n你需要确保节点上为 `emptyDir` 提供存储的文件系统支持项目配额。\n例如，XFS 和 ext4fs 文件系统都支持项目配额。\n\n{{< note >}}"}
{"en": "Project quotas let you monitor storage use; they do not enforce limits.", "zh": "项目配额可以帮你监视存储用量，但无法强制执行限制。\n{{< /note >}}"}
{"en": "Kubernetes uses project IDs starting from `1048576`. The IDs in use are\nregistered in `/etc/projects` and `/etc/projid`. If project IDs in\nthis range are used for other purposes on the system, those project\nIDs must be registered in `/etc/projects` and `/etc/projid` so that\nKubernetes does not use them.\n\nQuotas are faster and more accurate than directory scanning. When a\ndirectory is assigned to a project, all files created under a\ndirectory are created in that project, and the kernel merely has to\nkeep track of how many blocks are in use by files in that project.\nIf a file is created and deleted, but has an open file descriptor,\nit continues to consume space. Quota tracking records that space accurately\nwhereas directory scans overlook the storage used by deleted files.", "zh": "Kubernetes 所使用的项目 ID 始于 `1048576`。\n所使用的 IDs 会注册在 `/etc/projects` 和 `/etc/projid` 文件中。\n如果该范围中的项目 ID 已经在系统中被用于其他目的，则已占用的项目 ID\n也必须注册到 `/etc/projects` 和 `/etc/projid` 中，这样 Kubernetes\n才不会使用它们。\n\n配额方式与目录扫描方式相比速度更快，结果更精确。当某个目录被分配给某个项目时，\n该目录下所创建的所有文件都属于该项目，内核只需要跟踪该项目中的文件所使用的存储块个数。\n如果某文件被创建后又被删除，但对应文件描述符仍处于打开状态，\n该文件会继续耗用存储空间。配额跟踪技术能够精确第记录对应存储空间的状态，\n而目录扫描方式会忽略被删除文件所占用的空间。"}
{"en": "To use quotas to track a pod's resource usage, the pod must be in \na user namespace. Within user namespaces, the kernel restricts changes \nto projectIDs on the filesystem, ensuring the reliability of storage \nmetrics calculated by quotas.", "zh": "要使用配额来跟踪 Pod 的资源使用情况，Pod 必须位于用户命名空间中。\n在用户命名空间内，内核限制对文件系统上 projectID 的更改，从而确保按配额计算的存储指标的可靠性。"}
{"en": "If you want to use project quotas, you should:\n\n* Enable the `LocalStorageCapacityIsolationFSQuotaMonitoring=true`\n  [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n  using the `featureGates` field in the\n  [kubelet configuration](/docs/reference/config-api/kubelet-config.v1beta1/).", "zh": "如果你希望使用项目配额，你需要：\n\n* 在 [kubelet 配置](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/)中使用\n  `featureGates` 字段启用\n  `LocalStorageCapacityIsolationFSQuotaMonitoring=true` [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)。"}
{"en": "* Ensure the `UserNamespacesSupport` \n  [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n  is enabled, and that the kernel, CRI implementation and OCI runtime support user namespaces.", "zh": "* 确保 `UserNamespacesSupport` [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)已启用，\n  并且内核、CRI 实现和 OCI 运行时支持用户命名空间。"}
{"en": "* Ensure that the root filesystem (or optional runtime filesystem)\n  has project quotas enabled. All XFS filesystems support project quotas.\n  For ext4 filesystems, you need to enable the project quota tracking feature\n  while the filesystem is not mounted.\n\n  ```bash\n  # For ext4, with /dev/block-device not mounted\n  sudo tune2fs -O project -Q prjquota /dev/block-device\n  ```", "zh": "* 确保根文件系统（或者可选的运行时文件系统）启用了项目配额。所有 XFS\n  文件系统都支持项目配额。\n  对 extf 文件系统而言，你需要在文件系统尚未被挂载时启用项目配额跟踪特性：\n\n  ```bash\n  # 对 ext4 而言，在 /dev/block-device 尚未被挂载时执行下面操作\n  sudo tune2fs -O project -Q prjquota /dev/block-device\n  ```"}
{"en": "* Ensure that the root filesystem (or optional runtime filesystem) is\n  mounted with project quotas enabled. For both XFS and ext4fs, the\n  mount option is named `prjquota`.", "zh": "* 确保根文件系统（或者可选的运行时文件系统）在挂载时项目配额特性是被启用了的。\n  对于 XFS 和 ext4fs 而言，对应的挂载选项称作 `prjquota`。"}
{"en": "If you don't want to use project quotas, you should:\n\n* Disable the `LocalStorageCapacityIsolationFSQuotaMonitoring`\n  [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n  using the `featureGates` field in the\n  [kubelet configuration](/docs/reference/config-api/kubelet-config.v1beta1/).", "zh": "如果不想使用项目配额，你应该：\n\n* 使用 [kubelet 配置](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/)中的\n  `featureGates` 字段禁用 `LocalStorageCapacityIsolationFSQuotaMonitoring`\n  [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)。\n\n{{% /tab %}}\n{{< /tabs >}}"}
{"en": "## Extended resources\n\nExtended resources are fully-qualified resource names outside the\n`kubernetes.io` domain. They allow cluster operators to advertise and users to\nconsume the non-Kubernetes-built-in resources.\n\nThere are two steps required to use Extended Resources. First, the cluster\noperator must advertise an Extended Resource. Second, users must request the\nExtended Resource in Pods.", "zh": "## 扩展资源（Extended Resources）   {#extended-resources}\n\n扩展资源是 `kubernetes.io` 域名之外的标准资源名称。\n它们使得集群管理员能够颁布非 Kubernetes 内置资源，而用户可以使用他们。\n\n使用扩展资源需要两个步骤。首先，集群管理员必须颁布扩展资源。\n其次，用户必须在 Pod 中请求扩展资源。"}
{"en": "### Managing extended resources\n\n#### Node-level extended resources\n\nNode-level extended resources are tied to nodes.\n\n##### Device plugin managed resources\n\nSee [Device\nPlugin](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)\nfor how to advertise device plugin managed resources on each node.", "zh": "### 管理扩展资源   {#managing-extended-resources}\n\n#### 节点级扩展资源     {#node-level-extended-resources}\n\n节点级扩展资源绑定到节点。\n\n##### 设备插件管理的资源   {#device-plugin-managed-resources}\n\n有关如何颁布在各节点上由设备插件所管理的资源，\n请参阅[设备插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)。"}
{"en": "##### Other resources\n\nTo advertise a new node-level extended resource, the cluster operator can\nsubmit a `PATCH` HTTP request to the API server to specify the available\nquantity in the `status.capacity` for a node in the cluster. After this\noperation, the node's `status.capacity` will include a new resource. The\n`status.allocatable` field is updated automatically with the new resource\nasynchronously by the kubelet.", "zh": "##### 其他资源   {#other-resources}\n\n为了颁布新的节点级扩展资源，集群操作员可以向 API 服务器提交 `PATCH` HTTP 请求，\n以在集群中节点的 `status.capacity` 中为其配置可用数量。\n完成此操作后，节点的 `status.capacity` 字段中将包含新资源。\nkubelet 会异步地对 `status.allocatable` 字段执行自动更新操作，使之包含新资源。"}
{"en": "Because the scheduler uses the node's `status.allocatable` value when\nevaluating Pod fitness, the scheduler only takes account of the new value after\nthat asynchronous update. There may be a short delay between patching the\nnode capacity with a new resource and the time when the first Pod that requests\nthe resource can be scheduled on that node.", "zh": "由于调度器在评估 Pod 是否适合在某节点上执行时会使用节点的 `status.allocatable` 值，\n调度器只会考虑异步更新之后的新值。\n在更新节点容量使之包含新资源之后和请求该资源的第一个 Pod 被调度到该节点之间，\n可能会有短暂的延迟。"}
{"en": "**Example:**\n\nHere is an example showing how to use `curl` to form an HTTP request that\nadvertises five \"example.com/foo\" resources on node `k8s-node-1` whose master\nis `k8s-master`.", "zh": "**示例：**\n\n这是一个示例，显示了如何使用 `curl` 构造 HTTP 请求，公告主节点为 `k8s-master`\n的节点 `k8s-node-1` 上存在五个 `example.com/foo` 资源。\n\n```shell\ncurl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/capacity/example.com~1foo\", \"value\": \"5\"}]' \\\nhttp://k8s-master:8080/api/v1/nodes/k8s-node-1/status\n```\n\n{{< note >}}"}
{"en": "In the preceding request, `~1` is the encoding for the character `/`\nin the patch path. The operation path value in JSON-Patch is interpreted as a\nJSON-Pointer. For more details, see\n[IETF RFC 6901, section 3](https://tools.ietf.org/html/rfc6901#section-3).", "zh": "在前面的请求中，`~1` 是在 patch 路径中对字符 `/` 的编码。\nJSON-Patch 中的操作路径的值被视为 JSON-Pointer 类型。\n有关更多详细信息，请参见\n[IETF RFC 6901 第 3 节](https://tools.ietf.org/html/rfc6901#section-3)。\n{{< /note >}}"}
{"en": "#### Cluster-level extended resources\n\nCluster-level extended resources are not tied to nodes. They are usually managed\nby scheduler extenders, which handle the resource consumption and resource quota.\n\nYou can specify the extended resources that are handled by scheduler extenders\nin [scheduler configuration](/docs/reference/config-api/kube-scheduler-config.v1/)", "zh": "#### 集群层面的扩展资源   {#cluster-level-extended-resources}\n\n集群层面的扩展资源并不绑定到具体节点。\n它们通常由调度器扩展程序（Scheduler Extenders）管理，这些程序处理资源消耗和资源配额。\n\n你可以在[调度器配置](/zh-cn/docs/reference/config-api/kube-scheduler-config.v1/)\n中指定由调度器扩展程序处理的扩展资源。"}
{"en": "**Example:**\n\nThe following configuration for a scheduler policy indicates that the\ncluster-level extended resource \"example.com/foo\" is handled by the scheduler\nextender.\n\n- The scheduler sends a Pod to the scheduler extender only if the Pod requests\n     \"example.com/foo\".\n- The `ignoredByScheduler` field specifies that the scheduler does not check\n     the \"example.com/foo\" resource in its `PodFitsResources` predicate.", "zh": "**示例：**\n\n下面的调度器策略配置标明集群层扩展资源 \"example.com/foo\" 由调度器扩展程序处理。\n\n- 仅当 Pod 请求 \"example.com/foo\" 时，调度器才会将 Pod 发送到调度器扩展程序。\n- `ignoredByScheduler` 字段指定调度器不要在其 `PodFitsResources` 断言中检查\n  \"example.com/foo\" 资源。\n\n```json\n{\n  \"kind\": \"Policy\",\n  \"apiVersion\": \"v1\",\n  \"extenders\": [\n    {\n      \"urlPrefix\": \"<extender-endpoint>\",\n      \"bindVerb\": \"bind\",\n      \"managedResources\": [\n        {\n          \"name\": \"example.com/foo\",\n          \"ignoredByScheduler\": true\n        }\n      ]\n    }\n  ]\n}\n```"}
{"en": "### Consuming extended resources\n\nUsers can consume extended resources in Pod specs like CPU and memory.\nThe scheduler takes care of the resource accounting so that no more than the\navailable amount is simultaneously allocated to Pods.", "zh": "### 使用扩展资源  {#consuming-extended-resources}\n\n就像 CPU 和内存一样，用户可以在 Pod 的规约中使用扩展资源。\n调度器负责资源的核算，确保同时分配给 Pod 的资源总量不会超过可用数量。"}
{"en": "The API server restricts quantities of extended resources to whole numbers.\nExamples of _valid_ quantities are `3`, `3000m` and `3Ki`. Examples of\n_invalid_ quantities are `0.5` and `1500m` (because `1500m` would result in `1.5`).", "zh": "API 服务器将扩展资源的数量限制为整数。\n**有效** 数量的示例是 `3`、`3000m` 和 `3Ki`。\n**无效** 数量的示例是 `0.5` 和 `1500m`（因为 `1500m` 结果等同于 `1.5`）。\n\n{{< note >}}"}
{"en": "Extended resources replace Opaque Integer Resources.\nUsers can use any domain name prefix other than `kubernetes.io` which is reserved.", "zh": "扩展资源取代了非透明整数资源（Opaque Integer Resources，OIR）。\n用户可以使用 `kubernetes.io`（保留）以外的任何域名前缀。\n{{< /note >}}"}
{"en": "To consume an extended resource in a Pod, include the resource name as a key\nin the `spec.containers[].resources.limits` map in the container spec.", "zh": "要在 Pod 中使用扩展资源，请在容器规约的 `spec.containers[].resources.limits`\n映射中包含资源名称作为键。\n\n{{< note >}}"}
{"en": "Extended resources cannot be overcommitted, so request and limit\nmust be equal if both are present in a container spec.", "zh": "扩展资源不能过量使用，因此如果容器规约中同时存在请求和限制，则它们的取值必须相同。\n{{< /note >}}"}
{"en": "A Pod is scheduled only if all of the resource requests are satisfied, including\nCPU, memory and any extended resources. The Pod remains in the `PENDING` state\nas long as the resource request cannot be satisfied.\n\n**Example:**\n\nThe Pod below requests 2 CPUs and 1 \"example.com/foo\" (an extended resource).", "zh": "仅当所有资源请求（包括 CPU、内存和任何扩展资源）都被满足时，Pod 才能被调度。\n在资源请求无法满足时，Pod 会保持在 `PENDING` 状态。\n\n**示例：**\n\n下面的 Pod 请求 2 个 CPU 和 1 个 \"example.com/foo\"（扩展资源）。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-container\n    image: myimage\n    resources:\n      requests:\n        cpu: 2\n        example.com/foo: 1\n      limits:\n        example.com/foo: 1\n```"}
{"en": "## PID limiting\n\nProcess ID (PID) limits allow for the configuration of a kubelet\nto limit the number of PIDs that a given Pod can consume. See\n[PID Limiting](/docs/concepts/policy/pid-limiting/) for information.", "zh": "## PID 限制   {#pid-limiting}\n\n进程 ID（PID）限制允许对 kubelet 进行配置，以限制给定 Pod 可以消耗的 PID 数量。\n有关信息，请参见 [PID 限制](/zh-cn/docs/concepts/policy/pid-limiting/)。"}
{"en": "## Troubleshooting\n\n### My Pods are pending with event message `FailedScheduling`\n\nIf the scheduler cannot find any node where a Pod can fit, the Pod remains\nunscheduled until a place can be found. An\n[Event](/docs/reference/kubernetes-api/cluster-resources/event-v1/) is produced\neach time the scheduler fails to find a place for the Pod. You can use `kubectl`\nto view the events for a Pod; for example:", "zh": "## 疑难解答  {#troubleshooting}\n\n### 我的 Pod 处于悬决状态且事件信息显示 `FailedScheduling`  {#my-pods-are-pending-with-event-message-failedscheduling}\n\n如果调度器找不到该 Pod 可以匹配的任何节点，则该 Pod 将保持未被调度状态，\n直到找到一个可以被调度到的位置。每当调度器找不到 Pod 可以调度的地方时，\n会产生一个 [Event](/zh-cn/docs/reference/kubernetes-api/cluster-resources/event-v1/)。\n你可以使用 `kubectl` 来查看 Pod 的事件；例如：\n\n```shell\nkubectl describe pod frontend | grep -A 9999999999 Events\n```\n\n```\nEvents:\n  Type     Reason            Age   From               Message\n  ----     ------            ----  ----               -------\n  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpu\n```"}
{"en": "In the preceding example, the Pod named \"frontend\" fails to be scheduled due to\ninsufficient CPU resource on any node. Similar error messages can also suggest\nfailure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod\nis pending with a message of this type, there are several things to try:\n\n- Add more nodes to the cluster.\n- Terminate unneeded Pods to make room for pending Pods.\n- Check that the Pod is not larger than all the nodes. For example, if all the\n  nodes have a capacity of `cpu: 1`, then a Pod with a request of `cpu: 1.1` will\n  never be scheduled.\n- Check for node taints. If most of your nodes are tainted, and the new Pod does\n  not tolerate that taint, the scheduler only considers placements onto the\n  remaining nodes that don't have that taint.\n\nYou can check node capacities and amounts allocated with the\n`kubectl describe nodes` command. For example:", "zh": "在上述示例中，由于节点上的 CPU 资源不足，名为 “frontend” 的 Pod 无法被调度。\n由于内存不足（PodExceedsFreeMemory）而导致失败时，也有类似的错误消息。\n一般来说，如果 Pod 处于悬决状态且有这种类型的消息时，你可以尝试如下几件事情：\n\n- 向集群添加更多节点。\n- 终止不需要的 Pod，为悬决的 Pod 腾出空间。\n- 检查 Pod 所需的资源是否超出所有节点的资源容量。例如，如果所有节点的容量都是 `cpu：1`，\n  那么一个请求为 `cpu: 1.1` 的 Pod 永远不会被调度。\n- 检查节点上的污点设置。如果集群中节点上存在污点，而新的 Pod 不能容忍污点，\n  调度器只会考虑将 Pod 调度到不带有该污点的节点上。\n\n你可以使用 `kubectl describe nodes` 命令检查节点容量和已分配的资源数量。例如：\n\n```shell\nkubectl describe nodes e2e-test-node-pool-4lw4\n```\n\n```\nName:            e2e-test-node-pool-4lw4\n[ ... 这里忽略了若干行以便阅读 ...]\nCapacity:\n cpu:                               2\n memory:                            7679792Ki\n pods:                              110\nAllocatable:\n cpu:                               1800m\n memory:                            7474992Ki\n pods:                              110\n[ ... 这里忽略了若干行以便阅读 ...]\nNon-terminated Pods:        (5 in total)\n  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------    ----                                  ------------  ----------  ---------------  -------------\n  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)\n  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)\n  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)\n  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)\n  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  CPU Requests    CPU Limits    Memory Requests    Memory Limits\n  ------------    ----------    ---------------    -------------\n  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)\n```"}
{"en": "In the preceding output, you can see that if a Pod requests more than 1.120 CPUs\nor more than 6.23Gi of memory, that Pod will not fit on the node.\n\nBy looking at the “Pods” section, you can see which Pods are taking up space on\nthe node.", "zh": "在上面的输出中，你可以看到如果 Pod 请求超过 1.120 CPU 或者 6.23Gi 内存，节点将无法满足。\n\n通过查看 \"Pods\" 部分，你将看到哪些 Pod 占用了节点上的资源。"}
{"en": "The amount of resources available to Pods is less than the node capacity because\nsystem daemons use a portion of the available resources. Within the Kubernetes API,\neach Node has a `.status.allocatable` field\n(see [NodeStatus](/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeStatus)\nfor details).", "zh": "Pods 可用的资源量低于节点的资源总量，因为系统守护进程也会使用一部分可用资源。\n在 Kubernetes API 中，每个 Node 都有一个 `.status.allocatable` 字段\n（详情参见 [NodeStatus](/zh-cn/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeStatus)）。"}
{"en": "The `.status.allocatable` field describes the amount of resources that are available\nto Pods on that node (for example: 15 virtual CPUs and 7538 MiB of memory).\nFor more information on node allocatable resources in Kubernetes, see\n[Reserve Compute Resources for System Daemons](/docs/tasks/administer-cluster/reserve-compute-resources/).", "zh": "字段 `.status.allocatable` 描述节点上可以用于 Pod 的资源总量（例如：15 个虚拟\nCPU、7538 MiB 内存）。关于 Kubernetes 中节点可分配资源的信息，\n可参阅[为系统守护进程预留计算资源](/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/)。"}
{"en": "You can configure [resource quotas](/docs/concepts/policy/resource-quotas/)\nto limit the total amount of resources that a namespace can consume.\nKubernetes enforces quotas for objects in particular namespace when there is a\nResourceQuota in that namespace.\nFor example, if you assign specific namespaces to different teams, you\ncan add ResourceQuotas into those namespaces. Setting resource quotas helps to\nprevent one team from using so much of any resource that this over-use affects other teams.\n\nYou should also consider what access you grant to that namespace:\n**full** write access to a namespace allows someone with that access to remove any\nresource, including a configured ResourceQuota.", "zh": "你可以配置[资源配额](/zh-cn/docs/concepts/policy/resource-quotas/)功能特性以限制每个名字空间可以使用的资源总量。\n当某名字空间中存在 ResourceQuota 时，Kubernetes 会在该名字空间中的对象强制实施配额。\n例如，如果你为不同的团队分配名字空间，你可以为这些名字空间添加 ResourceQuota。\n设置资源配额有助于防止一个团队占用太多资源，以至于这种占用会影响其他团队。\n\n你还需要考虑为这些名字空间设置授权访问：\n为名字空间提供**全部**的写权限时，具有合适权限的人可能删除所有资源，\n包括所配置的 ResourceQuota。"}
{"en": "### My container is terminated\n\nYour container might get terminated because it is resource-starved. To check\nwhether a container is being killed because it is hitting a resource limit, call\n`kubectl describe pod` on the Pod of interest:", "zh": "### 我的容器被终止了  {#my-container-is-terminated}\n\n你的容器可能因为资源紧张而被终止。要查看容器是否因为遇到资源限制而被杀死，\n请针对相关的 Pod 执行 `kubectl describe pod`：\n\n```shell\nkubectl describe pod simmemleak-hra99\n```"}
{"en": "The output is similar to:", "zh": "输出类似于：\n\n```\nName:                           simmemleak-hra99\nNamespace:                      default\nImage(s):                       saadali/simmemleak\nNode:                           kubernetes-node-tf0f/10.240.216.66\nLabels:                         name=simmemleak\nStatus:                         Running\nReason:\nMessage:\nIP:                             10.244.2.75\nContainers:\n  simmemleak:\n    Image:  saadali/simmemleak:latest\n    Limits:\n      cpu:          100m\n      memory:       50Mi\n    State:          Running\n      Started:      Tue, 07 Jul 2019 12:54:41 -0700\n    Last State:     Terminated\n      Reason:       OOMKilled\n      Exit Code:    137\n      Started:      Fri, 07 Jul 2019 12:54:30 -0700\n      Finished:     Fri, 07 Jul 2019 12:54:33 -0700\n    Ready:          False\n    Restart Count:  5\nConditions:\n  Type      Status\n  Ready     False\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f\n  Normal  Pulled     41s   kubelet            Container image \"saadali/simmemleak:latest\" already present on machine\n  Normal  Created    41s   kubelet            Created container simmemleak\n  Normal  Started    40s   kubelet            Started container simmemleak\n  Normal  Killing    32s   kubelet            Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod\n```"}
{"en": "In the preceding example, the `Restart Count:  5` indicates that the `simmemleak`\ncontainer in the Pod was terminated and restarted five times (so far).\nThe `OOMKilled` reason shows that the container tried to use more memory than its limit.", "zh": "在上面的例子中，`Restart Count: 5` 意味着 Pod 中的 `simmemleak`\n容器被终止并且（到目前为止）重启了五次。\n原因 `OOMKilled` 显示容器尝试使用超出其限制的内存量。"}
{"en": "Your next step might be to check the application code for a memory leak. If you\nfind that the application is behaving how you expect, consider setting a higher\nmemory limit (and possibly request) for that container.", "zh": "你接下来要做的或许是检查应用代码，看看是否存在内存泄露。\n如果你发现应用的行为与你所预期的相同，则可以考虑为该容器设置一个更高的内存限制\n（也可能需要设置请求值）。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Get hands-on experience [assigning Memory resources to containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/).\n* Get hands-on experience [assigning CPU resources to containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/).\n* Read how the API reference defines a [container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container)\n  and its [resource requirements](/docs/reference/kubernetes-api/workload-resources/pod-v1/#resources)\n* Read about [project quotas](https://www.linux.org/docs/man8/xfs_quota.html) in XFS\n* Read more about the [kube-scheduler configuration reference (v1)](/docs/reference/config-api/kube-scheduler-config.v1/)\n* Read more about [Quality of Service classes for Pods](/docs/concepts/workloads/pods/pod-qos/)", "zh": "* 获取[分配内存资源给容器和 Pod](/zh-cn/docs/tasks/configure-pod-container/assign-memory-resource/) 的实践经验\n* 获取[分配 CPU 资源给容器和 Pod](/zh-cn/docs/tasks/configure-pod-container/assign-cpu-resource/) 的实践经验\n* 阅读 API 参考如何定义[容器](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container)\n  及其[资源请求](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#resources)。\n* 阅读 XFS 中[项目配额](https://www.linux.org/docs/man8/xfs_quota.html)的文档\n* 进一步阅读 [kube-scheduler 配置参考（v1）](/zh-cn/docs/reference/config-api/kube-scheduler-config.v1/)\n* 进一步阅读 [Pod 的服务质量等级](/zh-cn/docs/concepts/workloads/pods/pod-qos/)"}
{"en": "Use kubeconfig files to organize information about clusters, users, namespaces, and\nauthentication mechanisms. The `kubectl` command-line tool uses kubeconfig files to\nfind the information it needs to choose a cluster and communicate with the API server\nof a cluster.", "zh": "使用 kubeconfig 文件来组织有关集群、用户、命名空间和身份认证机制的信息。\n`kubectl` 命令行工具使用 kubeconfig 文件来查找选择集群所需的信息，并与集群的 API 服务器进行通信。\n\n{{< note >}}"}
{"en": "A file that is used to configure access to clusters is called\na *kubeconfig file*. This is a generic way of referring to configuration files.\nIt does not mean that there is a file named `kubeconfig`.", "zh": "用于配置集群访问的文件称为 **kubeconfig 文件**。\n这是引用到配置文件的通用方法，并不意味着有一个名为 `kubeconfig` 的文件。\n{{< /note >}}\n\n{{< warning >}}"}
{"en": "Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig file could result in malicious code execution or file exposure.\nIf you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell script.", "zh": "请务必仅使用来源可靠的 kubeconfig 文件。使用特制的 kubeconfig 文件可能会导致恶意代码执行或文件暴露。\n如果必须使用不受信任的 kubeconfig 文件，请首先像检查 Shell 脚本一样仔细检查此文件。\n{{< /warning>}}"}
{"en": "By default, `kubectl` looks for a file named `config` in the `$HOME/.kube` directory.\nYou can specify other kubeconfig files by setting the `KUBECONFIG` environment\nvariable or by setting the\n[`--kubeconfig`](/docs/reference/generated/kubectl/kubectl/) flag.", "zh": "默认情况下，`kubectl` 在 `$HOME/.kube` 目录下查找名为 `config` 的文件。\n你可以通过设置 `KUBECONFIG` 环境变量或者设置\n[`--kubeconfig`](/docs/reference/generated/kubectl/kubectl/)参数来指定其他 kubeconfig 文件。"}
{"en": "For step-by-step instructions on creating and specifying kubeconfig files, see\n[Configure Access to Multiple Clusters](/docs/tasks/access-application-cluster/configure-access-multiple-clusters).", "zh": "有关创建和指定 kubeconfig 文件的分步说明，\n请参阅[配置对多集群的访问](/zh-cn/docs/tasks/access-application-cluster/configure-access-multiple-clusters)。"}
{"en": "## Supporting multiple clusters, users, and authentication mechanisms", "zh": "## 支持多集群、用户和身份认证机制   {#support-clusters-users-and-authn}"}
{"en": "Suppose you have several clusters, and your users and components authenticate\nin a variety of ways. For example:", "zh": "假设你有多个集群，并且你的用户和组件以多种方式进行身份认证。比如："}
{"en": "- A running kubelet might authenticate using certificates.\n- A user might authenticate using tokens.\n- Administrators might have sets of certificates that they provide to individual users.", "zh": "- 正在运行的 kubelet 可能使用证书在进行认证。\n- 用户可能通过令牌进行认证。\n- 管理员可能拥有多个证书集合提供给各用户。"}
{"en": "With kubeconfig files, you can organize your clusters, users, and namespaces.\nYou can also define contexts to quickly and easily switch between\nclusters and namespaces.", "zh": "使用 kubeconfig 文件，你可以组织集群、用户和命名空间。你还可以定义上下文，以便在集群和命名空间之间快速轻松地切换。"}
{"en": "## Context", "zh": "## 上下文（Context）   {#context}"}
{"en": "A *context* element in a kubeconfig file is used to group access parameters\nunder a convenient name. Each context has three parameters: cluster, namespace, and user.\nBy default, the `kubectl` command-line tool uses parameters from\nthe *current context* to communicate with the cluster.", "zh": "通过 kubeconfig 文件中的 *context* 元素，使用简便的名称来对访问参数进行分组。\n每个 context 都有三个参数：cluster、namespace 和 user。\n默认情况下，`kubectl` 命令行工具使用 **当前上下文** 中的参数与集群进行通信。"}
{"en": "To choose the current context:", "zh": "选择当前上下文：\n\n```shell\nkubectl config use-context\n```"}
{"en": "## The KUBECONFIG environment variable", "zh": "## KUBECONFIG 环境变量   {#kubeconfig-env-var}"}
{"en": "The `KUBECONFIG` environment variable holds a list of kubeconfig files.\nFor Linux and Mac, the list is colon-delimited. For Windows, the list\nis semicolon-delimited. The `KUBECONFIG` environment variable is not\nrequired. If the `KUBECONFIG` environment variable doesn't exist,\n`kubectl` uses the default kubeconfig file, `$HOME/.kube/config`.", "zh": "`KUBECONFIG` 环境变量包含一个 kubeconfig 文件列表。\n对于 Linux 和 Mac，此列表以英文冒号分隔。对于 Windows，此列表以英文分号分隔。\n`KUBECONFIG` 环境变量不是必需的。\n如果 `KUBECONFIG` 环境变量不存在，`kubectl` 将使用默认的 kubeconfig 文件：`$HOME/.kube/config`。"}
{"en": "If the `KUBECONFIG` environment variable does exist, `kubectl` uses\nan effective configuration that is the result of merging the files\nlisted in the `KUBECONFIG` environment variable.", "zh": "如果 `KUBECONFIG` 环境变量存在，`kubectl` 将使用 `KUBECONFIG` 环境变量中列举的文件合并后的有效配置。"}
{"en": "## Merging kubeconfig files", "zh": "## 合并 kubeconfig 文件   {#merge-kubeconfig-files}"}
{"en": "To see your configuration, enter this command:", "zh": "要查看配置，输入以下命令：\n\n```shell\nkubectl config view\n```"}
{"en": "As described previously, the output might be from a single kubeconfig file,\nor it might be the result of merging several kubeconfig files.", "zh": "如前所述，输出可能来自单个 kubeconfig 文件，也可能是合并多个 kubeconfig 文件的结果。"}
{"en": "Here are the rules that `kubectl` uses when it merges kubeconfig files:", "zh": "以下是 `kubectl` 在合并 kubeconfig 文件时使用的规则。"}
{"en": "1. If the `--kubeconfig` flag is set, use only the specified file. Do not merge.\n   Only one instance of this flag is allowed.\n\n   Otherwise, if the `KUBECONFIG` environment variable is set, use it as a\n   list of files that should be merged.\n   Merge the files listed in the `KUBECONFIG` environment variable\n   according to these rules:\n\n   * Ignore empty filenames.\n   * Produce errors for files with content that cannot be deserialized.\n   * The first file to set a particular value or map key wins.\n   * Never change the value or map key.\n     Example: Preserve the context of the first file to set `current-context`.\n     Example: If two files specify a `red-user`, use only values from the first file's `red-user`.\n     Even if the second file has non-conflicting entries under `red-user`, discard them.", "zh": "1. 如果设置了 `--kubeconfig` 参数，则仅使用指定的文件。不进行合并。此参数只能使用一次。\n\n   否则，如果设置了 `KUBECONFIG` 环境变量，将它用作应合并的文件列表。根据以下规则合并 `KUBECONFIG` 环境变量中列出的文件：\n\n   * 忽略空文件名。\n   * 对于内容无法反序列化的文件，产生错误信息。\n   * 第一个设置特定值或者映射键的文件将生效。\n   * 永远不会更改值或者映射键。示例：保留第一个文件的上下文以设置 `current-context`。\n     示例：如果两个文件都指定了 `red-user`，则仅使用第一个文件的 `red-user` 中的值。\n     即使第二个文件在 `red-user` 下有非冲突条目，也要丢弃它们。"}
{"en": "For an example of setting the `KUBECONFIG` environment variable, see\n   [Setting the KUBECONFIG environment variable](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable).", "zh": "有关设置 `KUBECONFIG` 环境变量的示例，\n   请参阅[设置 KUBECONFIG 环境变量](/zh-cn/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable)。"}
{"en": "Otherwise, use the default kubeconfig file, `$HOME/.kube/config`, with no merging.", "zh": "否则，使用默认的 kubeconfig 文件（`$HOME/.kube/config`），不进行合并。"}
{"en": "1. Determine the context to use based on the first hit in this chain:\n\n    1. Use the `--context` command-line flag if it exists.\n    1. Use the `current-context` from the merged kubeconfig files.", "zh": "2. 根据此链中的第一个匹配确定要使用的上下文。\n\n    1. 如果存在上下文，则使用 `--context` 命令行参数。\n    2. 使用合并的 kubeconfig 文件中的 `current-context`。"}
{"en": "An empty context is allowed at this point.", "zh": "这种场景下允许空上下文。"}
{"en": "1. Determine the cluster and user. At this point, there might or might not be a context.\n   Determine the cluster and user based on the first hit in this chain,\n   which is run twice: once for user and once for cluster:\n\n   1. Use a command-line flag if it exists: `--user` or `--cluster`.\n   1. If the context is non-empty, take the user or cluster from the context.", "zh": "3. 确定集群和用户。此时，可能有也可能没有上下文。根据此链中的第一个匹配确定集群和用户，\n   这将运行两次：一次用于用户，一次用于集群。\n\n   1. 如果存在用户或集群，则使用命令行参数：`--user` 或者 `--cluster`。\n   2. 如果上下文非空，则从上下文中获取用户或集群。"}
{"en": "The user and cluster can be empty at this point.", "zh": "这种场景下用户和集群可以为空。"}
{"en": "1. Determine the actual cluster information to use. At this point, there might or\n   might not be cluster information.\n   Build each piece of the cluster information based on this chain; the first hit wins:\n\n   1. Use command line flags if they exist: `--server`, `--certificate-authority`, `--insecure-skip-tls-verify`.\n   1. If any cluster information attributes exist from the merged kubeconfig files, use them.\n   1. If there is no server location, fail.", "zh": "4. 确定要使用的实际集群信息。此时，可能有也可能没有集群信息。\n   基于此链构建每个集群信息；第一个匹配项会被采用：\n\n   1. 如果存在集群信息，则使用命令行参数：`--server`、`--certificate-authority` 和 `--insecure-skip-tls-verify`。\n   2. 如果合并的 kubeconfig 文件中存在集群信息属性，则使用这些属性。\n   3. 如果没有 server 配置，则配置无效。"}
{"en": "1. Determine the actual user information to use. Build user information using the same\n   rules as cluster information, except allow only one authentication\n   technique per user:\n\n   1. Use command line flags if they exist: `--client-certificate`, `--client-key`, `--username`, `--password`, `--token`.\n   1. Use the `user` fields from the merged kubeconfig files.\n   1. If there are two conflicting techniques, fail.", "zh": "5. 确定要使用的实际用户信息。使用与集群信息相同的规则构建用户信息，但对于每个用户只允许使用一种身份认证技术：\n\n   1. 如果存在用户信息，则使用命令行参数：`--client-certificate`、`--client-key`、`--username`、`--password` 和 `--token`。\n   2. 使用合并的 kubeconfig 文件中的 `user` 字段。\n   3. 如果存在两种冲突技术，则配置无效。"}
{"en": "1. For any information still missing, use default values and potentially\n   prompt for authentication information.", "zh": "6. 对于仍然缺失的任何信息，使用其对应的默认值，并可能提示输入身份认证信息。"}
{"en": "## File references", "zh": "## 文件引用   {#file-reference}"}
{"en": "File and path references in a kubeconfig file are relative to the location of the kubeconfig file.\nFile references on the command line are relative to the current working directory.\nIn `$HOME/.kube/config`, relative paths are stored relatively, and absolute paths\nare stored absolutely.", "zh": "kubeconfig 文件中的文件和路径引用是相对于 kubeconfig 文件的位置。\n命令行上的文件引用是相对于当前工作目录的。\n在 `$HOME/.kube/config` 中，相对路径按相对路径存储，而绝对路径按绝对路径存储。"}
{"en": "## Proxy\n\nYou can configure `kubectl` to use a proxy per cluster using `proxy-url` in your kubeconfig file, like this:", "zh": "## 代理   {#proxy}\n\n你可以在 `kubeconfig` 文件中，为每个集群配置 `proxy-url` 来让 `kubectl` 使用代理，例如：\n\n```yaml\napiVersion: v1\nkind: Config\n\nclusters:\n- cluster:\n    proxy-url: http://proxy.example.org:3128\n    server: https://k8s.example.org/k8s/clusters/c-xxyyzz\n  name: development\n\nusers:\n- name: developer\n\ncontexts:\n- context:\n  name: development\n```\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* [Configure Access to Multiple Clusters](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/)\n* [`kubectl config`](/docs/reference/generated/kubectl/kubectl-commands#config)", "zh": "* [配置对多集群的访问](/zh-cn/docs/tasks/access-application-cluster/configure-access-multiple-clusters/)\n* [`kubectl config`](/docs/reference/generated/kubectl/kubectl-commands#config)"}
{"en": "Kubernetes has various types of probes:\n\n- [Liveness probe](#liveness-probe)\n- [Readiness probe](#readiness-probe)\n- [Startup probe](#startup-probe)", "zh": "Kubernetes 提供了多种探针：\n\n- [存活探针](#liveness-probe)\n- [就绪探针](#readiness-probe)\n- [启动探针](#startup-probe)"}
{"en": "## Liveness probe\n\nLiveness probes determine when to restart a container. For example, liveness probes could catch a deadlock, when an application is running, but unable to make progress.", "zh": "## 存活探针   {#liveness-probe}\n\n存活探针决定何时重启容器。\n例如，当应用在运行但无法取得进展时，存活探针可以捕获这类死锁。"}
{"en": "If a container fails its liveness probe repeatedly, the kubelet restarts the container.", "zh": "如果一个容器的存活探针失败多次，kubelet 将重启该容器。"}
{"en": "Liveness probes do not wait for readiness probes to succeed. If you want to wait before\nexecuting a liveness probe you can either define `initialDelaySeconds`, or use a\n[startup probe](#startup-probe).", "zh": "存活探针不会等待就绪探针成功。\n如果你想在执行存活探针前等待，你可以定义 `initialDelaySeconds`，或者使用[启动探针](#startup-probe)。"}
{"en": "## Readiness probe\n\nReadiness probes determine when a container is ready to start accepting traffic. This is useful when waiting for an application to perform time-consuming initial tasks, such as establishing network connections, loading files, and warming caches.", "zh": "## 就绪探针   {#readiness-probe}\n\n就绪探针决定何时容器准备好开始接受流量。\n这种探针在等待应用执行耗时的初始任务时非常有用，例如建立网络连接、加载文件和预热缓存。"}
{"en": "If the readiness probe returns a failed state, Kubernetes removes the pod from all matching service endpoints.\n\nReadiness probes runs on the container during its whole lifecycle.", "zh": "如果就绪探针返回的状态为失败，Kubernetes 会将该 Pod 从所有对应服务的端点中移除。\n\n就绪探针在容器的整个生命期内持续运行。"}
{"en": "## Startup probe\n\nA startup probe verifies whether the application within a container is started. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet before they are up and running.", "zh": "## 启动探针   {#startup-probe}\n\n启动探针检查容器内的应用是否已启动。\n启动探针可以用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被 kubelet 杀掉。"}
{"en": "If such a probe is configured, it disables liveness and readiness checks until it succeeds.", "zh": "如果配置了这类探针，它会禁用存活检测和就绪检测，直到启动探针成功为止。"}
{"en": "This type of probe is only executed at startup, unlike liveness and readiness probes, which are run periodically.\n\n* Read more about the [Configure Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes).", "zh": "这类探针仅在启动时执行，不像存活探针和就绪探针那样周期性地运行。\n\n* 更多细节参阅[配置存活、就绪和启动探针](/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes)。"}
{"en": "overview \nThis page outlines the differences in how resources are managed between Linux and Windows.", "zh": "本页概述了 Linux 和 Windows 在资源管理方式上的区别。"}
{"en": "On Linux nodes, {{< glossary_tooltip text=\"cgroups\" term_id=\"cgroup\" >}} are used\nas a pod boundary for resource control. Containers are created within that boundary for network, process and file system isolation. The Linux cgroup APIs can be used to gather CPU, I/O, and memory use statistics.\n\nIn contrast, Windows uses a [_job object_](https://docs.microsoft.com/windows/win32/procthread/job-objects) per container with a system namespace filter\nto contain all processes in a container and provide logical isolation from the\nhost. (Job objects are a Windows process isolation mechanism and are different from what Kubernetes refers to as a {{< glossary_tooltip term_id=\"job\" text=\"Job\" >}}).\n\nThere is no way to run a Windows container without the namespace filtering in\nplace. This means that system privileges cannot be asserted in the context of the\nhost, and thus privileged containers are not available on Windows.\nContainers cannot assume an identity from the host because the Security Account Manager (SAM) is separate.", "zh": "在 Linux 节点上，{{< glossary_tooltip text=\"cgroup\" term_id=\"cgroup\" >}} 用作资源控制的 Pod 边界。\n在这个边界内创建容器以便于隔离网络、进程和文件系统。\nLinux cgroup API 可用于收集 CPU、I/O 和内存使用统计数据。\n\n与此相反，Windows 中每个容器对应一个[**作业对象**](https://docs.microsoft.com/zh-cn/windows/win32/procthread/job-objects)，\n与系统命名空间过滤器一起使用，将所有进程包含在一个容器中，提供与主机的逻辑隔离。\n（作业对象是一种 Windows 进程隔离机制，不同于 Kubernetes 提及的 {{< glossary_tooltip term_id=\"job\" text=\"Job\" >}})。\n\n如果没有命名空间过滤，就无法运行 Windows 容器。\n这意味着在主机环境中无法让系统特权生效，因此特权容器在 Windows 上不可用。\n容器不能使用来自主机的标识，因为安全帐户管理器（Security Account Manager，SAM）是独立的。"}
{"en": "## Memory management {#resource-management-memory}\n\nWindows does not have an out-of-memory process killer as Linux does. Windows always\ntreats all user-mode memory allocations as virtual, and pagefiles are mandatory.\n\nWindows nodes do not overcommit memory for processes. The\nnet effect is that Windows won't reach out of memory conditions the same way Linux\ndoes, and processes page to disk instead of being subject to out of memory (OOM)\ntermination. If memory is over-provisioned and all physical memory is exhausted,\nthen paging can slow down performance.", "zh": "## 内存管理 {#resource-management-memory}\n\nWindows 不像 Linux 一样提供杀手（killer）机制，杀死内存不足的进程。\nWindows 始终将所有用户态内存分配视为虚拟内存，并强制使用页面文件（pagefile）。\n\nWindows 节点不会为进程过量使用内存。\n最终结果是 Windows 不会像 Linux 那样达到内存不足的情况，Windows 将进程页面放到磁盘，\n不会因为内存不足（OOM）而终止进程。\n如果内存配置过量且所有物理内存都已耗尽，则换页性能就会降低。"}
{"en": "## CPU management {#resource-management-cpu}\n\nWindows can limit the amount of CPU time allocated for different processes but cannot\nguarantee a minimum amount of CPU time.\n\nOn Windows, the kubelet supports a command-line flag to set the\n[scheduling priority](https://docs.microsoft.com/windows/win32/procthread/scheduling-priorities) of the\nkubelet process: `--windows-priorityclass`. This flag allows the kubelet process to get\nmore CPU time slices when compared to other processes running on the Windows host.\nMore information on the allowable values and their meaning is available at\n[Windows Priority Classes](https://docs.microsoft.com/en-us/windows/win32/procthread/scheduling-priorities#priority-class).\nTo ensure that running Pods do not starve the kubelet of CPU cycles, set this flag to `ABOVE_NORMAL_PRIORITY_CLASS` or above.", "zh": "## CPU 管理 {#resource-management-cpu}\n\nWindows 可以限制为不同进程分配的 CPU 时间长度，但无法保证最小的 CPU 时间长度。\n\n在 Windows 上，kubelet 支持使用命令行标志来设置 kubelet 进程的[调度优先级](https://docs.microsoft.com/zh-cn/windows/win32/procthread/scheduling-priorities)：\n`--windows-priorityclass`。\n与 Windows 主机上运行的其他进程相比，此标志允许 kubelet 进程获取更多的 CPU 时间片。\n有关允许值及其含义的更多信息，请访问 [Windows 优先级类](https://docs.microsoft.com/zh-cn/windows/win32/procthread/scheduling-priorities#priority-class)。\n为了确保运行的 Pod 不会耗尽 kubelet 的 CPU 时钟周期，\n要将此标志设置为 `ABOVE_NORMAL_PRIORITY_CLASS` 或更高。"}
{"en": "## Resource reservation {#resource-reservation}\n\nTo account for memory and CPU used by the operating system, the container runtime, and by\nKubernetes host processes such as the kubelet, you can (and should) reserve\nmemory and CPU resources with the  `--kube-reserved` and/or `--system-reserved` kubelet flags.\nOn Windows these values are only used to calculate the node's\n[allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable) resources.", "zh": "## 资源预留 {#resource-reservation}\n\n为了满足操作系统、容器运行时和 kubelet 等 Kubernetes 主机进程使用的内存和 CPU，\n你可以（且应该）用 `--kube-reserved` 和/或 `--system-reserved` kubelet 标志来预留内存和 CPU 资源。\n在 Windows 上，这些值仅用于计算节点的[可分配](/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)资源。"}
{"en": "As you deploy workloads, set resource memory and CPU limits on containers.\nThis also subtracts from `NodeAllocatable` and helps the cluster-wide scheduler in determining which pods to place on which nodes.\n\nScheduling pods without limits may over-provision the Windows nodes and in extreme\ncases can cause the nodes to become unhealthy.", "zh": "{{< caution >}}\n在你部署工作负载时，需对容器设置内存和 CPU 资源的限制。\n这也会从 `NodeAllocatable` 中减去，帮助集群范围的调度器决定哪些 Pod 放到哪些节点上。\n\n若调度 Pod 时未设置限制值，可能对 Windows 节点过量配置资源。\n在极端情况下，这会让节点变得不健康。\n{{< /caution >}}"}
{"en": "On Windows, a good practice is to reserve at least 2GiB of memory.\n\nTo determine how much CPU to reserve,\nidentify the maximum pod density for each node and monitor the CPU usage of\nthe system services running there, then choose a value that meets your workload needs.", "zh": "在 Windows 上，一种好的做法是预留至少 2GiB 的内存。\n\n要决定预留多少 CPU，需明确每个节点的最大 Pod 密度，\n并监控节点上运行的系统服务的 CPU 使用率，然后选择一个满足工作负载需求的值。"}
{"en": "This page describes some of the security features that are built into the Linux\nkernel that you can use in your Kubernetes workloads. To learn how to apply\nthese features to your Pods and containers, refer to\n[Configure a SecurityContext for a Pod or Container](/docs/tasks/configure-pod-container/security-context/).\nYou should already be familiar with Linux and with the basics of Kubernetes\nworkloads.", "zh": "本页描述了一些 Linux 内核中内置的、你可以在 Kubernetes 工作负载中使用的安全特性。\n要了解如何将这些特性应用到你的 Pod 和容器，\n请参阅[为 Pod 或容器配置 SecurityContext](/zh-cn/docs/tasks/configure-pod-container/security-context/)。\n你须熟悉 Linux 和 Kubernetes 工作负载的基础知识。"}
{"en": "## Run workloads without root privileges {#run-without-root}\n\nWhen you deploy a workload in Kubernetes, use the Pod specification to restrict\nthat workload from running as the root user on the node. You can use the Pod\n`securityContext` to define the specific Linux user and group for the processes in\nthe Pod, and explicitly restrict containers from running as root users. Setting\nthese values in the Pod manifest takes precedence over similar values in the\ncontainer image, which is especially useful if you're running images that you\ndon't own.", "zh": "## 运行不具有 root 特权的工作负载   {#run-without-root}\n\n当你在 Kubernetes 中部署一个工作负载时，可以使用 Pod 规约来限制该工作负载以非 root 用户在节点上运行。\n你可以使用 Pod 的 `securityContext` 为 Pod 中的进程定义特定的 Linux 用户和组，\n并明确限制容器不可以 root 用户运行。在 Pod 清单中设置的这些值优先于容器镜像中的类似值，\n这对于运行非自有的镜像特别有用。\n\n{{< caution >}}"}
{"en": "Ensure that the user or group that you assign to the workload has the permissions\nrequired for the application to function correctly. Changing the user or group\nto one that doesn't have the correct permissions could lead to file access\nissues or failed operations.", "zh": "确保你分配给工作负载的用户或组具有应用正常运行所需的权限。\n将用户或组更改为没有适当权限的用户或组可能会导致文件访问问题或操作失败。\n{{< /caution >}}"}
{"en": "Configuring the kernel security features on this page provides fine-grained\ncontrol over the actions that processes in your cluster can take, but managing\nthese configurations can be challenging at scale. Running containers as\nnon-root, or in user namespaces if you need root privileges, helps to reduce the\nchance that you'll need to enforce your configured kernel security capabilities.", "zh": "配置本页所述的内核安全特性可以对集群中进程能够执行的操作进行细粒度的控制，但大规模管理这些配置可能会有挑战。\n以非 root 用户运行容器，或在需要 root 特权时在 user 命名空间中运行容器，有助于减少你因必须配置的内核安全权能的要求。"}
{"en": "## Security features in the Linux kernel {#linux-security-features}\n\nKubernetes lets you configure and use Linux kernel features to improve isolation\nand harden your containerized workloads. Common features include the following:", "zh": "## Linux 内核中的安全特性   {#linux-security-features}\n\nKubernetes 允许你配置和使用 Linux 内核特性来提高容器化的工作负载的隔离性，完成安全加固。\n常见的特性包括以下几种："}
{"en": "* **Secure computing mode (seccomp)**: Filter which system calls a process can\n  make\n* **AppArmor**: Restrict the access privileges of individual programs\n* **Security Enhanced Linux (SELinux)**: Assign security labels to objects for\n  more manageable security policy enforcement", "zh": "* **安全计算模式 (seccomp)**：过滤某个进程可以执行哪些系统调用\n* **AppArmor**：限制单个程序的访问特权\n* **安全增强 Linux (SELinux)**：为对象赋予安全标签，以便更好地管理安全策略的实施"}
{"en": "To configure settings for one of these features, the operating system that you\nchoose for your nodes must enable the feature in the kernel. For example,\nUbuntu 7.10 and later enable AppArmor by default. To learn whether your OS\nenables a specific feature, consult the OS documentation.", "zh": "要配置其中一个特性的设置，你为节点所选择的操作系统必须在内核中启用对应的特性。\n例如，Ubuntu 7.10 及更高版本默认启用 AppArmor。\n要了解你的操作系统是否启用了特定特性，请查阅对应的操作系统文档。"}
{"en": "You use the `securityContext` field in your Pod specification to define the\nconstraints that apply to those processes. The `securityContext` field also\nsupports other security settings, such as specific Linux capabilities or file\naccess permissions using UIDs and GIDs. To learn more, refer to\n[Configure a SecurityContext for a Pod or Container](/docs/tasks/configure-pod-container/security-context/).", "zh": "你可以使用 Pod 规约中的 `securityContext` 字段来定义适用于 Pod 中进程的约束。\n`securityContext` 字段还支持其他安全设置，例如使用特定 Linux 权能或基于 UID 和 GID 的文件访问权限。\n要了解更多信息，请参阅[为 Pod 或容器配置 SecurityContext](/zh-cn/docs/tasks/configure-pod-container/security-context/)。\n\n### seccomp"}
{"en": "Some of your workloads might need privileges to perform specific actions as the\nroot user on your node's host machine. Linux uses *capabilities* to divide the\navailable privileges into categories, so that processes can get the privileges\nrequired to perform specific actions without being granted all privileges. Each\ncapability has a set of system calls (syscalls) that a process can make. seccomp\nlets you restrict these individual syscalls.\nIt can be used to sandbox the privileges of a process, restricting the calls it\nis able to make from userspace into the kernel.", "zh": "你的某些工作负载可能需要在你的节点的主机上以 root 用户执行特定操作的权限。\nLinux 使用**权能（Capability）** 将可用的特权划分为不同类别，这样进程就能够获取执行特定操作所需的特权，\n而无需为其授予所有特权。每个权能都对应进程可以执行的一组系统调用（syscalls）。\nseccomp 允许你限制这些单独的系统调用。seccomp 可用于沙盒化进程的权限，限制其可以从用户空间向内核发出的调用。"}
{"en": "In Kubernetes, you use a *container runtime* on each node to run your\ncontainers. Example runtimes include CRI-O, Docker, or containerd. Each runtime\nallows only a subset of Linux capabilities by default. You can further limit the\nallowed syscalls individually by using a seccomp profile. Container runtimes\nusually include a default seccomp profile.\nKubernetes lets you automatically\napply seccomp profiles loaded onto a node to your Pods and containers.", "zh": "在 Kubernetes 中，你在每个节点上使用**容器运行时**来运行你的容器。\n运行时的例子包括 CRI-O、Docker 或 containerd。每个运行时默认仅允许一部分 Linux 权能。\n你可以使用 seccomp 配置文件进一步限制所允许的系统调用。容器运行时通常包含一个默认的 seccomp 配置文件。\nKubernetes 允许你自动将加载到某个节点上的那些 seccomp 配置文件应用到你的 Pod 和容器。\n\n{{<note>}}"}
{"en": "Kubernetes also has the `allowPrivilegeEscalation` setting for Pods and\ncontainers. When set to `false`, this prevents processes from gaining new\ncapabilities and restricts unprivileged users from changing the applied seccomp\nprofile to a more permissive profile.", "zh": "Kubernetes 还可以为 Pod 和容器设置 `allowPrivilegeEscalation`。当此字段设置为 `false` 时，\n将阻止进程获取新权能，并限制非特权用户将已应用的 seccomp 配置文件更改为某个更宽松的配置文件。\n{{</note>}}"}
{"en": "To learn how to implement seccomp in Kubernetes, refer to\n[Restrict a Container's Syscalls with seccomp](/docs/tutorials/security/seccomp/)\nor the [Seccomp node reference](/docs/reference/node/seccomp/)\n\nTo learn more about seccomp, see\n[Seccomp BPF](https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html)\nin the Linux kernel documentation.", "zh": "要了解如何在 Kubernetes 中实现 seccomp，\n请参阅[使用 seccomp 限制容器的系统调用](/zh-cn/docs/tutorials/security/seccomp/)或\n[Seccomp 节点参考](/zh-cn/docs/reference/node/seccomp/)。\n\n要了解 seccomp 的更多细节，请参阅 Linux 内核文档中的\n[Seccomp BPF](https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html)。"}
{"en": "#### Considerations for seccomp {#seccomp-considerations}\n\nseccomp is a low-level security configuration that you should only configure\nyourself if you require fine-grained control over Linux syscalls. Using\nseccomp, especially at scale, has the following risks:", "zh": "#### seccomp 的注意事项   {#seccomp-considerations}\n\nseccomp 是一种底层安全配置，只有在你需要对 Linux 系统调用进行细粒度控制时才应自行配置。\n使用 seccomp，尤其是在大规模使用时，会有以下风险："}
{"en": "* Configurations might break during application updates\n* Attackers can still use allowed syscalls to exploit vulnerabilities\n* Profile management for individual applications becomes challenging at scale", "zh": "* 在应用更新期间这些配置可能被破坏\n* 攻击者仍然可以使用被允许的系统调用来利用漏洞\n* 逐个应用地管理配置文件在规模较大时变得具有挑战性"}
{"en": "**Recommendation**: Use the default seccomp profile that's bundled with your\ncontainer runtime. If you need a more isolated environment, consider using a\nsandbox, such as gVisor. Sandboxes solve the preceding risks with custom\nseccomp profiles, but require more compute resources on your nodes and might\nhave compatibility issues with GPUs and other specialized hardware.", "zh": "**建议**：使用与你的容器运行时捆绑的默认 seccomp 配置文件。\n如果你需要一个隔离性更好的环境，请考虑使用沙箱，例如 gVisor。\n沙箱通过自定义 seccomp 配置文件解决了上述风险，但需要占用节点上的更多计算资源，\n并且可能与 GPU 和其他专用硬件存在兼容性问题。"}
{"en": "### AppArmor and SELinux: policy-based mandatory access control {#policy-based-mac}\n\nYou can use Linux policy-based mandatory access control (MAC) mechanisms, such\nas AppArmor and SELinux, to harden your Kubernetes workloads.", "zh": "### AppArmor 和 SELinux：基于策略的强制访问控制   {#policy-based-mac}\n\n你可以使用 Linux 上基于策略的强制访问控制（MAC）机制（例如 AppArmor 和 SELinux）来加固你的 Kubernetes 工作负载。\n\n#### AppArmor"}
{"en": "[AppArmor](https://apparmor.net/) is a Linux kernel security module that\nsupplements the standard Linux user and group based permissions to confine\nprograms to a limited set of resources. AppArmor can be configured for any\napplication to reduce its potential attack surface and provide greater in-depth\ndefense. It is configured through profiles tuned to allow the access needed by a\nspecific program or container, such as Linux capabilities, network access, and\nfile permissions. Each profile can be run in either enforcing mode, which blocks\naccess to disallowed resources, or complain mode, which only reports violations.", "zh": "[AppArmor](https://apparmor.net/) 是一个 Linux 内核安全模块，它在标准的基于 Linux 用户和组的权限基础上，\n进一步将程序限制在有限的资源集内。AppArmor 可以针对任何应用配置，以减小其潜在的攻击面并提供更深入的防御。\nAppArmor 通过调优的配置文件进行配置，以允许特定程序或容器所需的访问，例如 Linux 权能、网络访问和文件权限。\n每个配置文件要么在强制（Enforcing）模式下运行，即阻止访问不被允许的资源，\n要么在投诉（Complaining）模式下运行，只报告违规行为。"}
{"en": "AppArmor can help you to run a more secure deployment by restricting what\ncontainers are allowed to do, and/or provide better auditing through system\nlogs. The container runtime that you use might ship with a default AppArmor\nprofile, or you can use a custom profile.\n\nTo learn how to use AppArmor in Kubernetes, refer to\n[Restrict a Container's Access to Resources with AppArmor](/docs/tutorials/security/apparmor/).", "zh": "AppArmor 可以通过限制容器被允许执行哪些操作来帮助你运行更为安全的部署，还可以通过系统日志提供更好的审计。\n你使用的容器运行时可能附带默认的 AppArmor 配置文件，或者你也可以使用自定义的配置文件。\n\n要了解如何在 Kubernetes 中使用 AppArmor，\n请参阅[使用 AppArmor 限制容器对资源的访问](/zh-cn/docs/tutorials/security/apparmor/)。\n\n#### SELinux"}
{"en": "SELinux is a Linux kernel security module that lets you restrict the access\nthat a specific *subject*, such as a process, has to the files on your system.\nYou define security policies that apply to subjects that have specific SELinux\nlabels. When a process that has an SELinux label attempts to access a file, the\nSELinux server checks whether that process' security policy allows the access\nand makes an authorization decision.", "zh": "SELinux 是一个 Linux 内核安全模块，允许你限制特定**主体**（例如进程）对系统上文件的访问。\n你可以定义要应用到具有特定 SELinux 标签的主体的安全策略。\n当具有特定 SELinux 标签的进程试图访问某个文件时，SELinux 服务器会检查该进程的安全策略是否允许访问并做出鉴权决策。"}
{"en": "In Kubernetes, you can set an SELinux label in the `securityContext` field of\nyour manifest. The specified labels are assigned to those processes. If you\nhave configured security policies that affect those labels, the host OS kernel\nenforces these policies.\n\nTo learn how to use SELinux in Kubernetes, refer to\n[Assign SELinux labels to a container](/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container).", "zh": "在 Kubernetes 中，你可以在清单的 `securityContext` 字段中设置 SELinux 标签。\n所指定的标签被赋予给那些进程。如果你配置了影响这些标签的安全策略，则主机操作系统内核将强制执行这些策略。\n\n要了解如何在 Kubernetes 中使用 SELinux，\n请参阅[为容器分配 SELinux 标签](/zh-cn/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container)。"}
{"en": "#### Differences between AppArmor and SELinux {#apparmor-selinux-diff}\n\nThe operating system on your Linux nodes usually includes one of either\nAppArmor or SELinux. Both mechanisms provide similar types of protection, but\nhave differences such as the following:", "zh": "#### AppArmor 和 SELinux 之间的区别   {#apparmor-selinux-diff}\n\nLinux 节点上的操作系统通常包含 AppArmor 或 SELinux 其中之一。\n这两种机制都能提供类似的保护，但有以下区别："}
{"en": "* **Configuration**: AppArmor uses profiles to define access to resources.\n  SELinux uses policies that apply to specific labels.\n* **Policy application**: In AppArmor, you define resources using file paths.\n  SELinux uses the index node (inode) of a resource to identify the resource.", "zh": "* **配置**：AppArmor 使用配置文件定义对资源的访问。SELinux 使用适用于特定标签的策略。\n* **策略应用**：在 AppArmor 中，你使用文件路径来定义资源。SELinux 使用资源的索引节点 (inode) 来标识资源。"}
{"en": "### Summary of features {#summary}\n\nThe following table describes the use cases and scope of each security control.\nYou can use all of these controls together to build a more hardened system.", "zh": "### 特性摘要   {#summary}\n\n下表描述了每种安全控制机制的使用场景和范围。你可以同时使用所有这些控制机制来构建更稳固的系统。\n\n<table>"}
{"en": "<caption>Summary of Linux kernel security features</caption>", "zh": "<caption>Linux 内核安全特性摘要</caption>\n  <thead>\n    <tr>\n      <th>"}
{"en": "Security feature", "zh": "安全特性</th>\n      <th>"}
{"en": "Description", "zh": "描述</th>\n      <th>"}
{"en": "How to use", "zh": "使用方式</th>\n      <th>"}
{"en": "Example", "zh": "示例</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>seccomp</td>"}
{"en": "<td>Restrict individual kernel calls in the userspace. Reduces the\n      likelihood that a vulnerability that uses a restricted syscall would\n      compromise the system.</td>\n      <td>Specify a loaded seccomp profile in the Pod or container specification\n      to apply its constraints to the processes in the Pod.</td>\n      <td>Reject the <code>unshare</code> syscall, which was used in\n      <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-0185\">CVE-2022-0185</a>.</td>", "zh": "<td>限制用户空间中的各个内核调用。如果某漏洞使用了某受限的系统调用，这一机制可降低系统被破坏的可能性。</td>\n      <td>在 Pod 或容器规约中配置某已加载的 seccomp 配置文件，以将其约束应用于 Pod 中的进程。</td>\n      <td>拒绝曾在\n      <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-0185\">CVE-2022-0185</a>\n      中使用的 <code>unshare</code> 系统调用。</td>\n    </tr>\n    <tr>\n      <td>AppArmor</td>"}
{"en": "<td>Restrict program access to specific resources. Reduces the attack\n      surface of the program. Improves audit logging.</td>\n      <td>Specify a loaded AppArmor profile in the container specification.</td>\n      <td>Restrict a read-only program from writing to any file path\n      in the system.</td>", "zh": "<td>限制程序对特定资源的访问。减少程序的攻击面。改进审计日志。</td>\n      <td>在容器规约中设定某已加载的 AppArmor 配置文件。</td>\n      <td>限制只读程序，不允许其写入系统中的任何文件路径。</td>\n    </tr>\n    <tr>\n      <td>SELinux</td>"}
{"en": "<td>Restrict access to resources such as files, applications, ports, and\n      processes using labels and security policies.</td>\n      <td>Specify access restrictions for specific labels. Tag processes with\n      those labels to enforce the access restrictions related to the label.</td>\n      <td>Restrict a container from accessing files outside its own filesystem.</td>", "zh": "<td>使用标签和安全策略限制对文件、应用、端口和进程等资源的访问。</td>\n      <td>为特定标签设置访问限制。使用这些标签来标记进程，以强制执行与标签相关的访问限制。</td>\n      <td>限制容器访问其自身文件系统之外的文件。</td>\n    </tr>\n  </tbody>\n</table>\n\n{{< note >}}"}
{"en": "Mechanisms like AppArmor and SELinux can provide protection that extends beyond\nthe container. For example, you can use SELinux to help mitigate\n[CVE-2019-5736](https://access.redhat.com/security/cve/cve-2019-5736).", "zh": "像 AppArmor 和 SELinux 这样的机制可以提供超出容器范围的保护。例如，你可以使用 SELinux 帮助缓解\n[CVE-2019-5736](https://access.redhat.com/security/cve/cve-2019-5736)。\n{{< /note >}}"}
{"en": "### Considerations for managing custom configurations {#considerations-custom-configurations}\n\nseccomp, AppArmor, and SELinux usually have a default configuration that offers\nbasic protections.  You can also create custom profiles and policies that meet\nthe requirements of your workloads. Managing and distributing these custom\nconfigurations at scale might be challenging, especially if you use all three\nfeatures together. To help you to manage these configurations at scale, use a\ntool like the\n[Kubernetes Security Profiles Operator](https://github.com/kubernetes-sigs/security-profiles-operator).", "zh": "### 管理自定义配置的注意事项   {#considerations-custom-configurations}\n\nseccomp、AppArmor 和 SELinux 通常有一个默认配置来提供基本的保护。\n你还可以创建自定义配置文件和策略来满足你的工作负载的要求。\n大规模场景下管理和分发这些自定义配置可能具有挑战性，特别是当你同时使用这三种特性时。\n为了帮助你在大规模场景下管理这些配置，可以使用类似\n[Kubernetes Security Profiles Operator](https://github.com/kubernetes-sigs/security-profiles-operator)\n的工具。"}
{"en": "## Kernel-level security features and privileged containers {#kernel-security-features-privileged-containers}\n\nKubernetes lets you specify that some trusted containers can run in\n*privileged* mode. Any container in a Pod can run in privileged mode to use\noperating system administrative capabilities that would otherwise be\ninaccessible. This is available for both Windows and Linux.", "zh": "## 内核级安全特性和特权容器   {#kernel-security-features-privileged-containers}\n\nKubernetes 允许你指定一些被信任的容器能以**特权**模式运行。\nPod 中的所有容器都能够以特权模式运行，以使用操作系统的管理性质权能，这些权能在其他情况下是不可访问的。\n此特性在 Windows 和 Linux 上都可用。"}
{"en": "Privileged containers explicitly override some of the Linux kernel constraints\nthat you might use in your workloads, as follows:\n\n* **seccomp**: Privileged containers run as the `Unconfined` seccomp profile,\n  overriding any seccomp profile that you specified in your manifest.\n* **AppArmor**: Privileged containers ignore any applied AppArmor profiles.\n* **SELinux**: Privileged containers run as the `unconfined_t` domain.", "zh": "特权容器显式覆盖你可能在工作负载中使用的以下一些 Linux 内核约束：\n\n* **seccomp**：特权容器以 `Unconfined` 为 seccomp 配置文件运行，覆盖你在清单中指定的所有 seccomp 配置。\n* **AppArmor**：特权容器忽略任何已应用的 AppArmor 配置文件。\n* **SELinux**：特权容器以 `unconfined_t` 域运行。"}
{"en": "### Privileged containers {#privileged-containers}", "zh": "### 特权容器    {#privileged-containers}"}
{"en": "Any container in a Pod can enable *Privileged mode* if you set the\n`privileged: true` field in the\n[`securityContext`](/docs/tasks/configure-pod-container/security-context/)\nfield for the container. Privileged containers override or undo many other hardening settings such as the applied seccomp profile, AppArmor profile, or\nSELinux constraints. Privileged containers are given all Linux capabilities,\nincluding capabilities that they don't require. For example, a root user in a\nprivileged container might be able to use the `CAP_SYS_ADMIN` and\n`CAP_NET_ADMIN` capabilities on the node, bypassing the runtime seccomp\nconfiguration and other restrictions.", "zh": "如果你在容器的 [`securityContext`](/zh-cn/docs/tasks/configure-pod-container/security-context/)\n字段中设置 `privileged: true` 字段，则 Pod 中的所有容器都可以启用**特权模式**。\n特权容器会覆盖或使许多其他加固选项无效，例如已应用的 seccomp 配置文件、AppArmor 配置文件或 SELinux 约束。\n特权容器被赋予所有的 Linux 权能，包括它们所不需要的权能。例如，特权容器中的 root 用户可能能够绕过运行时的\nseccomp 配置和其他限制，在节点上使用 `CAP_SYS_ADMIN` 和 `CAP_NET_ADMIN` 权能。"}
{"en": "In most cases, you should avoid using privileged containers, and instead grant\nthe specific capabilities required by your container using the `capabilities`\nfield in the `securityContext` field. Only use privileged mode if you have a\ncapability that you can't grant with the securityContext. This is useful for\ncontainers that want to use operating system administrative capabilities such\nas manipulating the network stack or accessing hardware devices.", "zh": "在大多数情况下，你应避免使用特权容器，而是通过 `securityContext` 字段中的 `capabilities`\n字段来授予容器所需的特定权能。只有在你无法通过 securityContext 授予某个权能时，才使用特权模式。\n这对希望使用操作系统管理权能（如操纵网络栈或访问硬件设备）的容器来说特别有用。"}
{"en": "In Kubernetes version 1.26 and later, you can also run Windows containers in a\nsimilarly privileged mode by setting the `windowsOptions.hostProcess` flag on\nthe security context of the Pod spec. For details and instructions, see\n[Create a Windows HostProcess Pod](/docs/tasks/configure-pod-container/create-hostprocess-pod/).", "zh": "在 Kubernetes 1.26 及更高版本中，你还可以通过在 Pod 规约的安全上下文中设置 `windowsOptions.hostProcess` 标志，\n以类似的特权模式运行 Windows 容器。有关细节和说明，\n请参阅[创建 Windows HostProcess Pod](/zh-cn/docs/tasks/configure-pod-container/create-hostprocess-pod/)。"}
{"en": "## Recommendations and best practices {#recommendations-best-practices}\n\n* Before configuring kernel-level security capabilities, you should consider\n  implementing network-level isolation. For more information, read the\n  [Security Checklist](/docs/concepts/security/security-checklist/#network-security).\n* Unless necessary, run Linux workloads as non-root by setting specific user and\n  group IDs in your Pod manifest and by specifying `runAsNonRoot: true`.", "zh": "## 建议和最佳实践    {#recommendations-best-practices}\n\n* 在配置内核级安全权能之前，你应该考虑实施网络级别的隔离。\n  有关细节参阅[安全检查清单](/zh-cn/docs/concepts/security/security-checklist/#network-security)。\n* 除非必要，否则通过在 Pod 清单中设置特定的用户和组 ID 并指定 `runAsNonRoot: true`，以非 root 身份运行 Linux 工作负载。"}
{"en": "Additionally, you can run workloads in user namespaces by setting\n`hostUsers: false` in your Pod manifest. This lets you run containers as root\nusers in the user namespace, but as non-root users in the host namespace on the\nnode. This is still in early stages of development and might not have the level\nof support that you need. For instructions, refer to\n[Use a User Namespace With a Pod](/docs/tasks/configure-pod-container/user-namespaces/).", "zh": "此外，你可以通过在 Pod 清单中设置 `hostUsers: false` 来在 user 命名空间中运行工作负载。\n这使你可以以 user 命名空间中的 root 用户运行容器，但在节点上的主机命名空间中是非 root 用户。\n此特性仍处于早期开发阶段，可能不是你所需要的支持级别。\n有关说明，请参阅[为 Pod 配置 user 命名空间](/zh-cn/docs/tasks/configure-pod-container/user-namespaces/)。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* [Learn how to use AppArmor](/docs/tutorials/security/apparmor/)\n* [Learn how to use seccomp](/docs/tutorials/security/seccomp/)\n* [Learn how to use SELinux](/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container)\n* [Seccomp Node Reference](/docs/reference/node/seccomp/)", "zh": "* [学习如何使用 AppArmor](/zh-cn/docs/tutorials/security/apparmor/)\n* [学习如何使用 seccomp](/zh-cn/docs/tutorials/security/seccomp/)\n* [学习如何使用 SELinux](/zh-cn/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container)\n* [Seccomp 节点参考](/zh-cn/docs/reference/node/seccomp/)"}
{"en": "Kubernetes {{< glossary_tooltip text=\"RBAC\" term_id=\"rbac\" >}} is a key security control\nto ensure that cluster users and workloads have only the access to resources required to\nexecute their roles. It is important to ensure that, when designing permissions for cluster\nusers, the cluster administrator understands the areas where privilge escalation could occur,\nto reduce the risk of excessive access leading to security incidents.\n\nThe good practices laid out here should be read in conjunction with the general\n[RBAC documentation](/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update).", "zh": "Kubernetes {{< glossary_tooltip text=\"RBAC\" term_id=\"rbac\" >}}\n是一项重要的安全控制措施，用于保证集群用户和工作负载只能访问履行自身角色所需的资源。\n在为集群用户设计权限时，请务必确保集群管理员知道可能发生特权提级的地方，\n降低因过多权限而导致安全事件的风险。\n\n此文档的良好实践应该与通用\n[RBAC 文档](/zh-cn/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update)一起阅读。"}
{"en": "## General good practice\n\n### Least privilege", "zh": "## 通用的良好实践 {#general-good-practice}\n\n### 最小特权  {#least-privilege}"}
{"en": "Ideally, minimal RBAC rights should be assigned to users and service accounts. Only permissions\nexplicitly required for their operation should be used. While each cluster will be different,\nsome general rules that can be applied are :", "zh": "理想情况下，分配给用户和服务帐户的 RBAC 权限应该是最小的。\n仅应使用操作明确需要的权限，虽然每个集群会有所不同，但可以应用的一些常规规则："}
{"en": "- Assign permissions at the namespace level where possible. Use RoleBindings as opposed to\n   ClusterRoleBindings to give users rights only within a specific namespace.\n - Avoid providing wildcard permissions when possible, especially to all resources.\n   As Kubernetes is an extensible system, providing wildcard access gives rights\n   not just to all object types that currently exist in the cluster, but also to all object types\n   which are created in the future.\n - Administrators should not use `cluster-admin` accounts except where specifically needed.\n   Providing a low privileged account with\n   [impersonation rights](/docs/reference/access-authn-authz/authentication/#user-impersonation)\n   can avoid accidental modification of cluster resources.\n - Avoid adding users to the `system:masters` group. Any user who is a member of this group\n   bypasses all RBAC rights checks and will always have unrestricted superuser access, which cannot be\n   revoked by removing RoleBindings or ClusterRoleBindings. As an aside, if a cluster is\n   using an authorization webhook, membership of this group also bypasses that webhook (requests\n   from users who are members of that group are never sent to the webhook)", "zh": "- 尽可能在命名空间级别分配权限。授予用户在特定命名空间中的权限时使用 RoleBinding\n  而不是 ClusterRoleBinding。\n- 尽可能避免通过通配符设置权限，尤其是对所有资源的权限。\n  由于 Kubernetes 是一个可扩展的系统，因此通过通配符来授予访问权限不仅会授予集群中当前的所有对象类型，\n  还包含所有未来被创建的所有对象类型。\n- 管理员不应使用 `cluster-admin` 账号，除非特别需要。为低特权帐户提供\n  [伪装权限](/zh-cn/docs/reference/access-authn-authz/authentication/#user-impersonation)\n  可以避免意外修改集群资源。\n- 避免将用户添加到 `system:masters` 组。任何属于此组成员的用户都会绕过所有 RBAC 权限检查，\n  始终具有不受限制的超级用户访问权限，并且不能通过删除 `RoleBinding` 或 `ClusterRoleBinding`\n  来取消其权限。顺便说一句，如果集群使用 Webhook 鉴权，此组的成员身份也会绕过该\n  Webhook（来自属于该组成员的用户的请求永远不会发送到 Webhook）。"}
{"en": "### Minimize distribution of privileged tokens", "zh": "### 最大限度地减少特权令牌的分发 {#minimize-distribution-of-privileged-tokens}"}
{"en": "Ideally, pods shouldn't be assigned service accounts that have been granted powerful permissions\n(for example, any of the rights listed under [privilege escalation risks](#privilege-escalation-risks)).\nIn cases where a workload requires powerful permissions, consider the following practices:\n\n- Limit the number of nodes running powerful pods. Ensure that any DaemonSets you run\n  are necessary and are run with least privilege to limit the blast radius of container escapes.\n- Avoid running powerful pods alongside untrusted or publicly-exposed ones. Consider using \n  [Taints and Toleration](/docs/concepts/scheduling-eviction/taint-and-toleration/),\n  [NodeAffinity](/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity), or\n  [PodAntiAffinity](/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)\n  to ensure pods don't run alongside untrusted or less-trusted Pods. Pay especial attention to\n  situations where less-trustworthy Pods are not meeting the **Restricted** Pod Security Standard.", "zh": "理想情况下，不应为 Pod 分配具有强大权限（例如，在[特权提级的风险](#privilege-escalation-risks)中列出的任一权限）\n的服务帐户。如果工作负载需要比较大的权限，请考虑以下做法：\n\n- 限制运行此类 Pod 的节点数量。确保你运行的任何 DaemonSet 都是必需的，\n  并且以最小权限运行，以限制容器逃逸的影响范围。\n- 避免将此类 Pod 与不可信任或公开的 Pod 在一起运行。\n  考虑使用[污点和容忍度](/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/)、\n  [节点亲和性](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity)或\n  [Pod 反亲和性](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)确保\n  Pod 不会与不可信或不太受信任的 Pod 一起运行。\n  特别注意可信度不高的 Pod 不符合 **Restricted** Pod 安全标准的情况。"}
{"en": "### Hardening\n\nKubernetes defaults to providing access which may not be required in every cluster. Reviewing\nthe RBAC rights provided by default can provide opportunities for security hardening.\nIn general, changes should not be made to rights provided to `system:` accounts some options\nto harden cluster rights exist:", "zh": "### 加固 {#hardening}\n\nKubernetes 默认提供访问权限并非是每个集群都需要的。\n审查默认提供的 RBAC 权限为安全加固提供了机会。\n一般来说，不应该更改 `system:` 帐户的某些权限，有一些方式来强化现有集群的权限："}
{"en": "- Review bindings for the `system:unauthenticated` group and remove them where possible, as this gives \n  access to anyone who can contact the API server at a network level.\n- Avoid the default auto-mounting of service account tokens by setting\n  `automountServiceAccountToken: false`. For more details, see\n  [using default service account token](/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server).\n  Setting this value for a Pod will overwrite the service account setting, workloads\n  which require service account tokens can still mount them.", "zh": "- 审查 `system:unauthenticated` 组的绑定，并在可能的情况下将其删除，\n  因为这会给所有能够访问 API 服务器的人以网络级别的权限。\n- 通过设置 `automountServiceAccountToken: false` 来避免服务账号令牌的默认自动挂载，\n  有关更多详细信息，请参阅[使用默认服务账号令牌](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server)。\n  此参数可覆盖 Pod 服务账号设置，而需要服务账号令牌的工作负载仍可以挂载。"}
{"en": "### Periodic review\n\nIt is vital to periodically review the Kubernetes RBAC settings for redundant entries and\npossible privilege escalations.\nIf an attacker is able to create a user account with the same name as a deleted user,\nthey can automatically inherit all the rights of the deleted user, specially the\nrights assigned to that user.", "zh": "### 定期检查  {#periodic-review}\n\n定期检查 Kubernetes RBAC 设置是否有冗余条目和提权可能性是至关重要的。\n如果攻击者能够创建与已删除用户同名的用户账号，\n他们可以自动继承被删除用户的所有权限，尤其是分配给该用户的权限。"}
{"en": "## Kubernetes RBAC - privilege escalation risks {#privilege-escalation-risks}\n\nWithin Kubernetes RBAC there are a number of privileges which, if granted, can allow a user or a service account\nto escalate their privileges in the cluster or affect systems outside the cluster.\n\nThis section is intended to provide visibility of the areas where cluster operators\nshould take care, to ensure that they do not inadvertently allow for more access to clusters than intended.", "zh": "## Kubernetes RBAC - 权限提权的风险 {#privilege-escalation-risks}\n\n在 Kubernetes RBAC 中有许多特权，如果被授予，\n用户或服务帐户可以提升其在集群中的权限并可能影响集群外的系统。\n\n本节旨在提醒集群操作员需要注意的不同领域，\n以确保他们不会无意中授予超出预期的集群访问权限。"}
{"en": "### Listing secrets\n\nIt is generally clear that allowing `get` access on Secrets will allow a user to read their contents.\nIt is also important to note that `list` and `watch` access also effectively allow for users to reveal the Secret contents.\nFor example, when a List response is returned (for example, via `kubectl get secrets -A -o yaml`), the response\nincludes the contents of all Secrets.", "zh": "### 列举 Secret {#listing-secrets}\n\n大家都很清楚，若允许对 Secrets 执行 `get` 访问，用户就获得了访问 Secret 内容的能力。\n同样需要注意的是：`list` 和 `watch` 访问也会授权用户获取 Secret 的内容。\n例如，当返回 List 响应时（例如，通过\n`kubectl get secrets -A -o yaml`），响应包含所有 Secret 的内容。"}
{"en": "### Workload creation\n\nPermission to create workloads (either Pods, or\n[workload resources](/docs/concepts/workloads/controllers/) that manage Pods) in a namespace\nimplicitly grants access to many other resources in that namespace, such as Secrets, ConfigMaps, and\nPersistentVolumes that can be mounted in Pods. Additionally, since Pods can run as any\n[ServiceAccount](/docs/reference/access-authn-authz/service-accounts-admin/), granting permission\nto create workloads also implicitly grants the API access levels of any service account in that\nnamespace.", "zh": "### 工作负载的创建 {#workload-creation}\n\n在一个命名空间中创建工作负载（Pod 或管理 Pod 的[工作负载资源](/zh-cn/docs/concepts/workloads/controllers/)）\n的权限隐式地授予了对该命名空间中许多其他资源的访问权限，例如可以挂载在\nPod 中的 Secret、ConfigMap 和 PersistentVolume。\n此外，由于 Pod 可以被任何[服务账号](/zh-cn/docs/reference/access-authn-authz/service-accounts-admin/)运行，\n因此授予创建工作负载的权限也会隐式地授予该命名空间中任何服务账号的 API 访问级别。"}
{"en": "Users who can run privileged Pods can use that access to gain node access and potentially to\nfurther elevate their privileges. Where you do not fully trust a user or other principal\nwith the ability to create suitably secure and isolated Pods, you should enforce either the\n**Baseline** or **Restricted** Pod Security Standard.\nYou can use [Pod Security admission](/docs/concepts/security/pod-security-admission/)\nor other (third party) mechanisms to implement that enforcement.", "zh": "可以运行特权 Pod 的用户可以利用该访问权限获得节点访问权限，\n并可能进一步提升他们的特权。如果你不完全信任某用户或其他主体，\n不相信他们能够创建比较安全且相互隔离的 Pod，你应该强制实施 **Baseline**\n或 **Restricted** Pod 安全标准。你可以使用\n[Pod 安全性准入](/zh-cn/docs/concepts/security/pod-security-admission/)或其他（第三方）\n机制来强制实施这些限制。"}
{"en": "For these reasons, namespaces should be used to separate resources requiring different levels of\ntrust or tenancy. It is still considered best practice to follow [least privilege](#least-privilege)\nprinciples and assign the minimum set of permissions, but boundaries within a namespace should be\nconsidered weak.", "zh": "出于这些原因，命名空间应该用于隔离不同的信任级别或不同租户所需的资源。\n遵循[最小特权](#least-privilege)原则并分配最小权限集仍被认为是最佳实践，\n但命名空间内的边界概念应视为比较弱。"}
{"en": "### Persistent volume creation\n\nIf someone - or some application - is allowed to create arbitrary PersistentVolumes, that access\nincludes the creation of `hostPath` volumes, which then means that a Pod would get access\nto the underlying host filesystem(s) on the associated node. Granting that ability is a security risk.", "zh": "### 持久卷的创建 {#persistent-volume-creation}\n\n如果允许某人或某个应用创建任意的 PersistentVolume，则这种访问权限包括创建 `hostPath` 卷，\n这意味着 Pod 将可以访问对应节点上的下层主机文件系统。授予该能力会带来安全风险。"}
{"en": "There are many ways a container with unrestricted access to the host filesystem can escalate privileges, including\nreading data from other containers, and abusing the credentials of system services, such as Kubelet.\n\nYou should only allow access to create PersistentVolume objects for:", "zh": "不受限制地访问主机文件系统的容器可以通过多种方式提升特权，包括从其他容器读取数据以及滥用系统服务\n（例如 kubelet）的凭据。\n\n你应该只允许以下实体具有创建 PersistentVolume 对象的访问权限："}
{"en": "- Users (cluster operators) that need this access for their work, and who you trust,\n- The Kubernetes control plane components which creates PersistentVolumes based on PersistentVolumeClaims\n  that are configured for automatic provisioning.\n  This is usually setup by the Kubernetes provider or by the operator when installing a CSI driver.", "zh": "- 需要此访问权限才能工作的用户（集群操作员）以及你信任的人，\n- Kubernetes 控制平面组件，这些组件基于已配置为自动制备的 PersistentVolumeClaim 创建 PersistentVolume。\n  这通常由 Kubernetes 提供商或操作员在安装 CSI 驱动程序时进行设置。"}
{"en": "Where access to persistent storage is required trusted administrators should create\nPersistentVolumes, and constrained users should use PersistentVolumeClaims to access that storage.", "zh": "在需要访问持久存储的地方，受信任的管理员应创建 PersistentVolume，而受约束的用户应使用\nPersistentVolumeClaim 来访问该存储。"}
{"en": "### Access to `proxy` subresource of Nodes\n\nUsers with access to the proxy sub-resource of node objects have rights to the Kubelet API,\nwhich allows for command execution on every pod on the node(s) to which they have rights.\nThis access bypasses audit logging and admission control, so care should be taken before\ngranting rights to this resource.", "zh": "### 访问 Node 的 `proxy` 子资源  {#access-to-proxy-subresource-of-nodes}\n\n有权访问 Node 对象的 proxy 子资源的用户有权访问 kubelet API，\n这允许在他们有权访问的节点上的所有 Pod 上执行命令。\n此访问绕过审计日志记录和准入控制，因此在授予对此资源的权限前应小心。"}
{"en": "### Escalate verb\n\nGenerally, the RBAC system prevents users from creating clusterroles with more rights than the user possesses.\nThe exception to this is the `escalate` verb. As noted in the [RBAC documentation](/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update),\nusers with this right can effectively escalate their privileges.", "zh": "### esclate 动词 {#escalate-verb}\n\n通常，RBAC 系统会阻止用户创建比他所拥有的更多权限的 `ClusterRole`。\n而 `escalate` 动词是个例外。如\n[RBAC 文档](/zh-cn/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update)\n中所述，拥有此权限的用户可以有效地提升他们的权限。"}
{"en": "### Bind verb\n\nSimilar to the `escalate` verb, granting users this right allows for the bypass of Kubernetes\nin-built protections against privilege escalation, allowing users to create bindings to\nroles with rights they do not already have.", "zh": "### bind 动词  {#bind-verb}\n\n与 `escalate` 动作类似，授予此权限的用户可以绕过 Kubernetes\n对权限提升的内置保护，用户可以创建并绑定尚不具有的权限的角色。"}
{"en": "### Impersonate verb\n\nThis verb allows users to impersonate and gain the rights of other users in the cluster.\nCare should be taken when granting it, to ensure that excessive permissions cannot be gained\nvia one of the impersonated accounts.", "zh": "### impersonate 动词 {#impersonate-verb}\n\n此动词允许用户伪装并获得集群中其他用户的权限。\n授予它时应小心，以确保通过其中一个伪装账号不会获得过多的权限。"}
{"en": "### CSRs and certificate issuing\n\nThe CSR API allows for users with `create` rights to CSRs and `update` rights on `certificatesigningrequests/approval`\nwhere the signer is `kubernetes.io/kube-apiserver-client` to create new client certificates\nwhich allow users to authenticate to the cluster. Those client certificates can have arbitrary\nnames including duplicates of Kubernetes system components. This will effectively allow for privilege escalation.", "zh": "### CSR 和证书颁发 {#csrs-and-certificate-issuing}\n\nCSR API 允许用户拥有 `create` CSR 的权限和 `update`\n`certificatesigningrequests/approval` 的权限，\n其中签名者是 `kubernetes.io/kube-apiserver-client`，\n通过此签名创建的客户端证书允许用户向集群进行身份验证。\n这些客户端证书可以包含任意的名称，包括 Kubernetes 系统组件的副本。\n这将有利于特权提级。"}
{"en": "### Token request\n\nUsers with `create` rights on `serviceaccounts/token` can create TokenRequests to issue\ntokens for existing service accounts.", "zh": "### 令牌请求 {#token-request}\n\n拥有 `serviceaccounts/token` 的 `create` 权限的用户可以创建\nTokenRequest 来发布现有服务帐户的令牌。"}
{"en": "### Control admission webhooks\n\nUsers with control over `validatingwebhookconfigurations` or `mutatingwebhookconfigurations`\ncan control webhooks that can read any object admitted to the cluster, and in the case of\nmutating webhooks, also mutate admitted objects.", "zh": "### 控制准入 Webhook {#control-admission-webhooks}\n\n可以控制 `validatingwebhookconfigurations` 或 `mutatingwebhookconfigurations`\n的用户可以控制能读取任何允许进入集群的对象的 webhook，\n并且在有变更 webhook 的情况下，还可以变更准入的对象。"}
{"en": "### Namespace modification\n\nUsers who can perform **patch** operations on Namespace objects (through a namespaced RoleBinding to a Role with that access) can modify\nlabels on that namespace. In clusters where Pod Security Admission is used, this may allow a user to configure the namespace\nfor a more permissive policy than intended by the administrators.\nFor clusters where NetworkPolicy is used, users may be set labels that indirectly allow\naccess to services that an administrator did not intend to allow.", "zh": "### 命名空间修改 {#namespace-modification}\n可以对命名空间对象执行 **patch** 操作的用户（通过命名空间内的 RoleBinding 关联到具有该权限的 Role），\n可以修改该命名空间的标签。在使用 Pod 安全准入的集群中，这可能允许用户将命名空间配置为比管理员预期更宽松的策略。\n对于使用 NetworkPolicy 的集群，用户所设置的标签可能间接导致对某些本不应被允许访问的服务的访问权限被开放。"}
{"en": "## Kubernetes RBAC - denial of service risks {#denial-of-service-risks}\n\n### Object creation denial-of-service {#object-creation-dos}\n\nUsers who have rights to create objects in a cluster may be able to create sufficient large \nobjects to create a denial of service condition either based on the size or number of objects, as discussed in\n[etcd used by Kubernetes is vulnerable to OOM attack](https://github.com/kubernetes/kubernetes/issues/107325). This may be\nspecifically relevant in multi-tenant clusters if semi-trusted or untrusted users \nare allowed limited access to a system.\n\nOne option for mitigation of this issue would be to use\n[resource quotas](/docs/concepts/policy/resource-quotas/#object-count-quota)\nto limit the quantity of objects which can be created.", "zh": "## Kubernetes RBAC - 拒绝服务攻击的风险 {#denial-of-service-risks}\n\n### 对象创建拒绝服务 {#object-creation-dos}\n\n有权在集群中创建对象的用户根据创建对象的大小和数量可能会创建足够大的对象，\n产生拒绝服务状况，如 [Kubernetes 使用的 etcd 容易受到 OOM 攻击](https://github.com/kubernetes/kubernetes/issues/107325)中的讨论。\n允许太不受信任或者不受信任的用户对系统进行有限的访问在多租户集群中是特别重要的。\n\n缓解此问题的一种选择是使用[资源配额](/zh-cn/docs/concepts/policy/resource-quotas/#object-count-quota)以限制可以创建的对象数量。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* To learn more about RBAC, see the [RBAC documentation](/docs/reference/access-authn-authz/rbac/).", "zh": "* 了解有关 RBAC 的更多信息，请参阅 [RBAC 文档](/zh-cn/docs/reference/access-authn-authz/rbac/)。"}
{"en": "Kubernetes is based on a cloud-native architecture, and draws on advice from the\n{{< glossary_tooltip text=\"CNCF\" term_id=\"cncf\" >}} about good practice for\ncloud native information security.", "zh": "Kubernetes 基于云原生架构，并借鉴了\n{{< glossary_tooltip text=\"CNCF\" term_id=\"cncf\" >}}\n有关云原生信息安全良好实践的建议。"}
{"en": "Read on through this page for an overview of how Kubernetes is designed to\nhelp you deploy a secure cloud native platform.", "zh": "继续阅读本页，了解 Kubernetes 如何设计以帮助你部署安全的云原生平台。"}
{"en": "## Cloud native information security", "zh": "## 云原生信息安全\n\n{{< comment >}}"}
{"en": "There are localized versions available of this whitepaper; if you can link to one of those\nwhen localizing, that's even better.", "zh": "该白皮书有可用的本地化版本；\n如果在本地化时能链接到其中一个版本，那就更好了。\n{{< /comment >}}"}
{"en": "The CNCF [white paper](https://github.com/cncf/tag-security/blob/main/community/resources/security-whitepaper/v2/CNCF_cloud-native-security-whitepaper-May2022-v2.pdf)\non cloud native security defines security controls and practices that are\nappropriate to different _lifecycle phases_.", "zh": "CNCF 关于云原生安全的[白皮书](https://github.com/cncf/tag-security/blob/main/security-whitepaper/v1/cloud-native-security-whitepaper-simplified-chinese.md)\n介绍了适用于不同**生命周期阶段**的安全控制和实践。"}
{"en": "## _Develop_ lifecycle phase {#lifecycle-phase-develop}", "zh": "## **开发**阶段 {#lifecycle-phase-develop}"}
{"en": "- Ensure the integrity of development environments.\n- Design applications following good practice for information security,\n  appropriate for your context.\n- Consider end user security as part of solution design.", "zh": "- 确保开发环境的完整性。\n- 在设计应用时，遵循信息安全的良好实践，\n   并根据实际情况进行调整。\n- 将最终用户的安全作为解决方案设计的一部分。"}
{"en": "To achieve this, you can:", "zh": "要达到这些目的，你可以："}
{"en": "1. Adopt an architecture, such as [zero trust](https://glossary.cncf.io/zero-trust-architecture/),\n   that minimizes attack surfaces, even for internal threats.\n1. Define a code review process that considers security concerns.\n1. Build a _threat model_ of your system or application that identifies\n   trust boundaries. Use that to model to identify risks and to help find\n   ways to treat those risks.\n1. Incorporate advanced security automation, such as _fuzzing_ and\n   [security chaos engineering](https://glossary.cncf.io/security-chaos-engineering/),\n   where it's justified.", "zh": "1. 采用诸如[零信任](https://glossary.cncf.io/zh-cn/zero-trust-architecture/)类似的架构，\n   尽可能缩小攻击面，对内部威胁也有效。\n2. 建立考虑安全问题的代码审查流程。\n3. 构建系统或应用程序的**威胁模型**，确定信任边界。\n   利用该模型识别风险，并帮助找到处理这些风险的方法。\n4. 合理的采用高级的安全自动化机制，例如**模糊测试**和[**安全混沌工程**](https://glossary.cncf.io/zh-cn/security-chaos-engineering/)。"}
{"en": "## _Distribute_ lifecycle phase {#lifecycle-phase-distribute}", "zh": "## **分发**阶段 {#lifecycle-phase-distribute}"}
{"en": "- Ensure the security of the supply chain for container images you execute.\n- Ensure the security of the supply chain for the cluster and other components\n  that execute your application. An example of another component might be an\n  external database that your cloud-native application uses for persistence.", "zh": "- 针对你所运行的容器镜像，确保供应链安全。\n- 针对运行应用程序的集群或其他组件，保证其供应链安全。\n   例如：其他组件可能是你的云原生应用用于数据持久化的外部数据库。"}
{"en": "To achieve this, you can:", "zh": "要达到这些目的，你可以："}
{"en": "1. Scan container images and other artifacts for known vulnerabilities.", "zh": "1. 扫描容器镜像和其他制品以查找已知漏洞。"}
{"en": "1. Ensure that software distribution uses encryption in transit, with\n   a chain of trust for the software source.", "zh": "2. 确保软件分发时采用传输加密技术，并建立软件源的信任链。"}
{"en": "1. Adopt and follow processes to update dependencies when updates are\n   available, especially in response to security announcements.", "zh": "3. 在有更新，尤其时安全公告时，采用并遵循更新依赖项的流程。"}
{"en": "1. Use validation mechanisms such as digital certificates for supply\n   chain assurance.", "zh": "4. 使用数字证书等验证机制来保证供应链可信。"}
{"en": "1. Subscribe to feeds and other mechanisms to alert you to security\n   risks.", "zh": "5. 订阅信息反馈和其他机制，以提醒你安全风险。"}
{"en": "1. Restrict access to artifacts. Place container images in a\n   [private registry](/docs/concepts/containers/images/#using-a-private-registry)\n   that only allows authorized clients to pull images.", "zh": "6. 严格限制制品访问权限。将容器镜像存储在[私有仓库](/zh-cn/docs/concepts/containers/images/#using-a-private-registry)，\n   仅允许已授权客户端拉取镜像。"}
{"en": "## _Deploy_ lifecycle phase {#lifecycle-phase-deploy}", "zh": "## **部署**阶段 {#lifecycle-phase-deploy}"}
{"en": "Ensure appropriate restrictions on what can be deployed, who can deploy it,\nand where it can be deployed to.\nYou can enforce measures from the _distribute_ phase, such as verifying the\ncryptographic identity of container image artifacts.", "zh": "确保对要部署的内容、可部署的人员和可部署的位置进行适当限制。\n你可以采取分发阶段的举措，例如验证容器镜像制品的加密身份。"}
{"en": "When you deploy Kubernetes, you also set the foundation for your\napplications' runtime environment: a Kubernetes cluster (or\nmultiple clusters).\nThat IT infrastructure must provide the security guarantees that higher\nlayers expect.", "zh": "当你部署 Kubernetes 时，也是在为应用程序的运行环境奠定基础：一个或多个 Kubernetes 集群。\n该 IT 基础设施必须提供上层所期望的安全保障。"}
{"en": "## _Runtime_ lifecycle phase {#lifecycle-phase-runtime}", "zh": "## 运行阶段 {#lifecycle-phase-runtime}"}
{"en": "The Runtime phase comprises three critical areas: [access](#protection-runtime-access),\n[compute](#protection-runtime-compute), and [storage](#protection-runtime-storage).", "zh": "运行阶段包含三个关键领域：[访问](#protection-runtime-access)、\n[计算](#protection-runtime-compute)和[存储](#protection-runtime-storage)。"}
{"en": "### Runtime protection: access {#protection-runtime-access}", "zh": "### 运行阶段的防护：访问 {#protection-runtime-access}"}
{"en": "The Kubernetes API is what makes your cluster work. Protecting this API is key\nto providing effective cluster security.", "zh": "Kubernetes API 是集群运行的基础。保护 API 是提供可靠的集群安全性的关键。"}
{"en": "Other pages in the Kubernetes documentation have more detail about how to set up\nspecific aspects of access control. The [security checklist](/docs/concepts/security/security-checklist/)\nhas a set of suggested basic checks for your cluster.", "zh": "Kubernetes 文档中的其他页面更详细地介绍了如何设置访问控制的具体细节。\n[安全检查清单](/zh-cn/docs/concepts/security/security-checklist/)为你的集群提供了一套建议的基本检查。"}
{"en": "Beyond that, securing your cluster means implementing effective\n[authentication](/docs/concepts/security/controlling-access/#authentication) and\n[authorization](/docs/concepts/security/controlling-access/#authorization) for API access. Use [ServiceAccounts](/docs/concepts/security/service-accounts/) to\nprovide and manage security identities for workloads and cluster\ncomponents.", "zh": "除此之外，加固集群还意味着对访问 API 实施有效的[身份认证](/zh-cn/docs/concepts/security/controlling-access/#authentication)和\n[鉴权](/zh-cn/docs/concepts/security/controlling-access/#authorization)。\n使用 [ServiceAccount](/zh-cn/docs/concepts/security/service-accounts/) \n为工作负载和集群组件提供和管理安全身份。"}
{"en": "Kubernetes uses TLS to protect API traffic; make sure to deploy the cluster using\nTLS (including for traffic between nodes and the control plane), and protect the\nencryption keys. If you use Kubernetes' own API for\n[CertificateSigningRequests](/docs/reference/access-authn-authz/certificate-signing-requests/#certificate-signing-requests),\npay special attention to restricting misuse there.", "zh": "Kubernetes 使用 TLS 保护 API 流量；确保在部署集群时采用了 TLS（包含工作节点和控制平面间的流量） 加密方式，\n并保护好加密密钥。如果使用 Kubernetes 自带的\n[证书签名请求](/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests/#certificate-signing-requests) API，\n特别注意不要滥用它们。"}
{"en": "### Runtime protection: compute {#protection-runtime-compute}", "zh": "### 运行阶段的防护：计算 {#protection-runtime-compute}"}
{"en": "{{< glossary_tooltip text=\"Containers\" term_id=\"container\" >}} provide two\nthings: isolation between different applications, and a mechanism to combine\nthose isolated applications to run on the same host computer. Those two\naspects, isolation and aggregation, mean that runtime security involves\nidentifying trade-offs and finding an appropriate balance.", "zh": "{{< glossary_tooltip text=\"容器\" term_id=\"container\" >}} 提供了两种功能：\n不同应用程序间的隔离，以及将这些隔离的应用程序合并运行到同一台主机的机制。\n隔离和聚合这两个方面意味着运行时安全需要权衡利弊，并找到合适的平衡点。"}
{"en": "Kubernetes relies on a {{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}\nto actually set up and run containers. The Kubernetes project does\nnot recommend a specific container runtime and you should make sure that\nthe runtime(s) that you choose meet your information security needs.", "zh": "Kubernetes 依赖{{< glossary_tooltip text=\"容器运行时\" term_id=\"container-runtime\" >}}\n来设置和运行容器。 Kubernetes 项目不会推荐特定的容器运行时，你应当确保\n你选用的运行时符合你的信息安全需要。"}
{"en": "To protect your compute at runtime, you can:", "zh": "要在运行时保护计算资源，你可以："}
{"en": "1. Enforce [Pod security standards](/docs/concepts/security/pod-security-standards/)\n   for applications, to help ensure they run with only the necessary privileges.", "zh": "1. 为应用程序强制采用 [Pod 安全性标准](/zh-cn/docs/concepts/security/pod-security-standards/)，\n   确保它们仅以所需权限运行。"}
{"en": "1. Run a specialized operating system on your nodes that is designed specifically\n   for running containerized workloads. This is typically based on a read-only\n   operating system (_immutable image_) that provides only the services\n   essential for running containers.", "zh": "2. 在你的节点上运行专门为运行容器化工作负载的而设计的专用操作系统。\n   它通常基于只读操作系统（**不可变镜像**），只提供运行容器所必须的服务。"}
{"en": "Container-specific operating systems help to isolate system components and\n   present a reduced attack surface in case of a container escape.", "zh": "容器化专用操作系统有助于隔离系统组件，并在容器逃逸时减少攻击面。"}
{"en": "1. Define [ResourceQuotas](/docs/concepts/policy/resource-quotas/) to\n   fairly allocate shared resources, and use\n   mechanisms such as [LimitRanges](/docs/concepts/policy/limit-range/)\n   to ensure that Pods specify their resource requirements.", "zh": "3. 定义 [ResourceQuota](/zh-cn/docs/concepts/policy/resource-quotas/)\n   以公平分配共享资源，并使用\n   [LimitRange](/zh-cn/docs/concepts/policy/limit-range/) 等机制\n   确保 Pod 定义了资源需求。"}
{"en": "1. Partition workloads across different nodes.\n   Use [node isolation](/docs/concepts/scheduling-eviction/assign-pod-node/#node-isolation-restriction)\n   mechanisms, either from Kubernetes itself or from the ecosystem, to ensure that\n   Pods with different trust contexts are run on separate sets of nodes.", "zh": "4. 划分工作负载到不同节点上。\n   使用来自 Kubernetes 本身或生态系统的\n   [节点隔离](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#node-isolation-restriction)机制，\n   以确保具有不同信任上下文的 Pod 在不同的节点上运行。"}
{"en": "1. Use a {{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}\n   that provides security restrictions.", "zh": "5. 使用提供安全限制的\n   {{< glossary_tooltip text=\"容器运行时\" term_id=\"container-runtime\" >}}。"}
{"en": "1. On Linux nodes, use a Linux security module such as [AppArmor](/docs/tutorials/security/apparmor/)\n   or [seccomp](/docs/tutorials/security/seccomp/).", "zh": "6. 在 Linux 节点上，使用 Linux 安全模式，例如 [AppArmor](/zh-cn/docs/tutorials/security/apparmor/)\n  或者 [seccomp](zh-cn/docs/tutorials/security/seccomp/)。"}
{"en": "### Runtime protection: storage {#protection-runtime-storage}", "zh": "### 运行阶段的防护：存储 {#protection-runtime-storage}"}
{"en": "To protect storage for your cluster and the applications that run there, you can:", "zh": "要保护你的集群和应用运行使用的存储，你可以："}
{"en": "1. Integrate your cluster with an external storage plugin that provides encryption at\n   rest for volumes.", "zh": "1. 为你的集群集成提供静态加密的外部存储插件。"}
{"en": "1. Enable [encryption at rest](/docs/tasks/administer-cluster/encrypt-data/) for\n   API objects.", "zh": "2. 为 API 对象启用[静态加密](/zh-cn/docs/tasks/administer-cluster/encrypt-data/)。"}
{"en": "1. Protect data durability using backups. Verify that you can restore these, whenever you need to.", "zh": "3. 使用备份保证数据的持久性。在需要的时候，验证备份数据的可恢复性。"}
{"en": "1. Authenticate connections between cluster nodes and any network storage they rely\n   upon.", "zh": "4. 集群节点和它们所依赖的任何网络存储都需要认证才能连接。"}
{"en": "1. Implement data encryption within your own application.", "zh": "5. 在你的应用程序中实现数据加密。"}
{"en": "For encryption keys, generating these within specialized hardware provides\nthe best protection against disclosure risks. A _hardware security module_\ncan let you perform cryptographic operations without allowing the security\nkey to be copied elsewhere.", "zh": "对于加密密钥来说，在专用硬件中生成这些密钥是防范泄密风险的最佳防护。\n**硬件安全模块**可以让你在不允许将安全密钥拷贝到其他地方的情况下执行加密操作。"}
{"en": "### Networking and security", "zh": "### 网络和安全 {#networking-and-security}"}
{"en": "You should also consider network security measures, such as\n[NetworkPolicy](/docs/concepts/services-networking/network-policies/) or a\n[service mesh](https://glossary.cncf.io/service-mesh/).\nSome network plugins for Kubernetes provide encryption for your\ncluster network, using technologies such as a virtual\nprivate network (VPN) overlay.\nBy design, Kubernetes lets you use your own networking plugin for your\ncluster (if you use managed Kubernetes, the person or organization\nmanaging your cluster may have chosen a network plugin for you).", "zh": "你也应当考虑网络安全措施，\n例如 [NetworkPolicy](/zh-cn/docs/concepts/services-networking/network-policies/) 或者\n[服务网格](https://glossary.cncf.io/zh-cn/service-mesh/)。\n一些 Kubernetes 的网络插件使用虚拟专用网络（VPN）叠加等技术，\n可以为集群网络提供加密功能。\n从设计上，Kubernetes 允许你在你的集群中使用自有网络插件（如果你使用托管 Kubernetes，\n集群管理员或组织可能会为你选择一个网络插件）。"}
{"en": "The network plugin you choose and the way you integrate it can have a\nstrong impact on the security of information in transit.", "zh": "你选用的网络插件和集成方式会对传输中的信息安全产生重大影响。"}
{"en": "### Observability and runtime security", "zh": "### 可观测性和运行时安全 {#Observability-and-runtime-security}"}
{"en": "Kubernetes lets you extend your cluster with extra tooling. You can set up third\nparty solutions to help you monitor or troubleshoot your applications and the\nclusters they are running. You also get some basic observability features built\nin to Kubernetes itself. Your code running in containers can generate logs,\npublish metrics or provide other observability data; at deploy time, you need to\nmake sure your cluster provides an appropriate level of protection there.", "zh": "Kubernetes 允许你使用外部工具扩展集群。你可以选择第三方解决方案\n帮助你监控或排查应用程序或运行集群的故障。\nKubernetes 自身还内置了一些基本的可观测性功能。\n运行在容器中的代码可以生成日志，暴露指标或提供其他的可观测数据；\n在部署时，你需要确保你的集群提供适当级别的安全保护。"}
{"en": "If you set up a metrics dashboard or something similar, review the chain of components\nthat populate data into that dashboard, as well as the dashboard itself. Make sure\nthat the whole chain is designed with enough resilience and enough integrity protection\nthat you can rely on it even during an incident where your cluster might be degraded.", "zh": "如果你配置了指标看板或其他类似的组件，审查暴露指标数据到看板的组件链路和看板本身。\n确保整个链路设计具有足够的弹性和足够的完整性保护，\n只有这样，即便是在集群降级导致的事件发生时，你也可以依赖它。"}
{"en": "Where appropriate, deploy security measures below the level of Kubernetes\nitself, such as cryptographically measured boot, or authenticated distribution\nof time (which helps ensure the fidelity of logs and audit records).", "zh": "在适当的情况下，在 Kubernetes 层之下部署一些安全举措，\n例如加密后启动或验证分发时间（有助于确保日志和审计记录的真实性）。"}
{"en": "For a high assurance environment, deploy cryptographic protections to ensure that\nlogs are both tamper-proof and confidential.", "zh": "对于高安全级别需求环境，部署加密保护措施，以确保日志防篡改和保密。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "### Cloud native security {#further-reading-cloud-native}", "zh": "### 云原生安全 {#further-reading-cloud-native}"}
{"en": "* CNCF [white paper](https://github.com/cncf/tag-security/blob/main/community/resources/security-whitepaper/v2/CNCF_cloud-native-security-whitepaper-May2022-v2.pdf)\n  on cloud native security.", "zh": "* CNCF 有关云原生安全的[白皮书](https://github.com/cncf/tag-security/blob/main/community/resources/security-whitepaper/v2/CNCF_cloud-native-security-whitepaper-May2022-v2.pdf)。"}
{"en": "* CNCF [white paper](https://github.com/cncf/tag-security/blob/f80844baaea22a358f5b20dca52cd6f72a32b066/supply-chain-security/supply-chain-security-paper/CNCF_SSCP_v1.pdf)\n  on good practices for securing a software supply chain.", "zh": "* CNCF 有关加固软件供应链的最佳实践[白皮书](https://github.com/cncf/tag-security/blob/f80844baaea22a358f5b20dca52cd6f72a32b066/supply-chain-security/supply-chain-security-paper/CNCF_SSCP_v1.pdf)。\n* [Fixing the Kubernetes clusterf\\*\\*k: Understanding security from the kernel up](https://archive.fosdem.org/2020/schedule/event/kubernetes/) (FOSDEM 2020)"}
{"en": "* [Kubernetes Security Best Practices](https://www.youtube.com/watch?v=wqsUfvRyYpw) (Kubernetes Forum Seoul 2019)", "zh": "* [Kubernetes 安全最佳实践](https://www.youtube.com/watch?v=wqsUfvRyYpw) (Kubernetes Forum Seoul 2019)\n* [Towards Measured Boot Out of the Box](https://www.youtube.com/watch?v=EzSkU3Oecuw) (Linux Security Summit 2016)"}
{"en": "### Kubernetes and information security {#further-reading-k8s}", "zh": "### Kubernetes 和信息安全 {#further-reading-k8s}"}
{"en": "* [Kubernetes security](/docs/concepts/security/)\n* [Securing your cluster](/docs/tasks/administer-cluster/securing-a-cluster/)\n* [Data encryption in transit](/docs/tasks/tls/managing-tls-in-a-cluster/) for the control plane\n* [Data encryption at rest](/docs/tasks/administer-cluster/encrypt-data/)\n* [Secrets in Kubernetes](/docs/concepts/configuration/secret/)\n* [Controlling Access to the Kubernetes API](/docs/concepts/security/controlling-access)\n* [Network policies](/docs/concepts/services-networking/network-policies/) for Pods\n* [Pod security standards](/docs/concepts/security/pod-security-standards/)\n* [RuntimeClasses](/docs/concepts/containers/runtime-class)", "zh": "* [安全](/zh-cn/docs/concepts/security/)\n* [保护集群](/zh-cn/docs/tasks/administer-cluster/securing-a-cluster/)\n* 针对控制平面[传输中的数据加密](/zh-cn/docs/tasks/tls/managing-tls-in-a-cluster/) \n* [静态加密机密数据](/zh-cn/docs/tasks/administer-cluster/encrypt-data/)\n* [Secrets](/zh-cn/docs/concepts/configuration/secret/)\n* [Kubernetes API 访问控制](/zh-cn/docs/concepts/security/controlling-access)\n* 针对 Pods 的[网络策略](/zh-cn/docs/concepts/services-networking/network-policies/) \n* [Pod 安全性标准](/zh-cn/docs/concepts/security/pod-security-standards/)\n* [容器运行时类](/zh-cn/docs/concepts/containers/runtime-class)"}
{"en": "This page introduces the ServiceAccount object in Kubernetes, providing\ninformation about how service accounts work, use cases, limitations,\nalternatives, and links to resources for additional guidance.", "zh": "本页介绍 Kubernetes 中的 ServiceAccount 对象，\n讲述服务账号的工作原理、使用场景、限制、替代方案，还提供了一些资源链接方便查阅更多指导信息。"}
{"en": "## What are service accounts? {#what-are-service-accounts}", "zh": "## 什么是服务账号？  {#what-are-service-accounts}"}
{"en": "A service account is a type of non-human account that, in Kubernetes, provides\na distinct identity in a Kubernetes cluster. Application Pods, system\ncomponents, and entities inside and outside the cluster can use a specific\nServiceAccount's credentials to identify as that ServiceAccount. This identity\nis useful in various situations, including authenticating to the API server or\nimplementing identity-based security policies.", "zh": "服务账号是在 Kubernetes 中一种用于非人类用户的账号，在 Kubernetes 集群中提供不同的身份标识。\n应用 Pod、系统组件以及集群内外的实体可以使用特定 ServiceAccount 的凭据来将自己标识为该 ServiceAccount。\n这种身份可用于许多场景，包括向 API 服务器进行身份认证或实现基于身份的安全策略。"}
{"en": "Service accounts exist as ServiceAccount objects in the API server. Service\naccounts have the following properties:", "zh": "服务账号以 ServiceAccount 对象的形式存在于 API 服务器中。服务账号具有以下属性："}
{"en": "* **Namespaced:** Each service account is bound to a Kubernetes\n  {{<glossary_tooltip text=\"namespace\" term_id=\"namespace\">}}. Every namespace\n  gets a [`default` ServiceAccount](#default-service-accounts) upon creation.\n\n* **Lightweight:** Service accounts exist in the cluster and are\n  defined in the Kubernetes API. You can quickly create service accounts to\n  enable specific tasks.", "zh": "* **名字空间限定：** 每个服务账号都与一个 Kubernetes 名字空间绑定。\n  每个名字空间在创建时，会获得一个[名为 `default` 的 ServiceAccount](#default-service-accounts)。\n\n* **轻量级：** 服务账号存在于集群中，并在 Kubernetes API 中定义。你可以快速创建服务账号以支持特定任务。"}
{"en": "* **Portable:** A configuration bundle for a complex containerized workload\n  might include service account definitions for the system's components. The\n  lightweight nature of service accounts and the namespaced identities make\n  the configurations portable.", "zh": "* **可移植性：** 复杂的容器化工作负载的配置包中可能包括针对系统组件的服务账号定义。\n  服务账号的轻量级性质和名字空间作用域的身份使得这类配置可移植。"}
{"en": "Service accounts are different from user accounts, which are authenticated\nhuman users in the cluster. By default, user accounts don't exist in the Kubernetes\nAPI server; instead, the API server treats user identities as opaque\ndata. You can authenticate as a user account using multiple methods. Some\nKubernetes distributions might add custom extension APIs to represent user\naccounts in the API server.", "zh": "服务账号与用户账号不同，用户账号是集群中通过了身份认证的人类用户。默认情况下，\n用户账号不存在于 Kubernetes API 服务器中；相反，API 服务器将用户身份视为不透明数据。\n你可以使用多种方法认证为某个用户账号。某些 Kubernetes 发行版可能会添加自定义扩展 API\n来在 API 服务器中表示用户账号。"}
{"en": "Comparison between service accounts and users", "zh": "{{< table caption=\"服务账号与用户之间的比较\" >}}"}
{"en": "| Description | ServiceAccount | User or group |\n| --- | --- | --- |\n| Location | Kubernetes API (ServiceAccount object) | External |\n| Access control | Kubernetes RBAC or other [authorization mechanisms](/docs/reference/access-authn-authz/authorization/#authorization-modules) | Kubernetes RBAC or other identity and access management mechanisms |\n| Intended use | Workloads, automation | People |", "zh": "| 描述 | 服务账号 | 用户或组 |\n| --- | --- | --- |\n| 位置 | Kubernetes API（ServiceAccount 对象）| 外部 |\n| 访问控制 | Kubernetes RBAC 或其他[鉴权机制](/zh-cn/docs/reference/access-authn-authz/authorization/#authorization-modules) | Kubernetes RBAC 或其他身份和访问管理机制 |\n| 目标用途 | 工作负载、自动化工具 | 人 |\n{{< /table >}}"}
{"en": "### Default service accounts {#default-service-accounts}", "zh": "### 默认服务账号 {#default-service-accounts}"}
{"en": "When you create a cluster, Kubernetes automatically creates a ServiceAccount\nobject named `default` for every namespace in your cluster. The `default`\nservice accounts in each namespace get no permissions by default other than the\n[default API discovery permissions](/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings)\nthat Kubernetes grants to all authenticated principals if role-based access control (RBAC) is enabled.\nIf you delete the `default` ServiceAccount object in a namespace, the\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}}\nreplaces it with a new one.", "zh": "在你创建集群时，Kubernetes 会自动为集群中的每个名字空间创建一个名为 `default` 的 ServiceAccount 对象。\n在启用了基于角色的访问控制（RBAC）时，Kubernetes 为所有通过了身份认证的主体赋予\n[默认 API 发现权限](/zh-cn/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings)。\n每个名字空间中的 `default` 服务账号除了这些权限之外，默认没有其他访问权限。\n如果基于角色的访问控制（RBAC）被启用，当你删除名字空间中的 `default` ServiceAccount 对象时，\n{{< glossary_tooltip text=\"控制平面\" term_id=\"control-plane\" >}}会用新的 ServiceAccount 对象替换它。"}
{"en": "If you deploy a Pod in a namespace, and you don't\n[manually assign a ServiceAccount to the Pod](#assign-to-pod), Kubernetes\nassigns the `default` ServiceAccount for that namespace to the Pod.", "zh": "如果你在某个名字空间中部署 Pod，并且你没有[手动为 Pod 指派 ServiceAccount](#assign-to-pod)，\nKubernetes 将该名字空间的 `default` 服务账号指派给这一 Pod。"}
{"en": "## Use cases for Kubernetes service accounts {#use-cases}\n\nAs a general guideline, you can use service accounts to provide identities in\nthe following scenarios:", "zh": "## Kubernetes 服务账号的使用场景   {#use-cases}\n\n一般而言，你可以在以下场景中使用服务账号来提供身份标识："}
{"en": "* Your Pods need to communicate with the Kubernetes API server, for example in\n  situations such as the following:\n  * Providing read-only access to sensitive information stored in Secrets.\n  * Granting [cross-namespace access](#cross-namespace), such as allowing a\n    Pod in namespace `example` to read, list, and watch for Lease objects in\n    the `kube-node-lease` namespace.", "zh": "* 你的 Pod 需要与 Kubernetes API 服务器通信，例如在以下场景中：\n  * 提供对存储在 Secret 中的敏感信息的只读访问。\n  * 授予[跨名字空间访问](#cross-namespace)的权限，例如允许 `example` 名字空间中的 Pod 读取、列举和监视\n    `kube-node-lease` 名字空间中的 Lease 对象。"}
{"en": "* Your Pods need to communicate with an external service. For example, a\n  workload Pod requires an identity for a commercially available cloud API,\n  and the commercial provider allows configuring a suitable trust relationship.\n* [Authenticating to a private image registry using an `imagePullSecret`](/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account).", "zh": "* 你的 Pod 需要与外部服务进行通信。例如，工作负载 Pod 需要一个身份来访问某商业化的云 API，\n  并且商业化 API 的提供商允许配置适当的信任关系。\n* [使用 `imagePullSecret` 完成在私有镜像仓库上的身份认证](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account)。"}
{"en": "* An external service needs to communicate with the Kubernetes API server. For\n  example, authenticating to the cluster as part of a CI/CD pipeline.\n* You use third-party security software in your cluster that relies on the\n  ServiceAccount identity of different Pods to group those Pods into different\n  contexts.", "zh": "* 外部服务需要与 Kubernetes API 服务器进行通信。例如，作为 CI/CD 流水线的一部分向集群作身份认证。\n* 你在集群中使用了第三方安全软件，该软件依赖不同 Pod 的 ServiceAccount 身份，按不同上下文对这些 Pod 分组。"}
{"en": "## How to use service accounts {#how-to-use}\n\nTo use a Kubernetes service account, you do the following:", "zh": "## 如何使用服务账号  {#how-to-use}\n\n要使用 Kubernetes 服务账号，你需要执行以下步骤："}
{"en": "1. Create a ServiceAccount object using a Kubernetes\n   client like `kubectl` or a manifest that defines the object.\n1. Grant permissions to the ServiceAccount object using an authorization\n   mechanism such as\n   [RBAC](/docs/reference/access-authn-authz/rbac/).", "zh": "1. 使用像 `kubectl` 这样的 Kubernetes 客户端或定义对象的清单（manifest）创建 ServiceAccount 对象。\n2. 使用鉴权机制（如 [RBAC](/zh-cn/docs/reference/access-authn-authz/rbac/)）为 ServiceAccount 对象授权。"}
{"en": "1. Assign the ServiceAccount object to Pods during Pod creation.\n\n   If you're using the identity from an external service,\n   [retrieve the ServiceAccount token](#get-a-token) and use it from that\n   service instead.", "zh": "3. 在创建 Pod 期间将 ServiceAccount 对象指派给 Pod。\n\n   如果你所使用的是来自外部服务的身份，可以[获取 ServiceAccount 令牌](#get-a-token)，并在该服务中使用这一令牌。"}
{"en": "For instructions, refer to\n[Configure Service Accounts for Pods](/docs/tasks/configure-pod-container/configure-service-account/).", "zh": "有关具体操作说明，参阅[为 Pod 配置服务账号](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/)。"}
{"en": "### Grant permissions to a ServiceAccount {#grant-permissions}", "zh": "### 为 ServiceAccount 授权   {#grant-permissions}"}
{"en": "You can use the built-in Kubernetes\n[role-based access control (RBAC)](/docs/reference/access-authn-authz/rbac/)\nmechanism to grant the minimum permissions required by each service account.\nYou create a *role*, which grants access, and then *bind* the role to your\nServiceAccount. RBAC lets you define a minimum set of permissions so that the\nservice account permissions follow the principle of least privilege. Pods that\nuse that service account don't get more permissions than are required to\nfunction correctly.", "zh": "你可以使用 Kubernetes 内置的\n[基于角色的访问控制 (RBAC)](/zh-cn/docs/reference/access-authn-authz/rbac/)机制来为每个服务账号授予所需的最低权限。\n你可以创建一个用来授权的**角色**，然后将此角色**绑定**到你的 ServiceAccount 上。\nRBAC 可以让你定义一组最低权限，使得服务账号权限遵循最小特权原则。\n这样使用服务账号的 Pod 不会获得超出其正常运行所需的权限。"}
{"en": "For instructions, refer to\n[ServiceAccount permissions](/docs/reference/access-authn-authz/rbac/#service-account-permissions).", "zh": "有关具体操作说明，参阅 [ServiceAccount 权限](/zh-cn/docs/reference/access-authn-authz/rbac/#service-account-permissions)。"}
{"en": "#### Cross-namespace access using a ServiceAccount {#cross-namespace}", "zh": "#### 使用 ServiceAccount 进行跨名字空间访问   {#cross-namespace}"}
{"en": "You can use RBAC to allow service accounts in one namespace to perform actions\non resources in a different namespace in the cluster. For example, consider a\nscenario where you have a service account and Pod in the `dev` namespace and\nyou want your Pod to see Jobs running in the `maintenance` namespace. You could\ncreate a Role object that grants permissions to list Job objects. Then,\nyou'd create a RoleBinding object in the `maintenance` namespace to bind the\nRole to the ServiceAccount object. Now, Pods in the `dev` namespace can list\nJob objects in the `maintenance` namespace using that service account.", "zh": "你可以使用 RBAC 允许一个名字空间中的服务账号对集群中另一个名字空间的资源执行操作。\n例如，假设你在 `dev` 名字空间中有一个服务账号和一个 Pod，并且希望该 Pod 可以查看 `maintenance`\n名字空间中正在运行的 Job。你可以创建一个 Role 对象来授予列举 Job 对象的权限。\n随后在 `maintenance` 名字空间中创建 RoleBinding 对象将 Role 绑定到此 ServiceAccount 对象上。\n现在，`dev` 名字空间中的 Pod 可以使用该服务账号列出 `maintenance` 名字空间中的 Job 对象集合。"}
{"en": "### Assign a ServiceAccount to a Pod {#assign-to-pod}\n\nTo assign a ServiceAccount to a Pod, you set the `spec.serviceAccountName`\nfield in the Pod specification. Kubernetes then automatically provides the\ncredentials for that ServiceAccount to the Pod. In v1.22 and later, Kubernetes\ngets a short-lived, **automatically rotating** token using the `TokenRequest`\nAPI and mounts the token as a\n[projected volume](/docs/concepts/storage/projected-volumes/#serviceaccounttoken).", "zh": "### 将 ServiceAccount 指派给 Pod   {#assign-to-pod}\n\n要将某 ServiceAccount 指派给某 Pod，你需要在该 Pod 的规约中设置 `spec.serviceAccountName` 字段。\nKubernetes 将自动为 Pod 提供该 ServiceAccount 的凭据。在 Kubernetes v1.22 及更高版本中，\nKubernetes 使用 `TokenRequest` API 获取一个短期的、**自动轮换**的令牌，\n并以[投射卷](/zh-cn/docs/concepts/storage/projected-volumes/#serviceaccounttoken)的形式挂载此令牌。"}
{"en": "By default, Kubernetes provides the Pod\nwith the credentials for an assigned ServiceAccount, whether that is the\n`default` ServiceAccount or a custom ServiceAccount that you specify.\n\nTo prevent Kubernetes from automatically injecting\ncredentials for a specified ServiceAccount or the `default` ServiceAccount, set the\n`automountServiceAccountToken` field in your Pod specification to `false`.", "zh": "默认情况下，Kubernetes 会将所指派的 ServiceAccount\n（无论是 `default` 服务账号还是你指定的定制 ServiceAccount）的凭据提供给 Pod。\n\n要防止 Kubernetes 自动注入指定的 ServiceAccount 或 `default` ServiceAccount 的凭据，\n可以将 Pod 规约中的 `automountServiceAccountToken` 字段设置为 `false`。"}
{"en": "In versions earlier than 1.22, Kubernetes provides a long-lived, static token\nto the Pod as a Secret.", "zh": "在 Kubernetes 1.22 之前的版本中，Kubernetes 会将一个长期有效的静态令牌以 Secret 形式提供给 Pod。"}
{"en": "#### Manually retrieve ServiceAccount credentials {#get-a-token}\n\nIf you need the credentials for a ServiceAccount to mount in a non-standard\nlocation, or for an audience that isn't the API server, use one of the\nfollowing methods:", "zh": "#### 手动获取 ServiceAccount 凭据   {#get-a-token}\n\n如果你需要 ServiceAccount 的凭据并将其挂载到非标准位置，或者用于 API 服务器之外的受众，可以使用以下方法之一："}
{"en": "* [TokenRequest API](/docs/reference/kubernetes-api/authentication-resources/token-request-v1/)\n  (recommended): Request a short-lived service account token from within\n  your own *application code*. The token expires automatically and can rotate\n  upon expiration.\n  If you have a legacy application that is not aware of Kubernetes, you\n  could use a sidecar container within the same pod to fetch these tokens\n  and make them available to the application workload.", "zh": "* [TokenRequest API](/zh-cn/docs/reference/kubernetes-api/authentication-resources/token-request-v1/)（推荐）：\n  在你自己的**应用代码**中请求一个短期的服务账号令牌。此令牌会自动过期，并可在过期时被轮换。\n  如果你有一个旧的、对 Kubernetes 无感知能力的应用，你可以在同一个 Pod\n  内使用边车容器来获取这些令牌，并将其提供给应用工作负载。"}
{"en": "* [Token Volume Projection](/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection)\n  (also recommended): In Kubernetes v1.20 and later, use the Pod specification to\n  tell the kubelet to add the service account token to the Pod as a\n  *projected volume*. Projected tokens expire automatically, and the kubelet\n  rotates the token before it expires.", "zh": "* [令牌卷投射](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection)（同样推荐）：\n  在 Kubernetes v1.20 及更高版本中，使用 Pod 规约告知 kubelet 将服务账号令牌作为**投射卷**添加到 Pod 中。\n  所投射的令牌会自动过期，在过期之前 kubelet 会自动轮换此令牌。"}
{"en": "* [Service Account Token Secrets](/docs/tasks/configure-pod-container/configure-service-account/#manually-create-an-api-token-for-a-serviceaccount)\n  (not recommended): You can mount service account tokens as Kubernetes\n  Secrets in Pods. These tokens don't expire and don't rotate. In versions prior to v1.24, a permanent token was automatically created for each service account.\n  This method is not recommended anymore, especially at scale, because of the risks associated\n  with static, long-lived credentials. The [LegacyServiceAccountTokenNoAutoGeneration feature gate](/docs/reference/command-line-tools-reference/feature-gates-removed)\n  (which was enabled by default from Kubernetes v1.24 to v1.26),  prevented Kubernetes from automatically creating these tokens for\n  ServiceAccounts. The feature gate is removed in v1.27, because it was elevated to GA status; you can still create indefinite service account tokens manually, but should take into account the security implications.", "zh": "* [服务账号令牌 Secret](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#manually-create-an-api-token-for-a-serviceaccount)（不推荐）：\n  你可以将服务账号令牌以 Kubernetes Secret 的形式挂载到 Pod 中。这些令牌不会过期且不会轮换。\n  在 v1.24 版本之前，系统会为每个服务账户自动创建一个永久令牌。此方法已不再被推荐，\n  尤其是在大规模应用时，因为使用静态、长期有效的凭证存在风险。\n  [LegacyServiceAccountTokenNoAutoGeneration 特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates-removed)\n  （从 Kubernetes v1.24 至 v1.26 默认启用），阻止 Kubernetes 自动为 ServiceAccount 创建这些令牌。\n  此特性门控在 v1.27 版本中被移除，因为此特性已升级为正式发布（GA）状态；\n  你仍然可以手动为 ServiceAccount 创建无限期的服务账户令牌，但应考虑到安全影响。\n\n{{< note >}}"}
{"en": "For applications running outside your Kubernetes cluster, you might be considering\ncreating a long-lived ServiceAccount token that is stored in a Secret. This allows authentication, but the Kubernetes project recommends you avoid this approach.\nLong-lived bearer tokens represent a security risk as, once disclosed, the token\ncan be misused. Instead, consider using an alternative. For example, your external\napplication can authenticate using a well-protected private key `and` a certificate,\nor using a custom mechanism such as an [authentication webhook](/docs/reference/access-authn-authz/authentication/#webhook-token-authentication) that you implement yourself.", "zh": "对于运行在 Kubernetes 集群外的应用，你可能考虑创建一个长期有效的 ServiceAccount 令牌，\n并将其存储在 Secret 中。尽管这种方式可以实现身份认证，但 Kubernetes 项目建议你避免使用此方法。\n长期有效的持有者令牌（Bearer Token）会带来安全风险，一旦泄露，此令牌就可能被滥用。\n为此，你可以考虑使用其他替代方案。例如，你的外部应用可以使用一个保护得很好的私钥和证书进行身份认证，\n或者使用你自己实现的[身份认证 Webhook](/zh-cn/docs/reference/access-authn-authz/authentication/#webhook-token-authentication)\n这类自定义机制。"}
{"en": "You can also use TokenRequest to obtain short-lived tokens for your external application.", "zh": "你还可以使用 TokenRequest 为外部应用获取短期的令牌。\n{{< /note >}}"}
{"en": "### Restricting access to Secrets {#enforce-mountable-secrets}", "zh": "### 限制对 Secret 的访问   {#enforce-mountable-secrets}"}
{"en": "Kubernetes provides an annotation called `kubernetes.io/enforce-mountable-secrets`\nthat you can add to your ServiceAccounts. When this annotation is applied,\nthe ServiceAccount's secrets can only be mounted on specified types of resources,\nenhancing the security posture of your cluster.\n\nYou can add the annotation to a ServiceAccount using a manifest:", "zh": "Kubernetes 提供了名为 `kubernetes.io/enforce-mountable-secrets` 的注解，\n你可以添加到你的 ServiceAccount 中。当应用了这个注解后，\nServiceAccount 的 Secret 只能挂载到特定类型的资源上，从而增强集群的安全性。\n\n你可以使用以下清单将注解添加到一个 ServiceAccount 中：\n\n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  annotations:\n    kubernetes.io/enforce-mountable-secrets: \"true\"\n  name: my-serviceaccount\n  namespace: my-namespace\n```"}
{"en": "When this annotation is set to \"true\", the Kubernetes control plane ensures that\nthe Secrets from this ServiceAccount are subject to certain mounting restrictions.", "zh": "当此注解设置为 \"true\" 时，Kubernetes 控制平面确保来自该 ServiceAccount 的 Secret 受到特定挂载限制。"}
{"en": "1. The name of each Secret that is mounted as a volume in a Pod must appear in the `secrets` field of the\n   Pod's ServiceAccount.", "zh": "1. 在 Pod 中作为卷挂载的每个 Secret 的名称必须列在该 Pod 中 ServiceAccount 的 `secrets` 字段中。"}
{"en": "1. The name of each Secret referenced using `envFrom` in a Pod must also appear in the `secrets`\n   field of the Pod's ServiceAccount.", "zh": "2. 在 Pod 中使用 `envFrom` 引用的每个 Secret 的名称也必须列在该 Pod 中 ServiceAccount 的 `secrets` 字段中。"}
{"en": "1. The name of each Secret referenced using `imagePullSecrets` in a Pod must also appear in the `secrets`\n   field of the Pod's ServiceAccount.", "zh": "3. 在 Pod 中使用 `imagePullSecrets` 引用的每个 Secret 的名称也必须列在该 Pod 中\n   ServiceAccount 的 `secrets` 字段中。"}
{"en": "By understanding and enforcing these restrictions, cluster administrators can maintain a tighter security profile and ensure that secrets are accessed only by the appropriate resources.", "zh": "通过理解并执行这些限制，集群管理员可以维护更严格的安全配置，并确保 Secret 仅被适当的资源访问。"}
{"en": "## Authenticating service account credentials {#authenticating-credentials}", "zh": "## 对服务账号凭据进行鉴别   {#authenticating-credentials}"}
{"en": "ServiceAccounts use signed\n{{<glossary_tooltip term_id=\"jwt\" text=\"JSON Web Tokens\">}}  (JWTs)\nto authenticate to the Kubernetes API server, and to any other system where a\ntrust relationship exists. Depending on how the token was issued\n(either time-limited using a `TokenRequest` or using a legacy mechanism with\na Secret), a ServiceAccount token might also have an expiry time, an audience,\nand a time after which the token *starts* being valid. When a client that is\nacting as a ServiceAccount tries to communicate with the Kubernetes API server,\nthe client includes an `Authorization: Bearer <token>` header with the HTTP\nrequest. The API server checks the validity of that bearer token as follows:", "zh": "ServiceAccount 使用签名的 JSON Web Token (JWT) 来向 Kubernetes API\n服务器以及任何其他存在信任关系的系统进行身份认证。根据令牌的签发方式\n（使用 `TokenRequest` 限制时间或使用传统的 Secret 机制），ServiceAccount\n令牌也可能有到期时间、受众和令牌**开始**生效的时间点。\n当客户端以 ServiceAccount 的身份尝试与 Kubernetes API 服务器通信时，\n客户端会在 HTTP 请求中包含 `Authorization: Bearer <token>` 标头。\nAPI 服务器按照以下方式检查该持有者令牌的有效性："}
{"en": "1. Checks the token signature.\n1. Checks whether the token has expired.\n1. Checks whether object references in the token claims are currently valid.\n1. Checks whether the token is currently valid.\n1. Checks the audience claims.", "zh": "1. 检查令牌签名。\n1. 检查令牌是否已过期。\n1. 检查令牌申明中的对象引用是否当前有效。\n1. 检查令牌是否当前有效。\n1. 检查受众申明。"}
{"en": "The TokenRequest API produces _bound tokens_ for a ServiceAccount. This\nbinding is linked to the lifetime of the client, such as a Pod, that is acting\nas that ServiceAccount.  See [Token Volume Projection](/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection)\nfor an example of a bound pod service account token's JWT schema and payload.\n\nFor tokens issued using the `TokenRequest` API, the API server also checks that\nthe specific object reference that is using the ServiceAccount still exists,\nmatching by the {{< glossary_tooltip term_id=\"uid\" text=\"unique ID\" >}} of that\nobject. For legacy tokens that are mounted as Secrets in Pods, the API server\nchecks the token against the Secret.", "zh": "TokenRequest API 为 ServiceAccount 生成**绑定令牌**。这种绑定与以该 ServiceAccount\n身份运行的客户端（如 Pod）的生命期相关联。有关绑定 Pod 服务账号令牌的 JWT 模式和载荷的示例，\n请参阅[服务账号令牌卷投射](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection)。\n\n对于使用 `TokenRequest` API 签发的令牌，API 服务器还会检查正在使用 ServiceAccount 的特定对象引用是否仍然存在，\n方式是通过该对象的{{< glossary_tooltip term_id=\"uid\" text=\"唯一 ID\" >}} 进行匹配。\n对于以 Secret 形式挂载到 Pod 中的旧有令牌，API 服务器会基于 Secret 来检查令牌。"}
{"en": "For more information about the authentication process, refer to\n[Authentication](/docs/reference/access-authn-authz/authentication/#service-account-tokens).", "zh": "有关身份认证过程的更多信息，参考[身份认证](/zh-cn/docs/reference/access-authn-authz/authentication/#service-account-tokens)。"}
{"en": "### Authenticating service account credentials in your own code {#authenticating-in-code}\n\nIf you have services of your own that need to validate Kubernetes service\naccount credentials, you can use the following methods:", "zh": "### 在自己的代码中检查服务账号凭据   {#authenticating-in-code}\n\n如果你的服务需要检查 Kubernetes 服务账号凭据，可以使用以下方法："}
{"en": "* [TokenReview API](/docs/reference/kubernetes-api/authentication-resources/token-review-v1/)\n  (recommended)\n* OIDC discovery", "zh": "* [TokenReview API](/zh-cn/docs/reference/kubernetes-api/authentication-resources/token-review-v1/)（推荐）\n* OIDC 发现"}
{"en": "The Kubernetes project recommends that you use the TokenReview API, because\nthis method invalidates tokens that are bound to API objects such as Secrets,\nServiceAccounts, Pods or Nodes when those objects are deleted. For example, if you\ndelete the Pod that contains a projected ServiceAccount token, the cluster\ninvalidates that token immediately and a TokenReview immediately fails.\nIf you use OIDC validation instead, your clients continue to treat the token\nas valid until the token reaches its expiration timestamp.", "zh": "Kubernetes 项目建议你使用 TokenReview API，因为当你删除某些 API 对象\n（如 Secret、ServiceAccount、Pod 和 Node）的时候，此方法将使绑定到这些 API 对象上的令牌失效。\n例如，如果删除包含投射 ServiceAccount 令牌的 Pod，则集群立即使该令牌失效，\n并且 TokenReview 操作也会立即失败。\n如果你使用的是 OIDC 验证，则客户端将继续将令牌视为有效，直到令牌达到其到期时间戳。"}
{"en": "Your application should always define the audience that it accepts, and should\ncheck that the token's audiences match the audiences that the application\nexpects. This helps to minimize the scope of the token so that it can only be\nused in your application and nowhere else.", "zh": "你的应用应始终定义其所接受的受众，并检查令牌的受众是否与应用期望的受众匹配。\n这有助于将令牌的作用域最小化，这样它只能在你的应用内部使用，而不能在其他地方使用。"}
{"en": "## Alternatives\n\n* Issue your own tokens using another mechanism, and then use\n  [Webhook Token Authentication](/docs/reference/access-authn-authz/authentication/#webhook-token-authentication)\n  to validate bearer tokens using your own validation service.", "zh": "## 替代方案   {#alternatives}\n\n* 使用其他机制签发你自己的令牌，然后使用\n  [Webhook 令牌身份认证](/zh-cn/docs/reference/access-authn-authz/authentication/#webhook-token-authentication)通过你自己的验证服务来验证持有者令牌。"}
{"en": "* Provide your own identities to Pods.\n  * [Use the SPIFFE CSI driver plugin to provide SPIFFE SVIDs as X.509 certificate pairs to Pods](https://cert-manager.io/docs/projects/csi-driver-spiffe/).\n    {{% thirdparty-content single=\"true\" %}}\n  * [Use a service mesh such as Istio to provide certificates to Pods](https://istio.io/latest/docs/tasks/security/cert-management/plugin-ca-cert/).", "zh": "* 为 Pod 提供你自己的身份：\n  * [使用 SPIFFE CSI 驱动插件将 SPIFFE SVID 作为 X.509 证书对提供给 Pod](https://cert-manager.io/docs/projects/csi-driver-spiffe/)。\n    {{% thirdparty-content single=\"true\" %}}\n  * [使用 Istio 这类服务网格为 Pod 提供证书](https://istio.io/latest/zh/docs/tasks/security/cert-management/plugin-ca-cert/)。"}
{"en": "* Authenticate from outside the cluster to the API server without using service account tokens:\n  * [Configure the API server to accept OpenID Connect (OIDC) tokens from your identity provider](/docs/reference/access-authn-authz/authentication/#openid-connect-tokens).\n  * Use service accounts or user accounts created using an external Identity\n    and Access Management (IAM) service, such as from a cloud provider, to\n    authenticate to your cluster.\n  * [Use the CertificateSigningRequest API with client certificates](/docs/tasks/tls/managing-tls-in-a-cluster/).", "zh": "* 从集群外部向 API 服务器进行身份认证，而不使用服务账号令牌：\n  * [配置 API 服务器接受来自你自己的身份驱动的 OpenID Connect (OIDC) 令牌](/zh-cn/docs/reference/access-authn-authz/authentication/#openid-connect-tokens)。\n  * 使用来自云提供商等外部身份和访问管理 (IAM) 服务创建的服务账号或用户账号向集群进行身份认证。\n  * [使用 CertificateSigningRequest API 和客户端证书](/zh-cn/docs/tasks/tls/managing-tls-in-a-cluster/)。"}
{"en": "* [Configure the kubelet to retrieve credentials from an image registry](/docs/tasks/administer-cluster/kubelet-credential-provider/).\n* Use a Device Plugin to access a virtual Trusted Platform Module (TPM), which\n  then allows authentication using a private key.", "zh": "* [配置 kubelet 从镜像仓库中获取凭据](/zh-cn/docs/tasks/administer-cluster/kubelet-credential-provider/)。\n* 使用设备插件访问虚拟的可信平台模块 (TPM)，进而可以使用私钥进行身份认证。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn how to [manage your ServiceAccounts as a cluster administrator](/docs/reference/access-authn-authz/service-accounts-admin/).\n* Learn how to [assign a ServiceAccount to a Pod](/docs/tasks/configure-pod-container/configure-service-account/).\n* Read the [ServiceAccount API reference](/docs/reference/kubernetes-api/authentication-resources/service-account-v1/).", "zh": "* 学习如何[作为集群管理员管理你的 ServiceAccount](/zh-cn/docs/reference/access-authn-authz/service-accounts-admin/)。\n* 学习如何[将 ServiceAccount 指派给 Pod](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/)。\n* 阅读 [ServiceAccount API 参考文档](/zh-cn/docs/reference/kubernetes-api/authentication-resources/service-account-v1/)。"}
{"en": "overview", "zh": "{{<glossary_definition prepend=\"在 Kubernetes 中，Secret 是这样一个对象：\"\nterm_id=\"secret\" length=\"all\">}}"}
{"en": "The following good practices are intended for both cluster administrators and\napplication developers. Use these guidelines to improve the security of your\nsensitive information in Secret objects, as well as to more effectively manage\nyour Secrets.", "zh": "以下良好实践适用于集群管理员和应用开发者。遵从这些指导方针有助于提高 Secret\n对象中敏感信息的安全性，还可以更有效地管理你的 Secret。"}
{"en": "## Cluster administrators\n\nThis section provides good practices that cluster administrators can use to\nimprove the security of confidential information in the cluster.", "zh": "## 集群管理员   {#cluster-administrators}\n\n本节提供了集群管理员可用于提高集群中机密信息安全性的良好实践。"}
{"en": "### Configure encryption at rest\n\nBy default, Secret objects are stored unencrypted in {{<glossary_tooltip\nterm_id=\"etcd\" text=\"etcd\">}}. You should configure encryption of your Secret\ndata in `etcd`. For instructions, refer to\n[Encrypt Secret Data at Rest](/docs/tasks/administer-cluster/encrypt-data/).", "zh": "### 配置静态加密   {#configure-encryption-at-rest}\n\n默认情况下，Secret 对象以非加密的形式存储在 {{<glossary_tooltip term_id=\"etcd\" text=\"etcd\">}} 中。\n你配置对在 `etcd` 中存储的 Secret 数据进行加密。相关的指导信息，\n请参阅[静态加密 Secret 数据](/zh-cn/docs/tasks/administer-cluster/encrypt-data/)。"}
{"en": "### Configure least-privilege access to Secrets {#least-privilege-secrets}\n\nWhen planning your access control mechanism, such as Kubernetes\n{{<glossary_tooltip term_id=\"rbac\" text=\"Role-based Access Control\">}} [(RBAC)](/docs/reference/access-authn-authz/rbac/),\nconsider the following guidelines for access to `Secret` objects. You should\nalso follow the other guidelines in\n[RBAC good practices](/docs/concepts/security/rbac-good-practices).", "zh": "### 配置 Secret 资源的最小特权访问   {#least-privilege-secrets}\n\n当规划诸如 Kubernetes\n{{<glossary_tooltip term_id=\"rbac\" text=\"基于角色的访问控制\">}} [(RBAC)](/zh-cn/docs/reference/access-authn-authz/rbac/)\n这类访问控制机制时，需要注意访问 `Secret` 对象的以下指导信息。\n你还应遵从 [RBAC 良好实践](/zh-cn/docs/concepts/security/rbac-good-practices)中的其他指导信息。"}
{"en": "- **Components**: Restrict `watch` or `list` access to only the most\n  privileged, system-level components. Only grant `get` access for Secrets if\n  the component's normal behavior requires it.\n- **Humans**: Restrict `get`, `watch`, or `list` access to Secrets. Only allow\n  cluster administrators to access `etcd`. This includes read-only access. For\n  more complex access control, such as restricting access to Secrets with\n  specific annotations, consider using third-party authorization mechanisms.", "zh": "- **组件**：限制仅最高特权的系统级组件可以执行 `watch` 或 `list` 访问。\n  仅在组件的正常行为需要时才授予对 Secret 的 `get` 访问权限。\n- **人员**：限制对 Secret 的 `get`、`watch` 或 `list` 访问权限。仅允许集群管理员访问 `etcd`。\n  这包括只读访问。对于更复杂的访问控制，例如使用特定注解限制对 Secret 的访问，请考虑使用第三方鉴权机制。\n\n{{< caution >}}"}
{"en": "Granting `list` access to Secrets implicitly lets the subject fetch the\ncontents of the Secrets.", "zh": "授予对 Secret 的 `list` 访问权限将意味着允许对应主体获取 Secret 的内容。\n{{< /caution >}}"}
{"en": "A user who can create a Pod that uses a Secret can also see the value of that\nSecret. Even if cluster policies do not allow a user to read the Secret\ndirectly, the same user could have access to run a Pod that then exposes the\nSecret. You can detect or limit the impact caused by Secret data being exposed,\neither intentionally or unintentionally, by a user with this access. Some\nrecommendations include:", "zh": "如果一个用户可以创建使用某 Secret 的 Pod，则该用户也可以看到该 Secret 的值。\n即使集群策略不允许用户直接读取 Secret，同一用户也可能有权限运行 Pod 进而暴露该 Secret。\n你可以检测或限制具有此访问权限的用户有意或无意地暴露 Secret 数据所造成的影响。\n这里有一些建议："}
{"en": "*  Use short-lived Secrets\n*  Implement audit rules that alert on specific events, such as concurrent\n   reading of multiple Secrets by a single user", "zh": "* 使用生命期短暂的 Secret\n* 实现对特定事件发出警报的审计规则，例如同一用户并发读取多个 Secret 时发出警报"}
{"en": "#### Additional ServiceAccount annotations for Secret management\n\nYou can also use the `kubernetes.io/enforce-mountable-secrets` annotation on\na ServiceAccount to enforce specific rules on how Secrets are used in a Pod.\nFor more details, see the [documentation on this annotation](/docs/reference/labels-annotations-taints/#enforce-mountable-secrets).", "zh": "#### 用于 Secret 管理的附加 ServiceAccount 注解\n\n你还可以在 ServiceAccount 上使用 `kubernetes.io/enforce-mountable-secrets`\n注解来强制执行有关如何在 Pod 中使用 Secret 的特定规则。\n\n更多详细信息，请参阅[有关此注解的文档](/zh-cn/docs/reference/labels-annotations-taints/#enforce-mountable-secrets)。"}
{"en": "### Improve etcd management policies\n\nConsider wiping or shredding the durable storage used by `etcd` once it is\nno longer in use.\n\nIf there are multiple `etcd` instances, configure encrypted SSL/TLS\ncommunication between the instances to protect the Secret data in transit.", "zh": "### 改进 etcd 管理策略   {#improve-etcd-management-policies}\n\n不再使用 `etcd` 所使用的持久存储时，考虑擦除或粉碎这些数据。\n\n如果存在多个 `etcd` 实例，则在实例之间配置加密的 SSL/TLS 通信以保护传输中的 Secret 数据。"}
{"en": "### Configure access to external Secrets", "zh": "### 配置对外部 Secret 的访问权限   {#configure-access-to-external-secrets}\n\n{{% thirdparty-content %}}"}
{"en": "You can use third-party Secrets store providers to keep your confidential data\noutside your cluster and then configure Pods to access that information.\nThe [Kubernetes Secrets Store CSI Driver](https://secrets-store-csi-driver.sigs.k8s.io/)\nis a DaemonSet that lets the kubelet retrieve Secrets from external stores, and\nmount the Secrets as a volume into specific Pods that you authorize to access\nthe data.", "zh": "你可以使用第三方 Secret 存储提供商将机密数据保存在你的集群之外，然后配置 Pod 访问该信息。\n[Kubernetes Secret 存储 CSI 驱动](https://secrets-store-csi-driver.sigs.k8s.io/)是一个 DaemonSet，\n它允许 kubelet 从外部存储中检索 Secret，并将 Secret 作为卷挂载到特定的、你授权访问数据的 Pod。"}
{"en": "For a list of supported providers, refer to\n[Providers for the Secret Store CSI Driver](https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver).", "zh": "有关支持的提供商列表，请参阅\n[Secret 存储 CSI 驱动的提供商](https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver)。"}
{"en": "## Developers\n\nThis section provides good practices for developers to use to improve the\nsecurity of confidential data when building and deploying Kubernetes resources.", "zh": "## 开发者   {#developers}\n\n本节为开发者提供了构建和部署 Kubernetes 资源时用于改进机密数据安全性的良好实践。"}
{"en": "### Restrict Secret access to specific containers\n\nIf you are defining multiple containers in a Pod, and only one of those\ncontainers needs access to a Secret, define the volume mount or environment\nvariable configuration so that the other containers do not have access to that\nSecret.", "zh": "### 限制特定容器集合才能访问 Secret     {#restrict-secret-access-to-specific-containers}\n\n如果你在一个 Pod 中定义了多个容器，且仅其中一个容器需要访问 Secret，则可以定义卷挂载或环境变量配置，\n这样其他容器就不会有访问该 Secret 的权限。"}
{"en": "### Protect Secret data after reading\n\nApplications still need to protect the value of confidential information after\nreading it from an environment variable or volume. For example, your\napplication must avoid logging the secret data in the clear or transmitting it\nto an untrusted party.", "zh": "### 读取后保护 Secret 数据   {#protect-secret-data-after-reading}\n\n应用程序从一个环境变量或一个卷读取机密信息的值后仍然需要保护这些值。\n例如，你的应用程序必须避免以明文记录 Secret 数据，还必须避免将这些数据传输给不受信任的一方。"}
{"en": "### Avoid sharing Secret manifests\n\nIf you configure a Secret through a\n{{< glossary_tooltip text=\"manifest\" term_id=\"manifest\" >}}, with the secret\ndata encoded as base64, sharing this file or checking it in to a source\nrepository means the secret is available to everyone who can read the manifest.", "zh": "### 避免共享 Secret 清单   {#avoid-shareing-secret-manifests}\n\n如果你通过{{< glossary_tooltip text=\"清单（Manifest）\" term_id=\"manifest\" >}}配置 Secret，\n同时将该 Secret 数据编码为 base64，\n那么共享此文件或将其检入一个源代码仓库就意味着有权读取该清单的所有人都能使用该 Secret。\n\n{{< caution >}}"}
{"en": "Base64 encoding is _not_ an encryption method, it provides no additional\nconfidentiality over plain text.", "zh": "Base64 编码**不是**一种加密方法，它没有为纯文本提供额外的保密机制。\n{{< /caution >}}"}
{"en": "The Kubernetes API server is the main point of entry to a cluster for external parties\n(users and services) interacting with it.", "zh": "Kubernetes API 服务器是外部（用户和服务）与集群交互的主要入口。"}
{"en": "As part of this role, the API server has several key built-in security controls, such as\naudit logging and {{< glossary_tooltip text=\"admission controllers\" term_id=\"admission-controller\" >}}.\nHowever, there are ways to modify the configuration\nor content of the cluster that bypass these controls.", "zh": "API 服务器作为交互的主要入口，还提供了几种关键的内置安全控制，\n例如审计日志和{{< glossary_tooltip text=\"准入控制器\" term_id=\"admission-controller\" >}}。\n但有一些方式可以绕过这些安全控制从而修改集群的配置或内容。"}
{"en": "This page describes the ways in which the security controls built into the\nKubernetes API server can be bypassed, so that cluster operators\nand security architects can ensure that these bypasses are appropriately restricted.", "zh": "本页描述了绕过 Kubernetes API 服务器中内置安全控制的几种方式，\n以便集群运维人员和安全架构师可以确保这些绕过方式被适当地限制。"}
{"en": "## Static Pods {#static-pods}", "zh": "## 静态 Pod {#static-pods}"}
{"en": "The {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} on each node loads and\ndirectly manages any manifests that are stored in a named directory or fetched from\na specific URL as [*static Pods*](/docs/tasks/configure-pod-container/static-pod) in\nyour cluster. The API server doesn't manage these static Pods. An attacker with write\naccess to this location could modify the configuration of static pods loaded from that\nsource, or could introduce new static Pods.", "zh": "每个节点上的 {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}}\n会加载并直接管理集群中存储在指定目录中或从特定 URL\n获取的[**静态 Pod**](/zh-cn/docs/tasks/configure-pod-container/static-pod) 清单。\nAPI 服务器不管理这些静态 Pod。对该位置具有写入权限的攻击者可以修改从该位置加载的静态 Pod 的配置，或引入新的静态 Pod。"}
{"en": "Static Pods are restricted from accessing other objects in the Kubernetes API. For example,\nyou can't configure a static Pod to mount a Secret from the cluster. However, these Pods can\ntake other security sensitive actions, such as using `hostPath` mounts from the underlying\nnode.", "zh": "静态 Pod 被限制访问 Kubernetes API 中的其他对象。例如，你不能将静态 Pod 配置为从集群挂载 Secret。\n但是，这些 Pod 可以执行其他安全敏感的操作，例如挂载来自下层节点的 `hostPath` 卷。"}
{"en": "By default, the kubelet creates a {{< glossary_tooltip text=\"mirror pod\" term_id=\"mirror-pod\">}}\nso that the static Pods are visible in the Kubernetes API. However, if the attacker uses an invalid\nnamespace name when creating the Pod, it will not be visible in the Kubernetes API and can only\nbe discovered by tooling that has access to the affected host(s).", "zh": "默认情况下，kubelet 会创建一个{{< glossary_tooltip text=\"镜像 Pod（Mirror Pod）\" term_id=\"mirror-pod\">}}，\n以便静态 Pod 在 Kubernetes API 中可见。但是，如果攻击者在创建 Pod 时使用了无效的名字空间名称，\n则该 Pod 将在 Kubernetes API 中不可见，只能通过对受影响主机有访问权限的工具发现。"}
{"en": "If a static Pod fails admission control, the kubelet won't register the Pod with the\nAPI server. However, the Pod still runs on the node. For more information, refer to\n[kubeadm issue #1541](https://github.com/kubernetes/kubeadm/issues/1541#issuecomment-487331701).", "zh": "如果静态 Pod 无法通过准入控制，kubelet 不会将 Pod 注册到 API 服务器。但该 Pod 仍然在节点上运行。\n有关更多信息，请参阅 [kubeadm issue #1541](https://github.com/kubernetes/kubeadm/issues/1541#issuecomment-487331701)。"}
{"en": "### Mitigations {#static-pods-mitigations}", "zh": "### 缓解措施 {#static-pods-mitigations}"}
{"en": "- Only [enable the kubelet static Pod manifest functionality](/docs/tasks/configure-pod-container/static-pod/#static-pod-creation)\n  if required by the node.\n- If a node uses the static Pod functionality, restrict filesystem access to the static Pod manifest directory\n  or URL to users who need the access.\n- Restrict access to kubelet configuration parameters and files to prevent an attacker setting\n  a static Pod path or URL.\n- Regularly audit and centrally report all access to directories or web storage locations that host\n  static Pod manifests and kubelet configuration files.", "zh": "- 仅在节点需要时[启用 kubelet 静态 Pod 清单功能](/zh-cn/docs/tasks/configure-pod-container/static-pod/#static-pod-creation)。\n- 如果节点使用静态 Pod 功能，请将对静态 Pod 清单目录或 URL 的文件系统的访问权限限制为需要访问的用户。\n- 限制对 kubelet 配置参数和文件的访问，以防止攻击者设置静态 Pod 路径或 URL。\n- 定期审计并集中报告所有对托管静态 Pod 清单和 kubelet 配置文件的目录或 Web 存储位置的访问。"}
{"en": "## The kubelet API {#kubelet-api}", "zh": "## kubelet API {#kubelet-api}"}
{"en": "The kubelet provides an HTTP API that is typically exposed on TCP port 10250 on cluster\nworker nodes. The API might also be exposed on control plane nodes depending on the Kubernetes\ndistribution in use. Direct access to the API allows for disclosure of information about\nthe pods running on a node, the logs from those pods, and execution of commands in\nevery container running on the node.", "zh": "kubelet 提供了一个 HTTP API，通常暴露在集群工作节点上的 TCP 端口 10250 上。\n在某些 Kubernetes 发行版中，API 也可能暴露在控制平面节点上。\n对 API 的直接访问允许公开有关运行在节点上的 Pod、这些 Pod 的日志以及在节点上运行的每个容器中执行命令的信息。"}
{"en": "When Kubernetes cluster users have RBAC access to `Node` object sub-resources, that access\nserves as authorization to interact with the kubelet API. The exact access depends on\nwhich sub-resource access has been granted, as detailed in\n[kubelet authorization](/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authorization).", "zh": "当 Kubernetes 集群用户具有对 `Node` 对象子资源 RBAC 访问权限时，该访问权限可用作与 kubelet API 交互的授权。\n实际的访问权限取决于授予了哪些子资源访问权限，详见\n[kubelet 鉴权](/zh-cn/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authorization)。"}
{"en": "Direct access to the kubelet API is not subject to admission control and is not logged\nby Kubernetes audit logging. An attacker with direct access to this API may be able to\nbypass controls that detect or prevent certain actions.", "zh": "对 kubelet API 的直接访问不受准入控制影响，也不会被 Kubernetes 审计日志记录。\n能直接访问此 API 的攻击者可能会绕过能检测或防止某些操作的控制机制。"}
{"en": "The kubelet API can be configured to authenticate requests in a number of ways.\nBy default, the kubelet configuration allows anonymous access. Most Kubernetes providers\nchange the default to use webhook and certificate authentication. This lets the control plane\nensure that the caller is authorized to access the `nodes` API resource or sub-resources.\nThe default anonymous access doesn't make this assertion with the control plane.", "zh": "kubelet API 可以配置为以多种方式验证请求。\n默认情况下，kubelet 的配置允许匿名访问。大多数 Kubernetes 提供商将默认值更改为使用 Webhook 和证书身份认证。\n这使得控制平面能够确保调用者访问 `nodes` API 资源或子资源是经过授权的。但控制平面不能确保默认的匿名访问也是如此。"}
{"en": "### Mitigations", "zh": "### 缓解措施 {#mitigations}"}
{"en": "- Restrict access to sub-resources of the `nodes` API object using mechanisms such as\n  [RBAC](/docs/reference/access-authn-authz/rbac/). Only grant this access when required,\n  such as by monitoring services.\n- Restrict access to the kubelet port. Only allow specified and trusted IP address\n  ranges to access the port.\n- Ensure that [kubelet authentication](/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication).\n  is set to webhook or certificate mode.\n- Ensure that the unauthenticated \"read-only\" Kubelet port is not enabled on the cluster.", "zh": "- 使用 [RBAC](/zh-cn/docs/reference/access-authn-authz/rbac/) 等机制限制对 `nodes` API 对象的子资源的访问。\n  只在有需要时才授予此访问权限，例如监控服务。\n- 限制对 kubelet 端口的访问。只允许指定和受信任的 IP 地址段访问该端口。\n- 确保将\n  [kubelet 身份验证](/zh-cn/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication)\n  设置为 Webhook 或证书模式。\n- 确保集群上未启用不作身份认证的“只读” Kubelet 端口。"}
{"en": "## The etcd API", "zh": "## etcd API {#the-etcd-api}"}
{"en": "Kubernetes clusters use etcd as a datastore. The `etcd` service listens on TCP port 2379.\nThe only clients that need access are the Kubernetes API server and any backup tooling\nthat you use. Direct access to this API allows for disclosure or modification of any\ndata held in the cluster.", "zh": "Kubernetes 集群使用 etcd 作为数据存储。`etcd` 服务监听 TCP 端口 2379。\n只有 Kubernetes API 服务器和你所使用的备份工具需要访问此存储。对该 API 的直接访问允许公开或修改集群中保存的数据。"}
{"en": "Access to the etcd API is typically managed by client certificate authentication.\nAny certificate issued by a certificate authority that etcd trusts allows full access\nto the data stored inside etcd.", "zh": "对 etcd API 的访问通常通过客户端证书身份认证来管理。\n由 etcd 信任的证书颁发机构所颁发的任何证书都可以完全访问 etcd 中存储的数据。"}
{"en": "Direct access to etcd is not subject to Kubernetes admission control and is not logged\nby Kubernetes audit logging. An attacker who has read access to the API server's\netcd client certificate private key (or can create a new trusted client certificate) can gain\ncluster admin rights by accessing cluster secrets or modifying access rules. Even without\nelevating their Kubernetes RBAC privileges, an attacker who can modify etcd can retrieve any API object\nor create new workloads inside the cluster.", "zh": "对 etcd 的直接访问不受 Kubernetes 准入控制的影响，也不会被 Kubernetes 审计日志记录。\n具有对 API 服务器的 etcd 客户端证书私钥的读取访问权限（或可以创建一个新的受信任的客户端证书）\n的攻击者可以通过访问集群 Secret 或修改访问规则来获得集群管理员权限。\n即使不提升其 Kubernetes RBAC 权限，可以修改 etcd 的攻击者也可以在集群内检索所有 API 对象或创建新的工作负载。"}
{"en": "Many Kubernetes providers configure\netcd to use mutual TLS (both client and server verify each other's certificate for authentication).\nThere is no widely accepted implementation of authorization for the etcd API, although\nthe feature exists. Since there is no authorization model, any certificate\nwith client access to etcd can be used to gain full access to etcd. Typically, etcd client certificates\nthat are only used for health checking can also grant full read and write access.", "zh": "许多 Kubernetes 提供商配置 etcd 为使用双向 TLS（客户端和服务器都验证彼此的证书以进行身份验证）。\n尽管存在该特性，但目前还没有被广泛接受的 etcd API 鉴权实现。\n由于缺少鉴权模型，任何具有对 etcd 的客户端访问权限的证书都可以用于获得对 etcd 的完全访问权限。\n通常，仅用于健康检查的 etcd 客户端证书也可以授予完全读写访问权限。"}
{"en": "### Mitigations {#etcd-api-mitigations}", "zh": "### 缓解措施 {#etcd-api-mitigations}"}
{"en": "- Ensure that the certificate authority trusted by etcd is used only for the purposes of\n  authentication to that service.\n- Control access to the private key for the etcd server certificate, and to the API server's\n  client certificate and key.\n- Consider restricting access to the etcd port at a network level, to only allow access\n  from specified and trusted IP address ranges.", "zh": "- 确保 etcd 所信任的证书颁发机构仅用于该服务的身份认证。\n- 控制对 etcd 服务器证书的私钥以及 API 服务器的客户端证书和密钥的访问。\n- 考虑在网络层面限制对 etcd 端口的访问，仅允许来自特定和受信任的 IP 地址段的访问。"}
{"en": "## Container runtime socket {#runtime-socket}", "zh": "## 容器运行时套接字 {#runtime-socket}"}
{"en": "On each node in a Kubernetes cluster, access to interact with containers is controlled\nby the container runtime (or runtimes, if you have configured more than one). Typically,\nthe container runtime exposes a Unix socket that the kubelet can access. An attacker with\naccess to this socket can launch new containers or interact with running containers.", "zh": "在 Kubernetes 集群中的每个节点上，与容器交互的访问都由容器运行时控制。\n通常，容器运行时会公开一个 kubelet 可以访问的 UNIX 套接字。\n具有此套接字访问权限的攻击者可以启动新容器或与正在运行的容器进行交互。"}
{"en": "At the cluster level, the impact of this access depends on whether the containers that\nrun on the compromised node have access to Secrets or other confidential\ndata that an attacker could use to escalate privileges to other worker nodes or to\ncontrol plane components.", "zh": "在集群层面，这种访问造成的影响取决于在受威胁节点上运行的容器是否可以访问 Secret 或其他机密数据，\n攻击者可以使用这些机密数据将权限提升到其他工作节点或控制平面组件。"}
{"en": "### Mitigations {#runtime-socket-mitigations}", "zh": "### 缓解措施 {#runtime-socket-mitigations}"}
{"en": "- Ensure that you tightly control filesystem access to container runtime sockets.\n  When possible, restrict this access to the `root` user.\n- Isolate the kubelet from other components running on the node, using\n  mechanisms such as Linux kernel namespaces.\n- Ensure that you restrict or forbid the use of [`hostPath` mounts](/docs/concepts/storage/volumes/#hostpath)\n  that include the container runtime socket, either directly or by mounting a parent\n  directory. Also `hostPath` mounts must be set as read-only to mitigate risks\n  of attackers bypassing directory restrictions.\n- Restrict user access to nodes, and especially restrict superuser access to nodes.", "zh": "- 确保严格控制对容器运行时套接字所在的文件系统访问。如果可能，限制为仅 `root` 用户可访问。\n- 使用 Linux 内核命名空间等机制将 kubelet 与节点上运行的其他组件隔离。\n- 确保限制或禁止使用包含容器运行时套接字的 [`hostPath` 挂载](/zh-cn/docs/concepts/storage/volumes/#hostpath)，\n  无论是直接挂载还是通过挂载父目录挂载。此外，`hostPath` 挂载必须设置为只读，以降低攻击者绕过目录限制的风险。\n- 限制用户对节点的访问，特别是限制超级用户对节点的访问。"}
{"en": "This page provides an overview of available configuration options and best practices for cluster\nmulti-tenancy.", "zh": "此页面概述了集群多租户的可用配置选项和最佳实践。"}
{"en": "Sharing clusters saves costs and simplifies administration. However, sharing clusters also\npresents challenges such as security, fairness, and managing _noisy neighbors_.", "zh": "共享集群可以节省成本并简化管理。\n然而，共享集群也带来了诸如安全性、公平性和管理**嘈杂邻居**等挑战。"}
{"en": "Clusters can be shared in many ways. In some cases, different applications may run in the same\ncluster. In other cases, multiple instances of the same application may run in the same cluster,\none for each end user. All these types of sharing are frequently described using the umbrella term\n_multi-tenancy_.", "zh": "集群可以通过多种方式共享。在某些情况下，不同的应用可能会在同一个集群中运行。\n在其他情况下，同一应用的多个实例可能在同一个集群中运行，每个实例对应一个最终用户。\n所有这些类型的共享经常使用一个总括术语 **多租户（Multi-Tenancy）** 来表述。"}
{"en": "While Kubernetes does not have first-class concepts of end users or tenants, it provides several\nfeatures to help manage different tenancy requirements. These are discussed below.", "zh": "虽然 Kubernetes 没有最终用户或租户的一阶概念，\n它还是提供了几个特性来帮助管理不同的租户需求。下面将对此进行讨论。"}
{"en": "## Use cases", "zh": "## 用例 {#use-cases}"}
{"en": "The first step to determining how to share your cluster is understanding your use case, so you can\nevaluate the patterns and tools available. In general, multi-tenancy in Kubernetes clusters falls\ninto two broad categories, though many variations and hybrids are also possible.", "zh": "确定如何共享集群的第一步是理解用例，以便你可以评估可用的模式和工具。\n一般来说，Kubernetes 集群中的多租户分为两大类，但也可以有许多变体和混合。"}
{"en": "### Multiple teams", "zh": "### 多团队 {#multiple-teams}"}
{"en": "A common form of multi-tenancy is to share a cluster between multiple teams within an\norganization, each of whom may operate one or more workloads. These workloads frequently need to\ncommunicate with each other, and with other workloads located on the same or different clusters.", "zh": "多租户的一种常见形式是在组织内的多个团队之间共享一个集群，每个团队可以操作一个或多个工作负载。\n这些工作负载经常需要相互通信，并与位于相同或不同集群上的其他工作负载进行通信。"}
{"en": "In this scenario, members of the teams often have direct access to Kubernetes resources via tools\nsuch as `kubectl`, or indirect access through GitOps controllers or other types of release\nautomation tools. There is often some level of trust between members of different teams, but\nKubernetes policies such as RBAC, quotas, and network policies are essential to safely and fairly\nshare clusters.", "zh": "在这一场景中，团队成员通常可以通过类似 `kubectl` 等工具直接访问 Kubernetes 资源，\n或者通过 GitOps 控制器或其他类型的自动化发布工具间接访问 Kubernetes 资源。\n不同团队的成员之间通常存在某种程度的信任，\n但 RBAC、配额和网络策略等 Kubernetes 策略对于安全、公平地共享集群至关重要。"}
{"en": "### Multiple customers", "zh": "### 多客户 {#multiple-customers}"}
{"en": "The other major form of multi-tenancy frequently involves a Software-as-a-Service (SaaS) vendor\nrunning multiple instances of a workload for customers. This business model is so strongly\nassociated with this deployment style that many people call it \"SaaS tenancy.\" However, a better\nterm might be \"multi-customer tenancy,\" since SaaS vendors may also use other deployment models,\nand this deployment model can also be used outside of SaaS.", "zh": "多租户的另一种主要形式通常涉及为客户运行多个工作负载实例的软件即服务 (SaaS) 供应商。\n这种业务模型与其部署风格之间的相关非常密切，以至于许多人称之为 “SaaS 租户”。  \n但是，更好的术语可能是“多客户租户（Multi-Customer Tenancy）”，因为 SaaS 供应商也可以使用其他部署模型，\n并且这种部署模型也可以在 SaaS 之外使用。"}
{"en": "In this scenario, the customers do not have access to the cluster; Kubernetes is invisible from\ntheir perspective and is only used by the vendor to manage the workloads. Cost optimization is\nfrequently a critical concern, and Kubernetes policies are used to ensure that the workloads are\nstrongly isolated from each other.", "zh": "在这种情况下，客户无权访问集群；\n从他们的角度来看，Kubernetes 是不可见的，仅由供应商用于管理工作负载。\n成本优化通常是一个关键问题，Kubernetes 策略用于确保工作负载彼此高度隔离。"}
{"en": "## Terminology", "zh": "## 术语 {#terminology}"}
{"en": "### Tenants", "zh": "### 租户 {#tenants}"}
{"en": "When discussing multi-tenancy in Kubernetes, there is no single definition for a \"tenant\".\nRather, the definition of a tenant will vary depending on whether multi-team or multi-customer\ntenancy is being discussed.", "zh": "在讨论 Kubernetes 中的多租户时，“租户”没有单一的定义。\n相反，租户的定义将根据讨论的是多团队还是多客户租户而有所不同。"}
{"en": "In multi-team usage, a tenant is typically a team, where each team typically deploys a small\nnumber of workloads that scales with the complexity of the service. However, the definition of\n\"team\" may itself be fuzzy, as teams may be organized into higher-level divisions or subdivided\ninto smaller teams.", "zh": "在多团队使用中，租户通常是一个团队，\n每个团队通常部署少量工作负载，这些工作负载会随着服务的复杂性而发生规模伸缩。\n然而，“团队”的定义本身可能是模糊的，\n因为团队可能被组织成更高级别的部门或细分为更小的团队。"}
{"en": "By contrast, if each team deploys dedicated workloads for each new client, they are using a\nmulti-customer model of tenancy. In this case, a \"tenant\" is simply a group of users who share a\nsingle workload. This may be as large as an entire company, or as small as a single team at that\ncompany.", "zh": "相反，如果每个团队为每个新客户部署专用的工作负载，那么他们使用的是多客户租户模型。\n在这种情况下，“租户”只是共享单个工作负载的一组用户。\n这种租户可能大到整个公司，也可能小到该公司的一个团队。"}
{"en": "In many cases, the same organization may use both definitions of \"tenants\" in different contexts.\nFor example, a platform team may offer shared services such as security tools and databases to\nmultiple internal “customers” and a SaaS vendor may also have multiple teams sharing a development\ncluster. Finally, hybrid architectures are also possible, such as a SaaS provider using a\ncombination of per-customer workloads for sensitive data, combined with multi-tenant shared\nservices.", "zh": "在许多情况下，同一组织可能在不同的上下文中使用“租户”的两种定义。\n例如，一个平台团队可能向多个内部“客户”提供安全工具和数据库等共享服务，\n而 SaaS 供应商也可能让多个团队共享一个开发集群。\n最后，混合架构也是可能的，\n例如，某 SaaS 提供商为每个客户的敏感数据提供独立的工作负载，同时提供多租户共享的服务。"}
{"en": "{{< figure src=\"/images/docs/multi-tenancy.png\" title=\"A cluster showing coexisting tenancy models\" class=\"diagram-large\" >}}", "zh": "{{< figure src=\"/images/docs/multi-tenancy.png\" title=\"展示共存租户模型的集群\" class=\"diagram-large\" >}}"}
{"en": "### Isolation", "zh": "### 隔离 {#isolation}"}
{"en": "There are several ways to design and build multi-tenant solutions with Kubernetes. Each of these\nmethods comes with its own set of tradeoffs that impact the isolation level, implementation\neffort, operational complexity, and cost of service.", "zh": "使用 Kubernetes 设计和构建多租户解决方案有多种方法。\n每种方法都有自己的一组权衡，这些权衡会影响隔离级别、实现工作量、操作复杂性和服务成本。"}
{"en": "A Kubernetes cluster consists of a control plane which runs Kubernetes software, and a data plane\nconsisting of worker nodes where tenant workloads are executed as pods. Tenant isolation can be\napplied in both the control plane and the data plane based on organizational requirements.", "zh": "Kubernetes 集群由运行 Kubernetes 软件的控制平面和由工作节点组成的数据平面组成，\n租户工作负载作为 Pod 在工作节点上执行。\n租户隔离可以根据组织要求应用于控制平面和数据平面。"}
{"en": "The level of isolation offered is sometimes described using terms like “hard” multi-tenancy, which\nimplies strong isolation, and “soft” multi-tenancy, which implies weaker isolation. In particular,\n\"hard\" multi-tenancy is often used to describe cases where the tenants do not trust each other,\noften from security and resource sharing perspectives (e.g. guarding against attacks such as data\nexfiltration or DoS). Since data planes typically have much larger attack surfaces, \"hard\"\nmulti-tenancy often requires extra attention to isolating the data-plane, though control plane\nisolation  also remains critical.", "zh": "所提供的隔离级别有时会使用一些术语来描述，例如 “硬性（Hard）” 多租户意味着强隔离，\n而 “柔性（Soft）” 多租户意味着较弱的隔离。\n特别是，“硬性”多租户通常用于描述租户彼此不信任的情况，\n并且大多是从安全和资源共享的角度（例如，防范数据泄露或 DoS 攻击等）。\n由于数据平面通常具有更大的攻击面，“硬性”多租户通常需要额外注意隔离数据平面，\n尽管控制平面隔离也很关键。"}
{"en": "However, the terms \"hard\" and \"soft\" can often be confusing, as there is no single definition that\nwill apply to all users. Rather, \"hardness\" or \"softness\" is better understood as a broad\nspectrum, with many different techniques that can be used to maintain different types of isolation\nin your clusters, based on your requirements.", "zh": "但是，“硬性”和“柔性”这两个术语常常令人困惑，因为没有一种定义能够适用于所有用户。\n相反，依据“硬度（Hardness）”或“柔度（Softness）”所定义的广泛谱系则更容易理解，\n根据你的需求，可以使用许多不同的技术在集群中维护不同类型的隔离。"}
{"en": "In more extreme cases, it may be easier or necessary to forgo any cluster-level sharing at all and\nassign each tenant their dedicated cluster, possibly even running on dedicated hardware if VMs are\nnot considered an adequate security boundary. This may be easier with managed Kubernetes clusters,\nwhere the overhead of creating and operating clusters is at least somewhat taken on by a cloud\nprovider. The benefit of stronger tenant isolation must be evaluated against the cost and\ncomplexity of managing multiple clusters. The [Multi-cluster SIG](https://git.k8s.io/community/sig-multicluster/README.md)\nis responsible for addressing these types of use cases.", "zh": "在更极端的情况下，彻底放弃所有集群级别的共享并为每个租户分配其专用集群可能更容易或有必要，\n如果认为虚拟机所提供的安全边界还不够，甚至可以在专用硬件上运行。\n对于托管的 Kubernetes 集群而言，这种方案可能更容易，\n其中创建和操作集群的开销至少在一定程度上由云提供商承担。\n必须根据管理多个集群的成本和复杂性来评估更强的租户隔离的好处。\n[Multi-Cluster SIG](https://git.k8s.io/community/sig-multicluster/README.md) 负责解决这些类型的用例。"}
{"en": "The remainder of this page focuses on isolation techniques used for shared Kubernetes clusters.\nHowever, even if you are considering dedicated clusters, it may be valuable to review these\nrecommendations, as it will give you the flexibility to shift to shared clusters in the future if\nyour needs or capabilities change.", "zh": "本页的其余部分重点介绍用于共享 Kubernetes 集群的隔离技术。\n但是，即使你正在考虑使用专用集群，查看这些建议也可能很有价值，\n因为如果你的需求或功能发生变化，它可以让你在未来比较灵活地切换到共享集群。"}
{"en": "## Control plane isolation", "zh": "## 控制面隔离 {#control-plane-isolation}"}
{"en": "Control plane isolation ensures that different tenants cannot access or affect each others'\nKubernetes API resources.", "zh": "控制平面隔离确保不同租户无法访问或影响彼此的 Kubernetes API 资源。"}
{"en": "### Namespaces", "zh": "### 命名空间 {#namespaces}"}
{"en": "In Kubernetes, a {{< glossary_tooltip text=\"Namespace\" term_id=\"namespace\" >}} provides a\nmechanism for isolating groups of API resources within a single cluster. This isolation has two\nkey dimensions:", "zh": "在 Kubernetes 中，\n{{<glossary_tooltip text=\"命名空间\" term_id=\"namespace\" >}}提供了一种在单个集群中隔离 API 资源组的机制。\n这种隔离有两个关键维度："}
{"en": "1. Object names within a namespace can overlap with names in other namespaces, similar to files in\n   folders. This allows tenants to name their resources without having to consider what other\n   tenants are doing.", "zh": "1. 一个命名空间中的对象名称可以与其他命名空间中的名称重叠，类似于文件夹中的文件。\n   这允许租户命名他们的资源，而无需考虑其他租户在做什么。"}
{"en": "2. Many Kubernetes security policies are scoped to namespaces. For example, RBAC Roles and Network\n   Policies are namespace-scoped resources. Using RBAC, Users and Service Accounts can be\n   restricted to a namespace.", "zh": "2. 许多 Kubernetes 安全策略的作用域是命名空间。\n   例如，RBAC Role 和 NetworkPolicy 是命名空间作用域的资源。\n   使用 RBAC，可以将用户和服务帐户限制在一个命名空间中。"}
{"en": "In a multi-tenant environment, a Namespace helps segment a tenant's workload into a logical and\ndistinct management unit. In fact, a common practice is to isolate every workload in its own\nnamespace, even if multiple workloads are operated by the same tenant. This ensures that each\nworkload has its own identity and can be configured with an appropriate security policy.", "zh": "在多租户环境中，命名空间有助于将租户的工作负载划分到各不相同的逻辑管理单元中。\n事实上，一种常见的做法是将每个工作负载隔离在自己的命名空间中，\n即使多个工作负载由同一个租户操作。\n这可确保每个工作负载都有自己的身份，并且可以使用适当的安全策略进行配置。"}
{"en": "The namespace isolation model requires configuration of several other Kubernetes resources,\nnetworking plugins, and adherence to security best practices to properly isolate tenant workloads.\nThese considerations are discussed below.", "zh": "命名空间隔离模型需要配置其他几个 Kubernetes 资源、网络插件，\n并遵守安全最佳实践以正确隔离租户工作负载。\n这些考虑将在下面讨论。"}
{"en": "### Access controls", "zh": "### 访问控制 {#access-controls}"}
{"en": "The most important type of isolation for the control plane is authorization. If teams or their\nworkloads can access or modify each others' API resources, they can change or disable all other\ntypes of policies thereby negating any protection those policies may offer. As a result, it is\ncritical to ensure that each tenant has the appropriate access to only the namespaces they need,\nand no more. This is known as the \"Principle of Least Privilege.\"", "zh": "控制平面最重要的隔离类型是授权。如果各个团队或其工作负载可以访问或修改彼此的 API 资源，\n他们可以更改或禁用所有其他类型的策略，从而取消这些策略可能提供的任何保护。\n因此，确保每个租户只对他们需要的命名空间有适当的访问权，\n而不是更多，这一点至关重要。这被称为“最小特权原则（Principle of Least Privileges）”。"}
{"en": "Role-based access control (RBAC) is commonly used to enforce authorization in the Kubernetes\ncontrol plane, for both users and workloads (service accounts).\n[Roles](/docs/reference/access-authn-authz/rbac/#role-and-clusterrole) and\n[RoleBindings](/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding) are\nKubernetes objects that are used at a namespace level to enforce access control in your\napplication; similar objects exist for authorizing access to cluster-level objects, though these\nare less useful for multi-tenant clusters.", "zh": "基于角色的访问控制 (RBAC) 通常用于在 Kubernetes 控制平面中对用户和工作负载（服务帐户）强制执行鉴权。\n[角色](/zh-cn/docs/reference/access-authn-authz/rbac/#role-and-clusterrole)\n和[角色绑定](/zh-cn/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding)是两种\nKubernetes 对象，用来在命名空间级别对应用实施访问控制；\n对集群级别的对象访问鉴权也有类似的对象，不过这些对象对于多租户集群不太有用。"}
{"en": "In a multi-team environment, RBAC must be used to restrict tenants' access to the appropriate\nnamespaces, and ensure that cluster-wide resources can only be accessed or modified by privileged\nusers such as cluster administrators.", "zh": "在多团队环境中，必须使用 RBAC 来限制租户只能访问合适的命名空间，\n并确保集群范围的资源只能由集群管理员等特权用户访问或修改。"}
{"en": "If a policy ends up granting a user more permissions than they need, this is likely a signal that\nthe namespace containing the affected resources should be refactored into finer-grained\nnamespaces. Namespace management tools may simplify the management of these finer-grained\nnamespaces by applying common RBAC policies to different namespaces, while still allowing\nfine-grained policies where necessary.", "zh": "如果一个策略最终授予用户的权限比他们所需要的还多，\n这可能是一个信号，表明包含受影响资源的命名空间应该被重构为更细粒度的命名空间。\n命名空间管理工具可以通过将通用 RBAC 策略应用于不同的命名空间来简化这些细粒度命名空间的管理，\n同时在必要时仍允许细粒度策略。"}
{"en": "### Quotas", "zh": "### 配额 {#quotas}"}
{"en": "Kubernetes workloads consume node resources, like CPU and memory.  In a multi-tenant environment,\nyou can use [Resource Quotas](/docs/concepts/policy/resource-quotas/) to manage resource usage of\ntenant workloads.  For the multiple teams use case, where tenants have access to the Kubernetes\nAPI, you can use resource quotas to limit the number of API resources (for example: the number of\nPods, or the number of ConfigMaps) that a tenant can create. Limits on object count ensure\nfairness and aim to avoid _noisy neighbor_ issues from affecting other tenants that share a\ncontrol plane.", "zh": "Kubernetes 工作负载消耗节点资源，例如 CPU 和内存。在多租户环境中，\n你可以使用[资源配额](/zh-cn/docs/concepts/policy/resource-quotas/)来管理租户工作负载的资源使用情况。\n对于多团队场景，各个租户可以访问 Kubernetes API，你可以使用资源配额来限制租户可以创建的 API 资源的数量\n（例如：Pod 的数量，或 ConfigMap 的数量）。\n对对象计数的限制确保了公平性，并有助于避免**嘈杂邻居**问题影响共享控制平面的其他租户。"}
{"en": "Resource quotas are namespaced objects. By mapping tenants to namespaces, cluster admins can use\nquotas to ensure that a tenant cannot monopolize a cluster's resources or overwhelm its control\nplane. Namespace management tools simplify the administration of quotas. In addition, while\nKubernetes quotas only apply within a single namespace, some namespace management tools allow\ngroups of namespaces to share quotas, giving administrators far more flexibility with less effort\nthan built-in quotas.", "zh": "资源配额是命名空间作用域的对象。\n通过将租户映射到命名空间，\n集群管理员可以使用配额来确保租户不能垄断集群的资源或压垮控制平面。\n命名空间管理工具简化了配额的管理。\n此外，虽然 Kubernetes 配额仅针对单个命名空间，\n但一些命名空间管理工具允许多个命名空间组共享配额，\n与内置配额相比，降低了管理员的工作量，同时为其提供了更大的灵活性。"}
{"en": "Quotas prevent a single tenant from consuming greater than their allocated share of resources\nhence minimizing the “noisy neighbor” issue, where one tenant negatively impacts the performance\nof other tenants' workloads.", "zh": "配额可防止单个租户所消耗的资源超过其被分配的份额，从而最大限度地减少**嘈杂邻居**问题，\n即一个租户对其他租户工作负载的性能产生负面影响。"}
{"en": "When you apply a quota to namespace, Kubernetes requires you to also specify resource requests and\nlimits for each container. Limits are the upper bound for the amount of resources that a container\ncan consume. Containers that attempt to consume resources that exceed the configured limits will\neither be throttled or killed, based on the resource type. When resource requests are set lower\nthan limits, each container is guaranteed the requested amount but there may still be some\npotential for impact across workloads.", "zh": "当你对命名空间应用配额时，\nKubernetes 要求你还为每个容器指定资源请求和限制。\n限制是容器可以消耗的资源量的上限。\n根据资源类型，尝试使用超出配置限制的资源的容器将被限制或终止。\n当资源请求设置为低于限制时，\n每个容器所请求的数量都可以得到保证，但可能仍然存在跨工作负载的一些潜在影响。"}
{"en": "Quotas cannot protect against all kinds of resource sharing, such as network traffic.\nNode isolation (described below) may be a better solution for this problem.", "zh": "配额不能针对所共享的所有资源（例如网络流量）提供保护。\n节点隔离（如下所述）可能是解决此问题的更好方法。"}
{"en": "## Data Plane Isolation", "zh": "## 数据平面隔离 {#data-plane-isolation}"}
{"en": "Data plane isolation ensures that pods and workloads for different tenants are sufficiently\nisolated.", "zh": "数据平面隔离确保不同租户的 Pod 和工作负载之间被充分隔离。"}
{"en": "### Network isolation", "zh": "### 网络隔离 {#network-isolation}"}
{"en": "By default, all pods in a Kubernetes cluster are allowed to communicate with each other, and all\nnetwork traffic is unencrypted. This can lead to security vulnerabilities where traffic is\naccidentally or maliciously sent to an unintended destination, or is intercepted by a workload on\na compromised node.", "zh": "默认情况下，Kubernetes 集群中的所有 Pod 都可以相互通信，并且所有网络流量都是未加密的。\n这可能导致安全漏洞，导致流量被意外或恶意发送到非预期目的地，\n或被受感染节点上的工作负载拦截。"}
{"en": "Pod-to-pod communication can be controlled using [Network Policies](/docs/concepts/services-networking/network-policies/),\nwhich restrict communication between pods using namespace labels or IP address ranges.\nIn a multi-tenant environment where strict network isolation between tenants is required, starting\nwith a default policy that denies communication between pods is recommended with another rule that\nallows all pods to query the DNS server for name resolution. With such a default policy in place,\nyou can begin adding more permissive rules that allow for communication within a namespace.\nIt is also recommended not to use empty label selector '{}' for namespaceSelector field in network policy definition,\nin case traffic need to be allowed between namespaces.\nThis scheme can be further refined as required. Note that this only applies to pods within a single\ncontrol plane; pods that belong to different virtual control planes cannot talk to each other via\nKubernetes networking.", "zh": "Pod 之间的通信可以使用[网络策略](/zh-cn/docs/concepts/services-networking/network-policies/)来控制，\n它使用命名空间标签或 IP 地址范围来限制 Pod 之间的通信。\n在需要租户之间严格网络隔离的多租户环境中，\n建议从拒绝 Pod 之间通信的默认策略入手，\n然后添加一条允许所有 Pod 查询 DNS 服务器以进行名称解析的规则。\n有了这样的默认策略之后，你就可以开始添加允许在命名空间内进行通信的更多规则。\n另外建议不要在网络策略定义中对 namespaceSelector 字段使用空标签选择算符 “{}”，\n以防需要允许在命名空间之间传输流量。\n该方案可根据需要进一步细化。\n请注意，这仅适用于单个控制平面内的 Pod；\n属于不同虚拟控制平面的 Pod 不能通过 Kubernetes 网络相互通信。"}
{"en": "Namespace management tools may simplify the creation of default or common network policies.\nIn addition, some of these tools allow you to enforce a consistent set of namespace labels across\nyour cluster, ensuring that they are a trusted basis for your policies.", "zh": "命名空间管理工具可以简化默认或通用网络策略的创建。\n此外，其中一些工具允许你在整个集群中强制实施一组一致的命名空间标签，\n确保它们是你策略的可信基础。\n\n{{< warning >}}"}
{"en": "Network policies require a [CNI plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni)\nthat supports the implementation of network policies. Otherwise, NetworkPolicy resources will be ignored.", "zh": "网络策略需要一个支持网络策略实现的\n[CNI 插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni)。\n否则，NetworkPolicy 资源将被忽略。\n{{< /warning >}}"}
{"en": "More advanced network isolation may be provided by service meshes, which provide OSI Layer 7\npolicies based on workload identity, in addition to namespaces. These higher-level policies can\nmake it easier to manage namespace-based multi-tenancy, especially when multiple namespaces are\ndedicated to a single tenant. They frequently also offer encryption using mutual TLS, protecting\nyour data even in the presence of a compromised node, and work across dedicated or virtual clusters.\nHowever, they can be significantly more complex to manage and may not be appropriate for all users.", "zh": "服务网格可以提供更高级的网络隔离，\n除了命名空间之外，它还提供基于工作负载身份的 OSI 第 7 层策略。\n这些更高层次的策略可以更轻松地管理基于命名空间的多租户，\n尤其是存在多个命名空间专用于某一个租户时。\n服务网格还经常使用双向 TLS 提供加密能力，\n即使在存在受损节点的情况下也能保护你的数据，\n并且可以跨专用或虚拟集群工作。\n但是，它们的管理可能要复杂得多，并且可能并不适合所有用户。"}
{"en": "### Storage isolation", "zh": "### 存储隔离 {#storage-isolation}"}
{"en": "Kubernetes offers several types of volumes that can be used as persistent storage for workloads.\nFor security and data-isolation, [dynamic volume provisioning](/docs/concepts/storage/dynamic-provisioning/)\nis recommended and volume types that use node resources should be avoided.", "zh": "Kubernetes 提供了若干类型的卷，可以用作工作负载的持久存储。\n为了安全和数据隔离，建议使用[动态卷制备](/zh-cn/docs/concepts/storage/dynamic-provisioning/)，\n并且应避免使用节点资源的卷类型。"}
{"en": "[StorageClasses](/docs/concepts/storage/storage-classes/) allow you to describe custom \"classes\"\nof storage offered by your cluster, based on quality-of-service levels, backup policies, or custom\npolicies determined by the cluster administrators.", "zh": "[存储类（StorageClass）](/zh-cn/docs/concepts/storage/storage-classes/)允许你根据服务质量级别、\n备份策略或由集群管理员确定的自定义策略描述集群提供的自定义存储“类”。"}
{"en": "Pods can request storage using a [PersistentVolumeClaim](/docs/concepts/storage/persistent-volumes/).\nA PersistentVolumeClaim is a namespaced resource, which enables isolating portions of the storage\nsystem and dedicating it to tenants within the shared Kubernetes cluster.\nHowever, it is important to note that a PersistentVolume is a cluster-wide resource and has a\nlifecycle independent of workloads and namespaces.", "zh": "Pod 可以使用[持久卷申领（PersistentVolumeClaim）](/zh-cn/docs/concepts/storage/persistent-volumes/)请求存储。\nPersistentVolumeClaim 是一种命名空间作用域的资源，\n它可以隔离存储系统的不同部分，并将隔离出来的存储提供给共享 Kubernetes 集群中的租户专用。\n但是，重要的是要注意 PersistentVolume 是集群作用域的资源，\n并且其生命周期独立于工作负载和命名空间的生命周期。"}
{"en": "For example, you can configure a separate StorageClass for each tenant and use this to strengthen isolation.\nIf a StorageClass is shared, you should set a [reclaim policy of `Delete`](/docs/concepts/storage/storage-classes/#reclaim-policy)\nto ensure that a PersistentVolume cannot be reused across different namespaces.", "zh": "例如，你可以为每个租户配置一个单独的 StorageClass，并使用它来加强隔离。\n如果一个 StorageClass 是共享的，你应该设置一个[回收策略](/zh-cn/docs/concepts/storage/storage-classes/#reclaim-policy)\n以确保 PersistentVolume 不能在不同的命名空间中重复使用。"}
{"en": "### Sandboxing containers", "zh": "### 沙箱容器 {#sandboxing-containers}\n\n{{% thirdparty-content %}}"}
{"en": "Kubernetes pods are composed of one or more containers that execute on worker nodes.\nContainers utilize OS-level virtualization and hence offer a weaker isolation boundary than\nvirtual machines that utilize hardware-based virtualization.", "zh": "Kubernetes Pod 由在工作节点上执行的一个或多个容器组成。\n容器利用操作系统级别的虚拟化，\n因此提供的隔离边界比使用基于硬件虚拟化的虚拟机弱一些。"}
{"en": "In a shared environment, unpatched vulnerabilities in the application and system layers can be\nexploited by attackers for container breakouts and remote code execution that allow access to host\nresources. In some applications, like a Content Management System (CMS), customers may be allowed\nthe ability to upload and execute untrusted scripts or code. In either case, mechanisms to further\nisolate and protect workloads using strong isolation are desirable.", "zh": "在共享环境中，攻击者可以利用应用和系统层中未修补的漏洞实现容器逃逸和远程代码执行，\n从而允许访问主机资源。\n在某些应用中，例如内容管理系统（CMS），\n客户可能被授权上传和执行非受信的脚本或代码。\n无论哪种情况，都需要使用强隔离进一步隔离和保护工作负载的机制。"}
{"en": "Sandboxing provides a way to isolate workloads running in a shared cluster. It typically involves\nrunning each pod in a separate execution environment such as a virtual machine or a userspace\nkernel. Sandboxing is often recommended when you are running untrusted code, where workloads are\nassumed to be malicious. Part of the reason this type of isolation is necessary is because\ncontainers are processes running on a shared kernel; they mount file systems like `/sys` and `/proc`\nfrom the underlying host, making them less secure than an application that runs on a virtual\nmachine which has its own kernel. While controls such as seccomp, AppArmor, and SELinux can be\nused to strengthen the security of containers, it is hard to apply a universal set of rules to all\nworkloads running in a shared cluster. Running workloads in a sandbox environment helps to\ninsulate the host from container escapes, where an attacker exploits a vulnerability to gain\naccess to the host system and all the processes/files running on that host.", "zh": "沙箱提供了一种在共享集群中隔离运行中的工作负载的方法。\n它通常涉及在单独的执行环境（例如虚拟机或用户空间内核）中运行每个 Pod。\n当你运行不受信任的代码时（假定工作负载是恶意的），通常建议使用沙箱，\n这种隔离是必要的，部分原因是由于容器是在共享内核上运行的进程。\n它们从底层主机挂载像 /sys 和 /proc 这样的文件系统，\n这使得它们不如在具有自己内核的虚拟机上运行的应用安全。\n虽然 seccomp、AppArmor 和 SELinux 等控件可用于加强容器的安全性，\n但很难将一套通用规则应用于在共享集群中运行的所有工作负载。\n在沙箱环境中运行工作负载有助于将主机隔离开来，不受容器逃逸影响，\n在容器逃逸场景中，攻击者会利用漏洞来访问主机系统以及在该主机上运行的所有进程/文件。"}
{"en": "Virtual machines and userspace kernels are two popular approaches to sandboxing. The following\nsandboxing implementations are available:\n\n* [gVisor](https://gvisor.dev/) intercepts syscalls from containers and runs them through a\n  userspace kernel, written in Go, with limited access to the underlying host.\n* [Kata Containers](https://katacontainers.io/) provide a secure container runtime that allows you to run\n  containers in a VM. The hardware virtualization available in Kata offers an added layer of\n  security for containers running untrusted code.", "zh": "虚拟机和用户空间内核是两种流行的沙箱方法。\n可以使用以下沙箱实现：\n\n* [gVisor](https://gvisor.dev/) 拦截来自容器的系统调用，并通过用户空间内核运行它们，\n  用户空间内核采用 Go 编写，对底层主机的访问是受限的\n* [Kata Containers](https://katacontainers.io/) 提供了一个安全的容器运行时，\n  允许你在 VM 中运行容器。Kata 中提供的硬件虚拟化为运行不受信任代码的容器提供了额外的安全层。"}
{"en": "### Node Isolation", "zh": "### 节点隔离 {#node-isolation}"}
{"en": "Node isolation is another technique that you can use to isolate tenant workloads from each other.\nWith node isolation, a set of nodes is dedicated to running pods from a particular tenant and\nco-mingling of tenant pods is prohibited. This configuration reduces the noisy tenant issue, as\nall pods running on a node will belong to a single tenant. The risk of information disclosure is\nslightly lower with node isolation because an attacker that manages to escape from a container\nwill only have access to the containers and volumes mounted to that node.", "zh": "节点隔离是另一种可用于将租户工作负载相互隔离的技术。\n通过节点隔离，一组节点专用于运行来自特定租户的 Pod，并且禁止混合不同租户 Pod 集合。\n这种配置减少了嘈杂的租户问题，因为在一个节点上运行的所有 Pod 都将属于一个租户。\n节点隔离的信息泄露风险略低，\n因为成功实现容器逃逸的攻击者也只能访问挂载在该节点上的容器和卷。"}
{"en": "Although workloads from different tenants are running on different nodes, it is important to be\naware that the kubelet and (unless using virtual control planes) the API service are still shared\nservices. A skilled attacker could use the permissions assigned to the kubelet or other pods\nrunning on the node to move laterally within the cluster and gain access to tenant workloads\nrunning on other nodes. If this is a major concern, consider implementing compensating controls\nsuch as seccomp, AppArmor or SELinux or explore using sandboxed containers or creating separate\nclusters for each tenant.", "zh": "尽管来自不同租户的工作负载在不同的节点上运行，\n仍然很重要的是要注意 kubelet 和\n（除非使用虚拟控制平面）API 服务仍然是共享服务。\n熟练的攻击者可以使用分配给 kubelet 或节点上运行的其他 Pod\n的权限在集群内横向移动并获得对其他节点上运行的租户工作负载的访问权限。\n如果这是一个主要问题，请考虑实施补偿控制，\n例如使用 seccomp、AppArmor 或 SELinux，或者探索使用沙箱容器，或者为每个租户创建单独的集群。"}
{"en": "Node isolation is a little easier to reason about from a billing standpoint than sandboxing\ncontainers since you can charge back per node rather than per pod. It also has fewer compatibility\nand performance issues and may be easier to implement than sandboxing containers.\nFor example, nodes for each tenant can be configured with taints so that only pods with the\ncorresponding toleration can run on them. A mutating webhook could then be used to automatically\nadd tolerations and node affinities to pods deployed into tenant namespaces so that they run on a\nspecific set of nodes designated for that tenant.", "zh": "从计费的角度来看，节点隔离比沙箱容器更容易理解，\n因为你可以按节点而不是按 Pod 收费。\n它的兼容性和性能问题也较少，而且可能比沙箱容器更容易实现。\n例如，可以为每个租户的节点配置污点，\n以便只有具有相应容忍度的 Pod 才能在其上运行。\n然后可以使用变更性质的 Webhook 自动向部署到租户命名空间中的 Pod 添加容忍度和节点亲和性，\n以便它们在为该租户指定的一组特定节点上运行。"}
{"en": "Node isolation can be implemented using an [pod node selectors](/docs/concepts/scheduling-eviction/assign-pod-node/)\nor a [Virtual Kubelet](https://github.com/virtual-kubelet).", "zh": "节点隔离可以使用[将 Pod 指派给节点](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/)或\n[Virtual Kubelet](https://github.com/virtual-kubelet) 来实现。"}
{"en": "## Additional Considerations", "zh": "## 额外的注意事项 {#additional-considerations}"}
{"en": "This section discusses other Kubernetes constructs and patterns that are relevant for multi-tenancy.", "zh": "本节讨论与多租户相关的其他 Kubernetes 结构和模式。"}
{"en": "### API Priority and Fairness", "zh": "### API 优先级和公平性 {#api-priority-and-fairness}"}
{"en": "[API priority and fairness](/docs/concepts/cluster-administration/flow-control/) is a Kubernetes\nfeature that allows you to assign a priority to certain pods running within the cluster.\nWhen an application calls the Kubernetes API, the API server evaluates the priority assigned to pod.\nCalls from pods with higher priority are fulfilled before those with a lower priority.\nWhen contention is high, lower priority calls can be queued until the server is less busy or you\ncan reject the requests.", "zh": "[API 优先级和公平性](/zh-cn/docs/concepts/cluster-administration/flow-control/)是 Kubernetes 的一个特性，\n允许你为集群中运行的某些 Pod 赋予优先级。\n当应用调用 Kubernetes API 时，API 服务器会评估分配给 Pod 的优先级。\n来自具有较高优先级的 Pod 的调用会在具有较低优先级的 Pod 的调用之前完成。\n当争用很激烈时，较低优先级的调用可以排队，直到服务器不那么忙，或者你可以拒绝请求。"}
{"en": "Using API priority and fairness will not be very common in SaaS environments unless you are\nallowing customers to run applications that interface with the Kubernetes API, for example,\na controller.", "zh": "使用 API 优先级和公平性在 SaaS 环境中并不常见，\n除非你允许客户运行与 Kubernetes API 接口的应用，例如控制器。"}
{"en": "### Quality-of-Service (QoS) {#qos}", "zh": "### 服务质量 (QoS) {#qos}"}
{"en": "When you’re running a SaaS application, you may want the ability to offer different\nQuality-of-Service (QoS) tiers of service to different tenants. For example, you may have freemium\nservice that comes with fewer performance guarantees and features and a for-fee service tier with\nspecific performance guarantees. Fortunately, there are several Kubernetes constructs that can\nhelp you accomplish this within a shared cluster, including network QoS, storage classes, and pod\npriority and preemption. The idea with each of these is to provide tenants with the quality of\nservice that they paid for. Let’s start by looking at networking QoS.", "zh": "当你运行 SaaS 应用时，\n你可能希望能够为不同的租户提供不同的服务质量 (QoS) 层级。\n例如，你可能拥有具有性能保证和功能较差的免费增值服务，\n以及具有一定性能保证的收费服务层。\n幸运的是，有几个 Kubernetes 结构可以帮助你在共享集群中完成此任务，\n包括网络 QoS、存储类以及 Pod 优先级和抢占。\n这些都是为了给租户提供他们所支付的服务质量。\n让我们从网络 QoS 开始。"}
{"en": "Typically, all pods on a node share a network interface. Without network QoS, some pods may\nconsume an unfair share of the available bandwidth at the expense of other pods.\nThe Kubernetes [bandwidth plugin](https://www.cni.dev/plugins/current/meta/bandwidth/) creates an\n[extended resource](/docs/concepts/configuration/manage-resources-containers/#extended-resources)\nfor networking that allows you to use Kubernetes resources constructs, i.e. requests/limits, to\napply rate limits to pods by using Linux tc queues.\nBe aware that the plugin is considered experimental as per the\n[Network Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping)\ndocumentation and should be thoroughly tested before use in production environments.", "zh": "通常，节点上的所有 Pod 共享一个网络接口。\n如果没有网络 QoS，一些 Pod 可能会以牺牲其他 Pod 为代价不公平地消耗可用带宽。\nKubernetes [带宽插件](https://www.cni.dev/plugins/current/meta/bandwidth/)为网络创建\n[扩展资源](/zh-cn/docs/concepts/configuration/manage-resources-containers/#extended-resources)，\n以允许你使用 Kubernetes 的 resources 结构，即 requests 和 limits 设置。\n通过使用 Linux tc 队列将速率限制应用于 Pod。\n请注意，根据[支持流量整形](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping)文档，\n该插件被认为是实验性的，在生产环境中使用之前应该进行彻底的测试。"}
{"en": "For storage QoS, you will likely want to create different storage classes or profiles with\ndifferent performance characteristics. Each storage profile can be associated with a different\ntier of service that is optimized for different workloads such IO, redundancy, or throughput.\nAdditional logic might be necessary to allow the tenant to associate the appropriate storage\nprofile with their workload.", "zh": "对于存储 QoS，你可能希望创建具有不同性能特征的不同存储类或配置文件。\n每个存储配置文件可以与不同的服务层相关联，该服务层针对 IO、冗余或吞吐量等不同的工作负载进行优化。\n可能需要额外的逻辑来允许租户将适当的存储配置文件与其工作负载相关联。"}
{"en": "Finally, there’s [pod priority and preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\nwhere you can assign priority values to pods. When scheduling pods, the scheduler will try\nevicting pods with lower priority when there are insufficient resources to schedule pods that are\nassigned a higher priority. If you have a use case where tenants have different service tiers in a\nshared cluster e.g. free and paid, you may want to give higher priority to certain tiers using\nthis feature.", "zh": "最后，还有 [Pod 优先级和抢占](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/)，\n你可以在其中为 Pod 分配优先级值。\n在调度 Pod 时，当没有足够的资源来调度分配了较高优先级的 Pod 时，\n调度程序将尝试驱逐具有较低优先级的 Pod。\n如果你有一个用例，其中租户在共享集群中具有不同的服务层，例如免费和付费，\n你可能希望使用此功能为某些层级提供更高的优先级。"}
{"en": "### DNS", "zh": "### DNS {#dns}"}
{"en": "Kubernetes clusters include a Domain Name System (DNS) service to provide translations from names\nto IP addresses, for all Services and Pods. By default, the Kubernetes DNS service allows lookups\nacross all namespaces in the cluster.", "zh": "Kubernetes 集群包括一个域名系统（DNS）服务，\n可为所有服务和 Pod 提供从名称到 IP 地址的转换。\n默认情况下，Kubernetes DNS 服务允许在集群中的所有命名空间中进行查找。"}
{"en": "In multi-tenant environments where tenants can access pods and other Kubernetes resources, or where\nstronger isolation is required, it may be necessary to prevent pods from looking up services in other\nNamespaces.\nYou can restrict cross-namespace DNS lookups by configuring security rules for the DNS service.\nFor example, CoreDNS (the default DNS service for Kubernetes) can leverage Kubernetes metadata\nto restrict queries to Pods and Services within a namespace. For more information, read an\n[example](https://github.com/coredns/policy#kubernetes-metadata-multi-tenancy-policy) of\nconfiguring this within the CoreDNS documentation.", "zh": "在多租户环境中，租户可以访问 Pod 和其他 Kubernetes 资源，\n或者在需要更强隔离的情况下，可能需要阻止 Pod 在其他名称空间中查找服务。\n你可以通过为 DNS 服务配置安全规则来限制跨命名空间的 DNS 查找。\n例如，CoreDNS（Kubernetes 的默认 DNS 服务）可以利用 Kubernetes\n元数据来限制对命名空间内的 Pod 和服务的查询。\n有关更多信息，请阅读 CoreDNS 文档中配置此功能的\n[示例](https://github.com/coredns/policy#kubernetes-metadata-multi-tenancy-policy)。"}
{"en": "When a [Virtual Control Plane per tenant](#virtual-control-plane-per-tenant) model is used, a DNS\nservice must be configured per tenant or a multi-tenant DNS service must be used.\nHere is an example of a [customized version of CoreDNS](https://github.com/kubernetes-sigs/cluster-api-provider-nested/blob/main/virtualcluster/doc/tenant-dns.md)\nthat supports multiple tenants.", "zh": "当使用[各租户独立虚拟控制面](#virtual-control-plane-per-tenant)模型时，\n必须为每个租户配置 DNS 服务或必须使用多租户 DNS 服务。参见一个\n[CoreDNS 的定制版本](https://github.com/kubernetes-sigs/cluster-api-provider-nested/blob/main/virtualcluster/doc/tenant-dns.md)支持多租户的示例。"}
{"en": "### Operators", "zh": "### Operators {#operators}"}
{"en": "[Operators](/docs/concepts/extend-kubernetes/operator/) are Kubernetes controllers that manage\napplications. Operators can simplify the management of multiple instances of an application, like\na database service, which makes them a common building block in the multi-consumer (SaaS)\nmulti-tenancy use case.", "zh": "[Operator 模式](/zh-cn/docs/concepts/extend-kubernetes/operator/)是管理应用的 Kubernetes 控制器。\nOperator 可以简化应用的多个实例的管理，例如数据库服务，\n这使它们成为多消费者 (SaaS) 多租户用例中的通用构建块。"}
{"en": "Operators used in a multi-tenant environment should follow a stricter set of guidelines.\nSpecifically, the Operator should:\n\n* Support creating resources within different tenant namespaces, rather than just in the namespace\n  in which the Operator is deployed.\n* Ensure that the Pods are configured with resource requests and limits, to ensure scheduling and fairness.\n* Support configuration of Pods for data-plane isolation techniques such as node isolation and\n  sandboxed containers.", "zh": "在多租户环境中使用 Operators 应遵循一套更严格的准则。具体而言，Operator 应：\n\n* 支持在不同的租户命名空间内创建资源，而不仅仅是在部署 Operator 的命名空间内。\n* 确保 Pod 配置了资源请求和限制，以确保调度和公平。\n* 支持节点隔离、沙箱容器等数据平面隔离技术的 Pod 配置。"}
{"en": "## Implementations", "zh": "## 实现 {#implementations}\n\n{{% thirdparty-content %}}"}
{"en": "There are two primary ways to share a Kubernetes cluster for multi-tenancy: using Namespaces\n(that is, a Namespace per tenant) or by virtualizing the control plane (that is, virtual control\nplane per tenant).", "zh": "为多租户共享 Kubernetes 集群有两种主要方法：\n使用命名空间（即每个租户独立的命名空间）\n或虚拟化控制平面（即每个租户独立的虚拟控制平面）。"}
{"en": "In both cases, data plane isolation, and management of additional considerations such as API\nPriority and Fairness, is also recommended.", "zh": "在这两种情况下，还建议对数据平面隔离和其他考虑事项，如 API 优先级和公平性，进行管理。"}
{"en": "Namespace isolation is well-supported by Kubernetes, has a negligible resource cost, and provides\nmechanisms to allow tenants to interact appropriately, such as by allowing service-to-service\ncommunication. However, it can be difficult to configure, and doesn't apply to Kubernetes\nresources that can't be namespaced, such as Custom Resource Definitions, Storage Classes, and Webhooks.", "zh": "Kubernetes 很好地支持命名空间隔离，其资源开销可以忽略不计，并提供了允许租户适当交互的机制，\n例如允许服务之间的通信。\n但是，它可能很难配置，而且不适用于非命名空间作用域的 Kubernetes 资源，例如自定义资源定义、存储类和 Webhook 等。"}
{"en": "Control plane virtualization allows for isolation of non-namespaced resources at the cost of\nsomewhat higher resource usage and more difficult cross-tenant sharing. It is a good option when\nnamespace isolation is insufficient but dedicated clusters are undesirable, due to the high cost\nof maintaining them (especially on-prem) or due to their higher overhead and lack of resource\nsharing. However, even within a virtualized control plane, you will likely see benefits by using\nnamespaces as well.", "zh": "控制平面虚拟化允许以更高的资源使用率和更困难的跨租户共享为代价隔离非命名空间作用域的资源。\n当命名空间隔离不足但不希望使用专用集群时，这是一个不错的选择，\n因为维护专用集群的成本很高（尤其是本地集群），\n或者由于专用集群的额外开销较高且缺乏资源共享。\n但是，即使在虚拟化控制平面中，你也可能会看到使用命名空间的好处。"}
{"en": "The two options are discussed in more detail in the following sections.", "zh": "以下各节将更详细地讨论这两个选项："}
{"en": "### Namespace per tenant", "zh": "### 每个租户独立的命名空间 {#namespace-per-tenant}"}
{"en": "As previously mentioned, you should consider isolating each workload in its own namespace, even if\nyou are using dedicated clusters or virtualized control planes. This ensures that each workload\nonly has access to its own resources, such as ConfigMaps and Secrets, and allows you to tailor\ndedicated security policies for each workload. In addition, it is a best practice to give each\nnamespace names that are unique across your entire fleet (that is, even if they are in separate\nclusters), as this gives you the flexibility to switch between dedicated and shared clusters in\nthe future, or to use multi-cluster tooling such as service meshes.", "zh": "如前所述，你应该考虑将每个工作负载隔离在其自己的命名空间中，\n即使你使用的是专用集群或虚拟化控制平面。\n这可确保每个工作负载只能访问其自己的资源，例如 ConfigMap 和 Secret，\n并允许你为每个工作负载定制专用的安全策略。\n此外，最佳实践是为整个集群中的每个命名空间名称提供唯一的名称（即，即使它们位于单独的集群中），\n因为这使你将来可以灵活地在专用集群和共享集群之间切换，\n或者使用多集群工具，例如服务网格。"}
{"en": "Conversely, there are also advantages to assigning namespaces at the tenant level, not just the\nworkload level, since there are often policies that apply to all workloads owned by a single\ntenant. However, this raises its own problems. Firstly, this makes it difficult or impossible to\ncustomize policies to individual workloads, and secondly, it may be challenging to come up with a\nsingle level of \"tenancy\" that should be given a namespace. For example, an organization may have\ndivisions, teams, and subteams - which should be assigned a namespace?", "zh": "相反，在租户级别分配命名空间也有优势，\n而不仅仅是工作负载级别，\n因为通常有一些策略适用于单个租户拥有的所有工作负载。\n然而，这种方案也有自己的问题。\n首先，这使得为各个工作负载定制策略变得困难或不可能，\n其次，确定应该赋予命名空间的单一级别的 “租户” 可能很困难。\n例如，一个组织可能有部门、团队和子团队 - 哪些应该分配一个命名空间？"}
{"en": "To solve this, Kubernetes provides the [Hierarchical Namespace Controller (HNC)](https://github.com/kubernetes-sigs/hierarchical-namespaces),\nwhich allows you to organize your namespaces into hierarchies, and share certain policies and\nresources between them. It also helps you manage namespace labels, namespace lifecycles, and\ndelegated management, and share resource quotas across related namespaces. These capabilities can\nbe useful in both multi-team and multi-customer scenarios.", "zh": "为了解决这个问题，Kubernetes 提供了\n[Hierarchical Namespace Controller (HNC)](https://github.com/kubernetes-sigs/hierarchical-namespaces)，\n它允许你将多个命名空间组织成层次结构，并在它们之间共享某些策略和资源。\n它还可以帮助你管理命名空间标签、命名空间生命周期和委托管理，\n并在相关命名空间之间共享资源配额。\n这些功能在多团队和多客户场景中都很有用。"}
{"en": "Other projects that provide similar capabilities and aid in managing namespaced resources are\nlisted below.", "zh": "下面列出了提供类似功能并有助于管理命名空间资源的其他项目："}
{"en": "#### Multi-team tenancy", "zh": "#### 多团队租户 {#multi-team-tenancy}"}
{"en": "* [Capsule](https://github.com/clastix/capsule)", "zh": "* [Capsule](https://github.com/clastix/capsule)"}
{"en": "#### Multi-customer tenancy", "zh": "#### 多客户租户 {#multi-customer-tenancy}"}
{"en": "* [Kubeplus](https://github.com/cloud-ark/kubeplus)", "zh": "* [Kubeplus](https://github.com/cloud-ark/kubeplus)"}
{"en": "#### Policy engines", "zh": "#### 策略引擎 {#policy-engines}"}
{"en": "Policy engines provide features to validate and generate tenant configurations:", "zh": "策略引擎提供了验证和生成租户配置的特性："}
{"en": "* [Kyverno](https://kyverno.io/)\n* [OPA/Gatekeeper](https://github.com/open-policy-agent/gatekeeper)", "zh": "* [Kyverno](https://kyverno.io/)\n* [OPA/Gatekeeper](https://github.com/open-policy-agent/gatekeeper)"}
{"en": "### Virtual control plane per tenant", "zh": "### 每个租户独立的虚拟控制面    {#virtual-control-plane-per-tenant}"}
{"en": "Another form of control-plane isolation is to use Kubernetes extensions to provide each tenant a\nvirtual control-plane that enables segmentation of cluster-wide API resources.\n[Data plane isolation](#data-plane-isolation) techniques can be used with this model to securely\nmanage worker nodes across tenants.", "zh": "控制面隔离的另一种形式是使用 Kubernetes 扩展为每个租户提供一个虚拟控制面，\n以实现集群范围内 API 资源的分段。\n[数据平面隔离](#data-plane-isolation)技术可以与此模型一起使用，\n以安全地跨多个租户管理工作节点。"}
{"en": "The virtual control plane based multi-tenancy model extends namespace-based multi-tenancy by\nproviding each tenant with dedicated control plane components, and hence complete control over\ncluster-wide resources and add-on services. Worker nodes are shared across all tenants, and are\nmanaged by a Kubernetes cluster that is normally inaccessible to tenants.\nThis cluster is often referred to as a _super-cluster_ (or sometimes as a _host-cluster_).\nSince a tenant’s control-plane is not directly associated with underlying compute resources it is\nreferred to as a _virtual control plane_.", "zh": "基于虚拟控制面的多租户模型通过为每个租户提供专用控制面组件来扩展基于命名空间的多租户，\n从而完全控制集群范围的资源和附加服务。\n工作节点在所有租户之间共享，并由租户通常无法访问的 Kubernetes 集群管理。\n该集群通常被称为 **超集群（Super-Cluster）**（或有时称为 **host-cluster**）。\n由于租户的控制面不直接与底层计算资源相关联，因此它被称为**虚拟控制平面**。"}
{"en": "A virtual control plane typically consists of the Kubernetes API server,\nthe controller manager, and the etcd data store.\nIt interacts with the super cluster via a metadata synchronization controller\nwhich coordinates changes across tenant control planes and the control plane of the super-cluster.", "zh": "虚拟控制面通常由 Kubernetes API 服务器、控制器管理器和 etcd 数据存储组成。\n它通过元数据同步控制器与超集群交互，\n该控制器跨租户控制面和超集群控制面对变化进行协调。"}
{"en": "By using per-tenant dedicated control planes,\nmost of the isolation problems due to sharing one API server among all tenants are solved.\nExamples include noisy neighbors in the control plane,\nuncontrollable blast radius of policy misconfigurations,\nand conflicts between cluster scope objects such as webhooks and CRDs.\nHence, the virtual control plane model is particularly suitable for cases\nwhere each tenant requires access to a Kubernetes API server and expects the full cluster manageability.", "zh": "通过使用每个租户单独的专用控制面，可以解决由于所有租户共享一个 API 服务器而导致的大部分隔离问题。\n例如，控制平面中的嘈杂邻居、策略错误配置导致的不可控爆炸半径以及如\nWebhook 和 CRD 等集群范围对象之间的冲突。\n因此，虚拟控制平面模型特别适用于每个租户都需要访问\nKubernetes API 服务器并期望具有完整集群可管理性的情况。"}
{"en": "The improved isolation comes at the cost of running\nand maintaining an individual virtual control plane per tenant.\nIn addition, per-tenant control planes do not solve isolation problems in the data plane,\nsuch as node-level noisy neighbors or security threats.\nThese must still be addressed separately.", "zh": "改进的隔离是以每个租户运行和维护一个单独的虚拟控制平面为代价的。\n此外，租户层面的控制面不能解决数据面的隔离问题，\n例如节点级的嘈杂邻居或安全威胁。这些仍然必须单独解决。"}
{"en": "The Kubernetes [Cluster API - Nested (CAPN)](https://github.com/kubernetes-sigs/cluster-api-provider-nested/tree/main/virtualcluster)\nproject provides an implementation of virtual control planes.", "zh": "Kubernetes [Cluster API - Nested (CAPN)](https://github.com/kubernetes-sigs/cluster-api-provider-nested/tree/main/virtualcluster)\n项目提供了虚拟控制平面的实现。"}
{"en": "#### Other implementations", "zh": "#### 其他实现 {#other-implementations}\n\n* [Kamaji](https://github.com/clastix/kamaji)\n* [vcluster](https://github.com/loft-sh/vcluster)"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}"}
{"en": "The Kubernetes [Pod Security Standards](/docs/concepts/security/pod-security-standards/) define\ndifferent isolation levels for Pods. These standards let you define how you want to restrict the\nbehavior of pods in a clear, consistent fashion.", "zh": "Kubernetes [Pod 安全性标准（Security Standard）](/zh-cn/docs/concepts/security/pod-security-standards/)\n为 Pod 定义不同的隔离级别。这些标准能够让你以一种清晰、一致的方式定义如何限制 Pod 行为。"}
{"en": "Kubernetes offers a built-in _Pod Security_ {{< glossary_tooltip text=\"admission controller\"\nterm_id=\"admission-controller\" >}} to enforce the Pod Security Standards. Pod security restrictions\nare applied at the {{< glossary_tooltip text=\"namespace\" term_id=\"namespace\" >}} level when pods are\ncreated.", "zh": "Kubernetes 提供了一个内置的 **Pod Security**\n{{< glossary_tooltip text=\"准入控制器\" term_id=\"admission-controller\" >}}来执行 Pod 安全标准\n（Pod Security Standard）。\n创建 Pod 时在{{< glossary_tooltip text=\"名字空间\" term_id=\"namespace\" >}}级别应用这些 Pod 安全限制。"}
{"en": "### Built-in Pod Security admission enforcement\n\nThis page is part of the documentation for Kubernetes v{{< skew currentVersion >}}.\nIf you are running a different version of Kubernetes, consult the documentation for that release.", "zh": "### 内置 Pod 安全准入强制执行 {#built-in-pod-security-admission-enforcement}\n\n本页面是 Kubernetes v{{< skew currentVersion >}} 文档的一部分。\n如果你运行的是其他版本的 Kubernetes，请查阅该版本的文档。"}
{"en": "## Pod Security levels", "zh": "## Pod 安全性级别   {#pod-security-levels}"}
{"en": "Pod Security admission places requirements on a Pod's [Security\nContext](/docs/tasks/configure-pod-container/security-context/) and other related fields according\nto the three levels defined by the [Pod Security\nStandards](/docs/concepts/security/pod-security-standards): `privileged`, `baseline`, and\n`restricted`. Refer to the [Pod Security Standards](/docs/concepts/security/pod-security-standards)\npage for an in-depth look at those requirements.", "zh": "Pod 安全性准入插件对 Pod\n的[安全性上下文](/zh-cn/docs/tasks/configure-pod-container/security-context/)有一定的要求，\n并且依据 [Pod 安全性标准](/zh-cn/docs/concepts/security/pod-security-standards)所定义的三个级别\n（`privileged`、`baseline` 和 `restricted`）对其他字段也有要求。\n关于这些需求的更进一步讨论，请参阅\n[Pod 安全性标准](/zh-cn/docs/concepts/security/pod-security-standards/)页面。"}
{"en": "## Pod Security Admission labels for namespaces\n\nOnce the feature is enabled or the webhook is installed, you can configure namespaces to define the admission\ncontrol mode you want to use for pod security in each namespace. Kubernetes defines a set of \n{{< glossary_tooltip term_id=\"label\" text=\"labels\" >}} that you can set to define which of the \npredefined Pod Security Standard levels you want to use for a namespace. The label you select\ndefines what action the {{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}}\ntakes if a potential violation is detected:", "zh": "## 为名字空间设置 Pod 安全性准入控制标签 {#pod-security-admission-labels-for-namespaces}\n\n一旦特性被启用或者安装了 Webhook，你可以配置名字空间以定义每个名字空间中\nPod 安全性准入控制模式。\nKubernetes 定义了一组{{< glossary_tooltip term_id=\"label\" text=\"标签\" >}}，\n你可以设置这些标签来定义某个名字空间上要使用的预定义的 Pod 安全性标准级别。\n你所选择的标签定义了检测到潜在违例时，\n{{< glossary_tooltip text=\"控制面\" term_id=\"control-plane\" >}}要采取什么样的动作。"}
{"en": "Mode | Description\n:---------|:------------\n**enforce** | Policy violations will cause the pod to be rejected.\n**audit** | Policy violations will trigger the addition of an audit annotation to the event recorded in the [audit log](/docs/tasks/debug/debug-cluster/audit/), but are otherwise allowed.\n**warn** | Policy violations will trigger a user-facing warning, but are otherwise allowed.", "zh": "{{< table caption=\"Pod 安全准入模式\" >}}\n模式 | 描述\n:---------|:------------\n**enforce** | 策略违例会导致 Pod 被拒绝\n**audit** | 策略违例会触发[审计日志](/zh-cn/docs/tasks/debug/debug-cluster/audit/)中记录新事件时添加审计注解；但是 Pod 仍是被接受的。\n**warn** | 策略违例会触发用户可见的警告信息，但是 Pod 仍是被接受的。\n{{< /table >}}"}
{"en": "A namespace can configure any or all modes, or even set a different level for different modes.\n\nFor each mode, there are two labels that determine the policy used:", "zh": "名字空间可以配置任何一种或者所有模式，或者甚至为不同的模式设置不同的级别。\n\n对于每种模式，决定所使用策略的标签有两个："}
{"en": "# The per-mode level label indicates which policy level to apply for the mode.\n#\n# MODE must be one of `enforce`, `audit`, or `warn`.\n# LEVEL must be one of `privileged`, `baseline`, or `restricted`.\npod-security.kubernetes.io/<MODE>: <LEVEL>\n\n# Optional: per-mode version label that can be used to pin the policy to the\n# version that shipped with a given Kubernetes minor version (for example v{{< skew currentVersion >}}).\n#\n# MODE must be one of `enforce`, `audit`, or `warn`.\n# VERSION must be a valid Kubernetes minor version, or `latest`.\npod-security.kubernetes.io/<MODE>-version: <VERSION>", "zh": "```yaml\n# 模式的级别标签用来标示对应模式所应用的策略级别\n#\n# MODE 必须是 `enforce`、`audit` 或 `warn` 之一\n# LEVEL 必须是 `privileged`、baseline` 或 `restricted` 之一\npod-security.kubernetes.io/<MODE>: <LEVEL>\n\n# 可选：针对每个模式版本的版本标签可以将策略锁定到\n# 给定 Kubernetes 小版本号所附带的版本（例如 v{{< skew currentVersion >}}）\n#\n# MODE 必须是 `enforce`、`audit` 或 `warn` 之一\n# VERSION 必须是一个合法的 Kubernetes 小版本号或者 `latest`\npod-security.kubernetes.io/<MODE>-version: <VERSION>\n```"}
{"en": "Check out [Enforce Pod Security Standards with Namespace Labels](/docs/tasks/configure-pod-container/enforce-standards-namespace-labels) to see example usage.", "zh": "关于用法示例，可参阅[使用名字空间标签来强制实施 Pod 安全标准](/zh-cn/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/)。"}
{"en": "## Workload resources and Pod templates\n\nPods are often created indirectly, by creating a [workload\nobject](/docs/concepts/workloads/controllers/) such as a {{< glossary_tooltip\nterm_id=\"deployment\" >}} or {{< glossary_tooltip term_id=\"job\">}}. The workload object defines a\n_Pod template_ and a {{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}} for the\nworkload resource creates Pods based on that template. To help catch violations early, both the\naudit and warning modes are applied to the workload resources. However, enforce mode is **not**\napplied to workload resources, only to the resulting pod objects.", "zh": "## 负载资源和 Pod 模板    {#workload-resources-and-pod-templates}\n\nPod 通常是通过创建 {{< glossary_tooltip term_id=\"deployment\" >}} 或\n{{< glossary_tooltip term_id=\"job\">}}\n这类[工作负载对象](/zh-cn/docs/concepts/workloads/controllers/)来间接创建的。\n工作负载对象为工作负载资源定义一个 **Pod 模板**和一个对应的负责基于该模板来创建\nPod 的{{< glossary_tooltip term_id=\"controller\" text=\"控制器\" >}}。\n为了尽早地捕获违例状况，`audit` 和 `warn` 模式都应用到负载资源。\n不过，`enforce` 模式并**不**应用到工作负载资源，仅应用到所生成的 Pod 对象上。"}
{"en": "## Exemptions\n\nYou can define _exemptions_ from pod security enforcement in order to allow the creation of pods that\nwould have otherwise been prohibited due to the policy associated with a given namespace.\nExemptions can be statically configured in the\n[Admission Controller configuration](/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller).", "zh": "## 豁免   {#exemptions}\n\n你可以为 Pod 安全性的实施设置**豁免（Exemptions）** 规则，\n从而允许创建一些本来会被与给定名字空间相关的策略所禁止的 Pod。\n豁免规则可以在[准入控制器配置](/zh-cn/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller)\n中静态配置。"}
{"en": "Exemptions must be explicitly enumerated. Requests meeting exemption criteria are _ignored_ by the\nAdmission Controller (all `enforce`, `audit` and `warn` behaviors are skipped). Exemption dimensions include:", "zh": "豁免规则必须显式枚举。满足豁免标准的请求会被准入控制器**忽略**\n（所有 `enforce`、`audit` 和 `warn` 行为都会被略过）。\n豁免的维度包括："}
{"en": "- **Usernames:** requests from users with an exempt authenticated (or impersonated) username are\n  ignored.\n- **RuntimeClassNames:** pods and [workload resources](#workload-resources-and-pod-templates) specifying an exempt runtime class name are\n  ignored.\n- **Namespaces:** pods and [workload resources](#workload-resources-and-pod-templates) in an exempt namespace are ignored.", "zh": "- **Username：** 来自用户名已被豁免的、已认证的（或伪装的）的用户的请求会被忽略。\n- **RuntimeClassName：** 指定了已豁免的运行时类名称的 Pod\n  和[负载资源](#workload-resources-and-pod-templates)会被忽略。\n- **Namespace：** 位于被豁免的名字空间中的 Pod 和[负载资源](#workload-resources-and-pod-templates)会被忽略。\n\n{{< caution >}}"}
{"en": "Most pods are created by a controller in response to a [workload\nresource](#workload-resources-and-pod-templates), meaning that exempting an end user will only\nexempt them from enforcement when creating pods directly, but not when creating a workload resource.\nController service accounts (such as `system:serviceaccount:kube-system:replicaset-controller`)\nshould generally not be exempted, as doing so would implicitly exempt any user that can create the\ncorresponding workload resource.", "zh": "大多数 Pod 是作为对[工作负载资源](#workload-resources-and-pod-templates)的响应，\n由控制器所创建的，这意味着为某最终用户提供豁免时，只会当该用户直接创建 Pod\n时对其实施安全策略的豁免。用户创建工作负载资源时不会被豁免。\n控制器服务账号（例如：`system:serviceaccount:kube-system:replicaset-controller`）\n通常不应该被豁免，因为豁免这类服务账号隐含着对所有能够创建对应工作负载资源的用户豁免。\n{{< /caution >}}"}
{"en": "Updates to the following pod fields are exempt from policy checks, meaning that if a pod update\nrequest only changes these fields, it will not be denied even if the pod is in violation of the\ncurrent policy level:", "zh": "策略检查时会对以下 Pod 字段的更新操作予以豁免，这意味着如果 Pod\n更新请求仅改变这些字段时，即使 Pod 违反了当前的策略级别，请求也不会被拒绝。"}
{"en": "- Any metadata updates **except** changes to the seccomp or AppArmor annotations:\n  - `seccomp.security.alpha.kubernetes.io/pod` (deprecated)\n  - `container.seccomp.security.alpha.kubernetes.io/*` (deprecated)\n  - `container.apparmor.security.beta.kubernetes.io/*` (deprecated)\n- Valid updates to `.spec.activeDeadlineSeconds`\n- Valid updates to `.spec.tolerations`", "zh": "- 除了对 seccomp 或 AppArmor 注解之外的所有元数据（Metadata）更新操作：\n  - `seccomp.security.alpha.kubernetes.io/pod` （已弃用）\n  - `container.seccomp.security.alpha.kubernetes.io/*` （已弃用）\n  - `container.apparmor.security.beta.kubernetes.io/*`（已弃用）\n- 对 `.spec.activeDeadlineSeconds` 的合法更新\n- 对 `.spec.tolerations` 的合法更新"}
{"en": "## Metrics\n\nHere are the Prometheus metrics exposed by kube-apiserver:", "zh": "## 指标   {#metrics}\n\n以下是 kube-apiserver 公开的 Prometheus 指标："}
{"en": "- `pod_security_errors_total`: This metric indicates the number of errors preventing normal evaluation.\n  Non-fatal errors may result in the latest restricted profile being used for enforcement.\n- `pod_security_evaluations_total`: This metric indicates the number of policy evaluations that have occurred,\n  not counting ignored or exempt requests during exporting.\n- `pod_security_exemptions_total`: This metric indicates the number of exempt requests, not counting ignored\n  or out of scope requests.", "zh": "- `pod_security_errors_total`：此指标表示妨碍正常评估的错误数量。\n  如果错误是非致命的，kube-apiserver 可能会强制实施最新的受限配置。\n- `pod_security_evaluations_total`：此指标表示已发生的策略评估的数量，\n  不包括导出期间被忽略或豁免的请求。\n- `pod_security_exemptions_total`：该指标表示豁免请求的数量，\n  不包括被忽略或超出范围的请求。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- [Pod Security Standards](/docs/concepts/security/pod-security-standards)\n- [Enforcing Pod Security Standards](/docs/setup/best-practices/enforcing-pod-security-standards)\n- [Enforce Pod Security Standards by Configuring the Built-in Admission Controller](/docs/tasks/configure-pod-container/enforce-standards-admission-controller)\n- [Enforce Pod Security Standards with Namespace Labels](/docs/tasks/configure-pod-container/enforce-standards-namespace-labels)", "zh": "- [Pod 安全性标准](/zh-cn/docs/concepts/security/pod-security-standards/)\n- [强制实施 Pod 安全性标准](/zh-cn/docs/setup/best-practices/enforcing-pod-security-standards/)\n- [通过配置内置的准入控制器强制实施 Pod 安全性标准](/zh-cn/docs/tasks/configure-pod-container/enforce-standards-admission-controller/)\n- [使用名字空间标签来实施 Pod 安全性标准](/zh-cn/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/)"}
{"en": "If you are running an older version of Kubernetes and want to upgrade\nto a version of Kubernetes that does not include PodSecurityPolicies,\nread [migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller](/docs/tasks/configure-pod-container/migrate-from-psp).", "zh": "如果你正运行较老版本的 Kubernetes，想要升级到不包含 PodSecurityPolicy 的 Kubernetes 版本，\n可以参阅[从 PodSecurityPolicy 迁移到内置的 PodSecurity 准入控制器](/zh-cn/docs/tasks/configure-pod-container/migrate-from-psp)。"}
{"en": "This page provides an overview of controlling access to the Kubernetes API.", "zh": "本页面概述了对 Kubernetes API 的访问控制。"}
{"en": "Users access the [Kubernetes API](/docs/concepts/overview/kubernetes-api/) using `kubectl`,\nclient libraries, or by making REST requests.  Both human users and\n[Kubernetes service accounts](/docs/tasks/configure-pod-container/configure-service-account/) can be\nauthorized for API access.\nWhen a request reaches the API, it goes through several stages, illustrated in the\nfollowing diagram:", "zh": "用户使用 `kubectl`、客户端库或构造 REST 请求来访问 [Kubernetes API](/zh-cn/docs/concepts/overview/kubernetes-api/)。\n人类用户和 [Kubernetes 服务账号](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/)都可以被鉴权访问 API。\n当请求到达 API 时，它会经历多个阶段，如下图所示：\n\n![Kubernetes API 请求处理步骤示意图](/zh-cn/docs/images/access-control-overview.svg)"}
{"en": "## Transport security", "zh": "## 传输安全 {#transport-security}"}
{"en": "By default, the Kubernetes API server listens on port 6443 on the first non-localhost\nnetwork interface, protected by TLS. In a typical production Kubernetes cluster, the\nAPI serves on port 443. The port can be changed with the `--secure-port`, and the\nlistening IP address with the `--bind-address` flag.\n\nThe API server presents a certificate. This certificate may be signed using\na private certificate authority (CA), or based on a public key infrastructure linked\nto a generally recognized CA. The certificate and corresponding private key can be set\nby using the `--tls-cert-file` and `--tls-private-key-file` flags.", "zh": "默认情况下，Kubernetes API 服务器在第一个非 localhost 网络接口的 6443 端口上进行监听，\n受 TLS 保护。在一个典型的 Kubernetes 生产集群中，API 使用 443 端口。\n该端口可以通过 `--secure-port` 进行变更，监听 IP 地址可以通过 `--bind-address` 标志进行变更。\n\nAPI 服务器出示证书。该证书可以使用私有证书颁发机构（CA）签名，也可以基于链接到公认的 CA 的公钥基础架构签名。\n该证书和相应的私钥可以通过使用 `--tls-cert-file` 和 `--tls-private-key-file` 标志进行设置。"}
{"en": "If your cluster uses a private certificate authority, you need a copy of that CA\ncertificate configured into your `~/.kube/config` on the client, so that you can\ntrust the connection and be confident it was not intercepted.\n\nYour client can present a TLS client certificate at this stage.", "zh": "如果你的集群使用私有证书颁发机构，你需要在客户端的 `~/.kube/config` 文件中提供该 CA 证书的副本，\n以便你可以信任该连接并确认该连接没有被拦截。\n\n你的客户端可以在此阶段出示 TLS 客户端证书。"}
{"en": "## Authentication\n\nOnce TLS is established, the HTTP request moves to the Authentication step.\nThis is shown as step **1** in the diagram.\nThe cluster creation script or cluster admin configures the API server to run\none or more Authenticator modules.\nAuthenticators are described in more detail in\n[Authentication](/docs/reference/access-authn-authz/authentication/).", "zh": "## 认证 {#authentication}\n\n如上图步骤 **1** 所示，建立 TLS 后， HTTP 请求将进入认证（Authentication）步骤。\n集群创建脚本或者集群管理员配置 API 服务器，使之运行一个或多个身份认证组件。\n身份认证组件在[认证](/zh-cn/docs/reference/access-authn-authz/authentication/)节中有更详细的描述。"}
{"en": "The input to the authentication step is the entire HTTP request; however, it typically\nexamines the headers and/or client certificate.\n\nAuthentication modules include client certificates, password, and plain tokens,\nbootstrap tokens, and JSON Web Tokens (used for service accounts).\n\nMultiple authentication modules can be specified, in which case each one is tried in sequence,\nuntil one of them succeeds.", "zh": "认证步骤的输入整个 HTTP 请求；但是，通常组件只检查头部或/和客户端证书。\n\n认证模块包含客户端证书、密码、普通令牌、引导令牌和 JSON Web 令牌（JWT，用于服务账号）。\n\n可以指定多个认证模块，在这种情况下，服务器依次尝试每个验证模块，直到其中一个成功。"}
{"en": "If the request cannot be authenticated, it is rejected with HTTP status code 401.\nOtherwise, the user is authenticated as a specific `username`, and the user name\nis available to subsequent steps to use in their decisions.  Some authenticators\nalso provide the group memberships of the user, while other authenticators\ndo not.\n\nWhile Kubernetes uses usernames for access control decisions and in request logging,\nit does not have a `User` object nor does it store usernames or other information about\nusers in its API.", "zh": "如果请求认证不通过，服务器将以 HTTP 状态码 401 拒绝该请求。\n反之，该用户被认证为特定的 `username`，并且该用户名可用于后续步骤以在其决策中使用。\n部分验证器还提供用户的组成员身份，其他则不提供。"}
{"en": "## Authorization\n\nAfter the request is authenticated as coming from a specific user, the request must\nbe authorized. This is shown as step **2** in the diagram.\n\nA request must include the username of the requester, the requested action, and\nthe object affected by the action. The request is authorized if an existing policy\ndeclares that the user has permissions to complete the requested action.\n\nFor example, if Bob has the policy below, then he can read pods only in the namespace `projectCaribou`:", "zh": "## 鉴权 {#authorization}\n\n如上图的步骤 **2** 所示，将请求验证为来自特定的用户后，请求必须被鉴权。\n\n请求必须包含请求者的用户名、请求的行为以及受该操作影响的对象。\n如果现有策略声明用户有权完成请求的操作，那么该请求被鉴权通过。\n\n例如，如果 Bob 有以下策略，那么他只能在 `projectCaribou` 名称空间中读取 Pod。\n\n```json\n{\n    \"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\",\n    \"kind\": \"Policy\",\n    \"spec\": {\n        \"user\": \"bob\",\n        \"namespace\": \"projectCaribou\",\n        \"resource\": \"pods\",\n        \"readonly\": true\n    }\n}\n```"}
{"en": "If Bob makes the following request, the request is authorized because he is\nallowed to read objects in the `projectCaribou` namespace:", "zh": "如果 Bob 执行以下请求，那么请求会被鉴权，因为允许他读取 `projectCaribou` 名称空间中的对象。\n\n```json\n{\n  \"apiVersion\": \"authorization.k8s.io/v1beta1\",\n  \"kind\": \"SubjectAccessReview\",\n  \"spec\": {\n    \"resourceAttributes\": {\n      \"namespace\": \"projectCaribou\",\n      \"verb\": \"get\",\n      \"group\": \"unicorn.example.org\",\n      \"resource\": \"pods\"\n    }\n  }\n}\n```"}
{"en": "If Bob makes a request to write (`create` or `update`) to the objects in the\n`projectCaribou` namespace, his authorization is denied. If Bob makes a request\nto read (`get`) objects in a different namespace such as `projectFish`, then his authorization is denied.\n\nKubernetes authorization requires that you use common REST attributes to interact\nwith existing organization-wide or cloud-provider-wide access control systems.\nIt is important to use REST formatting because these control systems might\ninteract with other APIs besides the Kubernetes API.", "zh": "如果 Bob 在 `projectCaribou` 名字空间中请求写（`create` 或 `update`）对象，其鉴权请求将被拒绝。\n如果 Bob 在诸如 `projectFish` 这类其它名字空间中请求读取（`get`）对象，其鉴权也会被拒绝。\n\nKubernetes 鉴权要求使用公共 REST 属性与现有的组织范围或云提供商范围的访问控制系统进行交互。\n使用 REST 格式很重要，因为这些控制系统可能会与 Kubernetes API 之外的 API 交互。"}
{"en": "Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode.\nWhen an administrator creates a cluster, they configure the authorization modules that should be used in the API server.\nIf more than one authorization modules are configured, Kubernetes checks each module,\nand if any module authorizes the request, then the request can proceed.\nIf all of the modules deny the request, then the request is denied (HTTP status code 403).\n\nTo learn more about Kubernetes authorization, including details about creating\npolicies using the supported authorization modules, see [Authorization](/docs/reference/access-authn-authz/authorization/).", "zh": "Kubernetes 支持多种鉴权模块，例如 ABAC 模式、RBAC 模式和 Webhook 模式等。\n管理员创建集群时，他们配置应在 API 服务器中使用的鉴权模块。\n如果配置了多个鉴权模块，则 Kubernetes 会检查每个模块，任意一个模块鉴权该请求，请求即可继续；\n如果所有模块拒绝了该请求，请求将会被拒绝（HTTP 状态码 403）。\n\n要了解更多有关 Kubernetes 鉴权的更多信息，包括有关使用支持鉴权模块创建策略的详细信息，\n请参阅[鉴权](/zh-cn/docs/reference/access-authn-authz/authorization/)。"}
{"en": "## Admission control \n\nAdmission Control modules are software modules that can modify or reject requests.\nIn addition to the attributes available to Authorization modules, Admission\nControl modules can access the contents of the object that is being created or modified.\n\nAdmission controllers act on requests that create, modify, delete, or connect to (proxy) an object.\nAdmission controllers do not act on requests that merely read objects.\nWhen multiple admission controllers are configured, they are called in order.", "zh": "## 准入控制 {#admission-control}\n\n准入控制模块是可以修改或拒绝请求的软件模块。\n除鉴权模块可用的属性外，准入控制模块还可以访问正在创建或修改的对象的内容。\n\n准入控制器对创建、修改、删除或（通过代理）连接对象的请求进行操作。\n准入控制器不会对仅读取对象的请求起作用。\n有多个准入控制器被配置时，服务器将依次调用它们。"}
{"en": "This is shown as step **3** in the diagram.\n\nUnlike Authentication and Authorization modules, if any admission controller module\nrejects, then the request is immediately rejected.\n\nIn addition to rejecting objects, admission controllers can also set complex defaults for\nfields.\n\nThe available Admission Control modules are described in [Admission Controllers](/docs/reference/access-authn-authz/admission-controllers/).\n\nOnce a request passes all admission controllers, it is validated using the validation routines\nfor the corresponding API object, and then written to the object store (shown as step **4**).", "zh": "这一操作如上图的步骤 **3** 所示。\n\n与身份认证和鉴权模块不同，如果任何准入控制器模块拒绝某请求，则该请求将立即被拒绝。\n\n除了拒绝对象之外，准入控制器还可以为字段设置复杂的默认值。\n\n可用的准入控制模块在[准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/)中进行了描述。\n\n请求通过所有准入控制器后，将使用检验例程检查对应的 API 对象，然后将其写入对象存储（如步骤 **4** 所示）。"}
{"en": "## Auditing\n\nKubernetes auditing provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster.\nThe cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself.\n\nFor more information, see [Auditing](/docs/tasks/debug/debug-cluster/audit/).", "zh": "## 审计 {#auditing}\n\nKubernetes 审计提供了一套与安全相关的、按时间顺序排列的记录，其中记录了集群中的操作序列。\n集群对用户、使用 Kubernetes API 的应用程序以及控制平面本身产生的活动进行审计。\n\n更多信息请参考[审计](/zh-cn/docs/tasks/debug/debug-cluster/audit/)。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "Read more documentation on authentication, authorization and API access control:\n\n- [Authenticating](/docs/reference/access-authn-authz/authentication/)\n   - [Authenticating with Bootstrap Tokens](/docs/reference/access-authn-authz/bootstrap-tokens/)\n- [Admission Controllers](/docs/reference/access-authn-authz/admission-controllers/)\n   - [Dynamic Admission Control](/docs/reference/access-authn-authz/extensible-admission-controllers/)\n- [Authorization](/docs/reference/access-authn-authz/authorization/)\n   - [Role Based Access Control](/docs/reference/access-authn-authz/rbac/)\n   - [Attribute Based Access Control](/docs/reference/access-authn-authz/abac/)\n   - [Node Authorization](/docs/reference/access-authn-authz/node/)\n   - [Webhook Authorization](/docs/reference/access-authn-authz/webhook/)\n- [Certificate Signing Requests](/docs/reference/access-authn-authz/certificate-signing-requests/)\n   - including [CSR approval](/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection)\n     and [certificate signing](/docs/reference/access-authn-authz/certificate-signing-requests/#signing)\n- Service accounts\n  - [Developer guide](/docs/tasks/configure-pod-container/configure-service-account/)\n  - [Administration](/docs/reference/access-authn-authz/service-accounts-admin/)\n\nYou can learn about:\n- how Pods can use\n  [Secrets](/docs/concepts/configuration/secret/#service-accounts-automatically-create-and-attach-secrets-with-api-credentials)\n  to obtain API credentials.", "zh": "阅读更多有关身份认证、鉴权和 API 访问控制的文档：\n\n- [认证](/zh-cn/docs/reference/access-authn-authz/authentication/)\n  - [使用 Bootstrap 令牌进行身份认证](/zh-cn/docs/reference/access-authn-authz/bootstrap-tokens/)\n- [准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/)\n  - [动态准入控制](/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/)\n- [鉴权](/zh-cn/docs/reference/access-authn-authz/authorization/)\n  - [基于角色的访问控制](/zh-cn/docs/reference/access-authn-authz/rbac/)\n  - [基于属性的访问控制](/zh-cn/docs/reference/access-authn-authz/abac/)\n  - [节点鉴权](/zh-cn/docs/reference/access-authn-authz/node/)\n  - [Webhook 鉴权](/zh-cn/docs/reference/access-authn-authz/webhook/)\n- [证书签名请求](/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests/)\n  - 包括 [CSR 认证](/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection)\n    和[证书签名](/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests/#signing)\n- 服务账号\n  - [开发者指导](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/)\n  - [管理](/zh-cn/docs/reference/access-authn-authz/service-accounts-admin/)\n\n你可以了解：\n- Pod 如何使用\n  [Secret](/zh-cn/docs/concepts/configuration/secret/#service-accounts-automatically-create-and-attach-secrets-with-api-credentials)\n  获取 API 凭据。"}
{"en": "This checklist aims at providing a basic list of guidance with links to more\ncomprehensive documentation on each topic. It does not claim to be exhaustive\nand is meant to evolve.\n\nOn how to read and use this document:\n\n- The order of topics does not reflect an order of priority.\n- Some checklist items are detailed in the paragraph below the list of each section.", "zh": "本清单旨在提供一个基本的指导列表，其中包含链接，指向各个主题的更为全面的文档。\n此清单不求详尽无遗，是预计会不断演化的。\n\n关于如何阅读和使用本文档：\n\n- 主题的顺序并不代表优先级的顺序。\n- 在每章节的列表下面的段落中，都详细列举了一些检查清项目。\n\n{{< caution >}}"}
{"en": "Checklists are **not** sufficient for attaining a good security posture on their\nown. A good security posture requires constant attention and improvement, but a\nchecklist can be the first step on the never-ending journey towards security\npreparedness. Some of the recommendations in this checklist may be too\nrestrictive or too lax for your specific security needs. Since Kubernetes\nsecurity is not \"one size fits all\", each category of checklist items should be\nevaluated on its merits.", "zh": "单靠检查清单是**不够的**，无法获得良好的安全态势。\n实现良好的安全态势需要持续的关注和改进，实现安全上有备无患的目标道路漫长，清单可作为征程上的第一步。\n对于你的特定安全需求，此清单中的某些建议可能过于严格或过于宽松。\n由于 Kubernetes 的安全性并不是“一刀切”的，因此针对每一类检查清单项目都应该做价值评估。\n{{< /caution >}}"}
{"en": "## Authentication & Authorization\n\n- [ ] `system:masters` group is not used for user or component authentication after bootstrapping.\n- [ ] The kube-controller-manager is running with `--use-service-account-credentials`\n  enabled.\n- [ ] The root certificate is protected (either an offline CA, or a managed\n  online CA with effective access controls).\n- [ ] Intermediate and leaf certificates have an expiry date no more than 3\n  years in the future.\n- [ ] A process exists for periodic access review, and reviews occur no more\n  than 24 months apart.\n- [ ] The [Role Based Access Control Good Practices](/docs/concepts/security/rbac-good-practices/)\n  are followed for guidance related to authentication and authorization.", "zh": "## 认证和鉴权 {#authentication-authorization}\n\n- [ ] 在启动后 `system:masters` 组不用于用户或组件的身份验证。\n- [ ] kube-controller-manager 运行时要启用 `--use-service-account-credentials` 参数。\n- [ ] 根证书要受到保护（或离线 CA，或一个具有有效访问控制的托管型在线 CA）。\n- [ ] 中级证书和叶子证书的有效期不要超过未来 3 年。\n- [ ] 存在定期访问审查的流程，审查间隔不要超过 24 个月。\n- [ ] 遵循[基于角色的访问控制良好实践](/zh-cn/docs/concepts/security/rbac-good-practices/)，\n  以获得与身份验证和授权相关的指导。"}
{"en": "After bootstrapping, neither users nor components should authenticate to the\nKubernetes API as `system:masters`. Similarly, running all of\nkube-controller-manager as `system:masters` should be avoided. In fact,\n`system:masters` should only be used as a break-glass mechanism, as opposed to\nan admin user.", "zh": "在启动后，用户和组件都不应以 `system:masters` 身份向 Kubernetes API 进行身份验证。\n同样，应避免将任何 kube-controller-manager 以 `system:masters` 运行。\n事实上，`system:masters` 应该只用作一个例外机制，而不是管理员用户。"}
{"en": "## Network security\n\n- [ ] CNI plugins in-use supports network policies.\n- [ ] Ingress and egress network policies are applied to all workloads in the\n  cluster.\n- [ ] Default network policies within each namespace, selecting all pods, denying\n  everything, are in place.\n- [ ] If appropriate, a service mesh is used to encrypt all communications inside of the cluster.\n- [ ] The Kubernetes API, kubelet API and etcd are not exposed publicly on Internet.\n- [ ] Access from the workloads to the cloud metadata API is filtered.\n- [ ] Use of LoadBalancer and ExternalIPs is restricted.", "zh": "## 网络安全 {#network-security}\n\n- [ ] 使用的 CNI 插件可支持网络策略。\n- [ ] 对集群中的所有工作负载应用入站和出站的网络策略。\n- [ ] 落实每个名字空间内的默认网络策略，覆盖所有 Pod，拒绝一切访问。\n- [ ] 如果合适，使用服务网格来加密集群内的所有通信。\n- [ ] 不在互联网上公开 Kubernetes API、kubelet API 和 etcd。\n- [ ] 过滤工作负载对云元数据 API 的访问。\n- [ ] 限制使用 LoadBalancer 和 ExternalIP。"}
{"en": "A number of [Container Network Interface (CNI) plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)\nplugins provide the functionality to\nrestrict network resources that pods may communicate with. This is most commonly done\nthrough [Network Policies](/docs/concepts/services-networking/network-policies/)\nwhich provide a namespaced resource to define rules. Default network policies\nblocking everything egress and ingress, in each namespace, selecting all the\npods, can be useful to adopt an allow list approach, ensuring that no workloads\nis missed.", "zh": "许多[容器网络接口（Container Network Interface，CNI）插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)提供了限制\nPod 可能与之通信的网络资源的功能。\n这种限制通常通过[网络策略](/zh-cn/docs/concepts/services-networking/network-policies/)来完成，\n网络策略提供了一种名字空间作用域的资源来定义规则。\n在每个名字空间中，默认的网络策略会阻塞所有的出入站流量，并选择所有 Pod，\n采用允许列表的方法很有用，可以确保不遗漏任何工作负载。"}
{"en": "Not all CNI plugins provide encryption in transit. If the chosen plugin lacks this\nfeature, an alternative solution could be to use a service mesh to provide that\nfunctionality.\n\nThe etcd datastore of the control plane should have controls to limit access and\nnot be publicly exposed on the Internet. Furthermore, mutual TLS (mTLS) should\nbe used to communicate securely with it. The certificate authority for this\nshould be unique to etcd.", "zh": "并非所有 CNI 插件都在传输过程中提供加密。\n如果所选的插件缺少此功能，一种替代方案是可以使用服务网格来提供该功能。\n\n控制平面的 etcd 数据存储应该实施访问限制控制，并且不要在互联网上公开。\n此外，应使用双向 TLS（mTLS）与其进行安全通信。\n用在这里的证书机构应该仅用于 etcd。"}
{"en": "External Internet access to the Kubernetes API server should be restricted to\nnot expose the API publicly. Be careful as many managed Kubernetes distribution\nare publicly exposing the API server by default. You can then use a bastion host\nto access the server.\n\nThe [kubelet](/docs/reference/command-line-tools-reference/kubelet/) API access\nshould be restricted and not publicly exposed, the defaults authentication and\nauthorization settings, when no configuration file specified with the `--config`\nflag, are overly permissive.", "zh": "应该限制外部互联网对 Kubernetes API 服务器未公开的 API 的访问。\n请小心，因为许多托管的 Kubernetes 发行版在默认情况下公开了 API 服务器。\n当然，你可以使用堡垒机访问服务器。\n\n对 [kubelet](/zh-cn/docs/reference/command-line-tools-reference/kubelet/) API 的访问应该受到限制，\n并且不公开，当没有使用 `--config` 参数来设置配置文件时，默认的身份验证和鉴权设置是过于宽松的。"}
{"en": "If a cloud provider is used for hosting Kubernetes, the access from pods to the cloud\nmetadata API `169.254.169.254` should also be restricted or blocked if not needed\nbecause it may leak information.\n\nFor restricted LoadBalancer and ExternalIPs use, see\n[CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs](https://github.com/kubernetes/kubernetes/issues/97076)\nand the [DenyServiceExternalIPs admission controller](/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips)\nfor further information.", "zh": "如果使用云服务供应商托管的 Kubernetes，在没有明确需要的情况下，\n也应该限制或阻止从 Pod 对云元数据 API `169.254.169.254` 的访问，因为这可能泄露信息。\n\n关于限制使用 LoadBalancer 和 ExternalIP 请参阅\n[CVE-2020-8554：中间人使用 LoadBalancer 或 ExternalIP](https://github.com/kubernetes/kubernetes/issues/97076)\n和\n[DenyServiceExternalIPs 准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips)获取更多信息。"}
{"en": "## Pod security\n\n- [ ] RBAC rights to `create`, `update`, `patch`, `delete` workloads is only granted if necessary.\n- [ ] Appropriate Pod Security Standards policy is applied for all namespaces and enforced.\n- [ ] Memory limit is set for the workloads with a limit equal or inferior to the request.\n- [ ] CPU limit might be set on sensitive workloads.\n- [ ] For nodes that support it, Seccomp is enabled with appropriate syscalls\n  profile for programs.\n- [ ] For nodes that support it, AppArmor or SELinux is enabled with appropriate\n  profile for programs.", "zh": "## Pod 安全 {#pod-security}\n\n- [ ] 仅在必要时才授予 `create`、`update`、`patch`、`delete` 工作负载的 RBAC 权限。\n- [ ] 对所有名字空间实施适当的 Pod 安全标准策略，并强制执行。\n- [ ] 为工作负载设置内存限制值，并确保限制值等于或者不高于请求值。\n- [ ] 对敏感工作负载可以设置 CPU 限制。\n- [ ] 对于支持 Seccomp 的节点，可以为程序启用合适的系统调用配置文件。\n- [ ] 对于支持 AppArmor 或 SELinux 的系统，可以为程序启用合适的配置文件。"}
{"en": "RBAC authorization is crucial but\n[cannot be granular enough to have authorization on the Pods' resources](/docs/concepts/security/rbac-good-practices/#workload-creation)\n(or on any resource that manages Pods). The only granularity is the API verbs\non the resource itself, for example, `create` on Pods. Without\nadditional admission, the authorization to create these resources allows direct\nunrestricted access to the schedulable nodes of a cluster.", "zh": "RBAC 的授权是至关重要的，\n但[不能在足够细的粒度上对 Pod 的资源进行授权](/zh-cn/docs/concepts/security/rbac-good-practices/#workload-creation)，\n也不足以对管理 Pod 的任何资源进行授权。\n唯一的粒度是资源本身上的 API 动作，例如，对 Pod 的 `create`。\n在未指定额外许可的情况下，创建这些资源的权限允许直接不受限制地访问集群的可调度节点。"}
{"en": "The [Pod Security Standards](/docs/concepts/security/pod-security-standards/)\ndefine three different policies, privileged, baseline and restricted that limit\nhow fields can be set in the `PodSpec` regarding security.\nThese standards can be enforced at the namespace level with the new\n[Pod Security](/docs/concepts/security/pod-security-admission/) admission,\nenabled by default, or by third-party admission webhook. Please note that,\ncontrary to the removed PodSecurityPolicy admission it replaces,\n[Pod Security](/docs/concepts/security/pod-security-admission/)\nadmission can be easily combined with admission webhooks and external services.", "zh": "[Pod 安全性标准](/zh-cn/docs/concepts/security/pod-security-standards/)定义了三种不同的策略：\n特权策略（Privileged）、基线策略（Baseline）和限制策略（Restricted），它们限制了 `PodSpec` 中关于安全的字段的设置。\n这些标准可以通过默认启用的新的\n[Pod 安全性准入](/zh-cn/docs/concepts/security/pod-security-admission/)或第三方准入 Webhook 在名字空间级别强制执行。\n请注意，与它所取代的、已被移除的 PodSecurityPolicy 准入机制相反，\n[Pod 安全性准入](/zh-cn/docs/concepts/security/pod-security-admission/)可以轻松地与准入 Webhook 和外部服务相结合使用。"}
{"en": "Pod Security admission `restricted` policy, the most restrictive policy of the\n[Pod Security Standards](/docs/concepts/security/pod-security-standards/) set,\n[can operate in several modes](/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces),\n`warn`, `audit` or `enforce` to gradually apply the most appropriate\n[security context](/docs/tasks/configure-pod-container/security-context/)\naccording to security best practices. Nevertheless, pods'\n[security context](/docs/tasks/configure-pod-container/security-context/)\nshould be separately investigated to limit the privileges and access pods may\nhave on top of the predefined security standards, for specific use cases.", "zh": "`restricted` Pod 安全准入策略是 [Pod 安全性标准](/zh-cn/docs/concepts/security/pod-security-standards/)集中最严格的策略，\n可以在[多种种模式下运行](/zh-cn/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces)，\n根据最佳安全实践，逐步地采用 `warn`、`audit` 或者 `enforce`\n模式以应用最合适的[安全上下文（Security Context）](/zh-cn/docs/tasks/configure-pod-container/security-context/)。\n尽管如此，对于特定的用例，应该单独审查 Pod 的[安全上下文](/zh-cn/docs/tasks/configure-pod-container/security-context/)，\n以限制 Pod 在预定义的安全性标准之上可能具有的特权和访问权限。"}
{"en": "For a hands-on tutorial on [Pod Security](/docs/concepts/security/pod-security-admission/),\nsee the blog post\n[Kubernetes 1.23: Pod Security Graduates to Beta](/blog/2021/12/09/pod-security-admission-beta/).\n\n[Memory and CPU limits](/docs/concepts/configuration/manage-resources-containers/)\nshould be set in order to restrict the memory and CPU resources a pod can\nconsume on a node, and therefore prevent potential DoS attacks from malicious or\nbreached workloads. Such policy can be enforced by an admission controller.\nPlease note that CPU limits will throttle usage and thus can have unintended\neffects on auto-scaling features or efficiency i.e. running the process in best\neffort with the CPU resource available.", "zh": "有关 [Pod 安全性](/zh-cn/docs/concepts/security/pod-security-admission/)的实践教程，\n请参阅博文 [Kubernetes 1.23：Pod 安全性升级到 Beta](/blog/2021/12/09/pod-security-admission-beta/)。\n\n为了限制一个 Pod 可以使用的内存和 CPU 资源，\n应该设置 Pod 在节点上可消费的[内存和 CPU 限制](/zh-cn/docs/concepts/configuration/manage-resources-containers/),\n从而防止来自恶意的或已被攻破的工作负载的潜在 DoS 攻击。这种策略可以由准入控制器强制执行。\n请注意，CPU 限制设置可能会影响 CPU 用量，从而可能会对自动扩缩功能或效率产生意外的影响，\n换言之，系统会在可用的 CPU 资源下最大限度地运行进程。\n\n{{< caution >}}"}
{"en": "Memory limit superior to request can expose the whole node to OOM issues.", "zh": "内存限制高于请求的，可能会使整个节点面临 OOM 问题。\n{{< /caution >}}"}
{"en": "### Enabling Seccomp\n\nSeccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12.\nIt can be used to sandbox the privileges of a process, restricting the calls it is able to make\nfrom userspace into the kernel. Kubernetes lets you automatically apply seccomp profiles loaded onto\na node to your Pods and containers.\n\nSeccomp can improve the security of your workloads by reducing the Linux kernel syscall attack\nsurface available inside containers. The seccomp filter mode leverages BPF to create an allow or\ndeny list of specific syscalls, named profiles.\n\nSince Kubernetes 1.27, you can enable the use of `RuntimeDefault` as the default seccomp profile\nfor all workloads. A [security tutorial](/docs/tutorials/security/seccomp/) is available on this\ntopic. In addition, the\n[Kubernetes Security Profiles Operator](https://github.com/kubernetes-sigs/security-profiles-operator)\nis a project that facilitates the management and use of seccomp in clusters.", "zh": "### 启用 Seccomp {#enabling-seccomp}\n\nSeccomp 代表安全计算模式（Secure computing mode），这是一个自 Linux 内核版本 2.6.12 被加入的特性。\n它可以将进程的特权沙箱化，来限制从用户空间发起的对内核的调用。\nKubernetes 允许你将加载到节点上的 Seccomp 配置文件自动应用于你的 Pod 和容器。\n\nSeccomp 通过减少容器内对 Linux 内核的系统调用（System Call）以缩小攻击面，从而提高工作负载的安全性。\nSeccomp 过滤器模式借助 BPF 创建具体系统调用的允许清单或拒绝清单，名为配置文件（Profile）。\n\n从 Kubernetes 1.27 开始，你可以将 `RuntimeDefault` 设置为工作负载的默认 Seccomp 配置。\n你可以阅读相应的[安全教程](/zh-cn/docs/tutorials/security/seccomp/)。\n此外，[Kubernetes Security Profiles Operator](https://github.com/kubernetes-sigs/security-profiles-operator)\n是一个方便在集群中管理和使用 Seccomp 的项目。\n\n{{< note >}}"}
{"en": "Seccomp is only available on Linux nodes.", "zh": "Seccomp 仅适用于 Linux 节点。\n{{< /note >}}"}
{"en": "### Enabling AppArmor or SELinux", "zh": "### 启用 AppArmor 或 SELinux {#enabling-appArmor-or-SELinux}\n\n#### AppArmor"}
{"en": "[AppArmor](/docs/tutorials/security/apparmor/) is a Linux kernel security module that can\nprovide an easy way to implement Mandatory Access Control (MAC) and better\nauditing through system logs. A default AppArmor profile is enforced on nodes that support it, or a custom profile can be configured.\nLike seccomp, AppArmor is also configured\nthrough profiles, where each profile is either running in enforcing mode, which\nblocks access to disallowed resources or complain mode, which only reports\nviolations. AppArmor profiles are enforced on a per-container basis, with an\nannotation, allowing for processes to gain just the right privileges.", "zh": "[AppArmor](/zh-cn/docs/tutorials/security/apparmor/) 是一个 Linux 内核安全模块，\n可以提供一种简单的方法来实现强制访问控制（Mandatory Access Control, MAC）并通过系统日志进行更好地审计。\n默认 AppArmor 配置文件在支持它的节点上强制执行，或者可以配置自定义配置文件。\n与 Seccomp 一样，AppArmor 也通过配置文件进行配置，\n其中每个配置文件要么在强制（Enforcing）模式下运行，即阻止访问不允许的资源，要么在投诉（Complaining）模式下运行，只报告违规行为。\nAppArmor 配置文件是通过注解的方式，以容器为粒度强制执行的，允许进程获得刚好合适的权限。\n\n{{< note >}}"}
{"en": "AppArmor is only available on Linux nodes, and enabled in\n[some Linux distributions](https://gitlab.com/apparmor/apparmor/-/wikis/home#distributions-and-ports).", "zh": "AppArmor 仅在 Linux 节点上可用，\n在[一些 Linux 发行版](https://gitlab.com/apparmor/apparmor/-/wikis/home#distributions-and-ports)中已启用。\n{{< /note >}}\n\n#### SELinux"}
{"en": "[SELinux](https://github.com/SELinuxProject/selinux-notebook/blob/main/src/selinux_overview.md) is also a\nLinux kernel security module that can provide a mechanism for supporting access\ncontrol security policies, including Mandatory Access Controls (MAC). SELinux\nlabels can be assigned to containers or pods\n[via their `securityContext` section](/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container).", "zh": "[SELinux](https://github.com/SELinuxProject/selinux-notebook/blob/main/src/selinux_overview.md)\n也是一个 Linux 内核安全模块，可以提供支持访问控制安全策略的机制，包括强制访问控制（MAC）。\nSELinux 标签可以[通过 `securityContext` 节](/zh-cn/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container)指配给容器或 Pod。\n\n{{< note >}}"}
{"en": "SELinux is only available on Linux nodes, and enabled in\n[some Linux distributions](https://en.wikipedia.org/wiki/Security-Enhanced_Linux#Implementations).", "zh": "SELinux 仅在 Linux 节点上可用，\n在[一些 Linux 发行版](https://en.wikipedia.org/wiki/Security-Enhanced_Linux#Implementations)中已启用。\n{{< /note >}}"}
{"en": "## Pod placement\n\n- [ ] Pod placement is done in accordance with the tiers of sensitivity of the\n  application.\n- [ ] Sensitive applications are running isolated on nodes or with specific\n  sandboxed runtimes.", "zh": "## Pod 布局 {#pod-placement}\n\n- [ ] Pod 布局是根据应用程序的敏感级别来完成的。\n- [ ] 敏感应用程序在节点上隔离运行或使用特定的沙箱运行时运行。"}
{"en": "Pods that are on different tiers of sensitivity, for example, an application pod\nand the Kubernetes API server, should be deployed onto separate nodes. The\npurpose of node isolation is to prevent an application container breakout to\ndirectly providing access to applications with higher level of sensitivity to easily\npivot within the cluster. This separation should be enforced to prevent pods\naccidentally being deployed onto the same node. This could be enforced with the\nfollowing features:", "zh": "处于不同敏感级别的 Pod，例如，应用程序 Pod 和 Kubernetes API 服务器，应该部署到不同的节点上。\n节点隔离的目的是防止应用程序容器的逃逸，进而直接访问敏感度更高的应用，\n甚至轻松地改变集群工作机制。\n这种隔离应该被强制执行，以防止 Pod 集合被意外部署到同一节点上。\n可以通过以下功能实现："}
{"en": "[Node Selectors](/docs/concepts/scheduling-eviction/assign-pod-node/)\n: Key-value pairs, as part of the pod specification, that specify which nodes to\ndeploy onto. These can be enforced at the namespace and cluster level with the\n[PodNodeSelector](/docs/reference/access-authn-authz/admission-controllers/#podnodeselector)\nadmission controller.", "zh": "[节点选择器（Node Selector）](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/)\n: 作为 Pod 规约的一部分来设置的键值对，指定 Pod 可部署到哪些节点。\n  通过 [PodNodeSelector](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#podnodeselector)\n  准入控制器可以在名字空间和集群级别强制实施节点选择。"}
{"en": "[PodTolerationRestriction](/docs/reference/access-authn-authz/admission-controllers/#podtolerationrestriction)\n: An admission controller that allows administrators to restrict permitted\n[tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration/) within a\nnamespace. Pods within a namespace may only utilize the tolerations specified on\nthe namespace object annotation keys that provide a set of default and allowed\ntolerations.", "zh": "[PodTolerationRestriction](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#podtolerationrestriction)\n: [容忍度](/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/)准入控制器，\n  允许管理员设置在名字空间内允许使用的容忍度。\n  名字空间中的 Pod 只能使用名字空间对象的注解键上所指定的容忍度，这些键提供默认和允许的容忍度集合。"}
{"en": "[RuntimeClass](/docs/concepts/containers/runtime-class/)\n: RuntimeClass is a feature for selecting the container runtime configuration.\nThe container runtime configuration is used to run a Pod's containers and can\nprovide more or less isolation from the host at the cost of performance\noverhead.", "zh": "[RuntimeClass](/zh-cn/docs/concepts/containers/runtime-class/)\n: RuntimeClass 是一个用于选择容器运行时配置的特性，容器运行时配置用于运行 Pod 中的容器，\n  并以性能开销为代价提供或多或少的主机隔离能力。\n\n## Secrets {#secrets}"}
{"en": "- [ ] ConfigMaps are not used to hold confidential data.\n- [ ] Encryption at rest is configured for the Secret API.\n- [ ] If appropriate, a mechanism to inject secrets stored in third-party storage\n  is deployed and available.\n- [ ] Service account tokens are not mounted in pods that don't require them.\n- [ ] [Bound service account token volume](/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume)\n  is in-use instead of non-expiring tokens.", "zh": "- [ ] 不用 ConfigMap 保存机密数据。\n- [ ] 为 Secret API 配置静态加密。\n- [ ] 如果合适，可以部署和使用一种机制，负责注入保存在第三方存储中的 Secret。\n- [ ] 不应该将服务账号令牌挂载到不需要它们的 Pod 中。\n- [ ] 使用[绑定的服务账号令牌卷](/zh-cn/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume)，\n  而不要使用不会过期的令牌。"}
{"en": "Secrets required for pods should be stored within Kubernetes Secrets as opposed\nto alternatives such as ConfigMap. Secret resources stored within etcd should\nbe [encrypted at rest](/docs/tasks/administer-cluster/encrypt-data/).", "zh": "Pod 所需的秘密信息应该存储在 Kubernetes Secret 中，而不是像 ConfigMap 这样的替代品中。\n存储在 etcd 中的 Secret 资源应该被静态加密。"}
{"en": "Pods needing secrets should have these automatically mounted through volumes,\npreferably stored in memory like with the [`emptyDir.medium` option](/docs/concepts/storage/volumes/#emptydir).\nMechanism can be used to also inject secrets from third-party storages as\nvolume, like the [Secrets Store CSI Driver](https://secrets-store-csi-driver.sigs.k8s.io/).\nThis should be done preferentially as compared to providing the pods service\naccount RBAC access to secrets. This would allow adding secrets into the pod as\nenvironment variables or files. Please note that the environment variable method\nmight be more prone to leakage due to crash dumps in logs and the\nnon-confidential nature of environment variable in Linux, as opposed to the\npermission mechanism on files.", "zh": "需要 Secret 的 Pod 应该通过卷自动挂载这些信息，\n最好使用 [`emptyDir.medium` 选项](/zh-cn/docs/concepts/storage/volumes/#emptydir)存储在内存中。\n该机制还可以用于从第三方存储中注入 Secret 作为卷，如 [Secret Store CSI 驱动](https://secrets-store-csi-driver.sigs.k8s.io/)。\n与通过 RBAC 来允许 Pod 服务账号访问 Secret 相比，应该优先使用上述机制。这种机制允许将 Secret 作为环境变量或文件添加到 Pod 中。\n请注意，与带访问权限控制的文件相比，由于日志的崩溃转储，以及 Linux 的环境变量的非机密性，环境变量方法可能更容易发生泄漏。"}
{"en": "Service account tokens should not be mounted into pods that do not require them. This can be configured by setting\n[`automountServiceAccountToken`](/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server)\nto `false` either within the service account to apply throughout the namespace\nor specifically for a pod. For Kubernetes v1.22 and above, use\n[Bound Service Accounts](/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume)\nfor time-bound service account credentials.", "zh": "不应该将服务账号令牌挂载到不需要它们的 Pod 中。这可以通过在服务账号内将\n[`automountServiceAccountToken`](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server)\n设置为 `false` 来完成整个名字空间范围的配置，或者也可以单独在 Pod 层面定制。\n对于 Kubernetes v1.22 及更高版本，\n请使用[绑定服务账号](/zh-cn/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume)作为有时间限制的服务账号凭证。"}
{"en": "## Images\n\n- [ ] Minimize unnecessary content in container images.\n- [ ] Container images are configured to be run as unprivileged user.\n- [ ] References to container images are made by sha256 digests (rather than\ntags) or the provenance of the image is validated by verifying the image's\ndigital signature at deploy time [via admission control](/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures-with-admission-controller).\n- [ ] Container images are regularly scanned during creation and in deployment, and\n  known vulnerable software is patched.", "zh": "## 镜像 {#images}\n\n- [ ] 尽量减少容器镜像中不必要的内容。\n- [ ] 容器镜像配置为以非特权用户身份运行。\n- [ ] 对容器镜像的引用是通过 Sha256 摘要实现的，而不是标签（tags），\n  或者[通过准入控制器](/zh-cn/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures-with-admission-controller)在部署时验证镜像的数字签名来验证镜像的来源。\n- [ ] 在创建和部署过程中定期扫描容器镜像，并对已知的漏洞软件进行修补。"}
{"en": "Container image should contain the bare minimum to run the program they\npackage. Preferably, only the program and its dependencies, building the image\nfrom the minimal possible base. In particular, image used in production should not\ncontain shells or debugging utilities, as an\n[ephemeral debug container](/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container)\ncan be used for troubleshooting.", "zh": "容器镜像应该包含运行其所打包的程序所需要的最少内容。\n最好，只使用程序及其依赖项，基于最小的基础镜像来构建镜像。\n尤其是，在生产中使用的镜像不应包含 Shell 或调试工具，\n因为可以使用[临时调试容器](/zh-cn/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container)进行故障排除。"}
{"en": "Build images to directly start with an unprivileged user by using the\n[`USER` instruction in Dockerfile](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user).\nThe [Security Context](/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod)\nallows a container image to be started with a specific user and group with\n`runAsUser` and `runAsGroup`, even if not specified in the image manifest.\nHowever, the file permissions in the image layers might make it impossible to just\nstart the process with a new unprivileged user without image modification.", "zh": "构建镜像时使用 [Dockerfile 中的 `USER`](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user)\n指令直接开始使用非特权用户。\n[安全上下文（Security Context）](/zh-cn/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod)\n允许使用 `runAsUser` 和 `runAsGroup` 来指定使用特定的用户和组来启动容器镜像，\n即使没有在镜像清单文件（Manifest）中指定这些配置信息。\n不过，镜像层中的文件权限设置可能无法做到在不修改镜像的情况下，使用新的非特权用户来启动进程。"}
{"en": "Avoid using image tags to reference an image, especially the `latest` tag, the\nimage behind a tag can be easily modified in a registry. Prefer using the\ncomplete `sha256` digest which is unique to the image manifest. This policy can be\nenforced via an [ImagePolicyWebhook](/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook).\nImage signatures can also be automatically [verified with an admission controller](/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures-with-admission-controller)\nat deploy time to validate their authenticity and integrity.", "zh": "避免使用镜像标签来引用镜像，尤其是 `latest` 标签，因为标签对应的镜像可以在仓库中被轻松地修改。\n首选使用完整的 `Sha256` 摘要，该摘要对特定镜像清单文件而言是唯一的。\n可以通过 [ImagePolicyWebhook](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook)\n强制执行此策略。\n镜像签名还可以在部署时由[准入控制器自动验证](/zh-cn/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures-with-admission-controller)，\n以验证其真实性和完整性。"}
{"en": "Scanning a container image can prevent critical vulnerabilities from being\ndeployed to the cluster alongside the container image. Image scanning should be\ncompleted before deploying a container image to a cluster and is usually done\nas part of the deployment process in a CI/CD pipeline. The purpose of an image\nscan is to obtain information about possible vulnerabilities and their\nprevention in the container image, such as a\n[Common Vulnerability Scoring System (CVSS)](https://www.first.org/cvss/)\nscore. If the result of the image scans is combined with the pipeline\ncompliance rules, only properly patched container images will end up in\nProduction.", "zh": "扫描容器镜像可以防止关键性的漏洞随着容器镜像一起被部署到集群中。\n镜像扫描应在将容器镜像部署到集群之前完成，通常作为 CI/CD 流水线中的部署过程的一部分来完成。\n镜像扫描的目的是获取有关容器镜像中可能存在的漏洞及其预防措施的信息，\n例如使用[公共漏洞评分系统 （Common Vulnerability Scoring System，CVSS）](https://www.first.org/cvss/)评分。\n如果镜像扫描的结果与管道合性规则匹配，则只有经过正确修补的容器镜像才会最终进入生产环境。"}
{"en": "## Admission controllers\n\n- [ ] An appropriate selection of admission controllers is enabled.\n- [ ] A pod security policy is enforced by the Pod Security Admission or/and a\n  webhook admission controller.\n- [ ] The admission chain plugins and webhooks are securely configured.", "zh": "## 准入控制器 {#admission-controllers}\n\n- [ ] 选择启用适当的准入控制器。\n- [ ] Pod 安全策略由 Pod 安全准入强制执行，或者和 Webhook 准入控制器一起强制执行。\n- [ ] 保证准入链插件和 Webhook 的配置都是安全的。"}
{"en": "Admission controllers can help to improve the security of the cluster. However,\nthey can present risks themselves as they extend the API server and\n[should be properly secured](/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/).", "zh": "准入控制器可以帮助提高集群的安全性。\n然而，由于它们是对 API 服务器的扩展，其自身可能会带来风险，\n所以它们[应该得到适当的保护](/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/)。"}
{"en": "The following lists present a number of admission controllers that could be\nconsidered to enhance the security posture of your cluster and application. It\nincludes controllers that may be referenced in other parts of this document.", "zh": "下面列出了一些准入控制器，可以考虑用这些控制器来增强集群和应用程序的安全状况。\n列表中包括了可能在本文档其他部分曾提及的控制器。"}
{"en": "This first group of admission controllers includes plugins\n[enabled by default](/docs/reference/access-authn-authz/admission-controllers/#which-plugins-are-enabled-by-default),\nconsider to leave them enabled unless you know what you are doing:", "zh": "第一组准入控制器包括[默认启用的插件](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#which-plugins-are-enabled-by-default)，\n除非你知道自己在做什么，否则请考虑保持它们处于被启用的状态："}
{"en": "[`CertificateApproval`](/docs/reference/access-authn-authz/admission-controllers/#certificateapproval)\n: Performs additional authorization checks to ensure the approving user has\npermission to approve certificate request.", "zh": "[`CertificateApproval`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#certificateapproval)\n: 执行额外的授权检查，以确保审批用户具有审批证书请求的权限。"}
{"en": "[`CertificateSigning`](/docs/reference/access-authn-authz/admission-controllers/#certificatesigning)\n: Performs additional authorization checks to ensure the signing user has\npermission to sign certificate requests.", "zh": "[`CertificateSigning`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#certificatesigning)\n: 执行其他授权检查，以确保签名用户具有签名证书请求的权限。"}
{"en": "[`CertificateSubjectRestriction`](/docs/reference/access-authn-authz/admission-controllers/#certificatesubjectrestriction)\n: Rejects any certificate request that specifies a 'group' (or 'organization\nattribute') of `system:masters`.", "zh": "[`CertificateSubjectRestriction`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#certificatesubjectrestriction)\n: 拒绝将 `group`（或 `organization attribute`）设置为 `system:masters` 的所有证书请求。"}
{"en": "[`LimitRanger`](/docs/reference/access-authn-authz/admission-controllers/#limitranger)\n: Enforce the LimitRange API constraints.", "zh": "[`LimitRanger`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#limitranger)\n: 强制执行 LimitRange API 约束。"}
{"en": "[`MutatingAdmissionWebhook`](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\n: Allows the use of custom controllers through webhooks, these controllers may\nmutate requests that it reviews.", "zh": "[`MutatingAdmissionWebhook`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\n: 允许通过 Webhook 使用自定义控制器，这些控制器可能会变更它所审查的请求。"}
{"en": "[`PodSecurity`](/docs/reference/access-authn-authz/admission-controllers/#podsecurity)\n: Replacement for Pod Security Policy, restricts security contexts of deployed\nPods.", "zh": "[`PodSecurity`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#podsecurity)\n: Pod Security Policy 的替代品，用于约束所部署 Pod 的安全上下文。"}
{"en": "[`ResourceQuota`](/docs/reference/access-authn-authz/admission-controllers/#resourcequota)\n: Enforces resource quotas to prevent over-usage of resources.", "zh": "[`ResourceQuota`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#resourcequota)\n: 强制执行资源配额，以防止资源被过度使用。"}
{"en": "[`ValidatingAdmissionWebhook`](/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook)\n: Allows the use of custom controllers through webhooks, these controllers do\nnot mutate requests that it reviews.", "zh": "[`ValidatingAdmissionWebhook`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook)\n: 允许通过 Webhook 使用自定义控制器，这些控制器不变更它所审查的请求。"}
{"en": "The second group includes plugin that are not enabled by default but in general\navailability state and recommended to improve your security posture:", "zh": "第二组包括默认情况下没有启用、但处于正式发布状态的插件，建议启用这些插件以改善你的安全状况："}
{"en": "[`DenyServiceExternalIPs`](/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips)\n: Rejects all net-new usage of the `Service.spec.externalIPs` field. This is a mitigation for\n[CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs](https://github.com/kubernetes/kubernetes/issues/97076).", "zh": "[`DenyServiceExternalIPs`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips)\n: 拒绝使用 `Service.spec.externalIPs` 字段，已有的 Service 不受影响，新增或者变更时不允许使用。\n  这是 [CVE-2020-8554：中间人使用 LoadBalancer 或 ExternalIP](https://github.com/kubernetes/kubernetes/issues/97076)\n  的缓解措施。"}
{"en": "[`NodeRestriction`](/docs/reference/access-authn-authz/admission-controllers/#noderestriction)\n: Restricts kubelet's permissions to only modify the pods API resources they own\nor the node API resource that represent themselves. It also prevents kubelet\nfrom using the `node-restriction.kubernetes.io/` annotation, which can be used\nby an attacker with access to the kubelet's credentials to influence pod\nplacement to the controlled node.", "zh": "[`NodeRestriction`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#noderestriction)\n: 将 kubelet 的权限限制为只能修改其拥有的 Pod API 资源或代表其自身的节点 API 资源。\n  此插件还可以防止 kubelet 使用 `node-restriction.kubernetes.io/` 注解，\n  攻击者可以使用该注解来访问 kubelet 的凭证，从而影响所控制的节点上的 Pod 布局。"}
{"en": "The third group includes plugins that are not enabled by default but could be\nconsidered for certain use cases:", "zh": "第三组包括默认情况下未启用，但可以考虑在某些场景下启用的插件："}
{"en": "[`AlwaysPullImages`](/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages)\n: Enforces the usage of the latest version of a tagged image and ensures that the deployer\nhas permissions to use the image.", "zh": "[`AlwaysPullImages`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages)\n: 强制使用最新版本标记的镜像，并确保部署者有权使用该镜像。"}
{"en": "[`ImagePolicyWebhook`](/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook)\n: Allows enforcing additional controls for images through webhooks.", "zh": "[`ImagePolicyWebhook`](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook)\n: 允许通过 Webhook 对镜像强制执行额外的控制。"}
{"en": "## What's next\n\n- [Privilege escalation via Pod creation](/docs/reference/access-authn-authz/authorization/#privilege-escalation-via-pod-creation)\n  warns you about a specific access control risk; check how you're managing that\n  threat.\n  - If you use Kubernetes RBAC, read\n    [RBAC Good Practices](/docs/concepts/security/rbac-good-practices/) for\n    further information on authorization.\n- [Securing a Cluster](/docs/tasks/administer-cluster/securing-a-cluster/) for\n  information on protecting a cluster from accidental or malicious access.\n- [Cluster Multi-tenancy guide](/docs/concepts/security/multi-tenancy/) for\n  configuration options recommendations and best practices on multi-tenancy.\n- [Blog post \"A Closer Look at NSA/CISA Kubernetes Hardening Guidance\"](/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/#building-secure-container-images)\n  for complementary resource on hardening Kubernetes clusters.", "zh": "## 接下来  {#what-is-next}\n\n- [通过 Pod 创建进行权限升级](/zh-cn/docs/reference/access-authn-authz/authorization/#privilege-escalation-via-pod-creation)会警告你特定的访问控制风险；\n  请检查你如何管理该风险。\n  - 如果你使用 Kubernetes RBAC，请阅读\n    [RBAC 良好实践](/zh-cn/docs/concepts/security/rbac-good-practices/)获取有关鉴权的更多信息。\n- [保护集群](/zh-cn/docs/tasks/administer-cluster/securing-a-cluster/)提供如何保护集群免受意外或恶意访问的信息。\n- [集群多租户指南](/zh-cn/docs/concepts/security/multi-tenancy/)提供有关多租户的配置选项建议和最佳实践。\n- [博文“深入了解 NSA/CISA Kubernetes 强化指南”](/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/#building-secure-container-images)为强化\n  Kubernetes 集群提供补充资源。"}
{"en": "The Pod Security Standards define three different _policies_ to broadly cover the security\nspectrum. These policies are _cumulative_ and range from highly-permissive to highly-restrictive.\nThis guide outlines the requirements of each policy.", "zh": "Pod 安全性标准定义了三种不同的**策略（Policy）**，以广泛覆盖安全应用场景。\n这些策略是**叠加式的（Cumulative）**，安全级别从高度宽松至高度受限。\n本指南概述了每个策略的要求。"}
{"en": "| Profile | Description |\n| ------ | ----------- |\n| <strong style=\"white-space: nowrap\">Privileged</strong> | Unrestricted policy, providing the widest possible level of permissions. This policy allows for known privilege escalations. |\n| <strong style=\"white-space: nowrap\">Baseline</strong> | Minimally restrictive policy which prevents known privilege escalations. Allows the default (minimally specified) Pod configuration. |\n| <strong style=\"white-space: nowrap\">Restricted</strong> | Heavily restricted policy, following current Pod hardening best practices. |", "zh": "| Profile | 描述 |\n| ------ | ----------- |\n| <strong style=\"white-space: nowrap\">Privileged</strong> | 不受限制的策略，提供最大可能范围的权限许可。此策略允许已知的特权提升。 |\n| <strong style=\"white-space: nowrap\">Baseline</strong> | 限制性最弱的策略，禁止已知的特权提升。允许使用默认的（规定最少）Pod 配置。 |\n| <strong style=\"white-space: nowrap\">Restricted</strong> | 限制性非常强的策略，遵循当前的保护 Pod 的最佳实践。 |"}
{"en": "## Profile Details", "zh": "## Profile 细节    {#profile-details}\n\n### Privileged"}
{"en": "**The _Privileged_ policy is purposely-open, and entirely unrestricted.** This type of policy is\ntypically aimed at system- and infrastructure-level workloads managed by privileged, trusted users.\n\nThe Privileged policy is defined by an absence of restrictions. If you define a Pod where the Privileged\nsecurity policy applies, the Pod you define is able to bypass typical container isolation mechanisms.\nFor example, you can define a Pod that has access to the node's host network.", "zh": "**_Privileged_ 策略是有目的地开放且完全无限制的策略。**\n此类策略通常针对由特权较高、受信任的用户所管理的系统级或基础设施级负载。\n\nPrivileged 策略定义中限制较少。\n如果你定义应用了 Privileged 安全策略的 Pod，你所定义的这个 Pod 能够绕过典型的容器隔离机制。\n例如，你可以定义有权访问节点主机网络的 Pod。\n\n### Baseline"}
{"en": "**The _Baseline_ policy is aimed at ease of adoption for common containerized workloads while\npreventing known privilege escalations.** This policy is targeted at application operators and\ndevelopers of non-critical applications. The following listed controls should be\nenforced/disallowed:", "zh": "**_Baseline_ 策略的目标是便于常见的容器化应用采用，同时禁止已知的特权提升。**\n此策略针对的是应用运维人员和非关键性应用的开发人员。\n下面列举的控制应该被实施（禁止）：\n\n{{< note >}}"}
{"en": "In this table, wildcards (`*`) indicate all elements in a list. For example,\n`spec.containers[*].securityContext` refers to the Security Context object for _all defined\ncontainers_. If any of the listed containers fails to meet the requirements, the entire pod will\nfail validation.", "zh": "在下述表格中，通配符（`*`）意味着一个列表中的所有元素。\n例如 `spec.containers[*].securityContext` 表示**所定义的所有容器**的安全性上下文对象。\n如果所列出的任一容器不能满足要求，整个 Pod 将无法通过校验。\n{{< /note >}}\n\n<table>"}
{"en": "caption style=\"display:none\">Baseline policy specification</caption", "zh": "<caption style=\"display:none\">Baseline 策略规范</caption>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td>控制（Control）</td>\n\t\t\t<td>策略（Policy）</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">HostProcess</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "Windows Pods offer the ability to run <a href=\"/docs/tasks/configure-pod-container/create-hostprocess-pod\">HostProcess containers</a> which enables privileged access to the Windows host machine. Privileged access to the host is disallowed in the Baseline policy.", "zh": "Windows Pod 提供了运行\n\t\t\t\t<a href=\"/zh-cn/docs/tasks/configure-pod-container/create-hostprocess-pod\">HostProcess 容器</a>的能力，\n\t\t\t\t这使得对 Windows 宿主的特权访问成为可能。Baseline 策略中禁止对宿主的特权访问。\n\t\t\t\t{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}\n\t\t\t\t</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.securityContext.windowsOptions.hostProcess</code></li>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.windowsOptions.hostProcess</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.windowsOptions.hostProcess</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.windowsOptions.hostProcess</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/nil", "zh": "未定义、nil</li>\n\t\t\t\t\t<li><code>false</code></li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">"}
{"en": "Host Namespaces", "zh": "宿主名字空间</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "Sharing the host namespaces must be disallowed.", "zh": "必须禁止共享宿主上的名字空间。</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.hostNetwork</code></li>\n\t\t\t\t\t<li><code>spec.hostPID</code></li>\n\t\t\t\t\t<li><code>spec.hostIPC</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/nil", "zh": "未定义、nil</li>\n\t\t\t\t\t<li><code>false</code></li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">"}
{"en": "Privileged Containers", "zh": "特权容器</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "Privileged Pods disable most security mechanisms and must be disallowed.", "zh": "特权 Pod 会使大多数安全性机制失效，必须被禁止。</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.privileged</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.privileged</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.privileged</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/nil", "zh": "未定义、nil</li>\n\t\t\t\t\t<li><code>false</code></li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">"}
{"en": "Capabilities", "zh": "权能</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "Adding additional capabilities beyond those listed below must be disallowed.", "zh": "必须禁止添加除下列字段之外的权能。</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.capabilities.add</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.capabilities.add</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/nil", "zh": "未定义、nil</li>\n\t\t\t\t\t<li><code>AUDIT_WRITE</code></li>\n\t\t\t\t\t<li><code>CHOWN</code></li>\n\t\t\t\t\t<li><code>DAC_OVERRIDE</code></li>\n\t\t\t\t\t<li><code>FOWNER</code></li>\n\t\t\t\t\t<li><code>FSETID</code></li>\n\t\t\t\t\t<li><code>KILL</code></li>\n\t\t\t\t\t<li><code>MKNOD</code></li>\n\t\t\t\t\t<li><code>NET_BIND_SERVICE</code></li>\n\t\t\t\t\t<li><code>SETFCAP</code></li>\n\t\t\t\t\t<li><code>SETGID</code></li>\n\t\t\t\t\t<li><code>SETPCAP</code></li>\n\t\t\t\t\t<li><code>SETUID</code></li>\n\t\t\t\t\t<li><code>SYS_CHROOT</code></li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">"}
{"en": "HostPath Volumes", "zh": "HostPath 卷</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "HostPath volumes must be forbidden.", "zh": "必须禁止 HostPath 卷。</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.volumes[*].hostPath</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/nil", "zh": "未定义、nil</li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">"}
{"en": "Host Ports", "zh": "宿主端口</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "HostPorts should be disallowed entirely (recommended) or restricted to a known list.", "zh": "应该完全禁止使用宿主端口（推荐）或者至少限制只能使用某确定列表中的端口。</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.containers[*].ports[*].hostPort</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].ports[*].hostPort</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].ports[*].hostPort</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/nil", "zh": "未定义、nil</li>\n\t\t\t\t\t<li>"}
{"en": "Known list (not supported by the built-in <a href=\"/docs/concepts/security/pod-security-admission/\">Pod Security Admission controller</a>)", "zh": "已知列表（不支持内置的 <a href=\"/zh-cn/docs/concepts/security/pod-security-admission/\">Pod 安全性准入控制器</a> ）</li>\n\t\t\t\t\t<li><code>0</code></li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">AppArmor</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "On supported hosts, the <code>RuntimeDefault</code> AppArmor profile is applied by default. The baseline policy should prevent overriding or disabling the default AppArmor profile, or restrict overrides to an allowed set of profiles.", "zh": "在受支持的主机上，默认使用 <code>RuntimeDefault</code> AppArmor 配置。Baseline\n\t\t\t\t策略应避免覆盖或者禁用默认策略，以及限制覆盖一些配置集合的权限。\n\t\t\t\t</p>\n        <p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n        <ul>\n              <li><code>spec.securityContext.appArmorProfile.type</code></li>\n              <li><code>spec.containers[*].securityContext.appArmorProfile.type</code></li>\n              <li><code>spec.initContainers[*].securityContext.appArmorProfile.type</code></li>\n              <li><code>spec.ephemeralContainers[*].securityContext.appArmorProfile.type</code></li>\n        </ul>\n        <p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值<</strong></p>\n        <ul>\n              <li>Undefined/nil</li>\n              <li><code>RuntimeDefault</code></li>\n              <li><code>Localhost</code></li>\n        </ul>\n        <hr />\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>metadata.annotations[\"container.apparmor.security.beta.kubernetes.io/*\"]</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/nil", "zh": "未定义、nil</li>\n\t\t\t\t\t<li><code>runtime/default</code></li>\n\t\t\t\t\t<li><code>localhost/*</code></li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">SELinux</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "Setting the SELinux type is restricted, and setting a custom SELinux user or role option is forbidden.", "zh": "设置 SELinux 类型的操作是被限制的，设置自定义的 SELinux 用户或角色选项是被禁止的。\n\t\t\t\t</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.securityContext.seLinuxOptions.type</code></li>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.seLinuxOptions.type</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.seLinuxOptions.type</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.type</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/\"\"", "zh": "未定义、\"\"</li>\n\t\t\t\t\t<li><code>container_t</code></li>\n\t\t\t\t\t<li><code>container_init_t</code></li>\n\t\t\t\t\t<li><code>container_kvm_t</code></li>\n          <li><code>container_engine_t</code>"}
{"en": "(since Kubernetes 1.31)", "zh": "（自从 Kubernetes 1.31）</li>\n\t\t\t\t</ul>\n\t\t\t\t<hr />\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.securityContext.seLinuxOptions.user</code></li>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.seLinuxOptions.user</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.seLinuxOptions.user</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.user</code></li>\n\t\t\t\t\t<li><code>spec.securityContext.seLinuxOptions.role</code></li>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.seLinuxOptions.role</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.seLinuxOptions.role</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.role</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/\"\"", "zh": "未定义、\"\"</li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\"><code>/proc</code>"}
{"en": "Mount Type", "zh": "挂载类型</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "The default <code>/proc</code> masks are set up to reduce attack surface, and should be required.", "zh": "要求使用默认的 <code>/proc</code> 掩码以减小攻击面。</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.procMount</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.procMount</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.procMount</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/nil", "zh": "未定义、nil</li>\n\t\t\t\t\t<li><code>Default</code></li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n  \t\t\t<td>Seccomp</td>\n  \t\t\t<td>\n  \t\t\t\t<p>"}
{"en": "Seccomp profile must not be explicitly set to <code>Unconfined</code>.", "zh": "Seccomp 配置必须不能显式设置为 <code>Unconfined</code>。</p>\n  \t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.securityContext.seccompProfile.type</code></li>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.seccompProfile.type</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/nil", "zh": "未定义、nil</li>\n\t\t\t\t\t<li><code>RuntimeDefault</code></li>\n\t\t\t\t\t<li><code>Localhost</code></li>\n\t\t\t\t</ul>\n  \t\t\t</td>\n  \t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">Sysctls</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed \"safe\" subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node.", "zh": "sysctl 可以禁用安全机制或影响宿主上所有容器，因此除了若干“安全”的允许子集之外，其他都应该被禁止。\n\t\t\t\t如果某 sysctl 是受容器或 Pod 的名字空间限制，且与节点上其他 Pod 或进程相隔离，可认为是安全的。\n\t\t\t\t</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.securityContext.sysctls[*].name</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/nil", "zh": "未定义、nil</li>\n\t\t\t\t\t<li><code>kernel.shm_rmid_forced</code></li>\n\t\t\t\t\t<li><code>net.ipv4.ip_local_port_range</code></li>\n\t\t\t\t\t<li><code>net.ipv4.ip_unprivileged_port_start</code></li>\n\t\t\t\t\t<li><code>net.ipv4.tcp_syncookies</code></li>\n\t\t\t\t\t<li><code>net.ipv4.ping_group_range</code></li>\n\t\t\t\t\t<li><code>net.ipv4.ip_local_reserved_ports</code>"}
{"en": "(since Kubernetes 1.27)", "zh": "（从 Kubernetes 1.27 开始）</li>\n\t\t\t\t\t<li><code>net.ipv4.tcp_keepalive_time</code>"}
{"en": "(since Kubernetes 1.29)", "zh": "（从 Kubernetes 1.29 开始）</li>\n\t\t\t\t\t<li><code>net.ipv4.tcp_fin_timeout</code>"}
{"en": "(since Kubernetes 1.29)", "zh": "（从 Kubernetes 1.29 开始）</li>\n\t\t\t\t\t<li><code>net.ipv4.tcp_keepalive_intvl</code>"}
{"en": "(since Kubernetes 1.29)", "zh": "（从 Kubernetes 1.29 开始）</li>\n\t\t\t\t\t<li><code>net.ipv4.tcp_keepalive_probes</code>"}
{"en": "(since Kubernetes 1.29)", "zh": "（从 Kubernetes 1.29 开始）</li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n### Restricted"}
{"en": "**The _Restricted_ policy is aimed at enforcing current Pod hardening best practices, at the\nexpense of some compatibility.** It is targeted at operators and developers of security-critical\napplications, as well as lower-trust users. The following listed controls should be\nenforced/disallowed:", "zh": "**_Restricted_ 策略旨在实施当前保护 Pod 的最佳实践，尽管这样作可能会牺牲一些兼容性。**\n该类策略主要针对运维人员和安全性很重要的应用的开发人员，以及不太被信任的用户。\n下面列举的控制需要被实施（禁止）：\n\n{{< note >}}"}
{"en": "In this table, wildcards (`*`) indicate all elements in a list. For example,\n`spec.containers[*].securityContext` refers to the Security Context object for _all defined\ncontainers_. If any of the listed containers fails to meet the requirements, the entire pod will\nfail validation.", "zh": "在下述表格中，通配符（`*`）意味着一个列表中的所有元素。\n例如 `spec.containers[*].securityContext` 表示**所定义的所有容器**的安全性上下文对象。\n如果所列出的任一容器不能满足要求，整个 Pod 将无法通过校验。\n{{< /note >}}\n\n<table>\n\t<caption style=\"display:none\">"}
{"en": "Restricted policy specification", "zh": "Restricted 策略规范</caption>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td width=\"30%\"><strong>"}
{"en": "Control", "zh": "控制</strong></td>\n\t\t\t<td><strong>"}
{"en": "Policy", "zh": "策略</strong></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td colspan=\"2\"><em>"}
{"en": "Everything from the Baseline policy", "zh": "Baseline 策略的所有要求</em></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">"}
{"en": "Volume Types", "zh": "卷类型</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "The Restricted policy only permits the following volume types.", "zh": "Restricted 策略仅允许以下卷类型。\n\t\t\t\t</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.volumes[*]</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>"}
{"en": "Every item in the <code>spec.volumes[*]</code> list must set one of the following fields to a non-null value:", "zh": "<code>spec.volumes[*]</code> 列表中的每个条目必须将下面字段之一设置为非空值：\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.volumes[*].configMap</code></li>\n\t\t\t\t\t<li><code>spec.volumes[*].csi</code></li>\n\t\t\t\t\t<li><code>spec.volumes[*].downwardAPI</code></li>\n\t\t\t\t\t<li><code>spec.volumes[*].emptyDir</code></li>\n\t\t\t\t\t<li><code>spec.volumes[*].ephemeral</code></li>\n\t\t\t\t\t<li><code>spec.volumes[*].persistentVolumeClaim</code></li>\n\t\t\t\t\t<li><code>spec.volumes[*].projected</code></li>\n\t\t\t\t\t<li><code>spec.volumes[*].secret</code></li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">"}
{"en": "Privilege Escalation (v1.8+)", "zh": "特权提升（v1.8+）</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "Privilege escalation (such as via set-user-ID or set-group-ID file mode) should not be allowed. <em><a href=\"#os-specific-policy-controls\">This is Linux only policy</a> in v1.25+ <code>(spec.os.name != windows)</code></em>", "zh": "禁止（通过 SetUID 或 SetGID 文件模式）获得特权提升。<em><a href=\"#policies-specific-to-linux\">这是 v1.25+ 中仅针对 Linux 的策略</a> <code>(spec.os.name != windows)</code></em></p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.allowPrivilegeEscalation</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.allowPrivilegeEscalation</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.allowPrivilegeEscalation</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>false</code></li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">"}
{"en": "Running as Non-root", "zh": "以非 root 账号运行</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "Containers must be required to run as non-root users.", "zh": "容器必须以非 root 账号运行。</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.securityContext.runAsNonRoot</code></li>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.runAsNonRoot</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.runAsNonRoot</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.runAsNonRoot</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>true</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<small>"}
{"en": "The container fields may be undefined/<code>nil</code> if the pod-level\n\t\t\t\t\t<code>spec.securityContext.runAsNonRoot</code> is set to <code>true</code>.", "zh": "如果 Pod 级别 <code>spec.securityContext.runAsNonRoot</code> 设置为 <code>true</code>，则允许容器组的安全上下文字段设置为 未定义/<code>nil</code>。\n\t\t\t\t</small>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">"}
{"en": "Running as Non-root user (v1.23+)", "zh": "非 root 用户（v1.23+）</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "Containers must not set <tt>runAsUser</tt> to 0", "zh": "容器不可以将 <tt>runAsUser</tt> 设置为 0</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.securityContext.runAsUser</code></li>\n          <li><code>spec.containers[*].securityContext.runAsUser</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.runAsUser</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.runAsUser</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "any non-zero value", "zh": "所有的非零值</li>\n\t\t\t\t\t<li><code>undefined/null</code></li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td style=\"white-space: nowrap\">Seccomp (v1.19+)</td>\n\t\t\t<td>\n  \t\t\t\t<p>"}
{"en": "Seccomp profile must be explicitly set to one of the allowed values. Both the <code>Unconfined</code> profile and the <em>absence</em> of a profile are prohibited. <em><a href=\"#os-specific-policy-controls\">This is Linux only policy</a> in v1.25+ <code>(spec.os.name != windows)</code></em>", "zh": "Seccomp Profile 必须被显式设置成一个允许的值。禁止使用 <code>Unconfined</code> Profile 或者指定 <em>不存在的</em> Profile。<em><a href=\"#policies-specific-to-linux\">这是 v1.25+ 中仅针对 Linux 的策略</a> <code>(spec.os.name != windows)</code></em></p>\n  \t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.securityContext.seccompProfile.type</code></li>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.seccompProfile.type</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>RuntimeDefault</code></li>\n\t\t\t\t\t<li><code>Localhost</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<small>"}
{"en": "The container fields may be undefined/<code>nil</code> if the pod-level\n\t\t\t\t\t<code>spec.securityContext.seccompProfile.type</code> field is set appropriately.\n\t\t\t\t\tConversely, the pod-level field may be undefined/<code>nil</code> if _all_ container-\n\t\t\t\t\tlevel fields are set.", "zh": "如果 Pod 级别的 <code>spec.securityContext.seccompProfile.type</code>\n          已设置得当，容器级别的安全上下文字段可以为未定义/<code>nil</code>。\n          反之如果 <bold>所有的</bold> 容器级别的安全上下文字段已设置，\n          则 Pod 级别的字段可为 未定义/<code>nil</code>。\n\t\t\t\t</small>\n  \t\t\t</td>\n  \t\t</tr>\n\t\t  <tr>\n\t\t\t<td style=\"white-space: nowrap\">"}
{"en": "Capabilities (v1.22+)", "zh": "权能（v1.22+）</td>\n\t\t\t<td>\n\t\t\t\t<p>"}
{"en": "Containers must drop <code>ALL</code> capabilities, and are only permitted to add back\n\t\t\t\t\tthe <code>NET_BIND_SERVICE</code> capability. <em><a href=\"#os-specific-policy-controls\">This is Linux only policy</a> in v1.25+ <code>(.spec.os.name != \"windows\")</code></em>", "zh": "容器必须弃用 <code>ALL</code> 权能，并且只允许添加\n          <code>NET_BIND_SERVICE</code> 权能。<em><a href=\"#policies-specific-to-linux\">这是 v1.25+ 中仅针对 Linux 的策略</a> <code>(.spec.os.name != \"windows\")</code></em>\n\t\t\t\t</p>\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.capabilities.drop</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.capabilities.drop</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.capabilities.drop</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Any list of capabilities that includes <code>ALL</code>", "zh": "包括 <code>ALL</code> 在内的任意权能列表。</li>\n\t\t\t\t</ul>\n\t\t\t\t<hr />\n\t\t\t\t<p><strong>"}
{"en": "Restricted Fields", "zh": "限制的字段</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li><code>spec.containers[*].securityContext.capabilities.add</code></li>\n\t\t\t\t\t<li><code>spec.initContainers[*].securityContext.capabilities.add</code></li>\n\t\t\t\t\t<li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li>\n\t\t\t\t</ul>\n\t\t\t\t<p><strong>"}
{"en": "Allowed Values", "zh": "准许的取值</strong></p>\n\t\t\t\t<ul>\n\t\t\t\t\t<li>"}
{"en": "Undefined/nil", "zh": "未定义、nil</li>\n\t\t\t\t\t<li><code>NET_BIND_SERVICE</code></li>\n\t\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t</tbody>\n</table>"}
{"en": "## Policy Instantiation\n\nDecoupling policy definition from policy instantiation allows for a common understanding and\nconsistent language of policies across clusters, independent of the underlying enforcement\nmechanism.\n\nAs mechanisms mature, they will be defined below on a per-policy basis. The methods of enforcement\nof individual policies are not defined here.", "zh": "## 策略实例化   {#policy-instantiation}\n\n将策略定义从策略实例中解耦出来有助于形成跨集群的策略理解和语言陈述，\n以免绑定到特定的下层实施机制。\n\n随着相关机制的成熟，这些机制会按策略分别定义在下面。特定策略的实施方法不在这里定义。"}
{"en": "[**Pod Security Admission Controller**](/docs/concepts/security/pod-security-admission/)", "zh": "[**Pod 安全性准入控制器**](/zh-cn/docs/concepts/security/pod-security-admission/)\n\n- {{< example file=\"security/podsecurity-privileged.yaml\" >}}Privileged 名字空间{{< /example >}}\n- {{< example file=\"security/podsecurity-baseline.yaml\" >}}Baseline 名字空间{{< /example >}}\n- {{< example file=\"security/podsecurity-restricted.yaml\" >}}Restricted 名字空间{{< /example >}}"}
{"en": "### Alternatives", "zh": "### 替代方案   {#alternatives}\n\n{{% thirdparty-content %}}"}
{"en": "Other alternatives for enforcing policies are being developed in the Kubernetes ecosystem, such as:", "zh": "在 Kubernetes 生态系统中还在开发一些其他的替代方案，例如：\n\n- [Kubewarden](https://github.com/kubewarden)\n- [Kyverno](https://kyverno.io/policies/pod-security/)\n- [OPA Gatekeeper](https://github.com/open-policy-agent/gatekeeper)"}
{"en": "## Pod OS field\n\nKubernetes lets you use nodes that run either Linux or Windows. You can mix both kinds of\nnode in one cluster.\nWindows in Kubernetes has some limitations and differentiators from Linux-based\nworkloads. Specifically, many of the Pod `securityContext` fields\n[have no effect on Windows](/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext).", "zh": "## Pod OS 字段   {#pod-os-field}\n\nKubernetes 允许你使用运行 Linux 或 Windows 的节点。你可以在一个集群中混用两种类型的节点。\nKubernetes 中的 Windows 与基于 Linux 的工作负载相比有一些限制和差异。\n具体而言，许多 Pod `securityContext`\n字段[在 Windows 上不起作用](/zh-cn/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext)。\n\n\n{{< note >}}"}
{"en": "Kubelets prior to v1.24 don't enforce the pod OS field, and if a cluster has nodes on versions earlier than v1.24 the Restricted policies should be pinned to a version prior to v1.25.", "zh": "v1.24 之前的 kubelet 不强制处理 Pod OS 字段，如果集群中有些节点运行早于 v1.24 的版本，\n则应将 Restricted 策略锁定到 v1.25 之前的版本。\n{{< /note >}}"}
{"en": "### Restricted Pod Security Standard changes\nAnother important change, made in Kubernetes v1.25 is that the  _Restricted_ policy\nhas been updated to use the `pod.spec.os.name` field. Based on the OS name, certain policies that are specific\nto a particular OS can be relaxed for the other OS.", "zh": "### 限制性的 Pod Security Standard 变更   {#restricted-pod-security-standard-changes}\n\nKubernetes v1.25 中的另一个重要变化是 _Restricted_ 策略已更新，\n能够处理 `pod.spec.os.name` 字段。根据 OS 名称，专用于特定 OS 的某些策略对其他 OS 可以放宽限制。"}
{"en": "#### OS-specific policy controls\n\nRestrictions on the following controls are only required if `.spec.os.name` is not `windows`:\n- Privilege Escalation\n- Seccomp\n- Linux Capabilities", "zh": "#### OS 特定的策略控制\n\n仅当 `.spec.os.name` 不是 `windows` 时，才需要对以下控制进行限制：\n\n- 特权提升\n- Seccomp\n- Linux 权能"}
{"en": "## User namespaces\n\nUser Namespaces are a Linux-only feature to run workloads with increased\nisolation. How they work together with Pod Security Standards is described in\nthe [documentation](/docs/concepts/workloads/pods/user-namespaces#integration-with-pod-security-admission-checks) for Pods that use user namespaces.", "zh": "## 用户命名空间    {#user-namespaces}\n\n用户命名空间是 Linux 特有的功能，可在运行工作负载时提高隔离度。\n关于用户命名空间如何与 PodSecurityStandard 协同工作，\n请参阅[文档](/zh-cn/docs/concepts/workloads/pods/user-namespaces#integration-with-pod-security-admission-checks)了解\nPod 如何使用用户命名空间。"}
{"en": "## FAQ\n\n### Why isn't there a profile between Privileged and Baseline?", "zh": "## 常见问题    {#faq}\n\n### 为什么不存在介于 Privileged 和 Baseline 之间的策略类型   {#why-isnt-there-a-profile-between-privileged-and-baseline}"}
{"en": "The three profiles defined here have a clear linear progression from most secure (Restricted) to least\nsecure (Privileged), and cover a broad set of workloads. Privileges required above the Baseline\npolicy are typically very application specific, so we do not offer a standard profile in this\nniche. This is not to say that the privileged profile should always be used in this case, but that\npolicies in this space need to be defined on a case-by-case basis.\n\nSIG Auth may reconsider this position in the future, should a clear need for other profiles arise.", "zh": "这里定义的三种策略框架有一个明晰的线性递进关系，从最安全（Restricted）到最不安全（Privileged），\n并且覆盖了很大范围的工作负载。特权要求超出 Baseline 策略，这通常是特定于应用的需求，\n所以我们没有在这个范围内提供标准框架。这并不意味着在这样的情形下仍然只能使用 Privileged 框架，\n只是说处于这个范围的策略需要因地制宜地定义。\n\nSIG Auth 可能会在将来考虑这个范围的框架，前提是有对其他框架的需求。"}
{"en": "### What's the difference between a security profile and a security context?\n\n[Security Contexts](/docs/tasks/configure-pod-container/security-context/) configure Pods and\nContainers at runtime. Security contexts are defined as part of the Pod and container specifications\nin the Pod manifest, and represent parameters to the container runtime.", "zh": "### 安全配置与安全上下文的区别是什么？   {#whats-the-difference-between-security-profile-and-security-context}\n\n[安全上下文](/zh-cn/docs/tasks/configure-pod-container/security-context/)在运行时配置 Pod\n和容器。安全上下文是在 Pod 清单中作为 Pod 和容器规约的一部分来定义的，\n所代表的是传递给容器运行时的参数。"}
{"en": "Security profiles are control plane mechanisms to enforce specific settings in the Security Context,\nas well as other related parameters outside the Security Context. As of July 2021,\n[Pod Security Policies](/docs/concepts/security/pod-security-policy/) are deprecated in favor of the\nbuilt-in [Pod Security Admission Controller](/docs/concepts/security/pod-security-admission/).", "zh": "安全策略则是控制面用来对安全上下文以及安全性上下文之外的参数实施某种设置的机制。\n在 2021 年 7 月，\n[Pod 安全性策略](/zh-cn/docs/concepts/security/pod-security-policy/)已被废弃，\n取而代之的是内置的 [Pod 安全性准入控制器](/zh-cn/docs/concepts/security/pod-security-admission/)。"}
{"en": "### What about sandboxed Pods?\n\nThere is not currently an API standard that controls whether a Pod is considered sandboxed or\nnot. Sandbox Pods may be identified by the use of a sandboxed runtime (such as gVisor or Kata\nContainers), but there is no standard definition of what a sandboxed runtime is.", "zh": "### 沙箱（Sandboxed）Pod 怎么处理？  {#what-about-sandboxed-pods}\n\n现在还没有 API 标准来控制 Pod 是否被视作沙箱化 Pod。\n沙箱 Pod 可以通过其是否使用沙箱化运行时（如 gVisor 或 Kata Container）来辨别，\n不过目前还没有关于什么是沙箱化运行时的标准定义。"}
{"en": "The protections necessary for sandboxed workloads can differ from others. For example, the need to\nrestrict privileged permissions is lessened when the workload is isolated from the underlying\nkernel. This allows for workloads requiring heightened permissions to still be isolated.\n\nAdditionally, the protection of sandboxed workloads is highly dependent on the method of\nsandboxing. As such, no single recommended profile is recommended for all sandboxed workloads.", "zh": "沙箱化负载所需要的保护可能彼此各不相同。例如，当负载与下层内核直接隔离开来时，\n限制特权化操作的许可就不那么重要。这使得那些需要更多许可权限的负载仍能被有效隔离。\n\n此外，沙箱化负载的保护高度依赖于沙箱化的实现方法。\n因此，现在还没有针对所有沙箱化负载的建议配置。"}
{"en": "overview", "zh": "{{% alert title=\"被移除的特性\" color=\"warning\" %}}"}
{"en": "PodSecurityPolicy was [deprecated](/blog/2021/04/08/kubernetes-1-21-release-announcement/#podsecuritypolicy-deprecation)\nin Kubernetes v1.21, and removed from Kubernetes in v1.25.", "zh": "PodSecurityPolicy 在 Kubernetes v1.21\n中[被弃用](/blog/2021/04/08/kubernetes-1-21-release-announcement/#podsecuritypolicy-deprecation)，\n在 Kubernetes v1.25 中被移除。\n{{% /alert %}}"}
{"en": "Instead of using PodSecurityPolicy, you can enforce similar restrictions on Pods using\neither or both:", "zh": "作为替代，你可以使用下面任一方式执行类似的限制，或者同时使用下面这两种方式。"}
{"en": "- [Pod Security Admission](/docs/concepts/security/pod-security-admission/)\n- a 3rd party admission plugin, that you deploy and configure yourself", "zh": "- [Pod 安全准入](/zh-cn/docs/concepts/security/pod-security-admission/)\n- 自行部署并配置第三方准入插件"}
{"en": "For a migration guide, see [Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller](/docs/tasks/configure-pod-container/migrate-from-psp/).\nFor more information on the removal of this API,\nsee [PodSecurityPolicy Deprecation: Past, Present, and Future](/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/).", "zh": "有关如何迁移，\n参阅[从 PodSecurityPolicy 迁移到内置的 PodSecurity 准入控制器](/zh-cn/docs/tasks/configure-pod-container/migrate-from-psp/)。\n有关移除此 API 的更多信息，参阅\n[弃用 PodSecurityPolicy：过去、现在、未来](/zh-cn/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/)。"}
{"en": "If you are not running Kubernetes v{{< skew currentVersion >}}, check the documentation for\nyour version of Kubernetes.", "zh": "如果所运行的 Kubernetes 不是 v{{< skew currentVersion >}} 版本，则需要查看你所使用的 Kubernetes 版本的对应文档。"}
{"en": "This page describes security considerations and best practices specific to the Windows operating system.", "zh": "本篇介绍特定于 Windows 操作系统的安全注意事项和最佳实践。"}
{"en": "## Protection for Secret data on nodes", "zh": "## 保护节点上的 Secret 数据   {#protection-for-secret-data-on-nodes}"}
{"en": "On Windows, data from Secrets are written out in clear text onto the node's local\nstorage (as compared to using tmpfs / in-memory filesystems on Linux). As a cluster\noperator, you should take both of the following additional measures:", "zh": "在 Windows 上，来自 Secret 的数据以明文形式写入节点的本地存储\n（与在 Linux 上使用 tmpfs / 内存中文件系统不同）。\n作为集群操作员，你应该采取以下两项额外措施："}
{"en": "1. Use file ACLs to secure the Secrets' file location.\n1. Apply volume-level encryption using\n   [BitLocker](https://docs.microsoft.com/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server).", "zh": "1. 使用文件 ACL 来保护 Secret 的文件位置。\n2. 使用 [BitLocker](https://docs.microsoft.com/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server)\n   进行卷级加密。"}
{"en": "## Container users", "zh": "## 容器用户   {#container-users}"}
{"en": "[RunAsUsername](/docs/tasks/configure-pod-container/configure-runasusername)\ncan be specified for Windows Pods or containers to execute the container\nprocesses as specific user. This is roughly equivalent to\n[RunAsUser](/docs/concepts/security/pod-security-policy/#users-and-groups).", "zh": "可以为 Windows Pod 或容器指定 [RunAsUsername](/zh-cn/docs/tasks/configure-pod-container/configure-runasusername)\n以作为特定用户执行容器进程。这大致相当于 [RunAsUser](/zh-cn/docs/concepts/security/pod-security-policy/#users-and-groups)。"}
{"en": "Windows containers offer two default user accounts, ContainerUser and ContainerAdministrator.\nThe differences between these two user accounts are covered in\n[When to use ContainerAdmin and ContainerUser user accounts](https://docs.microsoft.com/virtualization/windowscontainers/manage-containers/container-security#when-to-use-containeradmin-and-containeruser-user-accounts)\nwithin Microsoft's _Secure Windows containers_ documentation.", "zh": "Windows 容器提供两个默认用户帐户，ContainerUser 和 ContainerAdministrator。\n在微软的 **Windows 容器安全** 文档\n[何时使用 ContainerAdmin 和 ContainerUser 用户帐户](https://docs.microsoft.com/zh-cn/virtualization/windowscontainers/manage-containers/container-security#when-to-use-containeradmin-and-containeruser-user-accounts)\n中介绍了这两个用户帐户之间的区别。"}
{"en": "Local users can be added to container images during the container build process.", "zh": "在容器构建过程中，可以将本地用户添加到容器镜像中。\n\n{{< note >}}"}
{"en": "* [Nano Server](https://hub.docker.com/_/microsoft-windows-nanoserver) based images run as\n  `ContainerUser` by default\n* [Server Core](https://hub.docker.com/_/microsoft-windows-servercore) based images run as\n  `ContainerAdministrator` by default", "zh": "* 基于 [Nano Server](https://hub.docker.com/_/microsoft-windows-nanoserver) 的镜像默认以 `ContainerUser` 运行\n* 基于 [Server Core](https://hub.docker.com/_/microsoft-windows-servercore) 的镜像默认以 `ContainerAdministrator` 运行\n{{< /note >}}"}
{"en": "Windows containers can also run as Active Directory identities by utilizing\n[Group Managed Service Accounts](/docs/tasks/configure-pod-container/configure-gmsa/)", "zh": "Windows 容器还可以通过使用[组管理的服务账号](/zh-cn/docs/tasks/configure-pod-container/configure-gmsa/)作为\nActive Directory 身份运行。"}
{"en": "## Pod-level security isolation", "zh": "## Pod 级安全隔离   {#pod-level-security-isolation}"}
{"en": "Linux-specific pod security context mechanisms (such as SELinux, AppArmor, Seccomp, or custom\nPOSIX capabilities) are not supported on Windows nodes.", "zh": "Windows 节点不支持特定于 Linux 的 Pod 安全上下文机制（例如 SELinux、AppArmor、Seccomp 或自定义 POSIX 权能字）。"}
{"en": "Privileged containers are [not supported](/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext)\non Windows.\nInstead [HostProcess containers](/docs/tasks/configure-pod-container/create-hostprocess-pod)\ncan be used on Windows to perform many of the tasks performed by privileged containers on Linux.", "zh": "Windows 上[不支持](/zh-cn/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext)特权容器。\n然而，可以在 Windows 上使用 [HostProcess 容器](/zh-cn/docs/tasks/configure-pod-container/create-hostprocess-pod)来执行\nLinux 上特权容器执行的许多任务。"}
{"en": "This section of the Kubernetes documentation aims to help you learn to run\nworkloads more securely, and about the essential aspects of keeping a\nKubernetes cluster secure.\n\nKubernetes is based on a cloud-native architecture, and draws on advice from the\n{{< glossary_tooltip text=\"CNCF\" term_id=\"cncf\" >}} about good practice for\ncloud native information security.", "zh": "Kubernetes 文档的这一部分内容的旨在引导你学习如何更安全地运行工作负载，\n以及维护 Kubernetes 集群的基本安全性。\n\nKubernetes 基于云原生架构，并借鉴了\n{{< glossary_tooltip text=\"CNCF\" term_id=\"cncf\" >}} 有关云原生信息安全良好实践的建议。"}
{"en": "Read [Cloud Native Security and Kubernetes](/docs/concepts/security/cloud-native-security/)\nfor the broader context about how to secure your cluster and the applications that\nyou're running on it.", "zh": "请阅读[云原生安全和 Kubernetes](/zh-cn/docs/concepts/security/cloud-native-security/)，\n了解有关如何保护集群及其上运行的应用程序的更广泛背景信息。"}
{"en": "## Kubernetes security mechanisms {#security-mechanisms}\n\nKubernetes includes several APIs and security controls, as well as ways to\ndefine [policies](#policies) that can form part of how you manage information security.", "zh": "## Kubernetes 安全机制 {#security-mechanisms}\n\nKubernetes 包含多个 API 和安全组件，\n以及定义[策略](#policies)的方法，这些策略可以作为你的信息安全管理的一部分。"}
{"en": "### Control plane protection\n\nA key security mechanism for any Kubernetes cluster is to\n[control access to the Kubernetes API](/docs/concepts/security/controlling-access).", "zh": "### 控制平面保护\n\n任何 Kubernetes 集群的一个关键安全机制是[控制对 Kubernetes API 的访问](/zh-cn/docs/concepts/security/controlling-access)。"}
{"en": "Kubernetes expects you to configure and use TLS to provide\n[data encryption in transit](/docs/tasks/tls/managing-tls-in-a-cluster/)\nwithin the control plane, and between the control plane and its clients.\nYou can also enable [encryption at rest](/docs/tasks/administer-cluster/encrypt-data/)\nfor the data stored within Kubernetes control plane; this is separate from using\nencryption at rest for your own workloads' data, which might also be a good idea.", "zh": "Kubernetes 希望你配置并使用 TLS，\n以便在控制平面内以及控制平面与其客户端之间提供[传输中的数据加密](/zh-cn/docs/tasks/tls/managing-tls-in-a-cluster/)。\n你还可以为 Kubernetes 控制平面中存储的数据启用静态加密；\n这与对你自己的工作负载数据使用静态加密不同，后者可能也是一个好主意。"}
{"en": "### Secrets\n\nThe [Secret](/docs/concepts/configuration/secret/) API provides basic protection for\nconfiguration values that require confidentiality.", "zh": "### Secret\n\n[Secret](/zh-cn/docs/concepts/configuration/secret/) API\n为需要保密的配置值提供基本保护。"}
{"en": "### Workload protection\n\nEnforce [Pod security standards](/docs/concepts/security/pod-security-standards/) to\nensure that Pods and their containers are isolated appropriately. You can also use\n[RuntimeClasses](/docs/concepts/containers/runtime-class) to define custom isolation\nif you need it.", "zh": "### 工具负载保护\n\n实施 [Pod 安全标准](/zh-cn/docs/concepts/security/pod-security-standards/)以确保\nPod 及其容器得到适当隔离。如果需要，你还可以使用\n[RuntimeClass](/zh-cn/docs/concepts/containers/runtime-class) 来配置自定义隔离。"}
{"en": "[Network policies](/docs/concepts/services-networking/network-policies/) let you control\nnetwork traffic between Pods, or between Pods and the network outside your cluster.\n\nYou can deploy security controls from the wider ecosystem to implement preventative\nor detective controls around Pods, their containers, and the images that run in them.", "zh": "[网络策略（NetworkPolicy)](/zh-cn/docs/concepts/services-networking/network-policies/)\n可让控制 Pod 之间或 Pod 与集群外部网络之间的网络流量。"}
{"en": "### Auditing\n\nKubernetes [audit logging](/docs/tasks/debug/debug-cluster/audit/) provides a\nsecurity-relevant, chronological set of records documenting the sequence of actions\nin a cluster. The cluster audits the activities generated by users, by applications\nthat use the Kubernetes API, and by the control plane itself.", "zh": "### 审计\n\nKubernetes [审计日志记录](/zh-cn/docs/tasks/debug/debug-cluster/audit/)提供了一组与安全相关、\n按时间顺序排列的记录，记录了集群中的操作序列。\n集群审计用户、使用 Kubernetes API 的应用程序以及控制平面本身生成的活动。"}
{"en": "## Cloud provider security\n\n{{% thirdparty-content vendor=\"true\" %}}\n\nIf you are running a Kubernetes cluster on your own hardware or a different cloud provider,\nconsult your documentation for security best practices.\nHere are links to some of the popular cloud providers' security documentation:", "zh": "## 云提供商安全\n\n{{% thirdparty-content vendor=\"true\" %}}\n\n如果你在自己的硬件或不同的云平台上运行 Kubernetes 集群，请参阅对应云平台的文档以了解安全最佳实践。\n以下是一些流行云提供商的安全文档的链接："}
{"en": "{{< table caption=\"Cloud provider security\" >}}\n\nIaaS Provider        | Link |\n-------------------- | ------------ |\nAlibaba Cloud | https://www.alibabacloud.com/trust-center |\nAmazon Web Services | https://aws.amazon.com/security |\nGoogle Cloud Platform | https://cloud.google.com/security |\nHuawei Cloud | https://www.huaweicloud.com/intl/en-us/securecenter/overallsafety |\nIBM Cloud | https://www.ibm.com/cloud/security |\nMicrosoft Azure | https://docs.microsoft.com/en-us/azure/security/azure-security |\nOracle Cloud Infrastructure | https://www.oracle.com/security |\nVMware vSphere | https://www.vmware.com/security/hardening-guides |\n\n{{< /table >}}", "zh": "{{< table caption=\"Cloud provider security\" >}}\n\nIaaS 提供商        | 链接 |\n-------------------- | ------------ |\n阿里云 | https://www.alibabacloud.com/trust-center |\n亚马逊网络服务 | https://aws.amazon.com/security |\n谷歌云平台 | https://cloud.google.com/security |\n华为云 | https://www.huaweicloud.com/intl/en-us/securecenter/overallsafety |\nIBM 云 | https://www.ibm.com/cloud/security |\n微软 Azure | https://docs.microsoft.com/en-us/azure/security/azure-security |\nOracle 云基础设施| https://www.oracle.com/security |\nVMware vSphere | https://www.vmware.com/security/hardening-guides |\n\n{{< /table >}}"}
{"en": "## Policies\n\nYou can define security policies using Kubernetes-native mechanisms,\nsuch as [NetworkPolicy](/docs/concepts/services-networking/network-policies/)\n(declarative control over network packet filtering) or\n[ValidatingAdmissionPolicy](/docs/reference/access-authn-authz/validating-admission-policy/) (declarative restrictions on what changes\nsomeone can make using the Kubernetes API).", "zh": "## 策略\n\n你可以使用 Kubernetes 原生机制定义安全策略，例如\n[NetworkPolicy](/zh-cn/docs/concepts/services-networking/network-policies/)（对网络数据包过滤的声明式控制）\n或 [ValidatingAdmissionPolicy](/zh-cn/docs/reference/access-authn-authz/validating-admission-policy/)\n（对某人可以使用 Kubernetes API 进行哪些更改的声明性限制）。"}
{"en": "However, you can also rely on policy implementations from the wider\necosystem around Kubernetes. Kubernetes provides extension mechanisms\nto let those ecosystem projects implement their own policy controls\non source code review, container image approval, API access controls,\nnetworking, and more.", "zh": "你还可以依赖 Kubernetes 周边更广泛的生态系统的策略实现。\nKubernetes 提供了扩展机制，让这些生态系统项目在源代码审查、\n容器镜像审批、API 访问控制、网络等方面实施自己的策略控制。"}
{"en": "For more information about policy mechanisms and Kubernetes,\nread [Policies](/docs/concepts/policy/).", "zh": "有关策略机制和 Kubernetes 的更多信息，请阅读[策略](/zh-cn/docs/concepts/policy/)。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "Learn about related Kubernetes security topics:\n\n* [Securing your cluster](/docs/tasks/administer-cluster/securing-a-cluster/)\n* [Known vulnerabilities](/docs/reference/issues-security/official-cve-feed/)\n  in Kubernetes (and links to further information)\n* [Data encryption in transit](/docs/tasks/tls/managing-tls-in-a-cluster/) for the control plane\n* [Data encryption at rest](/docs/tasks/administer-cluster/encrypt-data/)\n* [Controlling Access to the Kubernetes API](/docs/concepts/security/controlling-access)\n* [Network policies](/docs/concepts/services-networking/network-policies/) for Pods\n* [Secrets in Kubernetes](/docs/concepts/configuration/secret/)\n* [Pod security standards](/docs/concepts/security/pod-security-standards/)\n* [RuntimeClasses](/docs/concepts/containers/runtime-class)", "zh": "了解相关的 Kubernetes 安全主题：\n\n* [保护集群](/zh-cn/docs/tasks/administer-cluster/securing-a-cluster/)\n* Kubernetes 中的[已知漏洞](/zh-cn/docs/reference/issues-security/official-cve-feed/)（以及更多信息的链接）\n* [传输中的数据加密](/zh-cn/docs/tasks/tls/managing-tls-in-a-cluster/)（针对控制平面）\n* [静态数据加密](/zh-cn/docs/tasks/administer-cluster/encrypt-data/)\n* [控制对 Kubernetes API 的访问](/zh-cn/docs/concepts/security/controlling-access)\n* Pod 的 [网络策略](/zh-cn/docs/concepts/services-networking/network-policies/)\n* [Kubernetes 中的 Secret](/zh-cn/docs/concepts/configuration/secret/)\n* [Pod 安全标准](/zh-cn/docs/concepts/security/pod-security-standards/)\n* [运行时类](/zh-cn/docs/concepts/containers/runtime-class)"}
{"en": "Learn the context:", "zh": "了解上下文："}
{"en": "* [Cloud Native Security and Kubernetes](/docs/concepts/security/cloud-native-security/)", "zh": "* [云原生安全和 Kubernetes](/zh-cn/docs/concepts/security/cloud-native-security/)"}
{"en": "Get certified:\n\n* [Certified Kubernetes Security Specialist](https://training.linuxfoundation.org/certification/certified-kubernetes-security-specialist/)\n  certification and official training course.\n\nRead more in this section:", "zh": "获取认证：\n\n* [Kubernetes 安全专家认证](https://training.linuxfoundation.org/certification/certified-kubernetes-security-specialist/)和官方培训课程。\n\n阅读本节的更多内容："}
{"en": "Selecting the appropriate authentication mechanism(s) is a crucial aspect of securing your cluster.\nKubernetes provides several built-in mechanisms, each with its own strengths and weaknesses that \nshould be carefully considered when choosing the best authentication mechanism for your cluster.", "zh": "选择合适的身份认证机制是确保集群安全的一个重要方面。\nKubernetes 提供了多种内置机制，\n当为你的集群选择最好的身份认证机制时需要谨慎考虑每种机制的优缺点。"}
{"en": "In general, it is recommended to enable as few authentication mechanisms as possible to simplify \nuser management and prevent cases where users retain access to a cluster that is no longer required.", "zh": "通常情况下，建议启用尽可能少的身份认证机制，\n以简化用户管理，避免用户仍保有对其不再需要的集群的访问权限的情况。"}
{"en": "It is important to note that Kubernetes does not have an in-built user database within the cluster. \nInstead, it takes user information from the configured authentication system and uses that to make \nauthorization decisions. Therefore, to audit user access, you need to review credentials from every \nconfigured authentication source.", "zh": "值得注意的是 Kubernetes 集群中并没有内置的用户数据库。\n相反，它从已配置的身份认证系统中获取用户信息并依之做出鉴权决策。\n因此，要审计用户访问，你需要检视来自每个已配置身份认证数据源的凭据。"}
{"en": "For production clusters with multiple users directly accessing the Kubernetes API, it is \nrecommended to use external authentication sources such as OIDC. The internal authentication \nmechanisms, such as client certificates and service account tokens, described below, are not \nsuitable for this use-case.", "zh": "对于有多个用户直接访问 Kubernetes API 的生产集群来说，\n建议使用外部身份认证数据源，例如：OIDC。\n下文提到的客户端证书和服务账号令牌等内部身份认证机制则不适用这种情况。"}
{"en": "## X.509 client certificate authentication {#x509-client-certificate-authentication}", "zh": "## X.509 客户端证书身份认证 {#x509-client-certificate-authentication}"}
{"en": "Kubernetes leverages [X.509 client certificate](/docs/reference/access-authn-authz/authentication/#x509-client-certificates) \nauthentication for system components, such as when the Kubelet authenticates to the API Server. \nWhile this mechanism can also be used for user authentication, it might not be suitable for \nproduction use due to several restrictions:", "zh": "Kubernetes 采用 [X.509 客户端证书](/zh-cn/docs/reference/access-authn-authz/authentication/#x509-client-certificates)\n对系统组件进行身份认证，\n例如 Kubelet 对 API 服务器进行身份认证时。\n虽然这种机制也可以用于用户身份认证，但由于一些限制它可能不太适合在生产中使用："}
{"en": "- Client certificates cannot be individually revoked. Once compromised, a certificate can be used \n  by an attacker until it expires. To mitigate this risk, it is recommended to configure short \n  lifetimes for user authentication credentials created using client certificates.", "zh": "- 客户端证书无法独立撤销。\n  证书一旦被泄露，攻击者就可以使用它，直到证书过期。\n  为了降低这种风险，建议为使用客户端证书创建的用户身份认证凭据配置较短的有效期。"}
{"en": "- If a certificate needs to be invalidated, the certificate authority must be re-keyed, which \ncan introduce availability risks to the cluster.", "zh": "- 如果证书需要被作废，必须重新为证书机构设置密钥，但这样做可能给集群带来可用性风险。"}
{"en": "- There is no permanent record of client certificates created in the cluster. Therefore, all \nissued certificates must be recorded if you need to keep track of them.", "zh": "- 在集群中创建的客户端证书不会被永久记录。\n  因此，如果你要跟踪所有已签发的证书，就必须将它们记录下来。"}
{"en": "- Private keys used for client certificate authentication cannot be password-protected. Anyone \nwho can read the file containing the key will be able to make use of it.", "zh": "- 用于对客户端证书进行身份认证的私钥不可以启用密码保护。\n  任何可以读取包含密钥文件的人都可以利用该密钥。"}
{"en": "- Using client certificate authentication requires a direct connection from the client to the \nAPI server with no intervening TLS termination points, which can complicate network architectures.", "zh": "- 使用客户端证书身份认证需要客户端直连 API 服务器而不允许中间存在 TLS 终止节点，\n  这一约束可能会使网络架构变得复杂。"}
{"en": "- Group data is embedded in the `O` value of the client certificate, which means the user's group \nmemberships cannot be changed for the lifetime of the certificate.", "zh": "- 组数据包含在客户端证书的 `O` 值中，\n  这意味着在证书有效期内无法更改用户的组成员身份。"}
{"en": "## Static token file {#static-token-file}、", "zh": "## 静态令牌文件 {#static-token-file}"}
{"en": "Although Kubernetes allows you to load credentials from a \n[static token file](/docs/reference/access-authn-authz/authentication/#static-token-file) located \non the control plane node disks, this approach is not recommended for production servers due to \nseveral reasons:", "zh": "尽管 Kubernetes 允许你从控制平面节点的磁盘中加载\n[静态令牌文件](/zh-cn/docs/reference/access-authn-authz/authentication/#static-token-file)\n以获取凭据，但由于多种原因，在生产服务器上不建议采用这种方法："}
{"en": "- Credentials are stored in clear text on control plane node disks, which can be a security risk.", "zh": "- 凭据以明文的方式存储在控制平面节点的磁盘中，这可能是一种安全风险。"}
{"en": "- Changing any credential requires a restart of the API server process to take effect, which can \nimpact availability.", "zh": "- 修改任何凭据都需要重启 API 服务进程使其生效，这会影响可用性。"}
{"en": "- There is no mechanism available to allow users to rotate their credentials. To rotate a \ncredential, a cluster administrator must modify the token on disk and distribute it to the users.", "zh": "- 没有现成的机制让用户轮换其凭据数据。\n  要轮换凭据数据，集群管理员必须修改磁盘上的令牌并将其分发给用户。"}
{"en": "- There is no lockout mechanism available to prevent brute-force attacks.", "zh": "- 没有合适的锁机制用以防止暴力破解攻击。"}
{"en": "## Bootstrap tokens {#bootstrap-tokens}", "zh": "## 启动引导令牌 {#bootstrap-tokens}"}
{"en": "[Bootstrap tokens](/docs/reference/access-authn-authz/bootstrap-tokens/) are used for joining \nnodes to clusters and are not recommended for user authentication due to several reasons:", "zh": "[启动引导令牌](/zh-cn/docs/reference/access-authn-authz/bootstrap-tokens/)用于节点加入集群，\n因为下列的一些原因，不建议用于用户身份认证："}
{"en": "- They have hard-coded group memberships that are not suitable for general use, making them \nunsuitable for authentication purposes.", "zh": "- 启动引导令牌中包含有硬编码的组成员身份，不适合一般使用，\n  因此不适用于身份认证目的。"}
{"en": "- Manually generating bootstrap tokens can lead to weak tokens that can be guessed by an attacker, \nwhich can be a security risk.", "zh": "- 手动生成启动引导令牌有可能使较弱的令牌容易被攻击者猜到，\n  有可能成为安全隐患。"}
{"en": "- There is no lockout mechanism available to prevent brute-force attacks, making it easier for \nattackers to guess or crack the token.", "zh": "- 没有现成的加锁定机制用来防止暴力破解，\n  这使得攻击者更容易猜测或破解令牌。"}
{"en": "## ServiceAccount secret tokens {#serviceaccount-secret-tokens}", "zh": "## 服务账号令牌 {#serviceaccount-secret-tokens}"}
{"en": "[Service account secrets](/docs/reference/access-authn-authz/service-accounts-admin/#manual-secret-management-for-serviceaccounts) \nare available as an option to allow workloads running in the cluster to authenticate to the \nAPI server. In Kubernetes < 1.23, these were the default option, however, they are being replaced \nwith TokenRequest API tokens. While these secrets could be used for user authentication, they are \ngenerally unsuitable for a number of reasons:", "zh": "[服务账号令牌](/zh-cn/docs/reference/access-authn-authz/service-accounts-admin/#manual-secret-management-for-serviceaccounts) \n在运行于集群中的工作负载向 API 服务器进行身份认证时是个可选项。\n在 Kubernetes < 1.23 的版本中，服务账号令牌是默认选项，但现在已经被 TokenRequest API 取代。\n尽管这些密钥可以用于用户身份认证，但由于多种原因，它们通常并不合适："}
{"en": "- They cannot be set with an expiry and will remain valid until the associated service account is deleted.", "zh": "- 服务账号令牌无法设置有效期，在相关的服务账号被删除前一直有效。"}
{"en": "- The authentication tokens are visible to any cluster user who can read secrets in the namespace \nthat they are defined in.", "zh": "- 任何集群用户，只要能读取服务账号令牌定义所在的命名空间中的 Secret，就能看到身份认证令牌。"}
{"en": "- Service accounts cannot be added to arbitrary groups complicating RBAC management where they are used.", "zh": "- 服务账号无法被添加到任意组中，这一限制使得使用服务账号的 RBAC 管理变得复杂。"}
{"en": "## TokenRequest API tokens {#tokenrequest-api-tokens}", "zh": "## TokenRequest API 令牌 {#tokenrequest-api-tokens}"}
{"en": "The TokenRequest API is a useful tool for generating short-lived credentials for service \nauthentication to the API server or third-party systems. However, it is not generally recommended \nfor user authentication as there is no revocation method available, and distributing credentials \nto users in a secure manner can be challenging.", "zh": "TokenRequest API 是一种可生成短期凭据的有用工具，所生成的凭据可\n用于对 API 服务器或第三方系统执行服务身份认证。\n然而，通常不建议将此机制用于用户身份认证，因为没有办法撤销这些令牌，\n而且，如何以安全的方式向用户分发凭据信息也是挑战。"}
{"en": "When using TokenRequest tokens for service authentication, it is recommended to implement a short \nlifespan to reduce the impact of compromised tokens.", "zh": "当使用 TokenRequest 令牌进行服务身份认证时，\n建议使用较短的有效期以减少被泄露令牌可能带来的影响。"}
{"en": "## OpenID Connect token authentication {#openid-connect-token-authentication}", "zh": "## OpenID Connect 令牌身份认证 {#openid-connect-token-authentication}"}
{"en": "Kubernetes supports integrating external authentication services with the Kubernetes API using \n[OpenID Connect (OIDC)](/docs/reference/access-authn-authz/authentication/#openid-connect-tokens). \nThere is a wide variety of software that can be used to integrate Kubernetes with an identity \nprovider. However, when using OIDC authentication for Kubernetes, it is important to consider the \nfollowing hardening measures:", "zh": "Kubernetes 支持使用 [OpenID Connect (OIDC)](/zh-cn/docs/reference/access-authn-authz/authentication/#openid-connect-tokens) \n将外部身份认证服务与 Kubernetes API 集成。\n有多种软件可用于将 Kubernetes 与认证服务组件集成。\n不过，当为 Kubernetes 使用 OIDC 身份认证时，\n必须考虑以下加固措施："}
{"en": "- The software installed in the cluster to support OIDC authentication should be isolated from \ngeneral workloads as it will run with high privileges.", "zh": "- 安装在集群中用于支持 OIDC 身份认证的软件应该与普通的工作负载隔离，\n  因为它要以较高的特权来运行。"}
{"en": "- Some Kubernetes managed services are limited in the OIDC providers that can be used.", "zh": "- 有些 Kubernetes 托管服务对可使用的 OIDC 服务组件有限制。"}
{"en": "- As with TokenRequest tokens, OIDC tokens should have a short lifespan to reduce the impact of \ncompromised tokens.", "zh": "- 与 TokenRequest 令牌一样，OIDC 令牌的有效期也应较短，以减少被泄露的令牌所带来的影响。"}
{"en": "## Webhook token authentication {#webhook-token-authentication}", "zh": "## Webhook 令牌身份认证 {#webhook-token-authentication}"}
{"en": "[Webhook token authentication](/docs/reference/access-authn-authz/authentication/#webhook-token-authentication) \nis another option for integrating external authentication providers into Kubernetes. This mechanism \nallows for an authentication service, either running inside the cluster or externally, to be \ncontacted for an authentication decision over a webhook. It is important to note that the suitability \nof this mechanism will likely depend on the software used for the authentication service, and there \nare some Kubernetes-specific considerations to take into account.", "zh": "[Webhook 令牌身份认证](/zh-cn/docs/reference/access-authn-authz/authentication/#webhook-token-authentication)\n是另一种集成外部身份认证服务组件到 Kubernetes 中的可选项。\n这种机制允许通过 Webhook 的方式连接集群内部或外部运行的身份认证服务，\n以做出身份认证决策。值得注意的是，\n这种机制的适用性可能更取决于身份认证服务所使用的软件，\n而且还需要考虑一些特定于 Kubernetes 的因素。"}
{"en": "To configure Webhook authentication, access to control plane server filesystems is required. This \nmeans that it will not be possible with Managed Kubernetes unless the provider specifically makes it \navailable. Additionally, any software installed in the cluster to support this access should be \nisolated from general workloads, as it will run with high privileges.", "zh": "要配置 Webhook 身份认证的前提是需要提供控制平面服务器文件系统的访问权限。\n这意味着托管的 Kubernetes 无法实现这一点，除非供应商特别提供。\n此外，集群中安装的任何支持该访问的软件都应当与普通工作负载隔离，\n因为它需要以较高的特权来运行。"}
{"en": "## Authenticating proxy {#authenticating-proxy}", "zh": "## 身份认证代理 {#authenticating-proxy}"}
{"en": "Another option for integrating external authentication systems into Kubernetes is to use an \n[authenticating proxy](/docs/reference/access-authn-authz/authentication/#authenticating-proxy). \nWith this mechanism, Kubernetes expects to receive requests from the proxy with specific header \nvalues set, indicating the username and group memberships to assign for authorization purposes. \nIt is important to note that there are specific considerations to take into account when using \nthis mechanism.", "zh": "将外部身份认证系统集成到 Kubernetes 的另一种方式是使用\n[身份认证代理](/zh-cn/docs/reference/access-authn-authz/authentication/#authenticating-proxy)。\n在这种机制下，Kubernetes 接收到来自代理的请求，这些请求会携带特定的标头，\n标明为鉴权目的所赋予的用户名和组成员身份。\n值得注意的是，在使用这种机制时有一些特定的注意事项。"}
{"en": "Firstly, securely configured TLS must be used between the proxy and Kubernetes API server to \nmitigate the risk of traffic interception or sniffing attacks. This ensures that the communication \nbetween the proxy and Kubernetes API server is secure.", "zh": "首先，在代理和 Kubernetes API 服务器间必须以安全的方式配置 TLS 连接，\n从而降低流量劫持或嗅探攻击的风险。\nTLS 连接可以确保代理和 Kubernetes API 服务器间的通信是安全的。"}
{"en": "Secondly, it is important to be aware that an attacker who is able to modify the headers of the \nrequest may be able to gain unauthorized access to Kubernetes resources. As such, it is important \nto ensure that the headers are properly secured and cannot be tampered with.", "zh": "其次，需要注意的是，能够修改表头的攻击者可能会在未经授权的情况下访问 Kubernetes 资源。\n因此，确保标头得到妥善保护并且不会被篡改非常重要。"}
{"en": "You can constrain a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} so that it is\n_restricted_ to run on particular {{< glossary_tooltip text=\"node(s)\" term_id=\"node\" >}},\nor to _prefer_ to run on particular nodes.\nThere are several ways to do this and the recommended approaches all use\n[label selectors](/docs/concepts/overview/working-with-objects/labels/) to facilitate the selection.\nOften, you do not need to set any such constraints; the\n{{< glossary_tooltip text=\"scheduler\" term_id=\"kube-scheduler\" >}} will automatically do a reasonable placement\n(for example, spreading your Pods across nodes so as not place Pods on a node with insufficient free resources).\nHowever, there are some circumstances where you may want to control which node\nthe Pod deploys to, for example, to ensure that a Pod ends up on a node with an SSD attached to it,\nor to co-locate Pods from two different services that communicate a lot into the same availability zone.", "zh": "你可以约束一个 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}\n以便**限制**其只能在特定的{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}上运行，\n或优先在特定的节点上运行。有几种方法可以实现这点，\n推荐的方法都是用[标签选择算符](/zh-cn/docs/concepts/overview/working-with-objects/labels/)来进行选择。\n通常这样的约束不是必须的，因为调度器将自动进行合理的放置（比如，将 Pod 分散到节点上，\n而不是将 Pod 放置在可用资源不足的节点上等等）。但在某些情况下，你可能需要进一步控制\nPod 被部署到哪个节点。例如，确保 Pod 最终落在连接了 SSD 的机器上，\n或者将来自两个不同的服务且有大量通信的 Pod 被放置在同一个可用区。"}
{"en": "You can use any of the following methods to choose where Kubernetes schedules\nspecific Pods:\n\n- [nodeSelector](#nodeselector) field matching against [node labels](#built-in-node-labels)\n- [Affinity and anti-affinity](#affinity-and-anti-affinity)\n- [nodeName](#nodename) field\n- [Pod topology spread constraints](#pod-topology-spread-constraints)", "zh": "你可以使用下列方法中的任何一种来选择 Kubernetes 对特定 Pod 的调度：\n\n- 与[节点标签](#built-in-node-labels)匹配的 [nodeSelector](#nodeSelector)\n- [亲和性与反亲和性](#affinity-and-anti-affinity)\n- [nodeName](#nodename) 字段\n- [Pod 拓扑分布约束](#pod-topology-spread-constraints)"}
{"en": "## Node labels {#built-in-node-labels}\n\nLike many other Kubernetes objects, nodes have\n[labels](/docs/concepts/overview/working-with-objects/labels/). You can\n[attach labels manually](/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node).\nKubernetes also populates a [standard set of labels](/docs/reference/node/node-labels/)\non all nodes in a cluster.", "zh": "## 节点标签     {#built-in-node-labels}\n\n与很多其他 Kubernetes 对象类似，节点也有[标签](/zh-cn/docs/concepts/overview/working-with-objects/labels/)。\n你可以[手动地添加标签](/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node)。\nKubernetes 也会为集群中所有节点添加一些[标准的标签](/zh-cn/docs/reference/node/node-labels/)。\n\n{{< note >}}"}
{"en": "The value of these labels is cloud provider specific and is not guaranteed to be reliable.\nFor example, the value of `kubernetes.io/hostname` may be the same as the node name in some environments\nand a different value in other environments.", "zh": "这些标签的取值是取决于云提供商的，并且是无法在可靠性上给出承诺的。\n例如，`kubernetes.io/hostname` 的取值在某些环境中可能与节点名称相同，\n而在其他环境中会取不同的值。\n{{< /note >}}"}
{"en": "### Node isolation/restriction\n\nAdding labels to nodes allows you to target Pods for scheduling on specific\nnodes or groups of nodes. You can use this functionality to ensure that specific\nPods only run on nodes with certain isolation, security, or regulatory\nproperties.", "zh": "## 节点隔离/限制  {#node-isolation-restriction}\n\n通过为节点添加标签，你可以准备让 Pod 调度到特定节点或节点组上。\n你可以使用这个功能来确保特定的 Pod 只能运行在具有一定隔离性、安全性或监管属性的节点上。"}
{"en": "If you use labels for node isolation, choose label keys that the {{<glossary_tooltip text=\"kubelet\" term_id=\"kubelet\">}}\ncannot modify. This prevents a compromised node from setting those labels on\nitself so that the scheduler schedules workloads onto the compromised node.", "zh": "如果使用标签来实现节点隔离，建议选择节点上的\n{{<glossary_tooltip text=\"kubelet\" term_id=\"kubelet\">}}\n无法修改的标签键。\n这可以防止受感染的节点在自身上设置这些标签，进而影响调度器将工作负载调度到受感染的节点。"}
{"en": "The [`NodeRestriction` admission plugin](/docs/reference/access-authn-authz/admission-controllers/#noderestriction)\nprevents the kubelet from setting or modifying labels with a\n`node-restriction.kubernetes.io/` prefix.\n\nTo make use of that label prefix for node isolation:", "zh": "[`NodeRestriction` 准入插件](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#noderestriction)防止\nkubelet 使用 `node-restriction.kubernetes.io/` 前缀设置或修改标签。\n\n要使用该标签前缀进行节点隔离："}
{"en": "1. Ensure you are using the [Node authorizer](/docs/reference/access-authn-authz/node/) and have _enabled_ the `NodeRestriction` admission plugin.\n2. Add labels with the `node-restriction.kubernetes.io/` prefix to your nodes, and use those labels in your [node selectors](#nodeselector).\n   For example, `example.com.node-restriction.kubernetes.io/fips=true` or `example.com.node-restriction.kubernetes.io/pci-dss=true`.", "zh": "1. 确保你在使用[节点鉴权](/zh-cn/docs/reference/access-authn-authz/node/)机制并且已经启用了\n   [NodeRestriction 准入插件](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#noderestriction)。\n2. 将带有 `node-restriction.kubernetes.io/` 前缀的标签添加到 Node 对象，\n   然后在[节点选择算符](#nodeSelector)中使用这些标签。\n   例如，`example.com.node-restriction.kubernetes.io/fips=true` 或\n   `example.com.node-restriction.kubernetes.io/pci-dss=true`。\n\n## nodeSelector"}
{"en": "`nodeSelector` is the simplest recommended form of node selection constraint.\nYou can add the `nodeSelector` field to your Pod specification and specify the\n[node labels](#built-in-node-labels) you want the target node to have.\nKubernetes only schedules the Pod onto nodes that have each of the labels you\nspecify.", "zh": "`nodeSelector` 是节点选择约束的最简单推荐形式。你可以将 `nodeSelector` 字段添加到\nPod 的规约中设置你希望目标节点所具有的[节点标签](#built-in-node-labels)。\nKubernetes 只会将 Pod 调度到拥有你所指定的每个标签的节点上。"}
{"en": "See [Assign Pods to Nodes](/docs/tasks/configure-pod-container/assign-pods-nodes) for more\ninformation.", "zh": "进一步的信息可参见[将 Pod 指派给节点](/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes)。"}
{"en": "## Affinity and anti-affinity\n\n`nodeSelector` is the simplest way to constrain Pods to nodes with specific\nlabels. Affinity and anti-affinity expands the types of constraints you can\ndefine. Some of the benefits of affinity and anti-affinity include:", "zh": "## 亲和性与反亲和性  {#affinity-and-anti-affinity}\n\n`nodeSelector` 提供了一种最简单的方法来将 Pod 约束到具有特定标签的节点上。\n亲和性和反亲和性扩展了你可以定义的约束类型。使用亲和性与反亲和性的一些好处有："}
{"en": "- The affinity/anti-affinity language is more expressive. `nodeSelector` only\n  selects nodes with all the specified labels. Affinity/anti-affinity gives you\n  more control over the selection logic.\n- You can indicate that a rule is *soft* or *preferred*, so that the scheduler\n  still schedules the Pod even if it can't find a matching node.\n- You can constrain a Pod using labels on other Pods running on the node (or other topological domain),\n  instead of just node labels, which allows you to define rules for which Pods\n  can be co-located on a node.", "zh": "- 亲和性、反亲和性语言的表达能力更强。`nodeSelector` 只能选择拥有所有指定标签的节点。\n  亲和性、反亲和性为你提供对选择逻辑的更强控制能力。\n- 你可以标明某规则是“软需求”或者“偏好”，这样调度器在无法找到匹配节点时仍然调度该 Pod。\n- 你可以使用节点上（或其他拓扑域中）运行的其他 Pod 的标签来实施调度约束，\n  而不是只能使用节点本身的标签。这个能力让你能够定义规则允许哪些 Pod 可以被放置在一起。"}
{"en": "The affinity feature consists of two types of affinity:\n\n- *Node affinity* functions like the `nodeSelector` field but is more expressive and\n  allows you to specify soft rules.\n- *Inter-pod affinity/anti-affinity* allows you to constrain Pods against labels\n  on other Pods.", "zh": "亲和性功能由两种类型的亲和性组成：\n\n- **节点亲和性**功能类似于 `nodeSelector` 字段，但它的表达能力更强，并且允许你指定软规则。\n- Pod 间亲和性/反亲和性允许你根据其他 Pod 的标签来约束 Pod。"}
{"en": "### Node affinity\n\nNode affinity is conceptually similar to `nodeSelector`, allowing you to constrain which nodes your\nPod can be scheduled on based on node labels. There are two types of node\naffinity:", "zh": "### 节点亲和性   {#node-affinity}\n\n节点亲和性概念上类似于 `nodeSelector`，\n它使你可以根据节点上的标签来约束 Pod 可以调度到哪些节点上。\n节点亲和性有两种："}
{"en": "- `requiredDuringSchedulingIgnoredDuringExecution`: The scheduler can't\n  schedule the Pod unless the rule is met. This functions like `nodeSelector`,\n  but with a more expressive syntax.\n- `preferredDuringSchedulingIgnoredDuringExecution`: The scheduler tries to\n  find a node that meets the rule. If a matching node is not available, the\n  scheduler still schedules the Pod.", "zh": "- `requiredDuringSchedulingIgnoredDuringExecution`：\n  调度器只有在规则被满足的时候才能执行调度。此功能类似于 `nodeSelector`，\n  但其语法表达能力更强。\n- `preferredDuringSchedulingIgnoredDuringExecution`：\n  调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod。\n\n{{<note>}}"}
{"en": "In the preceding types, `IgnoredDuringExecution` means that if the node labels\nchange after Kubernetes schedules the Pod, the Pod continues to run.", "zh": "在上述类型中，`IgnoredDuringExecution` 意味着如果节点标签在 Kubernetes\n调度 Pod 后发生了变更，Pod 仍将继续运行。\n{{</note>}}"}
{"en": "You can specify node affinities using the `.spec.affinity.nodeAffinity` field in\nyour Pod spec.\n\nFor example, consider the following Pod spec:", "zh": "你可以使用 Pod 规约中的 `.spec.affinity.nodeAffinity` 字段来设置节点亲和性。\n例如，考虑下面的 Pod 规约：\n\n{{% code_sample file=\"pods/pod-with-node-affinity.yaml\" %}}"}
{"en": "In this example, the following rules apply:\n\n- The node *must* have a label with the key `topology.kubernetes.io/zone` and\n  the value of that label *must* be either `antarctica-east1` or `antarctica-west1`.\n- The node *preferably* has a label with the key `another-node-label-key` and\n  the value `another-node-label-value`.", "zh": "在这一示例中，所应用的规则如下：\n\n- 节点**必须**包含一个键名为 `topology.kubernetes.io/zone` 的标签，\n  并且该标签的取值**必须**为 `antarctica-east1` 或 `antarctica-west1`。\n- 节点**最好**具有一个键名为 `another-node-label-key` 且取值为\n  `another-node-label-value` 的标签。"}
{"en": "You can use the `operator` field to specify a logical operator for Kubernetes to use when\ninterpreting the rules. You can use `In`, `NotIn`, `Exists`, `DoesNotExist`,\n`Gt` and `Lt`.", "zh": "你可以使用 `operator` 字段来为 Kubernetes 设置在解释规则时要使用的逻辑操作符。\n你可以使用 `In`、`NotIn`、`Exists`、`DoesNotExist`、`Gt` 和 `Lt` 之一作为操作符。"}
{"en": "Read [Operators](#operators)\nto learn more about how these work.", "zh": "阅读[操作符](#operators)了解有关这些操作的更多信息。"}
{"en": "`NotIn` and `DoesNotExist` allow you to define node anti-affinity behavior.\nAlternatively, you can use [node taints](/docs/concepts/scheduling-eviction/taint-and-toleration/)\nto repel Pods from specific nodes.", "zh": "`NotIn` 和 `DoesNotExist` 可用来实现节点反亲和性行为。\n你也可以使用[节点污点](/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/)\n将 Pod 从特定节点上驱逐。\n\n{{< note >}}"}
{"en": "If you specify both `nodeSelector` and `nodeAffinity`, *both* must be satisfied\nfor the Pod to be scheduled onto a node.", "zh": "如果你同时指定了 `nodeSelector` 和 `nodeAffinity`，**两者**必须都要满足，\n才能将 Pod 调度到候选节点上。"}
{"en": "If you specify multiple terms in `nodeSelectorTerms` associated with `nodeAffinity`\ntypes, then the Pod can be scheduled onto a node if one of the specified terms\ncan be satisfied (terms are ORed).", "zh": "如果你在与 nodeAffinity 类型关联的 nodeSelectorTerms 中指定多个条件，\n只要其中一个 `nodeSelectorTerms` 满足（各个条件按逻辑或操作组合）的话，Pod 就可以被调度到节点上。"}
{"en": "If you specify multiple expressions in a single `matchExpressions` field associated with a\nterm in `nodeSelectorTerms`, then the Pod can be scheduled onto a node only\nif all the expressions are satisfied (expressions are ANDed).", "zh": "如果你在与 `nodeSelectorTerms` 中的条件相关联的单个 `matchExpressions` 字段中指定多个表达式，\n则只有当所有表达式都满足（各表达式按逻辑与操作组合）时，Pod 才能被调度到节点上。\n{{< /note >}}"}
{"en": "See [Assign Pods to Nodes using Node Affinity](/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/)\nfor more information.", "zh": "参阅[使用节点亲和性来为 Pod 指派节点](/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/)，\n以了解进一步的信息。"}
{"en": "#### Node affinity weight\n\nYou can specify a `weight` between 1 and 100 for each instance of the\n`preferredDuringSchedulingIgnoredDuringExecution` affinity type. When the\nscheduler finds nodes that meet all the other scheduling requirements of the Pod, the\nscheduler iterates through every preferred rule that the node satisfies and adds the\nvalue of the `weight` for that expression to a sum.", "zh": "#### 节点亲和性权重   {#node-affinity-weight}\n\n你可以为 `preferredDuringSchedulingIgnoredDuringExecution` 亲和性类型的每个实例设置\n`weight` 字段，其取值范围是 1 到 100。\n当调度器找到能够满足 Pod 的其他调度请求的节点时，调度器会遍历节点满足的所有的偏好性规则，\n并将对应表达式的 `weight` 值加和。"}
{"en": "The final sum is added to the score of other priority functions for the node.\nNodes with the highest total score are prioritized when the scheduler makes a\nscheduling decision for the Pod.\n\nFor example, consider the following Pod spec:", "zh": "最终的加和值会添加到该节点的其他优先级函数的评分之上。\n在调度器为 Pod 作出调度决定时，总分最高的节点的优先级也最高。\n\n例如，考虑下面的 Pod 规约：\n\n{{% code_sample file=\"pods/pod-with-affinity-preferred-weight.yaml\" %}}"}
{"en": "If there are two possible nodes that match the\n`preferredDuringSchedulingIgnoredDuringExecution` rule, one with the\n`label-1:key-1` label and another with the `label-2:key-2` label, the scheduler\nconsiders the `weight` of each node and adds the weight to the other scores for\nthat node, and schedules the Pod onto the node with the highest final score.", "zh": "如果存在两个候选节点，都满足 `preferredDuringSchedulingIgnoredDuringExecution` 规则，\n其中一个节点具有标签 `label-1:key-1`，另一个节点具有标签 `label-2:key-2`，\n调度器会考察各个节点的 `weight` 取值，并将该权重值添加到节点的其他得分值之上，\n\n{{< note >}}"}
{"en": "If you want Kubernetes to successfully schedule the Pods in this example, you\nmust have existing nodes with the `kubernetes.io/os=linux` label.", "zh": "如果你希望 Kubernetes 能够成功地调度此例中的 Pod，你必须拥有打了\n`kubernetes.io/os=linux` 标签的节点。\n{{< /note >}}"}
{"en": "#### Node affinity per scheduling profile", "zh": "#### 逐个调度方案中设置节点亲和性    {#node-affinity-per-scheduling-profile}\n\n{{< feature-state for_k8s_version=\"v1.20\" state=\"beta\" >}}"}
{"en": "When configuring multiple [scheduling profiles](/docs/reference/scheduling/config/#multiple-profiles), you can associate\na profile with a node affinity, which is useful if a profile only applies to a specific set of nodes.\nTo do so, add an `addedAffinity` to the `args` field of the [`NodeAffinity` plugin](/docs/reference/scheduling/config/#scheduling-plugins)\nin the [scheduler configuration](/docs/reference/scheduling/config/). For example:", "zh": "在配置多个[调度方案](/zh-cn/docs/reference/scheduling/config/#multiple-profiles)时，\n你可以将某个方案与节点亲和性关联起来，如果某个调度方案仅适用于某组特殊的节点时，\n这样做是很有用的。\n要实现这点，可以在[调度器配置](/zh-cn/docs/reference/scheduling/config/)中为\n[`NodeAffinity` 插件](/zh-cn/docs/reference/scheduling/config/#scheduling-plugins)的\n`args` 字段添加 `addedAffinity`。例如：\n\n```yaml\napiVersion: kubescheduler.config.k8s.io/v1beta3\nkind: KubeSchedulerConfiguration\n\nprofiles:\n  - schedulerName: default-scheduler\n  - schedulerName: foo-scheduler\n    pluginConfig:\n      - name: NodeAffinity\n        args:\n          addedAffinity:\n            requiredDuringSchedulingIgnoredDuringExecution:\n              nodeSelectorTerms:\n              - matchExpressions:\n                - key: scheduler-profile\n                  operator: In\n                  values:\n                  - foo\n```"}
{"en": "The `addedAffinity` is applied to all Pods that set `.spec.schedulerName` to `foo-scheduler`, in addition to the\nNodeAffinity specified in the PodSpec.\nThat is, in order to match the Pod, nodes need to satisfy `addedAffinity` and\nthe Pod's `.spec.NodeAffinity`.", "zh": "这里的 `addedAffinity` 除遵从 Pod 规约中设置的节点亲和性之外，\n还适用于将 `.spec.schedulerName` 设置为 `foo-scheduler`。\n换言之，为了匹配 Pod，节点需要满足 `addedAffinity` 和 Pod 的 `.spec.NodeAffinity`。"}
{"en": "Since the `addedAffinity` is not visible to end users, its behavior might be\nunexpected to them. Use node labels that have a clear correlation to the\nscheduler profile name.", "zh": "由于 `addedAffinity` 对最终用户不可见，其行为可能对用户而言是出乎意料的。\n应该使用与调度方案名称有明确关联的节点标签。\n\n{{< note >}}"}
{"en": "The DaemonSet controller, which [creates Pods for DaemonSets](/docs/concepts/workloads/controllers/daemonset/#how-daemon-pods-are-scheduled),\ndoes not support scheduling profiles. When the DaemonSet controller creates\nPods, the default Kubernetes scheduler places those Pods and honors any\n`nodeAffinity` rules in the DaemonSet controller.", "zh": "DaemonSet 控制器[为 DaemonSet 创建 Pod](/zh-cn/docs/concepts/workloads/controllers/daemonset/#how-daemon-pods-are-scheduled)，\n但该控制器不理会调度方案。\nDaemonSet 控制器创建 Pod 时，默认的 Kubernetes 调度器负责放置 Pod，\n并遵从 DaemonSet 控制器中设置的 `nodeAffinity` 规则。\n{{< /note >}}"}
{"en": "### Inter-pod affinity and anti-affinity\n\nInter-pod affinity and anti-affinity allow you to constrain which nodes your\nPods can be scheduled on based on the labels of **Pods** already running on that\nnode, instead of the node labels.", "zh": "### Pod 间亲和性与反亲和性  {#inter-pod-affinity-and-anti-affinity}\n\nPod 间亲和性与反亲和性使你可以基于已经在节点上运行的 **Pod** 的标签来约束\nPod 可以调度到的节点，而不是基于节点上的标签。"}
{"en": "Inter-pod affinity and anti-affinity rules take the form \"this\nPod should (or, in the case of anti-affinity, should not) run in an X if that X\nis already running one or more Pods that meet rule Y\", where X is a topology\ndomain like node, rack, cloud provider zone or region, or similar and Y is the\nrule Kubernetes tries to satisfy.", "zh": "Pod 间亲和性与反亲和性的规则格式为“如果 X 上已经运行了一个或多个满足规则 Y 的 Pod，\n则这个 Pod 应该（或者在反亲和性的情况下不应该）运行在 X 上”。\n这里的 X 可以是节点、机架、云提供商可用区或地理区域或类似的拓扑域，\nY 则是 Kubernetes 尝试满足的规则。"}
{"en": "You express these rules (Y) as [label selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors)\nwith an optional associated list of namespaces. Pods are namespaced objects in\nKubernetes, so Pod labels also implicitly have namespaces. Any label selectors\nfor Pod labels should specify the namespaces in which Kubernetes should look for those\nlabels.", "zh": "你通过[标签选择算符](/zh-cn/docs/concepts/overview/working-with-objects/labels/#label-selectors)\n的形式来表达规则（Y），并可根据需要指定选关联的名字空间列表。\nPod 在 Kubernetes 中是名字空间作用域的对象，因此 Pod 的标签也隐式地具有名字空间属性。\n针对 Pod 标签的所有标签选择算符都要指定名字空间，Kubernetes\n会在指定的名字空间内寻找标签。"}
{"en": "You express the topology domain (X) using a `topologyKey`, which is the key for\nthe node label that the system uses to denote the domain. For examples, see\n[Well-Known Labels, Annotations and Taints](/docs/reference/labels-annotations-taints/).", "zh": "你会通过 `topologyKey` 来表达拓扑域（X）的概念，其取值是系统用来标示域的节点标签键。\n相关示例可参见[常用标签、注解和污点](/zh-cn/docs/reference/labels-annotations-taints/)。\n\n{{< note >}}"}
{"en": "Inter-pod affinity and anti-affinity require substantial amounts of\nprocessing which can slow down scheduling in large clusters significantly. We do\nnot recommend using them in clusters larger than several hundred nodes.", "zh": "Pod 间亲和性和反亲和性都需要相当的计算量，因此会在大规模集群中显著降低调度速度。\n我们不建议在包含数百个节点的集群中使用这类设置。\n{{< /note >}}\n\n{{< note >}}"}
{"en": "Pod anti-affinity requires nodes to be consistently labeled, in other words,\nevery node in the cluster must have an appropriate label matching `topologyKey`.\nIf some or all nodes are missing the specified `topologyKey` label, it can lead\nto unintended behavior.", "zh": "Pod 反亲和性需要节点上存在一致性的标签。换言之，\n集群中每个节点都必须拥有与 `topologyKey` 匹配的标签。\n如果某些或者所有节点上不存在所指定的 `topologyKey` 标签，调度行为可能与预期的不同。\n{{< /note >}}"}
{"en": "#### Types of inter-pod affinity and anti-affinity\n\nSimilar to [node affinity](#node-affinity) are two types of Pod affinity and\nanti-affinity as follows:", "zh": "#### Pod 间亲和性与反亲和性的类型\n\n与[节点亲和性](#node-affinity)类似，Pod 的亲和性与反亲和性也有两种类型：\n\n- `requiredDuringSchedulingIgnoredDuringExecution`\n- `preferredDuringSchedulingIgnoredDuringExecution`"}
{"en": "For example, you could use\n`requiredDuringSchedulingIgnoredDuringExecution` affinity to tell the scheduler to\nco-locate Pods of two services in the same cloud provider zone because they\ncommunicate with each other a lot. Similarly, you could use\n`preferredDuringSchedulingIgnoredDuringExecution` anti-affinity to spread Pods\nfrom a service across multiple cloud provider zones.", "zh": "例如，你可以使用 `requiredDuringSchedulingIgnoredDuringExecution` 亲和性来告诉调度器，\n将两个服务的 Pod 放到同一个云提供商可用区内，因为它们彼此之间通信非常频繁。\n类似地，你可以使用 `preferredDuringSchedulingIgnoredDuringExecution`\n反亲和性来将同一服务的多个 Pod 分布到多个云提供商可用区中。"}
{"en": "To use inter-pod affinity, use the `affinity.podAffinity` field in the Pod spec.\nFor inter-pod anti-affinity, use the `affinity.podAntiAffinity` field in the Pod\nspec.", "zh": "要使用 Pod 间亲和性，可以使用 Pod 规约中的 `.affinity.podAffinity` 字段。\n对于 Pod 间反亲和性，可以使用 Pod 规约中的 `.affinity.podAntiAffinity` 字段。"}
{"en": "#### Scheduling a group of pods with inter-pod affinity to themselves\n\nIf the current Pod being scheduled is the first in a series that have affinity to themselves,\nit is allowed to be scheduled if it passes all other affinity checks. This is determined by\nverifying that no other pod in the cluster matches the namespace and selector of this pod,\nthat the pod matches its own terms, and the chosen node matches all requested topologies.\nThis ensures that there will not be a deadlock even if all the pods have inter-pod affinity\nspecified.", "zh": "#### 调度一组具有 Pod 间亲和性的 Pod   {#scheduling-a-group-of-pods-with-inter-pod-affinity-to-themselves}\n\n如果当前正被调度的 Pod 在具有自我亲和性的 Pod 序列中排在第一个，\n那么只要它满足其他所有的亲和性规则，它就可以被成功调度。\n这是通过以下方式确定的：确保集群中没有其他 Pod 与此 Pod 的名字空间和标签选择算符匹配；\n该 Pod 满足其自身定义的条件，并且选定的节点满足所指定的所有拓扑要求。\n这确保即使所有的 Pod 都配置了 Pod 间亲和性，也不会出现调度死锁的情况。"}
{"en": "#### Pod affinity example {#an-example-of-a-pod-that-uses-pod-affinity}\n\nConsider the following Pod spec:", "zh": "#### Pod 亲和性示例   {#an-example-of-a-pod-that-uses-pod-affinity}\n\n考虑下面的 Pod 规约：\n\n{{% code_sample file=\"pods/pod-with-pod-affinity.yaml\" %}}"}
{"en": "This example defines one Pod affinity rule and one Pod anti-affinity rule. The\nPod affinity rule uses the \"hard\"\n`requiredDuringSchedulingIgnoredDuringExecution`, while the anti-affinity rule\nuses the \"soft\" `preferredDuringSchedulingIgnoredDuringExecution`.", "zh": "本示例定义了一条 Pod 亲和性规则和一条 Pod 反亲和性规则。Pod 亲和性规则配置为\n`requiredDuringSchedulingIgnoredDuringExecution`，而 Pod 反亲和性配置为\n`preferredDuringSchedulingIgnoredDuringExecution`。"}
{"en": "The affinity rule specifies that the scheduler is allowed to place the example Pod\non a node only if that node belongs to a specific [zone](/docs/concepts/scheduling-eviction/topology-spread-constraints/)\nwhere other Pods have been labeled with `security=S1`.\nFor instance, if we have a cluster with a designated zone, let's call it \"Zone V,\"\nconsisting of nodes labeled with `topology.kubernetes.io/zone=V`, the scheduler can\nassign the Pod to any node within Zone V, as long as there is at least one Pod within\nZone V already labeled with `security=S1`. Conversely, if there are no Pods with `security=S1`\nlabels in Zone V, the scheduler will not assign the example Pod to any node in that zone.", "zh": "亲和性规则规定，只有节点属于特定的[区域](/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n且该区域中的其他 Pod 已打上 `security=S1` 标签时，调度器才可以将示例 Pod 调度到此节点上。\n例如，如果我们有一个具有指定区域（称之为 \"Zone V\"）的集群，此区域由带有 `topology.kubernetes.io/zone=V`\n标签的节点组成，那么只要 Zone V 内已经至少有一个 Pod 打了 `security=S1` 标签，\n调度器就可以将此 Pod 调度到 Zone V 内的任何节点。相反，如果 Zone V 中没有带有 `security=S1` 标签的 Pod，\n则调度器不会将示例 Pod 调度给该区域中的任何节点。"}
{"en": "The anti-affinity rule specifies that the scheduler should try to avoid scheduling the Pod\non a node if that node belongs to a specific [zone](/docs/concepts/scheduling-eviction/topology-spread-constraints/)\nwhere other Pods have been labeled with `security=S2`.\nFor instance, if we have a cluster with a designated zone, let's call it \"Zone R,\"\nconsisting of nodes labeled with `topology.kubernetes.io/zone=R`, the scheduler should avoid\nassigning the Pod to any node within Zone R, as long as there is at least one Pod within\nZone R already labeled with `security=S2`. Conversely, the anti-affinity rule does not impact\nscheduling into Zone R if there are no Pods with `security=S2` labels.", "zh": "反亲和性规则规定，如果节点属于特定的[区域](/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n且该区域中的其他 Pod 已打上 `security=S2` 标签，则调度器应尝试避免将 Pod 调度到此节点上。\n例如，如果我们有一个具有指定区域（我们称之为 \"Zone R\"）的集群，此区域由带有 `topology.kubernetes.io/zone=R`\n标签的节点组成，只要 Zone R 内已经至少有一个 Pod 打了 `security=S2` 标签，\n调度器应避免将 Pod 分配给 Zone R 内的任何节点。相反，如果 Zone R 中没有带有 `security=S2` 标签的 Pod，\n则反亲和性规则不会影响将 Pod 调度到 Zone R。"}
{"en": "To get yourself more familiar with the examples of Pod affinity and anti-affinity,\nrefer to the [design proposal](https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md).", "zh": "查阅[设计文档](https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md)\n以进一步熟悉 Pod 亲和性与反亲和性的示例。"}
{"en": "You can use the `In`, `NotIn`, `Exists` and `DoesNotExist` values in the\n`operator` field for Pod affinity and anti-affinity.", "zh": "你可以针对 Pod 间亲和性与反亲和性为其 `operator` 字段使用 `In`、`NotIn`、`Exists`、\n`DoesNotExist` 等值。"}
{"en": "Read [Operators](#operators)\nto learn more about how these work.", "zh": "阅读[操作符](#operators)了解有关这些操作的更多信息。"}
{"en": "In principle, the `topologyKey` can be any allowed label key with the following\nexceptions for performance and security reasons:", "zh": "原则上，`topologyKey` 可以是任何合法的标签键。出于性能和安全原因，`topologyKey`\n有一些限制："}
{"en": "- For Pod affinity and anti-affinity, an empty `topologyKey` field is not allowed in both `requiredDuringSchedulingIgnoredDuringExecution`\n  and `preferredDuringSchedulingIgnoredDuringExecution`.\n- For `requiredDuringSchedulingIgnoredDuringExecution` Pod anti-affinity rules,\n  the admission controller `LimitPodHardAntiAffinityTopology` limits\n  `topologyKey` to `kubernetes.io/hostname`. You can modify or disable the\n  admission controller if you want to allow custom topologies.", "zh": "- 对于 Pod 亲和性而言，在 `requiredDuringSchedulingIgnoredDuringExecution`\n  和 `preferredDuringSchedulingIgnoredDuringExecution` 中，`topologyKey`\n  不允许为空。\n- 对于 `requiredDuringSchedulingIgnoredDuringExecution` 要求的 Pod 反亲和性，\n  准入控制器 `LimitPodHardAntiAffinityTopology` 要求 `topologyKey` 只能是\n  `kubernetes.io/hostname`。如果你希望使用其他定制拓扑逻辑，\n  你可以更改准入控制器或者禁用之。"}
{"en": "In addition to `labelSelector` and `topologyKey`, you can optionally specify a list\nof namespaces which the `labelSelector` should match against using the\n`namespaces` field at the same level as `labelSelector` and `topologyKey`.\nIf omitted or empty, `namespaces` defaults to the namespace of the Pod where the\naffinity/anti-affinity definition appears.", "zh": "除了 `labelSelector` 和 `topologyKey`，你也可以指定 `labelSelector`\n要匹配的名字空间列表，方法是在 `labelSelector` 和 `topologyKey`\n所在层同一层次上设置 `namespaces`。\n如果 `namespaces` 被忽略或者为空，则默认为 Pod 亲和性/反亲和性的定义所在的名字空间。"}
{"en": "#### Namespace selector", "zh": "#### 名字空间选择算符  {#namespace-selector}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "You can also select matching namespaces using `namespaceSelector`, which is a label query over the set of namespaces.\nThe affinity term is applied to namespaces selected by both `namespaceSelector` and the `namespaces` field.\nNote that an empty `namespaceSelector` ({}) matches all namespaces, while a null or empty `namespaces` list and\nnull `namespaceSelector` matches the namespace of the Pod where the rule is defined.", "zh": "用户也可以使用 `namespaceSelector` 选择匹配的名字空间，`namespaceSelector`\n是对名字空间集合进行标签查询的机制。\n亲和性条件会应用到 `namespaceSelector` 所选择的名字空间和 `namespaces` 字段中所列举的名字空间之上。\n注意，空的 `namespaceSelector`（`{}`）会匹配所有名字空间，而 null 或者空的\n`namespaces` 列表以及 null 值 `namespaceSelector` 意味着“当前 Pod 的名字空间”。\n\n#### matchLabelKeys\n\n{{< feature-state feature_gate_name=\"MatchLabelKeysInPodAffinity\" >}}\n\n{{< note >}}"}
{"en": "The `matchLabelKeys` field is an beta-level field and is disabled by default in\nKubernetes {{< skew currentVersion >}}.\nWhen you want to disable it, you have to disable it explicitly via the\n`MatchLabelKeysInPodAffinity` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).", "zh": "`matchLabelKeys` 字段是一个 Beta 级别的字段，在 Kubernetes {{< skew currentVersion >}} 中默认被禁用。\n当你想要禁用此字段时，你必须通过 `MatchLabelKeysInPodAffinity`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)禁用它。\n{{< /note >}}"}
{"en": "Kubernetes includes an optional `matchLabelKeys` field for Pod affinity\nor anti-affinity. The field specifies keys for the labels that should  match with the incoming Pod's labels,\nwhen satisfying the Pod (anti)affinity.\n\nThe keys are used to look up values from the pod labels; those key-value labels are combined\n(using `AND`) with the match restrictions defined using the `labelSelector` field. The combined\nfiltering selects the set of existing pods that will be taken into Pod (anti)affinity calculation.", "zh": "Kubernetes 在 Pod 亲和性或反亲和性中包含一个可选的 `matchLabelKeys` 字段。\n此字段指定了应与传入 Pod 的标签匹配的标签键，以满足 Pod 的（反）亲和性。\n\n这些键用于从 Pod 的标签中查找值；这些键值标签与使用 `labelSelector` 字段定义的匹配限制组合（使用 `AND` 操作）。\n这种组合的过滤机制选择将用于 Pod（反）亲和性计算的现有 Pod 集合。"}
{"en": "A common use case is to use `matchLabelKeys` with `pod-template-hash` (set on Pods\nmanaged as part of a Deployment, where the value is unique for each revision).\nUsing `pod-template-hash` in `matchLabelKeys` allows you to target the Pods that belong\nto the same revision as the incoming Pod, so that a rolling upgrade won't break affinity.", "zh": "一个常见的用例是在 `matchLabelKeys` 中使用 `pod-template-hash`\n（设置在作为 Deployment 的一部分进行管理的 Pod 上，其中每个版本的值是唯一的）。\n在 `matchLabelKeys` 中使用 `pod-template-hash` 允许你定位与传入 Pod 相同版本的 Pod，\n确保滚动升级不会破坏亲和性。"}
{"en": "# Only Pods from a given rollout are taken into consideration when calculating pod affinity.\n# If you update the Deployment, the replacement Pods follow their own affinity rules\n# (if there are any defined in the new Pod template)", "zh": "```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: application-server\n...\nspec:\n  template:\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - database\n            topologyKey: topology.kubernetes.io/zone\n            # 只有在计算 Pod 亲和性时，才考虑指定上线的 Pod。\n            # 如果你更新 Deployment，替代的 Pod 将遵循它们自己的亲和性规则\n            # （如果在新的 Pod 模板中定义了任何规则）。\n            matchLabelKeys:\n            - pod-template-hash\n```\n\n#### mismatchLabelKeys\n\n{{< feature-state feature_gate_name=\"MatchLabelKeysInPodAffinity\" >}}\n\n{{< note >}}"}
{"en": "The `mismatchLabelKeys` field is an beta-level field and is disabled by default in\nKubernetes {{< skew currentVersion >}}.\nWhen you want to disable it, you have to disable it explicitly via the\n`MatchLabelKeysInPodAffinity` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).", "zh": "`mismatchLabelKeys` 字段是一个 Beta 级别的字段，在 Kubernetes {{< skew currentVersion >}} 中默认被禁用。\n当你想要禁用此字段时，你必须通过 `MatchLabelKeysInPodAffinity`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)禁用它。\n{{< /note >}}"}
{"en": "Kubernetes includes an optional `mismatchLabelKeys` field for Pod affinity\nor anti-affinity. The field specifies keys for the labels that should **not** match with the incoming Pod's labels,\nwhen satisfying the Pod (anti)affinity.\n\nOne example use case is to ensure Pods go to the topology domain (node, zone, etc) where only Pods from the same tenant or team are scheduled in.\nIn other words, you want to avoid running Pods from two different tenants on the same topology domain at the same time.", "zh": "Kubernetes 为 Pod 亲和性或反亲和性提供了一个可选的 `mismatchLabelKeys` 字段。\n此字段指定了在满足 Pod（反）亲和性时，**不**应与传入 Pod 的标签匹配的键。\n\n一个示例用例是确保 Pod 进入指定的拓扑域（节点、区域等），在此拓扑域中只调度来自同一租户或团队的 Pod。\n换句话说，你想要避免在同一拓扑域中同时运行来自两个不同租户的 Pod。"}
{"en": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    # Assume that all relevant Pods have a \"tenant\" label set\n    tenant: tenant-a\n...\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      # ensure that pods associated with this tenant land on the correct node pool\n      - matchLabelKeys:\n          - tenant\n        topologyKey: node-pool\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      # ensure that pods associated with this tenant can't schedule to nodes used for another tenant\n      - mismatchLabelKeys:\n        - tenant # whatever the value of the \"tenant\" label for this Pod, prevent\n                 # scheduling to nodes in any pool where any Pod from a different\n                 # tenant is running.\n        labelSelector:\n          # We have to have the labelSelector which selects only Pods with the tenant label,\n          # otherwise this Pod would hate Pods from daemonsets as well, for example,\n          # which aren't supposed to have the tenant label.\n          matchExpressions:\n          - key: tenant\n            operator: Exists\n        topologyKey: node-pool\n```", "zh": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    # 假设所有相关的 Pod 都设置了 “tenant” 标签\n    tenant: tenant-a\n...\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      # 确保与此租户关联的 Pod 落在正确的节点池上\n      - matchLabelKeys:\n          - tenant\n        topologyKey: node-pool\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      # 确保与此租户关联的 Pod 不能调度到用于其他租户的节点上\n      - mismatchLabelKeys:\n        - tenant # 无论此 Pod 的 “tenant” 标签的值是什么，\n                 # 如果节点池中有来自别的租户的任何 Pod 在运行，\n                 # 都会阻碍此 Pod 被调度到这些节点池中的节点上\n        labelSelector:\n          # 我们必须有一个 labelSelector，只选择具有 “tenant” 标签的 Pod，\n          # 否则此 Pod 也会与来自 DaemonSet 的 Pod 发生冲突，\n          # 而这些 Pod 不应该具有 “tenant” 标签\n          matchExpressions:\n          - key: tenant\n            operator: Exists\n        topologyKey: node-pool\n```"}
{"en": "#### More practical use-cases\n\nInter-pod affinity and anti-affinity can be even more useful when they are used with higher\nlevel collections such as ReplicaSets, StatefulSets, Deployments, etc. These\nrules allow you to configure that a set of workloads should\nbe co-located in the same defined topology; for example, preferring to place two related\nPods onto the same node.", "zh": "#### 更实际的用例\n\nPod 间亲和性与反亲和性在与更高级别的集合（例如 ReplicaSet、StatefulSet、\nDeployment 等）一起使用时，它们可能更加有用。\n这些规则使得你可以配置一组工作负载，使其位于所定义的同一拓扑中；\n例如优先将两个相关的 Pod 置于相同的节点上。"}
{"en": "For example: imagine a three-node cluster. You use the cluster to run a web application\nand also an in-memory cache (such as Redis). For this example, also assume that latency between\nthe web application and the memory cache should be as low as is practical. You could use inter-pod\naffinity and anti-affinity to co-locate the web servers with the cache as much as possible.", "zh": "以一个三节点的集群为例。你使用该集群运行一个带有内存缓存（例如 Redis）的 Web 应用程序。\n在此例中，还假设 Web 应用程序和内存缓存之间的延迟应尽可能低。\n你可以使用 Pod 间的亲和性和反亲和性来尽可能地将该 Web 服务器与缓存并置。"}
{"en": "In the following example Deployment for the Redis cache, the replicas get the label `app=store`. The\n`podAntiAffinity` rule tells the scheduler to avoid placing multiple replicas\nwith the `app=store` label on a single node. This creates each cache in a\nseparate node.", "zh": "在下面的 Redis 缓存 Deployment 示例中，副本上设置了标签 `app=store`。\n`podAntiAffinity` 规则告诉调度器避免将多个带有 `app=store` 标签的副本部署到同一节点上。\n因此，每个独立节点上会创建一个缓存实例。\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-cache\nspec:\n  selector:\n    matchLabels:\n      app: store\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: store\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - store\n            topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: redis-server\n        image: redis:3.2-alpine\n```"}
{"en": "The following example Deployment for the web servers creates replicas with the label `app=web-store`.\nThe Pod affinity rule tells the scheduler to place each replica on a node that has a Pod\nwith the label `app=store`. The Pod anti-affinity rule tells the scheduler never to place\nmultiple `app=web-store` servers on a single node.", "zh": "下例的 Deployment 为 Web 服务器创建带有标签 `app=web-store` 的副本。\nPod 亲和性规则告诉调度器将每个副本放到存在标签为 `app=store` 的 Pod 的节点上。\nPod 反亲和性规则告诉调度器决不要在单个节点上放置多个 `app=web-store` 服务器。\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\nspec:\n  selector:\n    matchLabels:\n      app: web-store\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-store\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - web-store\n            topologyKey: \"kubernetes.io/hostname\"\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - store\n            topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: web-app\n        image: nginx:1.16-alpine\n```"}
{"en": "Creating the two preceding Deployments results in the following cluster layout,\nwhere each web server is co-located with a cache, on three separate nodes.", "zh": "创建前面两个 Deployment 会产生如下的集群布局，每个 Web 服务器与一个缓存实例并置，\n并分别运行在三个独立的节点上。\n\n|    node-1     |    node-2     |    node-3     |\n|:-------------:|:-------------:|:-------------:|\n| *webserver-1* | *webserver-2* | *webserver-3* |\n|   *cache-1*   |   *cache-2*   |   *cache-3*   |"}
{"en": "The overall effect is that each cache instance is likely to be accessed by a single client that\nis running on the same node. This approach aims to minimize both skew (imbalanced load) and latency.", "zh": "总体效果是每个缓存实例都非常可能被在同一个节点上运行的某个客户端访问，\n这种方法旨在最大限度地减少偏差（负载不平衡）和延迟。"}
{"en": "You might have other reasons to use Pod anti-affinity.\nSee the [ZooKeeper tutorial](/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure)\nfor an example of a StatefulSet configured with anti-affinity for high\navailability, using the same technique as this example.", "zh": "你可能还有使用 Pod 反亲和性的一些其他原因。\n参阅 [ZooKeeper 教程](/zh-cn/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure)\n了解一个 StatefulSet 的示例，该 StatefulSet 配置了反亲和性以实现高可用，\n所使用的是与此例相同的技术。"}
{"en": "## nodeName\n\n`nodeName` is a more direct form of node selection than affinity or\n`nodeSelector`. `nodeName` is a field in the Pod spec. If the `nodeName` field\nis not empty, the scheduler ignores the Pod and the kubelet on the named node\ntries to place the Pod on that node. Using `nodeName` overrules using\n`nodeSelector` or affinity and anti-affinity rules.", "zh": "## nodeName\n\n`nodeName` 是比亲和性或者 `nodeSelector` 更为直接的形式。`nodeName` 是 Pod\n规约中的一个字段。如果 `nodeName` 字段不为空，调度器会忽略该 Pod，\n而指定节点上的 kubelet 会尝试将 Pod 放到该节点上。\n使用 `nodeName` 规则的优先级会高于使用 `nodeSelector` 或亲和性与非亲和性的规则。"}
{"en": "Some of the limitations of using `nodeName` to select nodes are:\n\n- If the named node does not exist, the Pod will not run, and in\n  some cases may be automatically deleted.\n- If the named node does not have the resources to accommodate the\n  Pod, the Pod will fail and its reason will indicate why,\n  for example OutOfmemory or OutOfcpu.\n- Node names in cloud environments are not always predictable or stable.", "zh": "使用 `nodeName` 来选择节点的方式有一些局限性：\n\n- 如果所指代的节点不存在，则 Pod 无法运行，而且在某些情况下可能会被自动删除。\n- 如果所指代的节点无法提供用来运行 Pod 所需的资源，Pod 会失败，\n  而其失败原因中会给出是否因为内存或 CPU 不足而造成无法运行。\n- 在云环境中的节点名称并不总是可预测的，也不总是稳定的。\n\n{{< warning >}}"}
{"en": "`nodeName` is intended for use by custom schedulers or advanced use cases where\nyou need to bypass any configured schedulers. Bypassing the schedulers might lead to\nfailed Pods if the assigned Nodes get oversubscribed. You can use [node affinity](#node-affinity)\nor a the [`nodeselector` field](#nodeselector) to assign a Pod to a specific Node without bypassing the schedulers.", "zh": "`nodeName` 旨在供自定义调度器或需要绕过任何已配置调度器的高级场景使用。\n如果已分配的 Node 负载过重，绕过调度器可能会导致 Pod 失败。\n你可以使用[节点亲和性](#node-affinity)或 [`nodeselector` 字段](#nodeselector)将\nPod 分配给特定 Node，而无需绕过调度器。\n{{</ warning >}}"}
{"en": "Here is an example of a Pod spec using the `nodeName` field:", "zh": "下面是一个使用 `nodeName` 字段的 Pod 规约示例：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n  nodeName: kube-01\n```"}
{"en": "The above Pod will only run on the node `kube-01`.", "zh": "上面的 Pod 只能运行在节点 `kube-01` 之上。"}
{"en": "## Pod topology spread constraints\n\nYou can use _topology spread constraints_ to control how {{< glossary_tooltip text=\"Pods\" term_id=\"Pod\" >}}\nare spread across your cluster among failure-domains such as regions, zones, nodes, or among any other\ntopology domains that you define. You might do this to improve performance, expected availability, or\noverall utilization.\n\nRead [Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)\nto learn more about how these work.", "zh": "## Pod 拓扑分布约束 {#pod-topology-spread-constraints}\n\n你可以使用 **拓扑分布约束（Topology Spread Constraints）** 来控制\n{{< glossary_tooltip text=\"Pod\" term_id=\"Pod\" >}} 在集群内故障域之间的分布，\n故障域的示例有区域（Region）、可用区（Zone）、节点和其他用户自定义的拓扑域。\n这样做有助于提升性能、实现高可用或提升资源利用率。\n\n阅读 [Pod 拓扑分布约束](/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n以进一步了解这些约束的工作方式。"}
{"en": "## Operators\n\nThe following are all the logical operators that you can use in the `operator` field for `nodeAffinity` and `podAffinity` mentioned above.", "zh": "## 操作符   {#operators}\n\n下面是你可以在上述 `nodeAffinity` 和 `podAffinity` 的 `operator`\n字段中可以使用的所有逻辑运算符。"}
{"en": "|    Operator    |    Behavior     |\n| :------------: | :-------------: |\n| `In` | The label value is present in the supplied set of strings |\n|   `NotIn`   | The label value is not contained in the supplied set of strings |\n| `Exists` | A label with this key exists on the object |\n| `DoesNotExist` | No label with this key exists on the object |", "zh": "| 操作符 | 行为 |\n| :------------: | :-------------: |\n| `In` | 标签值存在于提供的字符串集中 |\n| `NotIn`  | 标签值不包含在提供的字符串集中 |\n| `Exists` | 对象上存在具有此键的标签 |\n| `DoesNotExist` | 对象上不存在具有此键的标签 |"}
{"en": "The following operators can only be used with `nodeAffinity`.", "zh": "以下操作符只能与 `nodeAffinity` 一起使用。"}
{"en": "|    Operator    |    Behavior    |\n| :------------: | :-------------: |\n| `Gt` | The field value will be parsed as an integer, and that integer is less than the integer that results from parsing the value of a label named by this selector |\n| `Lt` | The field value will be parsed as an integer, and that integer is greater than the integer that results from parsing the value of a label named by this selector |", "zh": "| 操作符 | 行为 |\n| :------------: | :-------------: |\n| `Gt` | 字段值将被解析为整数，并且该整数小于通过解析此选择算符命名的标签的值所得到的整数 |\n| `Lt` | 字段值将被解析为整数，并且该整数大于通过解析此选择算符命名的标签的值所得到的整数 |\n\n{{<note>}}"}
{"en": "`Gt` and `Lt` operators will not work with non-integer values. If the given value\ndoesn't parse as an integer, the pod will fail to get scheduled. Also, `Gt` and `Lt`\nare not available for `podAffinity`.", "zh": "`Gt` 和 `Lt` 操作符不能与非整数值一起使用。\n如果给定的值未解析为整数，则该 Pod 将无法被调度。\n另外，`Gt` 和 `Lt` 不适用于 `podAffinity`。\n{{</note>}}\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Read more about [taints and tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration/).\n- Read the design docs for [node affinity](https://git.k8s.io/design-proposals-archive/scheduling/nodeaffinity.md)\n  and for [inter-pod affinity/anti-affinity](https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md).\n- Learn about how the [topology manager](/docs/tasks/administer-cluster/topology-manager/) takes part in node-level\n  resource allocation decisions.\n- Learn how to use [nodeSelector](/docs/tasks/configure-pod-container/assign-pods-nodes/).\n- Learn how to use [affinity and anti-affinity](/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/).", "zh": "- 进一步阅读[污点与容忍度](/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/)文档。\n- 阅读[节点亲和性](https://git.k8s.io/design-proposals-archive/scheduling/nodeaffinity.md)和\n  [Pod 间亲和性与反亲和性](https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md)的设计文档。\n- 了解[拓扑管理器](/zh-cn/docs/tasks/administer-cluster/topology-manager/)如何参与节点层面资源分配决定。\n- 了解如何使用 [nodeSelector](/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes/)。\n- 了解如何使用[亲和性和反亲和性](/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/)。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.19\" state=\"stable\" >}}"}
{"en": "The _scheduling framework_ is a pluggable architecture for the Kubernetes scheduler.\nIt consists of a set of \"plugin\" APIs that are compiled directly into the scheduler.\nThese APIs allow most scheduling features to be implemented as plugins,\nwhile keeping the scheduling \"core\" lightweight and maintainable. Refer to the\n[design proposal of the scheduling framework][kep] for more technical information on\nthe design of the framework.", "zh": "**调度框架**是面向 Kubernetes 调度器的一种插件架构，\n它由一组直接编译到调度程序中的“插件” API 组成。\n这些 API 允许大多数调度功能以插件的形式实现，同时使调度“核心”保持简单且可维护。\n请参考[调度框架的设计提案](https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md)\n获取框架设计的更多技术信息。\n\n[kep]: https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md"}
{"en": "## Framework workflow", "zh": "## 框架工作流程   {#framework-workflow}"}
{"en": "The Scheduling Framework defines a few extension points. Scheduler plugins\nregister to be invoked at one or more extension points. Some of these plugins\ncan change the scheduling decisions and some are informational only.", "zh": "调度框架定义了一些扩展点。调度器插件注册后在一个或多个扩展点处被调用。\n这些插件中的一些可以改变调度决策，而另一些仅用于提供信息。"}
{"en": "Each attempt to schedule one Pod is split into two phases, the\n**scheduling cycle** and the **binding cycle**.", "zh": "每次调度一个 Pod 的尝试都分为两个阶段，即**调度周期**和**绑定周期**。"}
{"en": "### Scheduling Cycle & Binding Cycle", "zh": "### 调度周期和绑定周期   {#scheduling-cycle-and-binding-cycle}"}
{"en": "The scheduling cycle selects a node for the Pod, and the binding cycle applies\nthat decision to the cluster. Together, a scheduling cycle and binding cycle are\nreferred to as a \"scheduling context\".", "zh": "调度周期为 Pod 选择一个节点，绑定周期将该决策应用于集群。\n调度周期和绑定周期一起被称为“调度上下文”。"}
{"en": "Scheduling cycles are run serially, while binding cycles may run concurrently.", "zh": "调度周期是串行运行的，而绑定周期可能是同时运行的。"}
{"en": "A scheduling or binding cycle can be aborted if the Pod is determined to\nbe unschedulable or if there is an internal error. The Pod will be returned to\nthe queue and retried.", "zh": "如果确定 Pod 不可调度或者存在内部错误，则可以终止调度周期或绑定周期。\nPod 将返回队列并重试。"}
{"en": "## Interfaces", "zh": "## 接口   {#interfaces}"}
{"en": "The following picture shows the scheduling context of a Pod and the interfaces\nthat the scheduling framework exposes.", "zh": "下图显示了一个 Pod 的调度上下文以及调度框架公开的接口。"}
{"en": "One plugin may implement multiple interfaces to perform more complex or\nstateful tasks.", "zh": "一个插件可能实现多个接口，以执行更为复杂或有状态的任务。"}
{"en": "Some interfaces match the scheduler extension points which can be configured through\n[Scheduler Configuration](/docs/reference/scheduling/config/#extension-points).", "zh": "某些接口与可以通过[调度器配置](/zh-cn/docs/reference/scheduling/config/#extension-points)来设置的调度器扩展点匹配。"}
{"en": "{{< figure src=\"/images/docs/scheduling-framework-extensions.png\" title=\"scheduling framework extension points\" class=\"diagram-large\">}}", "zh": "{{< figure src=\"/images/docs/scheduling-framework-extensions.png\" title=\"调度框架扩展点\" class=\"diagram-large\">}}"}
{"en": "### PreEnqueue {#pre-enqueue}", "zh": "### PreEnqueue {#pre-enqueue}"}
{"en": "These plugins are called prior to adding Pods to the internal active queue, where Pods are marked as\nready for scheduling.\n\nOnly when all PreEnqueue plugins return `Success`, the Pod is allowed to enter the active queue.\nOtherwise, it's placed in the internal unschedulable Pods list, and doesn't get an `Unschedulable` condition.\n\nFor more details about how internal scheduler queues work, read\n[Scheduling queue in kube-scheduler](https://github.com/kubernetes/community/blob/f03b6d5692bd979f07dd472e7b6836b2dad0fd9b/contributors/devel/sig-scheduling/scheduler_queues.md).", "zh": "这些插件在将 Pod 被添加到内部活动队列之前被调用，在此队列中 Pod 被标记为准备好进行调度。\n\n只有当所有 PreEnqueue 插件返回 `Success` 时，Pod 才允许进入活动队列。\n否则，它将被放置在内部无法调度的 Pod 列表中，并且不会获得 `Unschedulable` 状态。\n\n要了解有关内部调度器队列如何工作的更多详细信息，请阅读\n[kube-scheduler 调度队列](https://github.com/kubernetes/community/blob/f03b6d5692bd979f07dd472e7b6836b2dad0fd9b/contributors/devel/sig-scheduling/scheduler_queues.md)。\n\n### EnqueueExtension"}
{"en": "EnqueueExtension is the interface where the plugin can control\nwhether to retry scheduling of Pods rejected by this plugin, based on changes in the cluster.\nPlugins that implement PreEnqueue, PreFilter, Filter, Reserve or Permit should implement this interface.", "zh": "EnqueueExtension 作为一个接口，插件可以在此接口之上根据集群中的变化来控制是否重新尝试调度被此插件拒绝的\nPod。实现 PreEnqueue、PreFilter、Filter、Reserve 或 Permit 的插件应实现此接口。\n\n### QueueingHint\n\n{{< feature-state for_k8s_version=\"v1.28\" state=\"beta\" >}}"}
{"en": "QueueingHint is a callback function for deciding whether a Pod can be requeued to the active queue or backoff queue.\nIt's executed every time a certain kind of event or change happens in the cluster.\nWhen the QueueingHint finds that the event might make the Pod schedulable,\nthe Pod is put into the active queue or the backoff queue\nso that the scheduler will retry the scheduling of the Pod.", "zh": "QueueingHint 作为一个回调函数，用于决定是否将 Pod 重新排队到活跃队列或回退队列。\n每当集群中发生某种事件或变化时，此函数就会被执行。\n当 QueueingHint 发现事件可能使 Pod 可调度时，Pod 将被放入活跃队列或回退队列，\n以便调度器可以重新尝试调度 Pod。\n\n{{< note >}}"}
{"en": "QueueingHint evaluation during scheduling is a beta-level feature.\nThe v1.28 release series initially enabled the associated feature gate; however, after the\ndiscovery of an excessive memory footprint, the Kubernetes project set that feature gate\nto be disabled by default. In Kubernetes {{< skew currentVersion >}}, this feature gate is\ndisabled and you need to enable it manually.\nYou can enable it via the\n`SchedulerQueueingHints` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).", "zh": "在调度过程中对 QueueingHint 求值是一个 Beta 级别的特性。\nv1.28 的系列小版本最初都开启了这个特性的门控；但是发现了内存占用过多的问题，\n于是 Kubernetes 项目将该特性门控设置为默认禁用。\n在 Kubernetes 的 {{< skew currentVersion >}} 版本中，这个特性门控被禁用，你需要手动开启它。\n你可以通过 `SchedulerQueueingHints`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)来启用它。\n{{< /note >}}"}
{"en": "### QueueSort {#queue-sort}", "zh": "### 队列排序 {#queue-sort}"}
{"en": "These plugins are used to sort Pods in the scheduling queue. A queue sort plugin\nessentially provides a `Less(Pod1, Pod2)` function. Only one queue sort\nplugin may be enabled at a time.", "zh": "这些插件用于对调度队列中的 Pod 进行排序。\n队列排序插件本质上提供 `Less(Pod1, Pod2)` 函数。\n一次只能启动一个队列插件。"}
{"en": "### PreFilter {#pre-filter}", "zh": "### PreFilter {#pre-filter}"}
{"en": "These plugins are used to pre-process info about the Pod, or to check certain\nconditions that the cluster or the Pod must meet. If a PreFilter plugin returns\nan error, the scheduling cycle is aborted.", "zh": "这些插件用于预处理 Pod 的相关信息，或者检查集群或 Pod 必须满足的某些条件。\n如果 PreFilter 插件返回错误，则调度周期将终止。"}
{"en": "### Filter", "zh": "### Filter"}
{"en": "These plugins are used to filter out nodes that cannot run the Pod. For each\nnode, the scheduler will call filter plugins in their configured order. If any\nfilter plugin marks the node as infeasible, the remaining plugins will not be\ncalled for that node. Nodes may be evaluated concurrently.", "zh": "这些插件用于过滤出不能运行该 Pod 的节点。对于每个节点，\n调度器将按照其配置顺序调用这些过滤插件。如果任何过滤插件将节点标记为不可行，\n则不会为该节点调用剩下的过滤插件。节点可以被同时进行评估。"}
{"en": "### PostFilter {#post-filter}", "zh": "### PostFilter  {#post-filter}"}
{"en": "These plugins are called after the Filter phase, but only when no feasible nodes\nwere found for the pod. Plugins are called in their configured order. If\nany postFilter plugin marks the node as `Schedulable`, the remaining plugins\nwill not be called. A typical PostFilter implementation is preemption, which\ntries to make the pod schedulable by preempting other Pods.", "zh": "这些插件在 Filter 阶段后调用，但仅在该 Pod 没有可行的节点时调用。\n插件按其配置的顺序调用。如果任何 PostFilter 插件标记节点为 \"Schedulable\"，\n则其余的插件不会调用。典型的 PostFilter 实现是抢占，试图通过抢占其他 Pod\n的资源使该 Pod 可以调度。"}
{"en": "### PreScore {#pre-score}", "zh": "### PreScore {#pre-score}"}
{"en": "These plugins are used to perform \"pre-scoring\" work, which generates a sharable\nstate for Score plugins to use. If a PreScore plugin returns an error, the\nscheduling cycle is aborted.", "zh": "这些插件用于执行“前置评分（pre-scoring）”工作，即生成一个可共享状态供 Score 插件使用。\n如果 PreScore 插件返回错误，则调度周期将终止。"}
{"en": "### Score {#scoring}", "zh": "### Score  {#scoring}"}
{"en": "These plugins are used to rank nodes that have passed the filtering phase. The\nscheduler will call each scoring plugin for each node. There will be a well\ndefined range of integers representing the minimum and maximum scores. After the\n[NormalizeScore](#normalize-scoring) phase, the scheduler will combine node\nscores from all plugins according to the configured plugin weights.", "zh": "这些插件用于对通过过滤阶段的节点进行排序。调度器将为每个节点调用每个评分插件。\n将有一个定义明确的整数范围，代表最小和最大分数。\n在[标准化评分](#normalize-scoring)阶段之后，\n调度器将根据配置的插件权重合并所有插件的节点分数。"}
{"en": "### NormalizeScore {#normalize-scoring}", "zh": "### NormalizeScore   {#normalize-scoring}"}
{"en": "These plugins are used to modify scores before the scheduler computes a final\nranking of Nodes. A plugin that registers for this extension point will be\ncalled with the [Score](#scoring) results from the same plugin. This is called\nonce per plugin per scheduling cycle.", "zh": "这些插件用于在调度器计算 Node 排名之前修改分数。\n在此扩展点注册的插件被调用时会使用同一插件的 [Score](#scoring)\n结果。每个插件在每个调度周期调用一次。"}
{"en": "For example, suppose a plugin `BlinkingLightScorer` ranks Nodes based on how\nmany blinking lights they have.", "zh": "例如，假设一个 `BlinkingLightScorer` 插件基于具有的闪烁指示灯数量来对节点进行排名。\n\n```go\nfunc ScoreNode(_ *v1.pod, n *v1.Node) (int, error) {\n    return getBlinkingLightCount(n)\n}\n```"}
{"en": "However, the maximum count of blinking lights may be small compared to\n`NodeScoreMax`. To fix this, `BlinkingLightScorer` should also register for this\nextension point.", "zh": "然而，最大的闪烁灯个数值可能比 `NodeScoreMax` 小。要解决这个问题，\n`BlinkingLightScorer` 插件还应该注册该扩展点。\n\n```go\nfunc NormalizeScores(scores map[string]int) {\n    highest := 0\n    for _, score := range scores {\n        highest = max(highest, score)\n    }\n    for node, score := range scores {\n        scores[node] = score*NodeScoreMax/highest\n    }\n}\n```"}
{"en": "If any NormalizeScore plugin returns an error, the scheduling cycle is\naborted.", "zh": "如果任何 NormalizeScore 插件返回错误，则调度阶段将终止。\n\n{{< note >}}"}
{"en": "Plugins wishing to perform \"pre-reserve\" work should use the\nNormalizeScore extension point.", "zh": "希望执行“预保留”工作的插件应该使用 NormalizeScore 扩展点。\n{{< /note >}}\n\n### Reserve {#reserve}"}
{"en": "A plugin that implements the Reserve interface has two methods, namely `Reserve`\nand `Unreserve`, that back two informational scheduling phases called Reserve\nand Unreserve, respectively. Plugins which maintain runtime state (aka \"stateful\nplugins\") should use these phases to be notified by the scheduler when resources\non a node are being reserved and unreserved for a given Pod.", "zh": "实现了 Reserve 接口的插件，拥有两个方法，即 `Reserve` 和 `Unreserve`，\n他们分别支持两个名为 Reserve 和 Unreserve 的信息传递性质的调度阶段。\n维护运行时状态的插件（又称\"有状态插件\"）应该使用这两个阶段，\n以便在节点上的资源被保留和解除保留给特定的 Pod 时，得到调度器的通知。"}
{"en": "The Reserve phase happens before the scheduler actually binds a Pod to its\ndesignated node. It exists to prevent race conditions while the scheduler waits\nfor the bind to succeed. The `Reserve` method of each Reserve plugin may succeed\nor fail; if one `Reserve` method call fails, subsequent plugins are not executed\nand the Reserve phase is considered to have failed. If the `Reserve` method of\nall plugins succeed, the Reserve phase is considered to be successful and the\nrest of the scheduling cycle and the binding cycle are executed.", "zh": "Reserve 阶段发生在调度器实际将一个 Pod 绑定到其指定节点之前。\n它的存在是为了防止在调度器等待绑定成功时发生竞争情况。\n每个 Reserve 插件的 `Reserve` 方法可能成功，也可能失败；\n如果一个 `Reserve` 方法调用失败，后面的插件就不会被执行，Reserve 阶段被认为失败。\n如果所有插件的 `Reserve` 方法都成功了，Reserve 阶段就被认为是成功的，\n剩下的调度周期和绑定周期就会被执行。"}
{"en": "The Unreserve phase is triggered if the Reserve phase or a later phase fails.\nWhen this happens, the `Unreserve` method of **all** Reserve plugins will be\nexecuted in the reverse order of `Reserve` method calls. This phase exists to\nclean up the state associated with the reserved Pod.", "zh": "如果 Reserve 阶段或后续阶段失败了，则触发 Unreserve 阶段。\n发生这种情况时，**所有** Reserve 插件的 `Unreserve` 方法将按照\n`Reserve` 方法调用的相反顺序执行。\n这个阶段的存在是为了清理与保留的 Pod 相关的状态。\n\n{{< caution >}}"}
{"en": "The implementation of the `Unreserve` method in Reserve plugins must be\nidempotent and may not fail.", "zh": "Reserve 插件中 `Unreserve` 方法的实现必须是幂等的，并且不能失败。\n{{< /caution >}}"}
{"en": "This is the last step in a scheduling cycle. Once a Pod is in the reserved\nstate, it will either trigger [Unreserve](#unreserve) plugins (on failure) or\n[PostBind](#post-bind) plugins (on success) at the end of the binding cycle.", "zh": "这个是调度周期的最后一步。\n一旦 Pod 处于保留状态，它将在绑定周期结束时触发 [Unreserve](#unreserve) 插件（失败时）或\n[PostBind](#post-bind) 插件（成功时）。"}
{"en": "### Permit", "zh": "### Permit"}
{"en": "_Permit_ plugins are invoked at the end of the scheduling cycle for each Pod, to\nprevent or delay the binding to the candidate node. A permit plugin can do one of\nthe three things:", "zh": "**Permit** 插件在每个 Pod 调度周期的最后调用，用于防止或延迟 Pod 的绑定。\n一个允许插件可以做以下三件事之一："}
{"en": "1.  **approve** \\\n    Once all Permit plugins approve a Pod, it is sent for binding.", "zh": "1.  **批准** \\\n    一旦所有 Permit 插件批准 Pod 后，该 Pod 将被发送以进行绑定。"}
{"en": "1.  **deny** \\\n    If any Permit plugin denies a Pod, it is returned to the scheduling queue.\n    This will trigger the Unreserve phase in [Reserve plugins](#reserve).", "zh": "2.  **拒绝** \\\n    如果任何 Permit 插件拒绝 Pod，则该 Pod 将被返回到调度队列。\n    这将触发 [Reserve 插件](#reserve)中的 Unreserve 阶段。"}
{"en": "1.  **wait** (with a timeout) \\\n    If a Permit plugin returns \"wait\", then the Pod is kept in an internal \"waiting\"\n    Pods list, and the binding cycle of this Pod starts but directly blocks until it\n    gets approved. If a timeout occurs, **wait** becomes **deny**\n    and the Pod is returned to the scheduling queue, triggering the\n    Unreserve phase in [Reserve plugins](#reserve).", "zh": "3. **等待**（带有超时）\\\n    如果一个 Permit 插件返回“等待”结果，则 Pod 将保持在一个内部的“等待中”\n    的 Pod 列表，同时该 Pod 的绑定周期启动时即直接阻塞直到得到批准。\n    如果超时发生，**等待**变成**拒绝**，并且 Pod 将返回调度队列，从而触发\n    [Reserve 插件](#reserve)中的 Unreserve 阶段。\n\n{{< note >}}"}
{"en": "While any plugin can access the list of \"waiting\" Pods and approve them\n(see [`FrameworkHandle`](https://git.k8s.io/enhancements/keps/sig-scheduling/624-scheduling-framework#frameworkhandle)),\nwe expect only the permit plugins to approve binding of reserved Pods that are in \"waiting\" state.\nOnce a Pod is approved, it is sent to the [PreBind](#pre-bind) phase.", "zh": "尽管任何插件可以访问“等待中”状态的 Pod 列表并批准它们\n（查看 [`FrameworkHandle`](https://git.k8s.io/enhancements/keps/sig-scheduling/624-scheduling-framework#frameworkhandle)）。\n我们期望只有允许插件可以批准处于“等待中”状态的预留 Pod 的绑定。\n一旦 Pod 被批准了，它将发送到 [PreBind](#pre-bind) 阶段。\n{{< /note >}}"}
{"en": "### PreBind {#pre-bind}", "zh": "### PreBind  {#pre-bind}"}
{"en": "These plugins are used to perform any work required before a Pod is bound. For\nexample, a pre-bind plugin may provision a network volume and mount it on the\ntarget node before allowing the Pod to run there.", "zh": "这些插件用于执行 Pod 绑定前所需的所有工作。\n例如，一个 PreBind 插件可能需要制备网络卷并且在允许 Pod\n运行在该节点之前将其挂载到目标节点上。"}
{"en": "If any PreBind plugin returns an error, the Pod is [rejected](#reserve) and\nreturned to the scheduling queue.", "zh": "如果任何 PreBind 插件返回错误，则 Pod 将被[拒绝](#reserve)并且退回到调度队列中。"}
{"en": "### Bind", "zh": "### Bind"}
{"en": "These plugins are used to bind a Pod to a Node. Bind plugins will not be called\nuntil all PreBind plugins have completed. Each bind plugin is called in the\nconfigured order. A bind plugin may choose whether or not to handle the given\nPod. If a bind plugin chooses to handle a Pod, **the remaining bind plugins are\nskipped**.", "zh": "Bind 插件用于将 Pod 绑定到节点上。直到所有的 PreBind 插件都完成，Bind 插件才会被调用。\n各 Bind 插件按照配置顺序被调用。Bind 插件可以选择是否处理指定的 Pod。\n如果某 Bind 插件选择处理某 Pod，**剩余的 Bind 插件将被跳过**。"}
{"en": "### PostBind {#post-bind}", "zh": "### PostBind  {#post-bind}"}
{"en": "This is an informational interface. Post-bind plugins are called after a\nPod is successfully bound. This is the end of a binding cycle, and can be used\nto clean up associated resources.", "zh": "这是个信息传递性质的接口。\nPostBind 插件在 Pod 成功绑定后被调用。这是绑定周期的结尾，可用于清理相关的资源。"}
{"en": "## Plugin API", "zh": "## 插件 API   {#plugin-api}"}
{"en": "There are two steps to the plugin API. First, plugins must register and get\nconfigured, then they use the extension point interfaces. Extension point\ninterfaces have the following form.", "zh": "插件 API 分为两个步骤。首先，插件必须完成注册并配置，然后才能使用扩展点接口。\n扩展点接口具有以下形式。\n\n```go\ntype Plugin interface {\n    Name() string\n}\n\ntype QueueSortPlugin interface {\n    Plugin\n    Less(*v1.pod, *v1.pod) bool\n}\n\ntype PreFilterPlugin interface {\n    Plugin\n    PreFilter(context.Context, *framework.CycleState, *v1.pod) error\n}\n\n// ...\n```"}
{"en": "## Plugin configuration", "zh": "## 插件配置   {#plugin-configuration}"}
{"en": "You can enable or disable plugins in the scheduler configuration. If you are using\nKubernetes v1.18 or later, most scheduling\n[plugins](/docs/reference/scheduling/config/#scheduling-plugins) are in use and\nenabled by default.", "zh": "你可以在调度器配置中启用或禁用插件。\n如果你在使用 Kubernetes v1.18 或更高版本，\n大部分调度[插件](/zh-cn/docs/reference/scheduling/config/#scheduling-plugins)都在使用中且默认启用。"}
{"en": "In addition to default plugins, you can also implement your own scheduling\nplugins and get them configured along with default plugins. You can visit\n[scheduler-plugins](https://github.com/kubernetes-sigs/scheduler-plugins) for more details.", "zh": "除了默认的插件，你还可以实现自己的调度插件并且将它们与默认插件一起配置。\n你可以访问 [scheduler-plugins](https://github.com/kubernetes-sigs/scheduler-plugins)\n了解更多信息。"}
{"en": "If you are using Kubernetes v1.18 or later, you can configure a set of plugins as\na scheduler profile and then define multiple profiles to fit various kinds of workload.\nLearn more at [multiple profiles](/docs/reference/scheduling/config/#multiple-profiles).", "zh": "如果你正在使用 Kubernetes v1.18 或更高版本，你可以将一组插件设置为一个调度器配置文件，\n然后定义不同的配置文件来满足各类工作负载。\n了解更多关于[多配置文件](/zh-cn/docs/reference/scheduling/config/#multiple-profiles)。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.30\" state=\"stable\" >}}"}
{"en": "Pods were considered ready for scheduling once created. Kubernetes scheduler\ndoes its due diligence to find nodes to place all pending Pods. However, in a\nreal-world case, some Pods may stay in a \"miss-essential-resources\" state for a long period.\nThese Pods actually churn the scheduler (and downstream integrators like Cluster AutoScaler)\nin an unnecessary manner.\n\nBy specifying/removing a Pod's `.spec.schedulingGates`, you can control when a Pod is ready\nto be considered for scheduling.", "zh": "Pod 一旦创建就被认为准备好进行调度。\nKubernetes 调度程序尽职尽责地寻找节点来放置所有待处理的 Pod。\n然而，在实际环境中，会有一些 Pod 可能会长时间处于\"缺少必要资源\"状态。\n这些 Pod 实际上以一种不必要的方式扰乱了调度器（以及 Cluster AutoScaler 这类下游的集成方）。\n\n通过指定或删除 Pod 的 `.spec.schedulingGates`，可以控制 Pod 何时准备好被纳入考量进行调度。"}
{"en": "## Configuring Pod schedulingGates\n\nThe `schedulingGates` field contains a list of strings, and each string literal is perceived as a\ncriteria that Pod should be satisfied before considered schedulable. This field can be initialized\nonly when a Pod is created (either by the client, or mutated during admission). After creation,\neach schedulingGate can be removed in arbitrary order, but addition of a new scheduling gate is disallowed.", "zh": "## 配置 Pod schedulingGates  {#configuring-pod-schedulinggates}\n\n`schedulingGates` 字段包含一个字符串列表，每个字符串文字都被视为 Pod 在被认为可调度之前应该满足的标准。\n该字段只能在创建 Pod 时初始化（由客户端创建，或在准入期间更改）。\n创建后，每个 schedulingGate 可以按任意顺序删除，但不允许添加新的调度门控。\n\n{{< figure src=\"/zh-cn/docs/images/podSchedulingGates.svg\" alt=\"pod-scheduling-gates-diagram\" caption=\""}
{"en": "Figure. Pod SchedulingGates", "zh": "图：Pod SchedulingGates\" class=\"diagram-large\" link=\"https://mermaid.live/edit#pako:eNplUctqFEEU_ZWispOejNPd6UxKcBVxJQjZabuo1KO7mO6upqo6GoZZCSIikp2KYuKDJApidKP0CP5Memay8hesfinBWt17zuHec-pOIZGUQQS1wYZtCxwpnA723DALM2CfHiFwW1JQff9WPX5VzcsOdlt4dfawKo-rd2-qJ0fn5aOL56eLZyedxLskOfu6nH_qGL9lFp_fV69PV78OVm-ftozgCOyQmNEiEVl00zoC5z_K5cfy98_DVnH3yj0wGFy3vnp_TSt476tr_5tjAyxP5hcvP_Sb2jE2R3VwfBmzxhcvvgDQ52hRvzfftNZH_UUkwVpvMw4mYw24SBK05rkBYRuONkpOGFrjnHf14L6gJkZ-_sAhMpGq4a51M2wQR7uO9hztO6KZF2bQgSlTKRbUHmha7w-hiVnKQohsSbGahDDMZlaHCyN39jMCkVEFc2CR03_3hIjjRFuUUWGkutVevDl8r7zRMH-FicSU2XYKzX5eiyOhjRUTmXER1XihEgvHxuQaDYc1vR4JExe760SmQy1ojJWJ97aCYeAGY-x6LNj08IbnUbI72hpz1x9xunl15GI4mzkwx9kdKXunsz8c5u0b\" >}}"}
{"en": "## Usage example\n\nTo mark a Pod not-ready for scheduling, you can create it with one or more scheduling gates like this:", "zh": "## 用法示例  {#usage-example}\n\n要将 Pod 标记为未准备好进行调度，你可以在创建 Pod 时附带一个或多个调度门控，如下所示：\n\n{{% code_sample file=\"pods/pod-with-scheduling-gates.yaml\" %}}"}
{"en": "After the Pod's creation, you can check its state using:", "zh": "Pod 创建后，你可以使用以下方法检查其状态：\n\n```bash\nkubectl get pod test-pod\n```"}
{"en": "The output reveals it's in `SchedulingGated` state:", "zh": "输出显示它处于 `SchedulingGated` 状态：\n\n```none\nNAME       READY   STATUS            RESTARTS   AGE\ntest-pod   0/1     SchedulingGated   0          7s\n```"}
{"en": "You can also check its `schedulingGates` field by running:", "zh": "你还可以通过运行以下命令检查其 `schedulingGates` 字段：\n\n```bash\nkubectl get pod test-pod -o jsonpath='{.spec.schedulingGates}'\n```"}
{"en": "The output is:", "zh": "输出是：\n\n```none\n[{\"name\":\"example.com/foo\"},{\"name\":\"example.com/bar\"}]\n```"}
{"en": "To inform scheduler this Pod is ready for scheduling, you can remove its `schedulingGates` entirely\nby reapplying a modified manifest:", "zh": "要通知调度程序此 Pod 已准备好进行调度，你可以通过重新应用修改后的清单来完全删除其 `schedulingGates`：\n\n{{% code_sample file=\"pods/pod-without-scheduling-gates.yaml\" %}}"}
{"en": "You can check if the `schedulingGates` is cleared by running:", "zh": "你可以通过运行以下命令检查 `schedulingGates` 是否已被清空：\n\n```bash\nkubectl get pod test-pod -o jsonpath='{.spec.schedulingGates}'\n```"}
{"en": "The output is expected to be empty. And you can check its latest status by running:", "zh": "预计输出为空，你可以通过运行下面的命令来检查它的最新状态：\n\n```bash\nkubectl get pod test-pod -o wide\n```"}
{"en": "Given the test-pod doesn't request any CPU/memory resources, it's expected that this Pod's state get\ntransited from previous `SchedulingGated` to `Running`:", "zh": "鉴于 test-pod 不请求任何 CPU/内存资源，预计此 Pod 的状态会从之前的\n`SchedulingGated` 转变为 `Running`：\n\n```none\nNAME       READY   STATUS    RESTARTS   AGE   IP         NODE\ntest-pod   1/1     Running   0          15s   10.0.0.4   node-2\n```"}
{"en": "## Observability\n\nThe metric `scheduler_pending_pods` comes with a new label `\"gated\"` to distinguish whether a Pod\nhas been tried scheduling but claimed as unschedulable, or explicitly marked as not ready for\nscheduling. You can use `scheduler_pending_pods{queue=\"gated\"}` to check the metric result.", "zh": "## 可观测性  {#observability}\n\n指标 `scheduler_pending_pods` 带有一个新标签 `\"gated\"`，\n以区分 Pod 是否已尝试调度但被宣称不可调度，或明确标记为未准备好调度。\n你可以使用 `scheduler_pending_pods{queue=\"gated\"}` 来检查指标结果。"}
{"en": "## Mutable Pod scheduling directives", "zh": "## 可变 Pod 调度指令    {#mutable-pod-scheduling-directives}"}
{"en": "You can mutate scheduling directives of Pods while they have scheduling gates, with certain constraints.\nAt a high level, you can only tighten the scheduling directives of a Pod. In other words, the updated\ndirectives would cause the Pods to only be able to be scheduled on a subset of the nodes that it would\npreviously match. More concretely, the rules for updating a Pod's scheduling directives are as follows:", "zh": "当 Pod 具有调度门控时，你可以在某些约束条件下改变 Pod 的调度指令。\n在高层次上，你只能收紧 Pod 的调度指令。换句话说，更新后的指令将导致\nPod 只能被调度到它之前匹配的节点子集上。\n更具体地说，更新 Pod 的调度指令的规则如下："}
{"en": "1. For `.spec.nodeSelector`, only additions are allowed. If absent, it will be allowed to be set.\n\n2. For `spec.affinity.nodeAffinity`, if nil, then setting anything is allowed.", "zh": "1. 对于 `.spec.nodeSelector`，只允许增加。如果原来未设置，则允许设置此字段。\n\n2. 对于 `spec.affinity.nodeAffinity`，如果当前值为 nil，则允许设置为任意值。"}
{"en": "3. If `NodeSelectorTerms` was empty, it will be allowed to be set.\n   If not empty, then only additions of `NodeSelectorRequirements` to `matchExpressions`\n   or `fieldExpressions` are allowed, and no changes to existing `matchExpressions`\n   and `fieldExpressions` will be allowed. This is because the terms in\n   `.requiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms`, are ORed\n   while the expressions in `nodeSelectorTerms[].matchExpressions` and\n   `nodeSelectorTerms[].fieldExpressions` are ANDed.", "zh": "3. 如果 `NodeSelectorTerms` 之前为空，则允许设置该字段。\n   如果之前不为空，则仅允许增加 `NodeSelectorRequirements` 到 `matchExpressions`\n   或 `fieldExpressions`，且不允许更改当前的 `matchExpressions` 和 `fieldExpressions`。\n   这是因为 `.requiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms`\n   中的条目被执行逻辑或运算，而 `nodeSelectorTerms[].matchExpressions` 和\n   `nodeSelectorTerms[].fieldExpressions` 中的表达式被执行逻辑与运算。"}
{"en": "4. For `.preferredDuringSchedulingIgnoredDuringExecution`, all updates are allowed.\n   This is because preferred terms are not authoritative, and so policy controllers\n   don't validate those terms.", "zh": "4. 对于 `.preferredDuringSchedulingIgnoredDuringExecution`，所有更新都被允许。\n   这是因为首选条目不具有权威性，因此策略控制器不会验证这些条目。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read the [PodSchedulingReadiness KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/3521-pod-scheduling-readiness) for more details", "zh": "* 阅读 [PodSchedulingReadiness KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/3521-pod-scheduling-readiness)\n  了解更多详情"}
{"en": "title: Node-pressure Eviction\ncontent_type: concept\nweight: 100", "zh": "{{<glossary_definition term_id=\"node-pressure-eviction\" length=\"short\">}}</br>\n\n{{< feature-state feature_gate_name=\"KubeletSeparateDiskGC\" >}}\n\n{{<note>}}"}
{"en": "The _split image filesystem_ feature, which enables support for the `containerfs`\nfilesystem, adds several new eviction signals, thresholds and metrics. To use\n`containerfs`, the Kubernetes release v{{< skew currentVersion >}} requires the\n`KubeletSeparateDiskGC` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nto be enabled. Currently, only CRI-O (v1.29 or higher) offers the `containerfs`\nfilesystem support.", "zh": "**拆分镜像文件系统** 功能支持 `containerfs` 文件系统，并增加了几个新的驱逐信号、阈值和指标。\n要使用 `containerfs`，Kubernetes 版本 v{{< skew currentVersion >}} 需要启用 `KubeletSeparateDiskGC`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)。\n目前，只有 CRI-O（v1.29 或更高版本）提供对 `containerfs` 文件系统的支持。\n{{</note>}}"}
{"en": "The {{<glossary_tooltip term_id=\"kubelet\" text=\"kubelet\">}} monitors resources\nlike memory, disk space, and filesystem inodes on your cluster's nodes.\nWhen one or more of these resources reach specific consumption levels, the\nkubelet can proactively fail one or more pods on the node to reclaim resources\nand prevent starvation.\n\nDuring a node-pressure eviction, the kubelet sets the [phase](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase) for the\nselected pods to `Failed`, and terminates the Pod.\n\nNode-pressure eviction is not the same as\n[API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/).", "zh": "{{<glossary_tooltip term_id=\"kubelet\" text=\"kubelet\">}}\n监控集群节点的内存、磁盘空间和文件系统的 inode 等资源。\n当这些资源中的一个或者多个达到特定的消耗水平，\nkubelet 可以主动地使节点上一个或者多个 Pod 失效，以回收资源防止饥饿。\n\n在节点压力驱逐期间，kubelet 将所选 Pod 的[阶段](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase)\n设置为 `Failed` 并终止 Pod。\n\n节点压力驱逐不同于 [API 发起的驱逐](/zh-cn/docs/concepts/scheduling-eviction/api-eviction/)。"}
{"en": "The kubelet does not respect your configured {{<glossary_tooltip term_id=\"pod-disruption-budget\" text=\"PodDisruptionBudget\">}}\nor the pod's\n`terminationGracePeriodSeconds`. If you use [soft eviction thresholds](#soft-eviction-thresholds),\nthe kubelet respects your configured `eviction-max-pod-grace-period`. If you use\n[hard eviction thresholds](#hard-eviction-thresholds), the kubelet uses a `0s` grace period (immediate shutdown) for termination.", "zh": "kubelet 并不理会你配置的 {{<glossary_tooltip term_id=\"pod-disruption-budget\" text=\"PodDisruptionBudget\">}}\n或者是 Pod 的 `terminationGracePeriodSeconds`。\n如果你使用了[软驱逐条件](#soft-eviction-thresholds)，kubelet 会考虑你所配置的\n`eviction-max-pod-grace-period`。\n如果你使用了[硬驱逐条件](#hard-eviction-thresholds)，kubelet 使用 `0s`\n宽限期（立即关闭）来终止 Pod。"}
{"en": "## Self healing behavior\n\nThe kubelet attempts to [reclaim node-level resources](#reclaim-node-resources)\nbefore it terminates end-user pods. For example, it removes unused container\nimages when disk resources are starved.", "zh": "## 自我修复行为   {#self-healing-behavior}\n\nkubelet 在终止最终用户 Pod 之前会尝试[回收节点级资源](#reclaim-node-resources)。\n例如，它会在磁盘资源不足时删除未使用的容器镜像。"}
{"en": "If the pods are managed by a {{< glossary_tooltip text=\"workload\" term_id=\"workload\" >}}\nresource (such as {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}\nor {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}) that\nreplaces failed pods, the control plane or `kube-controller-manager` creates new\npods in place of the evicted pods.", "zh": "如果 Pod 是由替换失败 Pod 的{{< glossary_tooltip text=\"工作负载\" term_id=\"workload\" >}}资源\n（例如 {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}\n或者 {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}）管理，\n则控制平面或 `kube-controller-manager` 会创建新的 Pod 来代替被驱逐的 Pod。"}
{"en": "### Self healing for static pods", "zh": "### 静态 Pod 的自我修复   {#self-healing-for-static-pods}"}
{"en": "If you are running a [static pod](/docs/concepts/workloads/pods/#static-pods)\non a node that is under resource pressure, the kubelet may evict that static\nPod. The kubelet then tries to create a replacement, because static Pods always\nrepresent an intent to run a Pod on that node.", "zh": "如果你在面临资源压力的节点上运行静态 Pod，则 kubelet 可能会驱逐该静态 Pod。\n由于静态 Pod 始终表示在该节点上运行 Pod 的意图，kubelet 会尝试创建替代 Pod。"}
{"en": "The kubelet takes the _priority_ of the static pod into account when creating\na replacement. If the static pod manifest specifies a low priority, and there\nare higher-priority Pods defined within the cluster's control plane, and the\nnode is under resource pressure, the kubelet may not be able to make room for\nthat static pod. The kubelet continues to attempt to run all static pods even\nwhen there is resource pressure on a node.", "zh": "创建替代 Pod 时，kubelet 会考虑静态 Pod 的优先级。如果静态 Pod 清单指定了低优先级，\n并且集群的控制平面内定义了优先级更高的 Pod，并且节点面临资源压力，则 kubelet\n可能无法为该静态 Pod 腾出空间。\n即使节点上存在资源压力，kubelet 也会继续尝试运行所有静态 pod。"}
{"en": "## Eviction signals and thresholds\n\nThe kubelet uses various parameters to make eviction decisions, like the following:\n\n- Eviction signals\n- Eviction thresholds\n- Monitoring intervals", "zh": "## 驱逐信号和阈值  {#eviction-signals-and-thresholds}\n\nkubelet 使用各种参数来做出驱逐决定，如下所示：\n\n- 驱逐信号\n- 驱逐条件\n- 监控间隔"}
{"en": "### Eviction signals {#eviction-signals}\n\nEviction signals are the current state of a particular resource at a specific\npoint in time. The kubelet uses eviction signals to make eviction decisions by\ncomparing the signals to eviction thresholds, which are the minimum amount of\nthe resource that should be available on the node.\n\nThe kubelet uses the following eviction signals:", "zh": "### 驱逐信号 {#eviction-signals}\n\n驱逐信号是特定资源在特定时间点的当前状态。\nkubelet 使用驱逐信号，通过将信号与驱逐条件进行比较来做出驱逐决定，\n驱逐条件是节点上应该可用资源的最小量。\n\nkubelet 使用以下驱逐信号：\n\n| 驱逐信号                  | 描述                                                                                  | 仅限于 Linux |\n|--------------------------|---------------------------------------------------------------------------------------|------------|\n| `memory.available`       | `memory.available` := `node.status.capacity[memory]` - `node.stats.memory.workingSet` |            |\n| `nodefs.available`       | `nodefs.available` := `node.stats.fs.available`                                       |            |\n| `nodefs.inodesFree`      | `nodefs.inodesFree` := `node.stats.fs.inodesFree`                                     |      •     |\n| `imagefs.available`      | `imagefs.available` := `node.stats.runtime.imagefs.available`                         |            |\n| `imagefs.inodesFree`     | `imagefs.inodesFree` := `node.stats.runtime.imagefs.inodesFree`                       |      •     |\n| `containerfs.available`  | `containerfs.available` := `node.stats.runtime.containerfs.available`                 |            |\n| `containerfs.inodesFree` | `containerfs.inodesFree` := `node.stats.runtime.containerfs.inodesFree`               |      •     |\n| `pid.available`          | `pid.available` := `node.stats.rlimit.maxpid` - `node.stats.rlimit.curproc`           |      •     |"}
{"en": "In this table, the **Description** column shows how kubelet gets the value of the\nsignal. Each signal supports either a percentage or a literal value. The Kubelet\ncalculates the percentage value relative to the total capacity associated with\nthe signal.", "zh": "在上表中，**描述**列显示了 kubelet 如何获取信号的值。每个信号支持百分比值或者是字面值。\nkubelet 计算相对于与信号有关的总量的百分比值。"}
{"en": "#### Memory signals\n\nOn Linux nodes, the value for `memory.available` is derived from the cgroupfs instead of tools\nlike `free -m`. This is important because `free -m` does not work in a\ncontainer, and if users use the [node allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\nfeature, out of resource decisions\nare made local to the end user Pod part of the cgroup hierarchy as well as the\nroot node. This [script](/examples/admin/resource/memory-available.sh) or\n[cgroupv2 script](/examples/admin/resource/memory-available-cgroupv2.sh)\nreproduces the same set of steps that the kubelet performs to calculate\n`memory.available`. The kubelet excludes inactive_file (the number of bytes of\nfile-backed memory on inactive LRU list) from its calculation as it assumes that\nmemory is reclaimable under pressure.", "zh": "#### 内存信号 {#memory-signals}\n\n在 Linux 节点上，`memory.available` 的值来自 cgroupfs，而不是像 `free -m` 这样的工具。\n这很重要，因为 `free -m` 在容器中不起作用，如果用户使用\n[节点可分配资源](/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\n这一功能特性，资源不足的判定是基于 cgroup 层次结构中的用户 Pod 所处的局部及 cgroup 根节点作出的。\n这个[脚本](/zh-cn/examples/admin/resource/memory-available.sh)或者\n[cgroupv2 脚本](/zh-cn/examples/admin/resource/memory-available-cgroupv2.sh)\n重现了 kubelet 为计算 `memory.available` 而执行的相同步骤。\nkubelet 在其计算中排除了 inactive_file（非活动 LRU 列表上基于文件来虚拟的内存的字节数），\n因为它假定在压力下内存是可回收的。"}
{"en": "On Windows nodes, the value for `memory.available` is derived from the node's global\nmemory commit levels (queried through the [`GetPerformanceInfo()`](https://learn.microsoft.com/windows/win32/api/psapi/nf-psapi-getperformanceinfo)\nsystem call) by subtracting the node's global [`CommitTotal`](https://learn.microsoft.com/windows/win32/api/psapi/ns-psapi-performance_information) from the node's [`CommitLimit`](https://learn.microsoft.com/windows/win32/api/psapi/ns-psapi-performance_information). Please note that `CommitLimit` can change if the node's page-file size changes!", "zh": "在 Windows 节点上，`memory.available` 的值来自节点的全局内存提交级别\n（通过 [`GetPerformanceInfo()`](https://learn.microsoft.com/windows/win32/api/psapi/nf-psapi-getperformanceinfo)系统调用查询），\n方法是从节点的 [`CommitLimit`](https://learn.microsoft.com/windows/win32/api/psapi/ns-psapi-performance_information)减去节点的全局\n[`CommitTotal`](https://learn.microsoft.com/windows/win32/api/psapi/ns-psapi-performance_information)。\n请注意，如果节点的页面文件大小发生变化，`CommitLimit` 也会发生变化！"}
{"en": "#### Filesystem signals\n\nThe kubelet recognizes three specific filesystem identifiers that can be used with\neviction signals (`<identifier>.inodesFree` or `<identifier>.available`):\n\n1. `nodefs`: The node's main filesystem, used for local disk volumes,\n    emptyDir volumes not backed by memory, log storage, ephemeral storage,\n    and more. For example, `nodefs` contains `/var/lib/kubelet`.\n\n1. `imagefs`: An optional filesystem that container runtimes can use to store\n   container images (which are the read-only layers) and container writable\n   layers.\n\n1. `containerfs`: An optional filesystem that container runtime can use to\n   store the writeable layers. Similar to the main filesystem (see `nodefs`),\n   it's used to store local disk volumes, emptyDir volumes not backed by memory,\n   log storage, and ephemeral storage, except for the container images. When\n   `containerfs` is used, the `imagefs` filesystem can be split to only store\n   images (read-only layers) and nothing else.", "zh": "#### 文件系统信号 {#filesystem-signals}\n\nkubelet 可识别三个可与驱逐信号一起使用的特定文件系统标识符（`<identifier>.inodesFree` 或 `<identifier>.available`）：\n\n1. `nodefs`：节点的主文件系统，用于本地磁盘卷、\n   非内存介质的 emptyDir 卷、日志存储、临时存储等。\n   例如，`nodefs` 包含 `/var/lib/kubelet`。\n\n1. `imagefs`：可供容器运行时存储容器镜像（只读层）和容器可写层的可选文件系统。\n\n1. `containerfs`：可供容器运行时存储可写层的可选文件系统。\n   与主文件系统（参见 `nodefs`）类似，\n   它用于存储本地磁盘卷、非内存介质的 emptyDir 卷、\n   日志存储和临时存储，但容器镜像除外。\n   当使用 `containerfs` 时，`imagefs` 文件系统可以分割为仅存储镜像（只读层）而不存储其他任何内容。"}
{"en": "As such, kubelet generally allows three options for container filesystems:\n\n- Everything is on the single `nodefs`, also referred to as \"rootfs\" or\n  simply \"root\", and there is no dedicated image filesystem.\n\n- Container storage (see `nodefs`) is on a dedicated disk, and `imagefs`\n  (writable and read-only layers) is separate from the root filesystem.\n  This is often referred to as \"split disk\" (or \"separate disk\") filesystem.\n\n- Container filesystem `containerfs` (same as `nodefs` plus writable\n  layers) is on root and the container images (read-only layers) are\n  stored on separate `imagefs`. This is often referred to as \"split image\"\n  filesystem.", "zh": "因此，kubelet 通常允许三种容器文件系统选项：\n\n- 所有内容都位于单个 `nodefs` 上，也称为 “rootfs” 或简称为 “root”，\n  并且没有专用镜像文件系统。\n\n- 容器存储（参见 `nodefs`）位于专用磁盘上，\n  而 `imagefs`（可写和只读层）与根文件系统分开。\n  这通常称为“分割磁盘”（或“单独磁盘”）文件系统。\n\n- 容器文件系统 `containerfs`（与 `nodefs` 加上可写层相同）位于根文件系统上，\n  容器镜像（只读层）存储在单独的 `imagefs` 上。 这通常称为“分割镜像”文件系统。"}
{"en": "The kubelet will attempt to auto-discover these filesystems with their current\nconfiguration directly from the underlying container runtime and will ignore\nother local node filesystems.\n\nThe kubelet does not support other container filesystems or storage configurations,\nand it does not currently support multiple filesystems for images and containers.", "zh": "kubelet 将尝试直接从底层容器运行时自动发现这些文件系统及其当前配置，并忽略其他本地节点文件系统。\n\nkubelet 不支持其他容器文件系统或存储配置，并且目前不支持为镜像和容器提供多个文件系统。"}
{"en": "### Deprecated kubelet garbage collection features\n\nSome kubelet garbage collection features are deprecated in favor of eviction:\n\n| Existing Flag | Rationale |\n| ------------- | --------- |\n| `--maximum-dead-containers` | deprecated once old logs are stored outside of container's context |\n| `--maximum-dead-containers-per-container` | deprecated once old logs are stored outside of container's context |\n| `--minimum-container-ttl-duration` | deprecated once old logs are stored outside of container's context |", "zh": "### 弃用的 kubelet 垃圾收集功能 {#deprecated-kubelet-garbage-collection-features}\n\n一些 kubelet 垃圾收集功能已被弃用，以鼓励使用驱逐机制。\n\n| 现有标志                                   | 原因                                  |\n| ----------------------------------------- |  ----------------------------------- |\n| `--maximum-dead-containers`               | 一旦旧的日志存储在容器的上下文之外就会被弃用 |\n| `--maximum-dead-containers-per-container` | 一旦旧的日志存储在容器的上下文之外就会被弃用 |\n| `--minimum-container-ttl-duration`        | 一旦旧的日志存储在容器的上下文之外就会被弃用 |"}
{"en": "### Eviction thresholds\n\nYou can specify custom eviction thresholds for the kubelet to use when it makes\neviction decisions. You can configure [soft](#soft-eviction-thresholds) and\n[hard](#hard-eviction-thresholds) eviction thresholds.\n\nEviction thresholds have the form `[eviction-signal][operator][quantity]`, where:\n\n- `eviction-signal` is the [eviction signal](#eviction-signals) to use.\n- `operator` is the [relational operator](https://en.wikipedia.org/wiki/Relational_operator#Standard_relational_operators)\n  you want, such as `<` (less than).\n- `quantity` is the eviction threshold amount, such as `1Gi`. The value of `quantity`\n  must match the quantity representation used by Kubernetes. You can use either\n  literal values or percentages (`%`).", "zh": "### 驱逐条件 {#eviction-thresholds}\n\n你可以为 kubelet 指定自定义驱逐条件，以便在作出驱逐决定时使用。\n你可以设置[软性的](#soft-eviction-thresholds)和[硬性的](#hard-eviction-thresholds)驱逐阈值。\n\n驱逐条件的形式为 `[eviction-signal][operator][quantity]`，其中：\n\n- `eviction-signal` 是要使用的[驱逐信号](#eviction-signals)。\n- `operator` 是你想要的[关系运算符](https://en.wikipedia.org/wiki/Relational_operator#Standard_relational_operators)，\n  比如 `<`（小于）。\n- `quantity` 是驱逐条件数量，例如 `1Gi`。\n  `quantity` 的值必须与 Kubernetes 使用的数量表示相匹配。\n  你可以使用文字值或百分比（`%`）。"}
{"en": "For example, if a node has 10GiB of total memory and you want trigger eviction if\nthe available memory falls below 1GiB, you can define the eviction threshold as\neither `memory.available<10%` or `memory.available<1Gi` (you cannot use both).\n\nYou can configure soft and hard eviction thresholds.", "zh": "例如，如果一个节点的总内存为 10GiB 并且你希望在可用内存低于 1GiB 时触发驱逐，\n则可以将驱逐条件定义为 `memory.available<10%` 或\n`memory.available< 1G`（你不能同时使用二者）。\n\n你可以配置软和硬驱逐条件。"}
{"en": "#### Soft eviction thresholds {#soft-eviction-thresholds}\n\nA soft eviction threshold pairs an eviction threshold with a required\nadministrator-specified grace period. The kubelet does not evict pods until the\ngrace period is exceeded. The kubelet returns an error on startup if you do\nnot specify a grace period.", "zh": "#### 软驱逐条件 {#soft-eviction-thresholds}\n\n软驱逐条件将驱逐条件与管理员所必须指定的宽限期配对。\n在超过宽限期之前，kubelet 不会驱逐 Pod。\n如果没有指定的宽限期，kubelet 会在启动时返回错误。"}
{"en": "You can specify both a soft eviction threshold grace period and a maximum\nallowed pod termination grace period for kubelet to use during evictions. If you\nspecify a maximum allowed grace period and the soft eviction threshold is met,\nthe kubelet uses the lesser of the two grace periods. If you do not specify a\nmaximum allowed grace period, the kubelet kills evicted pods immediately without\ngraceful termination.", "zh": "你可以既指定软驱逐条件宽限期，又指定 Pod 终止宽限期的上限，给 kubelet 在驱逐期间使用。\n如果你指定了宽限期的上限并且 Pod 满足软驱逐阈条件，则 kubelet 将使用两个宽限期中的较小者。\n如果你没有指定宽限期上限，kubelet 会立即杀死被驱逐的 Pod，不允许其体面终止。"}
{"en": "You can use the following flags to configure soft eviction thresholds:\n\n- `eviction-soft`: A set of eviction thresholds like `memory.available<1.5Gi`\n  that can trigger pod eviction if held over the specified grace period.\n- `eviction-soft-grace-period`: A set of eviction grace periods like `memory.available=1m30s`\n  that define how long a soft eviction threshold must hold before triggering a Pod eviction.\n- `eviction-max-pod-grace-period`: The maximum allowed grace period (in seconds)\n  to use when terminating pods in response to a soft eviction threshold being met.", "zh": "你可以使用以下标志来配置软驱逐条件：\n\n- `eviction-soft`：一组驱逐条件，如 `memory.available<1.5Gi`，\n  如果驱逐条件持续时长超过指定的宽限期，可以触发 Pod 驱逐。\n- `eviction-soft-grace-period`：一组驱逐宽限期，\n  如 `memory.available=1m30s`，定义软驱逐条件在触发 Pod 驱逐之前必须保持多长时间。\n- `eviction-max-pod-grace-period`：在满足软驱逐条件而终止 Pod 时使用的最大允许宽限期（以秒为单位）。"}
{"en": "#### Hard eviction thresholds {#hard-eviction-thresholds}\n\nA hard eviction threshold has no grace period. When a hard eviction threshold is\nmet, the kubelet kills pods immediately without graceful termination to reclaim\nthe starved resource.\n\nYou can use the `eviction-hard` flag to configure a set of hard eviction\nthresholds like `memory.available<1Gi`.", "zh": "#### 硬驱逐条件 {#hard-eviction-thresholds}\n\n硬驱逐条件没有宽限期。当达到硬驱逐条件时，\nkubelet 会立即杀死 pod，而不会正常终止以回收紧缺的资源。\n\n你可以使用 `eviction-hard` 标志来配置一组硬驱逐条件，\n例如 `memory.available<1Gi`。"}
{"en": "The kubelet has the following default hard eviction thresholds:\n\n- `memory.available<100Mi` (Linux nodes)\n- `memory.available<500Mi` (Windows nodes)\n- `nodefs.available<10%`\n- `imagefs.available<15%`\n- `nodefs.inodesFree<5%` (Linux nodes)\n- `imagefs.inodesFree<5%` (Linux nodes)", "zh": "kubelet 具有以下默认硬驱逐条件：\n\n- `memory.available<100Mi`（Linux 节点）\n- `nodefs.available<10%`（Windows 节点）\n- `imagefs.available<15%`\n- `nodefs.inodesFree<5%`（Linux 节点）\n- `imagefs.inodesFree<5%` (Linux 节点)"}
{"en": "These default values of hard eviction thresholds will only be set if none\nof the parameters is changed. If you change the value of any parameter,\nthen the values of other parameters will not be inherited as the default\nvalues and will be set to zero. In order to provide custom values, you\nshould provide all the thresholds respectively.", "zh": "只有在没有更改任何参数的情况下，硬驱逐阈值才会被设置成这些默认值。\n如果你更改了任何参数的值，则其他参数的取值不会继承其默认值设置，而将被设置为零。\n为了提供自定义值，你应该分别设置所有阈值。"}
{"en": "The `containerfs.available` and `containerfs.inodesFree` (Linux nodes) default\neviction thresholds will be set as follows:\n\n- If a single filesystem is used for everything, then `containerfs` thresholds\n  are set the same as `nodefs`.\n\n- If separate filesystems are configured for both images and containers,\n  then `containerfs` thresholds are set the same as `imagefs`.\n\nSetting custom overrides for thresholds related to `containersfs` is currently\nnot supported, and a warning will be issued if an attempt to do so is made; any\nprovided custom values will, as such, be ignored.", "zh": "`containerfs.available` 和 `containerfs.inodesFree`（Linux 节点）默认驱逐阈值将被设置如下：\n\n- 如果所有数据都使用同一文件系统，则 `containerfs` 阈值将设置为与 `nodefs` 相同。\n\n- 如果为镜像和容器配置了单独的文件系统，则 `containerfs` 阈值将设置为与 `imagefs` 相同。\n\n目前不支持为与 `containersfs` 相关的阈值设置自定义覆盖，如果尝试这样做，将发出警告；\n因此，所提供的所有自定义值都将被忽略。"}
{"en": "## Eviction monitoring interval\n\nThe kubelet evaluates eviction thresholds based on its configured `housekeeping-interval`,\nwhich defaults to `10s`.", "zh": "## 驱逐监测间隔   {#eviction-monitoring-interval}\n\nkubelet 根据其配置的 `housekeeping-interval`（默认为 `10s`）评估驱逐条件。"}
{"en": "## Node conditions {#node-conditions}\n\nThe kubelet reports [node conditions](/docs/concepts/architecture/nodes/#condition)\nto reflect that the node is under pressure because hard or soft eviction\nthreshold is met, independent of configured grace periods.", "zh": "## 节点状况 {#node-conditions}\n\nkubelet 报告[节点状况](/zh-cn/docs/concepts/architecture/nodes/#condition)以反映节点处于压力之下，\n原因是满足硬性的或软性的驱逐条件，与配置的宽限期无关。"}
{"en": "The kubelet maps eviction signals to node conditions as follows:\n\n| Node Condition    | Eviction Signal                                                                       | Description                                                                                |\n|-------------------|---------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|\n| `MemoryPressure`  | `memory.available`                                                                    | Available memory on the node has satisfied an eviction threshold                           |\n| `DiskPressure`    | `nodefs.available`, `nodefs.inodesFree`, `imagefs.available`, `imagefs.inodesFree`, `containerfs.available`, or `containerfs.inodesFree` | Available disk space and inodes on either the node's root filesystem, image filesystem, or container filesystem has satisfied an eviction threshold              |\n| `PIDPressure`     | `pid.available`                                                                       | Available processes identifiers on the (Linux) node has fallen below an eviction threshold |\n\nThe control plane also [maps](/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-nodes-by-condition)\nthese node conditions to taints.\n\nThe kubelet updates the node conditions based on the configured\n`--node-status-update-frequency`, which defaults to `10s`.", "zh": "kubelet 根据下表将驱逐信号映射为节点状况：\n\n| 节点条件 | 驱逐信号 | 描述 |\n|---------|--------|------|\n| `MemoryPressure` | `memory.available` | 节点上的可用内存已满足驱逐条件 |\n| `DiskPressure`   | `nodefs.available`, `nodefs.inodesFree`, `imagefs.available`, `imagefs.inodesFree`, `containerfs.available`, 或 `containerfs.inodesFree` | 节点的根文件系统、镜像文件系统或容器文件系统上的可用磁盘空间和 inode 已满足驱逐阈值 |\n| `PIDPressure`    | `pid.available` | (Linux) 节点上的可用进程标识符已低于驱逐条件 |\n\n控制平面还将这些节点状况[映射](/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-nodes-by-condition)为其污点。\n\nkubelet 根据配置的 `--node-status-update-frequency` 更新节点条件，默认为 `10s`。"}
{"en": "### Node condition oscillation\n\nIn some cases, nodes oscillate above and below soft eviction thresholds without\nholding for the defined grace periods. This causes the reported node condition\nto constantly switch between `true` and `false`, leading to bad eviction decisions.\n\nTo protect against oscillation, you can use the `eviction-pressure-transition-period`\nflag, which controls how long the kubelet must wait before transitioning a node\ncondition to a different state. The transition period has a default value of `5m`.", "zh": "### 节点状况波动   {#node-condition-oscillation}\n\n在某些情况下，节点在软驱逐条件上下振荡，而没有保持定义的宽限期。\n这会导致报告的节点条件在 `true` 和 `false` 之间不断切换，从而导致错误的驱逐决策。\n\n为了防止振荡，你可以使用 `eviction-pressure-transition-period` 标志，\n该标志控制 kubelet 在将节点条件转换为不同状态之前必须等待的时间。\n过渡期的默认值为 `5m`。"}
{"en": "### Reclaiming node level resources {#reclaim-node-resources}\n\nThe kubelet tries to reclaim node-level resources before it evicts end-user pods.\n\nWhen a `DiskPressure` node condition is reported, the kubelet reclaims node-level\nresources based on the filesystems on the node.", "zh": "### 回收节点级资源 {#reclaim-node-resources}\n\nkubelet 在驱逐最终用户 Pod 之前会先尝试回收节点级资源。\n\n当报告 `DiskPressure` 节点状况时，kubelet 会根据节点上的文件系统回收节点级资源。"}
{"en": "#### Without `imagefs` or `containerfs`\n\nIf the node only has a `nodefs` filesystem that meets eviction thresholds,\nthe kubelet frees up disk space in the following order:\n\n1. Garbage collect dead pods and containers.\n1. Delete unused images.", "zh": "#### 没有 `imagefs` 或 `containerfs` {#without-imagefs-or-containerfs}\n\n如果节点只有一个 `nodefs` 文件系统且该文件系统达到驱逐阈值，\nkubelet 将按以下顺序释放磁盘空间：\n\n1. 对已死亡的 Pod 和容器执行垃圾收集操作。\n\n1. 删除未使用的镜像。"}
{"en": "#### With `imagefs`\n\nIf the node has a dedicated `imagefs` filesystem for container runtimes to use,\nthe kubelet does the following:\n\n- If the `nodefs` filesystem meets the eviction thresholds, the kubelet garbage\n  collects dead pods and containers.\n- If the `imagefs` filesystem meets the eviction thresholds, the kubelet\n  deletes all unused images.", "zh": "#### 有 `imagefs`\n\n如果节点有一个专用的 `imagefs` 文件系统供容器运行时使用，kubelet 会执行以下操作：\n\n- 如果 `nodefs` 文件系统满足驱逐条件，kubelet 垃圾收集死亡 Pod 和容器。\n- 如果 `imagefs` 文件系统满足驱逐条件，kubelet 将删除所有未使用的镜像。"}
{"en": "#### With `imagefs` and `containerfs`\n\nIf the node has a dedicated `containerfs` alongside the `imagefs` filesystem\nconfigured for the container runtimes to use, then kubelet will attempt to\nreclaim resources as follows:\n\n- If the `containerfs` filesystem meets the eviction thresholds, the kubelet\n  garbage collects dead pods and containers.\n\n- If the `imagefs` filesystem meets the eviction thresholds, the kubelet\n  deletes all unused images.", "zh": "#### 使用 `imagefs` 和 `containerfs` {#with-imagefs-and-containerfs}\n\n如果节点除了 `imagefs` 文件系统之外还配置了专用的 `containerfs` 以供容器运行时使用，\n则 kubelet 将尝试按如下方式回收资源：\n\n- 如果 `containerfs` 文件系统满足驱逐阈值，则 kubelet 将垃圾收集死机的 pod 和容器。\n\n- 如果 `imagefs` 文件系统满足驱逐阈值，则 kubelet 将删除所有未使用的镜像。"}
{"en": "### Pod selection for kubelet eviction\n\nIf the kubelet's attempts to reclaim node-level resources don't bring the eviction\nsignal below the threshold, the kubelet begins to evict end-user pods.\n\nThe kubelet uses the following parameters to determine the pod eviction order:\n\n1. Whether the pod's resource usage exceeds requests\n1. [Pod Priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\n1. The pod's resource usage relative to requests", "zh": "### kubelet 驱逐时 Pod 的选择\n\n如果 kubelet 回收节点级资源的尝试没有使驱逐信号低于条件，\n则 kubelet 开始驱逐最终用户 Pod。\n\nkubelet 使用以下参数来确定 Pod 驱逐顺序：\n\n1. Pod 的资源使用是否超过其请求\n1. [Pod 优先级](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/)\n1. Pod 相对于请求的资源使用情况"}
{"en": "As a result, kubelet ranks and evicts pods in the following order:\n\n1. `BestEffort` or `Burstable` pods where the usage exceeds requests. These pods\n   are evicted based on their Priority and then by how much their usage level\n   exceeds the request.\n1. `Guaranteed` pods and `Burstable` pods where the usage is less than requests\n   are evicted last, based on their Priority.", "zh": "因此，kubelet 按以下顺序排列和驱逐 Pod：\n\n1. 首先考虑资源使用量超过其请求的 `BestEffort` 或 `Burstable` Pod。\n   这些 Pod 会根据它们的优先级以及它们的资源使用级别超过其请求的程度被逐出。\n1. 资源使用量少于请求量的 `Guaranteed` Pod 和 `Burstable` Pod 根据其优先级被最后驱逐。\n\n{{<note>}}"}
{"en": "The kubelet does not use the pod's [QoS class](/docs/concepts/workloads/pods/pod-qos/) to determine the eviction order.\nYou can use the QoS class to estimate the most likely pod eviction order when\nreclaiming resources like memory. QoS classification does not apply to EphemeralStorage requests,\nso the above scenario will not apply if the node is, for example, under `DiskPressure`.", "zh": "kubelet 不使用 Pod 的 [QoS 类](/zh-cn/docs/concepts/workloads/pods/pod-qos/)来确定驱逐顺序。\n在回收内存等资源时，你可以使用 QoS 类来估计最可能的 Pod 驱逐顺序。\nQoS 分类不适用于临时存储（EphemeralStorage）请求，\n因此如果节点在 `DiskPressure` 下，则上述场景将不适用。\n{{</note>}}"}
{"en": "`Guaranteed` pods are guaranteed only when requests and limits are specified for\nall the containers and they are equal. These pods will never be evicted because\nof another pod's resource consumption. If a system daemon (such as `kubelet`\nand `journald`) is consuming more resources than were reserved via\n`system-reserved` or `kube-reserved` allocations, and the node only has\n`Guaranteed` or `Burstable` pods using less resources than requests left on it,\nthen the kubelet must choose to evict one of these pods to preserve node stability\nand to limit the impact of resource starvation on other pods. In this case, it\nwill choose to evict pods of lowest Priority first.", "zh": "仅当 `Guaranteed` Pod 中所有容器都被指定了请求和限制并且二者相等时，才保证 Pod 不被驱逐。\n这些 Pod 永远不会因为另一个 Pod 的资源消耗而被驱逐。\n如果系统守护进程（例如 `kubelet` 和 `journald`）\n消耗的资源比通过 `system-reserved` 或 `kube-reserved` 分配保留的资源多，\n并且该节点只有 `Guaranteed` 或 `Burstable` Pod 使用的资源少于其上剩余的请求，\n那么 kubelet 必须选择驱逐这些 Pod 中的一个以保持节点稳定性并减少资源匮乏对其他 Pod 的影响。\n在这种情况下，它会选择首先驱逐最低优先级的 Pod。"}
{"en": "If you are running a [static pod](/docs/concepts/workloads/pods/#static-pods)\nand want to avoid having it evicted under resource pressure, set the\n`priority` field for that Pod directly. Static pods do not support the\n`priorityClassName` field.", "zh": "如果你正在运行[静态 Pod](/zh-cn/docs/concepts/workloads/pods/#static-pods)\n并且希望避免其在资源压力下被驱逐，请直接为该 Pod 设置 `priority` 字段。\n静态 Pod 不支持 `priorityClassName` 字段。"}
{"en": "When the kubelet evicts pods in response to inode or process ID starvation, it uses\nthe Pods' relative priority to determine the eviction order, because inodes and PIDs have no\nrequests.\n\nThe kubelet sorts pods differently based on whether the node has a dedicated\n`imagefs` or `containerfs` filesystem:", "zh": "当 kubelet 因 inode 或 进程 ID 不足而驱逐 Pod 时，\n它使用 Pod 的相对优先级来确定驱逐顺序，因为 inode 和 PID 没有对应的请求字段。\n\nkubelet 根据节点是否具有专用的 `imagefs` 文件系统 或者 `containerfs` 文件系统对 Pod 进行不同的排序："}
{"en": "#### Without `imagefs` or `containerfs` (`nodefs` and `imagefs` use the same filesystem) {#without-imagefs}\n\n- If `nodefs` triggers evictions, the kubelet sorts pods based on their\n  total disk usage (`local volumes + logs and a writable layer of all containers`).\n\n#### With `imagefs` (`nodefs` and `imagefs` filesystems are separate) {#with-imagefs}\n\n- If `nodefs` triggers evictions, the kubelet sorts pods based on `nodefs`\n  usage (`local volumes + logs of all containers`).\n\n- If `imagefs` triggers evictions, the kubelet sorts pods based on the\n  writable layer usage of all containers.", "zh": "#### 没有 `imagefs` 或 `containerfs`（`nodefs` 和 `imagefs` 使用相同的文件系统）{#without-imagefs}\n\n- 如果 `nodefs` 触发驱逐，kubelet 将根据 Pod 的总磁盘使用量（`本地卷 + 日志和所有容器的可写层`）对 Pod 进行排序。\n\n#### 有 `imagefs`（`nodefs` 和 `imagefs` 文件系统是独立的）{#with-imagefs}\n\n- 如果 `nodefs` 触发驱逐，kubelet 将根据 `nodefs` 使用量（`本地卷 + 所有容器的日志`）对 Pod 进行排序。\n\n- 如果 `imagefs` 触发驱逐，kubelet 将根据所有容器的可写层用量对 Pod 进行排序。"}
{"en": "#### With `imagesfs` and `containerfs` (`imagefs` and `containerfs` have been split) {#with-containersfs}\n\n- If `containerfs` triggers evictions, the kubelet sorts pods based on\n  `containerfs` usage (`local volumes + logs and a writable layer of all containers`).\n\n- If `imagefs` triggers evictions, the kubelet sorts pods based on the\n  `storage of images` rank, which represents the disk usage of a given image.", "zh": "#### 有 `imagesfs` 和 `containerfs`（`imagefs` 和 `containerfs` 已拆分）{#with-containersfs}\n\n- 如果 `containerfs` 触发驱逐，kubelet 将根据\n  `containerfs` 使用情况（`本地卷 + 日志和所有容器的可写层`）对 Pod 进行排序。\n\n- 如果 `imagefs` 触发驱逐，kubelet 将根据\n  `镜像存储` 用量对 Pod 进行排序，该用量表示给定镜像的磁盘使用情况。"}
{"en": "### Minimum eviction reclaim\n\n{{<note>}}\nAs of Kubernetes v{{< skew currentVersion >}}, you cannot set a custom value\nfor the `containerfs.available` metric. The configuration for this specific\nmetric will be set automatically to reflect values set for either the `nodefs`\nor `imagefs`, depending on the configuration.\n{{</note>}}\n\nIn some cases, pod eviction only reclaims a small amount of the starved resource.\nThis can lead to the kubelet repeatedly hitting the configured eviction thresholds\nand triggering multiple evictions.", "zh": "### 最小驱逐回收 {#minimum-eviction-reclaim}\n\n{{<note>}}\n在 Kubernetes v{{< skew currentVersion >}} 中，你无法为 `containerfs.available` 指标设置自定义值。\n此特定指标的配置将自动设置为反映为 `nodefs` 或 `imagefs` 设置的值，具体取决于配置。\n{{</note>}}\n\n在某些情况下，驱逐 Pod 只会回收少量的紧俏资源。\n这可能导致 kubelet 反复达到配置的驱逐条件并触发多次驱逐。"}
{"en": "You can use the `--eviction-minimum-reclaim` flag or a [kubelet config file](/docs/tasks/administer-cluster/kubelet-config-file/)\nto configure a minimum reclaim amount for each resource. When the kubelet notices\nthat a resource is starved, it continues to reclaim that resource until it\nreclaims the quantity you specify.\n\nFor example, the following configuration sets minimum reclaim amounts:", "zh": "你可以使用 `--eviction-minimum-reclaim` 标志或\n[kubelet 配置文件](/zh-cn/docs/tasks/administer-cluster/kubelet-config-file/)\n为每个资源配置最小回收量。\n当 kubelet 注意到某个资源耗尽时，它会继续回收该资源，直到回收到你所指定的数量为止。\n\n例如，以下配置设置最小回收量：\n\n```yaml\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nevictionHard:\n  memory.available: \"500Mi\"\n  nodefs.available: \"1Gi\"\n  imagefs.available: \"100Gi\"\nevictionMinimumReclaim:\n  memory.available: \"0Mi\"\n  nodefs.available: \"500Mi\"\n  imagefs.available: \"2Gi\"\n```"}
{"en": "In this example, if the `nodefs.available` signal meets the eviction threshold,\nthe kubelet reclaims the resource until the signal reaches the threshold of 1GiB,\nand then continues to reclaim the minimum amount of 500MiB, until the available\nnodefs storage value reaches 1.5GiB.\n\nSimilarly, the kubelet tries to reclaim the `imagefs` resource until the `imagefs.available`\nvalue reaches `102Gi`, representing 102 GiB of available container image storage. If the amount\nof storage that the kubelet could reclaim is less than 2GiB, the kubelet doesn't reclaim anything.\n\nThe default `eviction-minimum-reclaim` is `0` for all resources.", "zh": "在这个例子中，如果 `nodefs.available` 信号满足驱逐条件，\nkubelet 会回收资源，直到信号达到 1GiB 的条件，\n然后继续回收至少 500MiB 直到信号达到 1.5GiB。\n\n类似地，kubelet 尝试回收 `imagefs` 资源，直到 `imagefs.available` 值达到 `102Gi`，\n即 102 GiB 的可用容器镜像存储。如果 kubelet 可以回收的存储量小于 2GiB，\n则 kubelet 不会回收任何内容。\n\n对于所有资源，默认的 `eviction-minimum-reclaim` 为 `0`。"}
{"en": "## Node out of memory behavior\n\nIf the node experiences an _out of memory_ (OOM) event prior to the kubelet\nbeing able to reclaim memory, the node depends on the [oom_killer](https://lwn.net/Articles/391222/)\nto respond.\n\nThe kubelet sets an `oom_score_adj` value for each container based on the QoS for the pod.", "zh": "## 节点内存不足行为   {#node-out-of-memory-behavior}\n\n如果节点在 kubelet 能够回收内存之前遇到**内存不足**（OOM）事件，\n则节点依赖 [oom_killer](https://lwn.net/Articles/391222/) 来响应。\n\nkubelet 根据 Pod 的服务质量（QoS）为每个容器设置一个 `oom_score_adj` 值。\n\n| 服务质量            | `oom_score_adj`                                                                        |\n|--------------------|---------------------------------------------------------------------------------------|\n| `Guaranteed`       | -997                                                                                  |\n| `BestEffort`       | 1000                                                                                  |\n| `Burstable`        | **min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)** |\n\n{{<note>}}"}
{"en": "The kubelet also sets an `oom_score_adj` value of `-997` for any containers in Pods that have\n`system-node-critical` {{<glossary_tooltip text=\"Priority\" term_id=\"pod-priority\">}}.", "zh": "kubelet 还将具有 `system-node-critical`\n{{<glossary_tooltip text=\"优先级\" term_id=\"pod-priority\">}}\n的任何 Pod 中的容器 `oom_score_adj` 值设为 `-997`。\n{{</note>}}"}
{"en": "If the kubelet can't reclaim memory before a node experiences OOM, the\n`oom_killer` calculates an `oom_score` based on the percentage of memory it's\nusing on the node, and then adds the `oom_score_adj` to get an effective `oom_score`\nfor each container. It then kills the container with the highest score.\n\nThis means that containers in low QoS pods that consume a large amount of memory\nrelative to their scheduling requests are killed first.\n\nUnlike pod eviction, if a container is OOM killed, the kubelet can restart it\nbased on its `restartPolicy`.", "zh": "如果 kubelet 在节点遇到 OOM 之前无法回收内存，\n则 `oom_killer` 根据它在节点上使用的内存百分比计算 `oom_score`，\n然后加上 `oom_score_adj` 得到每个容器有效的 `oom_score`。\n然后它会杀死得分最高的容器。\n\n这意味着低 QoS Pod 中相对于其调度请求消耗内存较多的容器，将首先被杀死。\n\n与 Pod 驱逐不同，如果容器被 OOM 杀死，\n`kubelet` 可以根据其 `restartPolicy` 重新启动它。"}
{"en": "## Good practices {#node-pressure-eviction-good-practices}\n\nThe following sections describe good practices for eviction configuration.", "zh": "### 良好实践 {#node-pressure-eviction-good-practices}\n\n以下各小节阐述驱逐配置的好的做法。"}
{"en": "### Schedulable resources and eviction policies\n\nWhen you configure the kubelet with an eviction policy, you should make sure that\nthe scheduler will not schedule pods if they will trigger eviction because they\nimmediately induce memory pressure.", "zh": "#### 可调度的资源和驱逐策略\n\n当你为 kubelet 配置驱逐策略时，\n你应该确保调度程序不会在 Pod 触发驱逐时对其进行调度，因为这类 Pod 会立即引起内存压力。"}
{"en": "Consider the following scenario:\n\n- Node memory capacity: 10GiB\n- Operator wants to reserve 10% of memory capacity for system daemons (kernel, `kubelet`, etc.)\n- Operator wants to evict Pods at 95% memory utilization to reduce incidence of system OOM.", "zh": "考虑以下场景：\n\n* 节点内存容量：10GiB\n* 操作员希望为系统守护进程（内核、`kubelet` 等）保留 10% 的内存容量\n* 操作员希望在节点内存利用率达到 95% 以上时驱逐 Pod，以减少系统 OOM 的概率。"}
{"en": "For this to work, the kubelet is launched as follows:", "zh": "为此，kubelet 启动设置如下：\n\n```none\n--eviction-hard=memory.available<500Mi\n--system-reserved=memory=1.5Gi\n```"}
{"en": "In this configuration, the `--system-reserved` flag reserves 1.5GiB of memory\nfor the system, which is `10% of the total memory + the eviction threshold amount`.\n\nThe node can reach the eviction threshold if a pod is using more than its request,\nor if the system is using more than 1GiB of memory, which makes the `memory.available`\nsignal fall below 500MiB and triggers the threshold.", "zh": "在此配置中，`--system-reserved` 标志为系统预留了 1GiB 的内存，\n即 `总内存的 10% + 驱逐条件量`。\n\n如果 Pod 使用的内存超过其请求值或者系统使用的内存超过 `1Gi`，\n则节点可以达到驱逐条件，这使得 `memory.available` 信号低于 500MiB 并触发条件。"}
{"en": "### DaemonSets and node-pressure eviction {#daemonset}\n\nPod priority is a major factor in making eviction decisions. If you do not want\nthe kubelet to evict pods that belong to a DaemonSet, give those pods a high\nenough priority by specifying a suitable `priorityClassName` in the pod spec.\nYou can also use a lower priority, or the default, to only allow pods from that\nDaemonSet to run when there are enough resources.", "zh": "### DaemonSets 和节点压力驱逐  {#daemonset}\n\nPod 优先级是做出驱逐决定的主要因素。\n如果你不希望 kubelet 驱逐属于 DaemonSet 的 Pod，\n请在 Pod 规约中通过指定合适的 `priorityClassName` 为这些 Pod\n提供足够高的 `priorityClass`。\n你还可以使用较低优先级或默认优先级，以便\n仅在有足够资源时才运行 `DaemonSet` Pod。"}
{"en": "## Known issues\n\nThe following sections describe known issues related to out of resource handling.", "zh": "## 已知问题   {#known-issues}\n\n以下部分描述了与资源不足处理相关的已知问题。"}
{"en": "### kubelet may not observe memory pressure right away\n\nBy default, the kubelet polls cAdvisor to collect memory usage stats at a\nregular interval. If memory usage increases within that window rapidly, the\nkubelet may not observe `MemoryPressure` fast enough, and the OOM killer\nwill still be invoked.", "zh": "#### kubelet 可能不会立即观察到内存压力\n\n默认情况下，kubelet 轮询 cAdvisor 以定期收集内存使用情况统计信息。\n如果该轮询时间窗口内内存使用量迅速增加，kubelet 可能无法足够快地观察到 `MemoryPressure`，\n但是 OOM killer 仍将被调用。"}
{"en": "You can use the `--kernel-memcg-notification` flag to enable the `memcg`\nnotification API on the kubelet to get notified immediately when a threshold\nis crossed.\n\nIf you are not trying to achieve extreme utilization, but a sensible measure of\novercommit, a viable workaround for this issue is to use the `--kube-reserved`\nand `--system-reserved` flags to allocate memory for the system.", "zh": "你可以使用 `--kernel-memcg-notification`\n标志在 kubelet 上启用 `memcg` 通知 API，以便在超过条件时立即收到通知。\n\n如果你不是追求极端利用率，而是要采取合理的过量使用措施，\n则解决此问题的可行方法是使用 `--kube-reserved` 和 `--system-reserved` 标志为系统分配内存。"}
{"en": "### active_file memory is not considered as available memory\n\nOn Linux, the kernel tracks the number of bytes of file-backed memory on active\nleast recently used (LRU) list as the `active_file` statistic. The kubelet treats `active_file` memory\nareas as not reclaimable. For workloads that make intensive use of block-backed\nlocal storage, including ephemeral local storage, kernel-level caches of file\nand block data means that many recently accessed cache pages are likely to be\ncounted as `active_file`. If enough of these kernel block buffers are on the\nactive LRU list, the kubelet is liable to observe this as high resource use and\ntaint the node as experiencing memory pressure - triggering pod eviction.", "zh": "### active_file 内存未被视为可用内存\n\n在 Linux 上，内核跟踪活动最近最少使用（LRU）列表上的基于文件所虚拟的内存字节数作为 `active_file` 统计信息。\nkubelet 将 `active_file` 内存区域视为不可回收。\n对于大量使用块设备形式的本地存储（包括临时本地存储）的工作负载，\n文件和块数据的内核级缓存意味着许多最近访问的缓存页面可能被计为 `active_file`。\n如果这些内核块缓冲区中在活动 LRU 列表上有足够多，\nkubelet 很容易将其视为资源用量过量并为节点设置内存压力污点，从而触发 Pod 驱逐。"}
{"en": "For more details, see [https://github.com/kubernetes/kubernetes/issues/43916](https://github.com/kubernetes/kubernetes/issues/43916)\n\nYou can work around that behavior by setting the memory limit and memory request\nthe same for containers likely to perform intensive I/O activity. You will need\nto estimate or measure an optimal memory limit value for that container.", "zh": "更多细节请参见 [https://github.com/kubernetes/kubernetes/issues/43916](https://github.com/kubernetes/kubernetes/issues/43916)。\n\n你可以通过为可能执行 I/O 密集型活动的容器设置相同的内存限制和内存请求来应对该行为。\n你将需要估计或测量该容器的最佳内存限制值。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Learn about [API-initiated Eviction](/docs/concepts/scheduling-eviction/api-eviction/)\n- Learn about [Pod Priority and Preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\n- Learn about [PodDisruptionBudgets](/docs/tasks/run-application/configure-pdb/)\n- Learn about [Quality of Service](/docs/tasks/configure-pod-container/quality-service-pod/) (QoS)\n- Check out the [Eviction API](/docs/reference/generated/kubernetes-api/{{<param \"version\">}}/#create-eviction-pod-v1-core)", "zh": "- 了解 [API 发起的驱逐](/zh-cn/docs/concepts/scheduling-eviction/api-eviction/)\n- 了解 [Pod 优先级和抢占](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/)\n- 了解 [PodDisruptionBudgets](/zh-cn/docs/tasks/run-application/configure-pdb/)\n- 了解[服务质量](/zh-cn/docs/tasks/configure-pod-container/quality-service-pod/)（QoS）\n- 查看[驱逐 API](/docs/reference/generated/kubernetes-api/{{<param \"version\">}}/#create-eviction-pod-v1-core)"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.14\" state=\"stable\" >}}"}
{"en": "[Pods](/docs/concepts/workloads/pods/) can have _priority_. Priority indicates the\nimportance of a Pod relative to other Pods. If a Pod cannot be scheduled, the\nscheduler tries to preempt (evict) lower priority Pods to make scheduling of the\npending Pod possible.", "zh": "[Pod](/zh-cn/docs/concepts/workloads/pods/) 可以有**优先级**。\n优先级表示一个 Pod 相对于其他 Pod 的重要性。\n如果一个 Pod 无法被调度，调度程序会尝试抢占（驱逐）较低优先级的 Pod，\n以使悬决 Pod 可以被调度。"}
{"en": "body", "zh": "{{< warning >}}"}
{"en": "In a cluster where not all users are trusted, a malicious user could create Pods\nat the highest possible priorities, causing other Pods to be evicted/not get\nscheduled.\nAn administrator can use ResourceQuota to prevent users from creating pods at\nhigh priorities.\n\nSee [limit Priority Class consumption by default](/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default)\nfor details.", "zh": "在一个并非所有用户都是可信的集群中，恶意用户可能以最高优先级创建 Pod，\n导致其他 Pod 被驱逐或者无法被调度。\n管理员可以使用 ResourceQuota 来阻止用户创建高优先级的 Pod。\n参见[默认限制优先级消费](/zh-cn/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default)。\n\n{{< /warning >}}"}
{"en": "## How to use priority and preemption\n\nTo use priority and preemption:\n\n1.  Add one or more [PriorityClasses](#priorityclass).\n\n1.  Create Pods with[`priorityClassName`](#pod-priority) set to one of the added\n    PriorityClasses. Of course you do not need to create the Pods directly;\n    normally you would add `priorityClassName` to the Pod template of a\n    collection object like a Deployment.\n\nKeep reading for more information about these steps.", "zh": "## 如何使用优先级和抢占 {#how-to-use-priority-and-preemption}\n\n要使用优先级和抢占：\n\n1.  新增一个或多个 [PriorityClass](#priorityclass)。\n\n1.  创建 Pod，并将其 [`priorityClassName`](#pod-priority) 设置为新增的 PriorityClass。\n    当然你不需要直接创建 Pod；通常，你将会添加 `priorityClassName` 到集合对象（如 Deployment）\n    的 Pod 模板中。\n\n继续阅读以获取有关这些步骤的更多信息。\n\n{{< note >}}"}
{"en": "Kubernetes already ships with two PriorityClasses:\n`system-cluster-critical` and `system-node-critical`.\nThese are common classes and are used to [ensure that critical components are always scheduled first](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).", "zh": "Kubernetes 已经提供了 2 个 PriorityClass：\n`system-cluster-critical` 和 `system-node-critical`。\n这些是常见的类，用于[确保始终优先调度关键组件](/zh-cn/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/)。\n{{< /note >}}"}
{"en": "## PriorityClass\n\nA PriorityClass is a non-namespaced object that defines a mapping from a\npriority class name to the integer value of the priority. The name is specified\nin the `name` field of the PriorityClass object's metadata. The value is\nspecified in the required `value` field. The higher the value, the higher the\npriority.\nThe name of a PriorityClass object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names),\nand it cannot be prefixed with `system-`.", "zh": "## PriorityClass {#priorityclass}\n\nPriorityClass 是一个无命名空间对象，它定义了从优先级类名称到优先级整数值的映射。\n名称在 PriorityClass 对象元数据的 `name` 字段中指定。\n值在必填的 `value` 字段中指定。值越大，优先级越高。\nPriorityClass 对象的名称必须是有效的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)，\n并且它不能以 `system-` 为前缀。"}
{"en": "A PriorityClass object can have any 32-bit integer value smaller than or equal\nto 1 billion. This means that the range of values for a PriorityClass object is\nfrom -2147483648 to 1000000000 inclusive. Larger numbers are reserved for\nbuilt-in PriorityClasses that represent critical system Pods. A cluster\nadmin should create one PriorityClass object for each such mapping that they want.\n\nPriorityClass also has two optional fields: `globalDefault` and `description`.\nThe `globalDefault` field indicates that the value of this PriorityClass should\nbe used for Pods without a `priorityClassName`. Only one PriorityClass with\n`globalDefault` set to true can exist in the system. If there is no\nPriorityClass with `globalDefault` set, the priority of Pods with no\n`priorityClassName` is zero.\n\nThe `description` field is an arbitrary string. It is meant to tell users of the\ncluster when they should use this PriorityClass.", "zh": "PriorityClass 对象可以设置任何小于或等于 10 亿的 32 位整数值。\n这意味着 PriorityClass 对象的值范围是从 -2,147,483,648 到 1,000,000,000（含）。\n保留更大的数字，用于表示关键系统 Pod 的内置 PriorityClass。\n集群管理员应该为这类映射分别创建独立的 PriorityClass 对象。\n\nPriorityClass 还有两个可选字段：`globalDefault` 和 `description`。\n`globalDefault` 字段表示这个 PriorityClass 的值应该用于没有 `priorityClassName` 的 Pod。\n系统中只能存在一个 `globalDefault` 设置为 true 的 PriorityClass。\n如果不存在设置了 `globalDefault` 的 PriorityClass，\n则没有 `priorityClassName` 的 Pod 的优先级为零。\n\n`description` 字段是一个任意字符串。\n它用来告诉集群用户何时应该使用此 PriorityClass。"}
{"en": "### Notes about PodPriority and existing clusters\n\n-   If you upgrade an existing cluster without this feature, the priority\n    of your existing Pods is effectively zero.\n\n-   Addition of a PriorityClass with `globalDefault` set to `true` does not\n    change the priorities of existing Pods. The value of such a PriorityClass is\n    used only for Pods created after the PriorityClass is added.\n\n-   If you delete a PriorityClass, existing Pods that use the name of the\n    deleted PriorityClass remain unchanged, but you cannot create more Pods that\n    use the name of the deleted PriorityClass.", "zh": "### 关于 PodPriority 和现有集群的注意事项 {#notes-about-podpriority-and-existing-clusters}\n\n- 如果你升级一个已经存在的但尚未使用此特性的集群，该集群中已经存在的 Pod 的优先级等效于零。\n\n- 添加一个将 `globalDefault` 设置为 `true` 的 PriorityClass 不会改变现有 Pod 的优先级。\n  此类 PriorityClass 的值仅用于添加 PriorityClass 后创建的 Pod。\n\n- 如果你删除了某个 PriorityClass 对象，则使用被删除的 PriorityClass 名称的现有 Pod 保持不变，\n  但是你不能再创建使用已删除的 PriorityClass 名称的 Pod。"}
{"en": "### Example PriorityClass", "zh": "### PriorityClass 示例 {#example-priorityclass}\n\n```yaml\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000000\nglobalDefault: false\ndescription: \"此优先级类应仅用于 XYZ 服务 Pod。\"\n```"}
{"en": "## Non-preempting PriorityClass {#non-preempting-priority-class}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}\n\nPods with `preemptionPolicy: Never` will be placed in the scheduling queue\nahead of lower-priority pods,\nbut they cannot preempt other pods.\nA non-preempting pod waiting to be scheduled will stay in the scheduling queue,\nuntil sufficient resources are free,\nand it can be scheduled.\nNon-preempting pods,\nlike other pods,\nare subject to scheduler back-off.\nThis means that if the scheduler tries these pods and they cannot be scheduled,\nthey will be retried with lower frequency,\nallowing other pods with lower priority to be scheduled before them.\n\nNon-preempting pods may still be preempted by other,\nhigh-priority pods.", "zh": "## 非抢占式 PriorityClass {#non-preempting-priority-class}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}\n\n配置了 `preemptionPolicy: Never` 的 Pod 将被放置在调度队列中较低优先级 Pod 之前，\n但它们不能抢占其他 Pod。等待调度的非抢占式 Pod 将留在调度队列中，直到有足够的可用资源，\n它才可以被调度。非抢占式 Pod，像其他 Pod 一样，受调度程序回退的影响。\n这意味着如果调度程序尝试这些 Pod 并且无法调度它们，它们将以更低的频率被重试，\n从而允许其他优先级较低的 Pod 排在它们之前。\n\n非抢占式 Pod 仍可能被其他高优先级 Pod 抢占。"}
{"en": "`preemptionPolicy` defaults to `PreemptLowerPriority`,\nwhich will allow pods of that PriorityClass to preempt lower-priority pods\n(as is existing default behavior).\nIf `preemptionPolicy` is set to `Never`,\npods in that PriorityClass will be non-preempting.\n\nAn example use case is for data science workloads.\nA user may submit a job that they want to be prioritized above other workloads,\nbut do not wish to discard existing work by preempting running pods.\nThe high priority job with `preemptionPolicy: Never` will be scheduled\nahead of other queued pods,\nas soon as sufficient cluster resources \"naturally\" become free.", "zh": "`preemptionPolicy` 默认为 `PreemptLowerPriority`，\n这将允许该 PriorityClass 的 Pod 抢占较低优先级的 Pod（现有默认行为也是如此）。\n如果 `preemptionPolicy` 设置为 `Never`，则该 PriorityClass 中的 Pod 将是非抢占式的。\n\n数据科学工作负载是一个示例用例。用户可以提交他们希望优先于其他工作负载的作业，\n但不希望因为抢占运行中的 Pod 而导致现有工作被丢弃。\n设置为 `preemptionPolicy: Never` 的高优先级作业将在其他排队的 Pod 之前被调度，\n只要足够的集群资源“自然地”变得可用。"}
{"en": "### Example Non-preempting PriorityClass", "zh": "### 非抢占式 PriorityClass 示例   {#example-non-preempting-priorityclass}\n\n```yaml\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority-nonpreempting\nvalue: 1000000\npreemptionPolicy: Never\nglobalDefault: false\ndescription: \"This priority class will not cause other pods to be preempted.\"\n```"}
{"en": "## Pod priority\n\nAfter you have one or more PriorityClasses, you can create Pods that specify one\nof those PriorityClass names in their specifications. The priority admission\ncontroller uses the `priorityClassName` field and populates the integer value of\nthe priority. If the priority class is not found, the Pod is rejected.\n\nThe following YAML is an example of a Pod configuration that uses the\nPriorityClass created in the preceding example. The priority admission\ncontroller checks the specification and resolves the priority of the Pod to\n1000000.", "zh": "## Pod 优先级 {#pod-priority}\n\n在你拥有一个或多个 PriorityClass 对象之后，\n你可以创建在其规约中指定这些 PriorityClass 名称之一的 Pod。\n优先级准入控制器使用 `priorityClassName` 字段并填充优先级的整数值。\n如果未找到所指定的优先级类，则拒绝 Pod。\n\n以下 YAML 是 Pod 配置的示例，它使用在前面的示例中创建的 PriorityClass。\n优先级准入控制器检查 Pod 规约并将其优先级解析为 1000000。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  priorityClassName: high-priority\n```"}
{"en": "### Effect of Pod priority on scheduling order\n\nWhen Pod priority is enabled, the scheduler orders pending Pods by\ntheir priority and a pending Pod is placed ahead of other pending Pods\nwith lower priority in the scheduling queue. As a result, the higher\npriority Pod may be scheduled sooner than Pods with lower priority if\nits scheduling requirements are met. If such Pod cannot be scheduled, the\nscheduler will continue and try to schedule other lower priority Pods.", "zh": "### Pod 优先级对调度顺序的影响 {#effect-of-pod-priority-on-scheduling-order}\n\n当启用 Pod 优先级时，调度程序会按优先级对悬决 Pod 进行排序，\n并且每个悬决的 Pod 会被放置在调度队列中其他优先级较低的悬决 Pod 之前。\n因此，如果满足调度要求，较高优先级的 Pod 可能会比具有较低优先级的 Pod 更早调度。\n如果无法调度此类 Pod，调度程序将继续并尝试调度其他较低优先级的 Pod。"}
{"en": "## Preemption\n\nWhen Pods are created, they go to a queue and wait to be scheduled. The\nscheduler picks a Pod from the queue and tries to schedule it on a Node. If no\nNode is found that satisfies all the specified requirements of the Pod,\npreemption logic is triggered for the pending Pod. Let's call the pending Pod P.\nPreemption logic tries to find a Node where removal of one or more Pods with\nlower priority than P would enable P to be scheduled on that Node. If such a\nNode is found, one or more lower priority Pods get evicted from the Node. After\nthe Pods are gone, P can be scheduled on the Node.", "zh": "## 抢占    {#preemption}\n\nPod 被创建后会进入队列等待调度。\n调度器从队列中挑选一个 Pod 并尝试将它调度到某个节点上。\n如果没有找到满足 Pod 的所指定的所有要求的节点，则触发对悬决 Pod 的抢占逻辑。\n让我们将悬决 Pod 称为 P。抢占逻辑试图找到一个节点，\n在该节点中删除一个或多个优先级低于 P 的 Pod，则可以将 P 调度到该节点上。\n如果找到这样的节点，一个或多个优先级较低的 Pod 会被从节点中驱逐。\n被驱逐的 Pod 消失后，P 可以被调度到该节点上。"}
{"en": "### User exposed information\n\nWhen Pod P preempts one or more Pods on Node N, `nominatedNodeName` field of Pod\nP's status is set to the name of Node N. This field helps the scheduler track\nresources reserved for Pod P and also gives users information about preemptions\nin their clusters.\n\nPlease note that Pod P is not necessarily scheduled to the \"nominated Node\".\nThe scheduler always tries the \"nominated Node\" before iterating over any other nodes.\nAfter victim Pods are preempted, they get their graceful termination period. If\nanother node becomes available while scheduler is waiting for the victim Pods to\nterminate, scheduler may use the other node to schedule Pod P. As a result\n`nominatedNodeName` and `nodeName` of Pod spec are not always the same. Also, if\nthe scheduler preempts Pods on Node N, but then a higher priority Pod than Pod P\narrives, the scheduler may give Node N to the new higher priority Pod. In such a\ncase, scheduler clears `nominatedNodeName` of Pod P. By doing this, scheduler\nmakes Pod P eligible to preempt Pods on another Node.", "zh": "### 用户暴露的信息 {#user-exposed-information}\n\n当 Pod P 抢占节点 N 上的一个或多个 Pod 时，\nPod P 状态的 `nominatedNodeName` 字段被设置为节点 N 的名称。\n该字段帮助调度程序跟踪为 Pod P 保留的资源，并为用户提供有关其集群中抢占的信息。\n\n请注意，Pod P 不一定会调度到“被提名的节点（Nominated Node）”。\n调度程序总是在迭代任何其他节点之前尝试“指定节点”。\n在 Pod 因抢占而牺牲时，它们将获得体面终止期。\n如果调度程序正在等待牺牲者 Pod 终止时另一个节点变得可用，\n则调度程序可以使用另一个节点来调度 Pod P。\n因此，Pod 规约中的 `nominatedNodeName` 和 `nodeName` 并不总是相同。\n此外，如果调度程序抢占节点 N 上的 Pod，但随后比 Pod P 更高优先级的 Pod 到达，\n则调度程序可能会将节点 N 分配给新的更高优先级的 Pod。\n在这种情况下，调度程序会清除 Pod P 的 `nominatedNodeName`。\n通过这样做，调度程序使 Pod P 有资格抢占另一个节点上的 Pod。"}
{"en": "### Limitations of preemption\n\n#### Graceful termination of preemption victims\n\nWhen Pods are preempted, the victims get their\n[graceful termination period](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination).\nThey have that much time to finish their work and exit. If they don't, they are\nkilled. This graceful termination period creates a time gap between the point\nthat the scheduler preempts Pods and the time when the pending Pod (P) can be\nscheduled on the Node (N). In the meantime, the scheduler keeps scheduling other\npending Pods. As victims exit or get terminated, the scheduler tries to schedule\nPods in the pending queue. Therefore, there is usually a time gap between the\npoint that scheduler preempts victims and the time that Pod P is scheduled. In\norder to minimize this gap, one can set graceful termination period of lower\npriority Pods to zero or a small number.", "zh": "### 抢占的限制 {#limitations-of-preemption}\n\n#### 被抢占牺牲者的体面终止\n\n当 Pod 被抢占时，牺牲者会得到他们的\n[体面终止期](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)。\n它们可以在体面终止期内完成工作并退出。如果它们不这样做就会被杀死。\n这个体面终止期在调度程序抢占 Pod 的时间点和待处理的 Pod (P)\n可以在节点 (N) 上调度的时间点之间划分出了一个时间跨度。\n同时，调度器会继续调度其他待处理的 Pod。当牺牲者退出或被终止时，\n调度程序会尝试在待处理队列中调度 Pod。\n因此，调度器抢占牺牲者的时间点与 Pod P 被调度的时间点之间通常存在时间间隔。\n为了最小化这个差距，可以将低优先级 Pod 的体面终止时间设置为零或一个小数字。"}
{"en": "#### PodDisruptionBudget is supported, but not guaranteed\n\nA [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) (PDB)\nallows application owners to limit the number of Pods of a replicated application\nthat are down simultaneously from voluntary disruptions. Kubernetes supports\nPDB when preempting Pods, but respecting PDB is best effort. The scheduler tries\nto find victims whose PDB are not violated by preemption, but if no such victims\nare found, preemption will still happen, and lower priority Pods will be removed\ndespite their PDBs being violated.", "zh": "#### 支持 PodDisruptionBudget，但不保证\n\n[PodDisruptionBudget](/zh-cn/docs/concepts/workloads/pods/disruptions/)\n(PDB) 允许多副本应用程序的所有者限制因自愿性质的干扰而同时终止的 Pod 数量。\nKubernetes 在抢占 Pod 时支持 PDB，但对 PDB 的支持是基于尽力而为原则的。\n调度器会尝试寻找不会因被抢占而违反 PDB 的牺牲者，但如果没有找到这样的牺牲者，\n抢占仍然会发生，并且即使违反了 PDB 约束也会删除优先级较低的 Pod。"}
{"en": "#### Inter-Pod affinity on lower-priority Pods\n\nA Node is considered for preemption only when the answer to this question is\nyes: \"If all the Pods with lower priority than the pending Pod are removed from\nthe Node, can the pending Pod be scheduled on the Node?\"", "zh": "#### 与低优先级 Pod 之间的 Pod 间亲和性\n\n只有当这个问题的答案是肯定的时，才考虑在一个节点上执行抢占操作：\n“如果从此节点上删除优先级低于悬决 Pod 的所有 Pod，悬决 Pod 是否可以在该节点上调度？”\n\n{{< note >}}"}
{"en": "Preemption does not necessarily remove all lower-priority\nPods. If the pending Pod can be scheduled by removing fewer than all\nlower-priority Pods, then only a portion of the lower-priority Pods are removed.\nEven so, the answer to the preceding question must be yes. If the answer is no,\nthe Node is not considered for preemption.", "zh": "抢占并不一定会删除所有较低优先级的 Pod。\n如果悬决 Pod 可以通过删除少于所有较低优先级的 Pod 来调度，\n那么只有一部分较低优先级的 Pod 会被删除。\n即便如此，上述问题的答案必须是肯定的。\n如果答案是否定的，则不考虑在该节点上执行抢占。\n{{< /note >}}"}
{"en": "If a pending Pod has inter-pod {{< glossary_tooltip text=\"affinity\" term_id=\"affinity\" >}}\nto one or more of the lower-priority Pods on the Node, the inter-Pod affinity\nrule cannot be satisfied in the absence of those lower-priority Pods. In this case,\nthe scheduler does not preempt any Pods on the Node. Instead, it looks for another\nNode. The scheduler might find a suitable Node or it might not. There is no\nguarantee that the pending Pod can be scheduled.\n\nOur recommended solution for this problem is to create inter-Pod affinity only\ntowards equal or higher priority Pods.", "zh": "如果悬决 Pod 与节点上的一个或多个较低优先级 Pod 具有 Pod 间{{< glossary_tooltip text=\"亲和性\" term_id=\"affinity\" >}}，\n则在没有这些较低优先级 Pod 的情况下，无法满足 Pod 间亲和性规则。\n在这种情况下，调度程序不会抢占节点上的任何 Pod。\n相反，它寻找另一个节点。调度程序可能会找到合适的节点，\n也可能不会。无法保证悬决 Pod 可以被调度。\n\n我们针对此问题推荐的解决方案是仅针对同等或更高优先级的 Pod 设置 Pod 间亲和性。"}
{"en": "#### Cross node preemption\n\nSuppose a Node N is being considered for preemption so that a pending Pod P can\nbe scheduled on N. P might become feasible on N only if a Pod on another Node is\npreempted. Here's an example:\n\n*   Pod P is being considered for Node N.\n*   Pod Q is running on another Node in the same Zone as Node N.\n*   Pod P has Zone-wide anti-affinity with Pod Q (`topologyKey:\n    topology.kubernetes.io/zone`).\n*   There are no other cases of anti-affinity between Pod P and other Pods in\n    the Zone.\n*   In order to schedule Pod P on Node N, Pod Q can be preempted, but scheduler\n    does not perform cross-node preemption. So, Pod P will be deemed\n    unschedulable on Node N.\n\nIf Pod Q were removed from its Node, the Pod anti-affinity violation would be\ngone, and Pod P could possibly be scheduled on Node N.\n\nWe may consider adding cross Node preemption in future versions if there is\nenough demand and if we find an algorithm with reasonable performance.", "zh": "#### 跨节点抢占\n\n假设正在考虑在一个节点 N 上执行抢占，以便可以在 N 上调度待处理的 Pod P。\n只有当另一个节点上的 Pod 被抢占时，P 才可能在 N 上变得可行。\n下面是一个例子：\n\n* 调度器正在考虑将 Pod P 调度到节点 N 上。\n* Pod Q 正在与节点 N 位于同一区域的另一个节点上运行。\n* Pod P 与 Pod Q 具有 Zone 维度的反亲和（`topologyKey:topology.kubernetes.io/zone`）设置。\n* Pod P 与 Zone 中的其他 Pod 之间没有其他反亲和性设置。\n* 为了在节点 N 上调度 Pod P，可以抢占 Pod Q，但调度器不会进行跨节点抢占。\n  因此，Pod P 将被视为在节点 N 上不可调度。\n\n如果将 Pod Q 从所在节点中移除，则不会违反 Pod 间反亲和性约束，\n并且 Pod P 可能会被调度到节点 N 上。\n\n如果有足够的需求，并且如果我们找到性能合理的算法，\n我们可能会考虑在未来版本中添加跨节点抢占。"}
{"en": "## Troubleshooting\n\nPod priority and preemption can have unwanted side effects. Here are some\nexamples of potential problems and ways to deal with them.", "zh": "## 故障排除 {#troubleshooting}\n\nPod 优先级和抢占可能会产生不必要的副作用。以下是一些潜在问题的示例以及处理这些问题的方法。"}
{"en": "### Pods are preempted unnecessarily\n\nPreemption removes existing Pods from a cluster under resource pressure to make\nroom for higher priority pending Pods. If you give high priorities to\ncertain Pods by mistake, these unintentionally high priority Pods may cause\npreemption in your cluster. Pod priority is specified by setting the\n`priorityClassName` field in the Pod's specification. The integer value for\npriority is then resolved and populated to the `priority` field of `podSpec`.\n\nTo address the problem, you can change the `priorityClassName` for those Pods\nto use lower priority classes, or leave that field empty. An empty\n`priorityClassName` is resolved to zero by default.\n\nWhen a Pod is preempted, there will be events recorded for the preempted Pod.\nPreemption should happen only when a cluster does not have enough resources for\na Pod. In such cases, preemption happens only when the priority of the pending\nPod (preemptor) is higher than the victim Pods. Preemption must not happen when\nthere is no pending Pod, or when the pending Pods have equal or lower priority\nthan the victims. If preemption happens in such scenarios, please file an issue.", "zh": "### Pod 被不必要地抢占\n\n抢占在资源压力较大时从集群中删除现有 Pod，为更高优先级的悬决 Pod 腾出空间。\n如果你错误地为某些 Pod 设置了高优先级，这些无意的高优先级 Pod 可能会导致集群中出现抢占行为。\nPod 优先级是通过设置 Pod 规约中的 `priorityClassName` 字段来指定的。\n优先级的整数值然后被解析并填充到 `podSpec` 的 `priority` 字段。\n\n为了解决这个问题，你可以将这些 Pod 的 `priorityClassName` 更改为使用较低优先级的类，\n或者将该字段留空。默认情况下，空的 `priorityClassName` 解析为零。\n\n当 Pod 被抢占时，集群会为被抢占的 Pod 记录事件。只有当集群没有足够的资源用于 Pod 时，\n才会发生抢占。在这种情况下，只有当悬决 Pod（抢占者）的优先级高于受害 Pod 时才会发生抢占。\n当没有悬决 Pod，或者悬决 Pod 的优先级等于或低于牺牲者时，不得发生抢占。\n如果在这种情况下发生抢占，请提出问题。"}
{"en": "### Pods are preempted, but the preemptor is not scheduled\n\nWhen pods are preempted, they receive their requested graceful termination\nperiod, which is by default 30 seconds. If the victim Pods do not terminate within\nthis period, they are forcibly terminated. Once all the victims go away, the\npreemptor Pod can be scheduled.\n\nWhile the preemptor Pod is waiting for the victims to go away, a higher priority\nPod may be created that fits on the same Node. In this case, the scheduler will\nschedule the higher priority Pod instead of the preemptor.\n\nThis is expected behavior: the Pod with the higher priority should take the place\nof a Pod with a lower priority.", "zh": "### 有 Pod 被抢占，但抢占者并没有被调度\n\n当 Pod 被抢占时，它们会收到请求的体面终止期，默认为 30 秒。\n如果受害 Pod 在此期限内没有终止，它们将被强制终止。\n一旦所有牺牲者都离开，就可以调度抢占者 Pod。\n\n在抢占者 Pod 等待牺牲者离开的同时，可能某个适合同一个节点的更高优先级的 Pod 被创建。\n在这种情况下，调度器将调度优先级更高的 Pod 而不是抢占者。\n\n这是预期的行为：具有较高优先级的 Pod 应该取代具有较低优先级的 Pod。"}
{"en": "### Higher priority Pods are preempted before lower priority pods\n\nThe scheduler tries to find nodes that can run a pending Pod. If no node is\nfound, the scheduler tries to remove Pods with lower priority from an arbitrary\nnode in order to make room for the pending pod.\nIf a node with low priority Pods is not feasible to run the pending Pod, the scheduler\nmay choose another node with higher priority Pods (compared to the Pods on the\nother node) for preemption. The victims must still have lower priority than the\npreemptor Pod.\n\nWhen there are multiple nodes available for preemption, the scheduler tries to\nchoose the node with a set of Pods with lowest priority. However, if such Pods\nhave PodDisruptionBudget that would be violated if they are preempted then the\nscheduler may choose another node with higher priority Pods.\n\nWhen multiple nodes exist for preemption and none of the above scenarios apply,\nthe scheduler chooses a node with the lowest priority.", "zh": "### 优先级较高的 Pod 在优先级较低的 Pod 之前被抢占\n\n调度程序尝试查找可以运行悬决 Pod 的节点。如果没有找到这样的节点，\n调度程序会尝试从任意节点中删除优先级较低的 Pod，以便为悬决 Pod 腾出空间。\n如果具有低优先级 Pod 的节点无法运行悬决 Pod，\n调度器可能会选择另一个具有更高优先级 Pod 的节点（与其他节点上的 Pod 相比）进行抢占。\n牺牲者的优先级必须仍然低于抢占者 Pod。\n\n当有多个节点可供执行抢占操作时，调度器会尝试选择具有一组优先级最低的 Pod 的节点。\n但是，如果此类 Pod 具有 PodDisruptionBudget，当它们被抢占时，\n则会违反 PodDisruptionBudget，那么调度程序可能会选择另一个具有更高优先级 Pod 的节点。\n\n当存在多个节点抢占且上述场景均不适用时，调度器会选择优先级最低的节点。"}
{"en": "## Interactions between Pod priority and quality of service {#interactions-of-pod-priority-and-qos}\n\nPod priority and {{< glossary_tooltip text=\"QoS class\" term_id=\"qos-class\" >}}\nare two orthogonal features with few interactions and no default restrictions on\nsetting the priority of a Pod based on its QoS classes. The scheduler's\npreemption logic does not consider QoS when choosing preemption targets.\nPreemption considers Pod priority and attempts to choose a set of targets with\nthe lowest priority. Higher-priority Pods are considered for preemption only if\nthe removal of the lowest priority Pods is not sufficient to allow the scheduler\nto schedule the preemptor Pod, or if the lowest priority Pods are protected by\n`PodDisruptionBudget`.", "zh": "## Pod 优先级和服务质量之间的相互作用 {#interactions-of-pod-priority-and-qos}\n\nPod 优先级和 {{<glossary_tooltip text=\"QoS 类\" term_id=\"qos-class\" >}}\n是两个正交特征，交互很少，并且对基于 QoS 类设置 Pod 的优先级没有默认限制。\n调度器的抢占逻辑在选择抢占目标时不考虑 QoS。\n抢占会考虑 Pod 优先级并尝试选择一组优先级最低的目标。\n仅当移除优先级最低的 Pod 不足以让调度程序调度抢占式 Pod，\n或者最低优先级的 Pod 受 PodDisruptionBudget 保护时，才会考虑优先级较高的 Pod。"}
{"en": "The kubelet uses Priority to determine pod order for [node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\nYou can use the QoS class to estimate the order in which pods are most likely\nto get evicted. The kubelet ranks pods for eviction based on the following factors:\n\n  1. Whether the starved resource usage exceeds requests\n  1. Pod Priority\n  1. Amount of resource usage relative to requests\n\nSee [Pod selection for kubelet eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction)\nfor more details.\n\nkubelet node-pressure eviction does not evict Pods when their\nusage does not exceed their requests. If a Pod with lower priority is not\nexceeding its requests, it won't be evicted. Another Pod with higher priority\nthat exceeds its requests may be evicted.", "zh": "kubelet 使用优先级来确定\n[节点压力驱逐](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/) Pod 的顺序。\n你可以使用 QoS 类来估计 Pod 最有可能被驱逐的顺序。kubelet 根据以下因素对 Pod 进行驱逐排名：\n\n1. 对紧俏资源的使用是否超过请求值\n1. Pod 优先级\n1. 相对于请求的资源使用量\n\n有关更多详细信息，请参阅\n[kubelet 驱逐时 Pod 的选择](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction)。\n\n当某 Pod 的资源用量未超过其请求时，kubelet 节点压力驱逐不会驱逐该 Pod。\n如果优先级较低的 Pod 的资源使用量没有超过其请求，则不会被驱逐。\n另一个优先级较高且资源使用量超过其请求的 Pod 可能会被驱逐。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about using ResourceQuotas in connection with PriorityClasses: [limit Priority Class consumption by default](/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default)\n* Learn about [Pod Disruption](/docs/concepts/workloads/pods/disruptions/)\n* Learn about [API-initiated Eviction](/docs/concepts/scheduling-eviction/api-eviction/)\n* Learn about [Node-pressure Eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/)", "zh": "* 阅读有关将 ResourceQuota 与 PriorityClass 结合使用的信息：\n  [默认限制优先级消费](/zh-cn/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default)\n* 了解 [Pod 干扰](/zh-cn/docs/concepts/workloads/pods/disruptions/)\n* 了解 [API 发起的驱逐](/zh-cn/docs/concepts/scheduling-eviction/api-eviction/)\n* 了解[节点压力驱逐](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)"}
{"en": "Core Dynamic Resource Allocation with structured parameters:", "zh": "使用结构化参数进行核心动态资源分配：\n\n{{< feature-state feature_gate_name=\"DynamicResourceAllocation\" >}}"}
{"en": "Dynamic Resource Allocation with control plane controller:", "zh": "使用控制平面控制器进行动态资源分配：\n\n{{< feature-state feature_gate_name=\"DRAControlPlaneController\" >}}"}
{"en": "Dynamic resource allocation is an API for requesting and sharing resources\nbetween pods and containers inside a pod. It is a generalization of the\npersistent volumes API for generic resources. Typically those resources\nare devices like GPUs.\n\nThird-party resource drivers are\nresponsible for tracking and preparing resources, with allocation of\nresources handled by Kubernetes via _structured parameters_ (introduced in Kubernetes 1.30).\nDifferent kinds of resources support arbitrary parameters for defining requirements and\ninitialization.\n\nWhen a driver provides a _control plane controller_, the driver itself\nhandles allocation in cooperation with the Kubernetes scheduler.", "zh": "动态资源分配是一个用于在 Pod 之间和 Pod 内部容器之间请求和共享资源的 API。\n它是持久卷 API 针对一般资源的泛化。通常这些资源是 GPU 这类设备。\n\n第三方资源驱动程序负责跟踪和准备资源，\nKubernetes 通过**结构化参数**（在 Kubernetes 1.30 中引入）处理资源的分配。\n不同类别的资源支持任意参数来定义要求和初始化。\n\n当驱动程序提供**控制平面控制器**时，驱动程序本身与 Kubernetes 调度器合作一起处理资源分配。\n\n## {{% heading \"prerequisites\" %}}"}
{"en": "Kubernetes v{{< skew currentVersion >}} includes cluster-level API support for\ndynamic resource allocation, but it [needs to be enabled](#enabling-dynamic-resource-allocation)\nexplicitly. You also must install a resource driver for specific resources that\nare meant to be managed using this API. If you are not running Kubernetes\nv{{< skew currentVersion>}}, check the documentation for that version of Kubernetes.", "zh": "Kubernetes v{{< skew currentVersion >}} 包含用于动态资源分配的集群级 API 支持，\n但它需要被[显式启用](#enabling-dynamic-resource-allocation)。\n你还必须为此 API 要管理的特定资源安装资源驱动程序。\n如果你未运行 Kubernetes v{{< skew currentVersion>}}，\n请查看对应版本的 Kubernetes 文档。"}
{"en": "body", "zh": "## API"}
{"en": "The `resource.k8s.io/v1alpha3`\n{{< glossary_tooltip text=\"API group\" term_id=\"api-group\" >}} provides these types:", "zh": "`resource.k8s.io/v1alpha3`\n{{< glossary_tooltip text=\"API 组\" term_id=\"api-group\" >}}\n提供了以下类型："}
{"en": "ResourceClaim\n: Describes a request for access to resources in the cluster,\n  for use by workloads. For example, if a workload needs an accelerator device\n  with specific properties, this is how that request is expressed. The status\n  stanza tracks whether this claim has been satisfied and what specific\n  resources have been allocated.", "zh": "ResourceClaim\n: 描述对集群中资源的访问请求，工作负载需要使用这些资源。\n  例如，如果工作负载需要具有特定属性的加速器设备，就可以通过这种方式表达该请求。\n  状态部分跟踪此请求是否已被满足以及具体已分配了哪些资源。"}
{"en": "ResourceClaimTemplate\n: Defines the spec and some metadata for creating\n  ResourceClaims. Created by a user when deploying a workload.\n  The per-Pod ResourceClaims are then created and removed by Kubernetes\n  automatically.", "zh": "ResourceClaimTemplate\n: 定义用于创建 ResourceClaim 的规约和一些元数据。\n  部署工作负载时由用户创建。\n  每个 Pod 的 ResourceClaim 随后会被 Kubernetes 自动创建和移除。"}
{"en": "DeviceClass\n: Contains pre-defined selection criteria for certain devices and\n  configuration for them. DeviceClasses are created by a cluster administrator\n  when installing a resource driver. Each request to allocate a device\n  in a ResourceClaim must reference exactly one DeviceClass.", "zh": "DeviceClass\n: 包含某些设备的预定义选择标准和配置。\n  DeviceClass 由集群管理员在安装资源驱动程序时创建。\n  对 ResourceClaim 中某个设备的每个分配请求都必须准确引用一个 DeviceClass。"}
{"en": "PodSchedulingContext\n: Used internally by the control plane and resource drivers\n  to coordinate pod scheduling when ResourceClaims need to be allocated\n  for a Pod and those ResourceClaims use a control plane controller.\n\nResourceSlice\n: Used with structured parameters to publish information about resources\n  that are available in the cluster.", "zh": "PodSchedulingContext\n: 供控制平面和资源驱动程序内部使用，\n  在需要为 Pod 分配 ResourceClaim 且这些 ResourceClaim 使用控制平面控制器时协调 Pod 调度。\n\nResourceSlice\n: 与结构化参数一起使用，以发布有关集群中可用资源的信息。"}
{"en": "The developer of a resource driver decides whether they want to handle\nallocation themselves with a control plane controller or instead rely on allocation\nthrough Kubernetes with structured parameters. A\ncustom controller provides more flexibility, but cluster autoscaling is not\ngoing to work reliably for node-local resources. Structured parameters enable\ncluster autoscaling, but might not satisfy all use-cases.", "zh": "资源驱动程序的开发者决定他们是要使用控制平面控制器自己处理资源分配，\n还是依赖 Kubernetes 使用结构化参数来处理资源分配。\n自定义控制器提供更多的灵活性，但对于节点本地资源，集群自动扩缩可能无法可靠工作。\n结构化参数使集群自动扩缩成为可能，但可能无法满足所有使用场景。"}
{"en": "When a driver uses structured parameters, all parameters that select devices\nare defined in the ResourceClaim and DeviceClass with in-tree types. Configuration\nparameters can be embedded there as arbitrary JSON objects.", "zh": "当驱动程序使用结构化参数时，所有选择设备的参数都在\nResourceClaim 和 DeviceClass 中以树内类型被定义。\n配置参数可以作为任意 JSON 对象嵌入其中。"}
{"en": "The `core/v1` `PodSpec` defines ResourceClaims that are needed for a Pod in a\n`resourceClaims` field. Entries in that list reference either a ResourceClaim\nor a ResourceClaimTemplate. When referencing a ResourceClaim, all Pods using\nthis PodSpec (for example, inside a Deployment or StatefulSet) share the same\nResourceClaim instance. When referencing a ResourceClaimTemplate, each Pod gets\nits own instance.", "zh": "`core/v1` 的 `PodSpec` 在 `resourceClaims` 字段中定义 Pod 所需的 ResourceClaim。\n该列表中的条目引用 ResourceClaim 或 ResourceClaimTemplate。\n当引用 ResourceClaim 时，使用此 PodSpec 的所有 Pod\n（例如 Deployment 或 StatefulSet 中的 Pod）共享相同的 ResourceClaim 实例。\n引用 ResourceClaimTemplate 时，每个 Pod 都有自己的实例。"}
{"en": "The `resources.claims` list for container resources defines whether a container gets\naccess to these resource instances, which makes it possible to share resources\nbetween one or more containers.\n\nHere is an example for a fictional resource driver. Two ResourceClaim objects\nwill get created for this Pod and each container gets access to one of them.", "zh": "容器资源的 `resources.claims` 列表定义容器可以访问的资源实例，\n从而可以实现在一个或多个容器之间共享资源。\n\n下面是一个虚构的资源驱动程序的示例。\n该示例将为此 Pod 创建两个 ResourceClaim 对象，每个容器都可以访问其中一个。\n\n```yaml\napiVersion: resource.k8s.io/v1alpha3\nkind: DeviceClass\nname: resource.example.com\nspec:\n  selectors:\n  - cel:\n      expression: device.driver == \"resource-driver.example.com\"\n---\napiVersion: resource.k8s.io/v1alpha2\nkind: ResourceClaimTemplate\nmetadata:\n  name: large-black-cat-claim-template\nspec:\n  spec:\n    devices:\n      requests:\n      - name: req-0\n        deviceClassName: resource.example.com\n        selectors:\n        - cel:\n           expression: |-\n              device.attributes[\"resource-driver.example.com\"].color == \"black\" &&\n              device.attributes[\"resource-driver.example.com\"].size == \"large\"\n–--\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-cats\nspec:\n  containers:\n  - name: container0\n    image: ubuntu:20.04\n    command: [\"sleep\", \"9999\"]\n    resources:\n      claims:\n      - name: cat-0\n  - name: container1\n    image: ubuntu:20.04\n    command: [\"sleep\", \"9999\"]\n    resources:\n      claims:\n      - name: cat-1\n  resourceClaims:\n  - name: cat-0\n    resourceClaimTemplateName: large-black-cat-claim-template\n  - name: cat-1\n    resourceClaimTemplateName: large-black-cat-claim-template\n```"}
{"en": "## Scheduling", "zh": "## 调度  {#scheduling}"}
{"en": "### With control plane controller", "zh": "### 使用控制平面控制器  {#with-control-plane-controller}"}
{"en": "In contrast to native resources (CPU, RAM) and extended resources (managed by a\ndevice plugin, advertised by kubelet), without structured parameters\nthe scheduler has no knowledge of what\ndynamic resources are available in a cluster or how they could be split up to\nsatisfy the requirements of a specific ResourceClaim. Resource drivers are\nresponsible for that. They mark ResourceClaims as \"allocated\" once resources\nfor it are reserved. This also then tells the scheduler where in the cluster a\nResourceClaim is available.", "zh": "与原生资源（CPU、RAM）和扩展资源（由设备插件管理，并由 kubelet 公布）不同，\n如果没有结构化参数，调度器无法知道集群中有哪些动态资源，\n也不知道如何将它们拆分以满足特定 ResourceClaim 的要求。\n资源驱动程序负责这些任务。\n资源驱动程序在为 ResourceClaim 保留资源后将其标记为“已分配（Allocated）”。\n然后告诉调度器集群中可用的 ResourceClaim 的位置。"}
{"en": "When a pod gets scheduled, the scheduler checks all ResourceClaims needed by a Pod and\ncreates a PodScheduling object where it informs the resource drivers\nresponsible for those ResourceClaims about nodes that the scheduler considers\nsuitable for the Pod. The resource drivers respond by excluding nodes that\ndon't have enough of the driver's resources left. Once the scheduler has that\ninformation, it selects one node and stores that choice in the PodScheduling\nobject. The resource drivers then allocate their ResourceClaims so that the\nresources will be available on that node. Once that is complete, the Pod\ngets scheduled.", "zh": "当 Pod 被调度时，调度器检查 Pod 所需的所有 ResourceClaim，并创建一个 PodScheduling 对象，\n通知负责这些 ResourceClaim 的资源驱动程序，告知它们调度器认为适合该 Pod 的节点。\n资源驱动程序通过排除没有足够剩余资源的节点来响应调度器。\n一旦调度器有了这些信息，它就会选择一个节点，并将该选择存储在 PodScheduling 对象中。\n然后，资源驱动程序为其分配 ResourceClaim，以便资源可用于该节点。\n完成后，Pod 就会被调度。"}
{"en": "As part of this process, ResourceClaims also get reserved for the\nPod. Currently ResourceClaims can either be used exclusively by a single Pod or\nan unlimited number of Pods.", "zh": "作为此过程的一部分，ResourceClaim 会为 Pod 保留。\n目前，ResourceClaim 可以由单个 Pod 独占使用或不限数量的多个 Pod 使用。"}
{"en": "One key feature is that Pods do not get scheduled to a node unless all of\ntheir resources are allocated and reserved. This avoids the scenario where a Pod\ngets scheduled onto one node and then cannot run there, which is bad because\nsuch a pending Pod also blocks all other resources like RAM or CPU that were\nset aside for it.", "zh": "除非 Pod 的所有资源都已分配和保留，否则 Pod 不会被调度到节点，这是一个重要特性。\n这避免了 Pod 被调度到一个节点但无法在那里运行的情况，\n这种情况很糟糕，因为被挂起 Pod 也会阻塞为其保留的其他资源，如 RAM 或 CPU。\n\n{{< note >}}"}
{"en": "Scheduling of pods which use ResourceClaims is going to be slower because of\nthe additional communication that is required. Beware that this may also impact\npods that don't use ResourceClaims because only one pod at a time gets\nscheduled, blocking API calls are made while handling a pod with\nResourceClaims, and thus scheduling the next pod gets delayed.", "zh": "由于需要额外的通信，使用 ResourceClaim 的 Pod 的调度将会变慢。\n请注意，这也可能会影响不使用 ResourceClaim 的 Pod，因为一次仅调度一个\nPod，在使用 ResourceClaim 处理 Pod 时会进行阻塞 API 调用，\n从而推迟调度下一个 Pod。\n{{< /note >}}"}
{"en": "### With structured parameters", "zh": "### 使用结构化参数 {#with-structured-parameters}"}
{"en": "When a driver uses structured parameters, the scheduler takes over the\nresponsibility of allocating resources to a ResourceClaim whenever a pod needs\nthem. It does so by retrieving the full list of available resources from\nResourceSlice objects, tracking which of those resources have already been\nallocated to existing ResourceClaims, and then selecting from those resources\nthat remain.", "zh": "当驱动程序使用结构化参数时，调度器负责在 Pod 需要资源时为 ResourceClaim 分配资源。\n通过从 ResourceSlice 对象中检索可用资源的完整列表，\n跟踪已分配给现有 ResourceClaim 的资源，然后从剩余的资源中进行选择。"}
{"en": "The only kind of supported resources at the moment are devices. A device\ninstance has a name and several attributes and capacities. Devices get selected\nthrough CEL expressions which check those attributes and capacities. In\naddition, the set of selected devices also can be restricted to sets which meet\ncertain constraints.", "zh": "目前唯一支持的资源类别是设备。\n设备实例具有名称以及多个属性和容量信息。\n设备通过 CEL 表达式被选择，这些表达式检查设备的属性和容量。\n此外，所选择的设备集合还可以限制为满足特定约束的集合。"}
{"en": "The chosen resource is recorded in the ResourceClaim status together with any\nvendor-specific configuration, so when a pod is about to start on a node, the\nresource driver on the node has all the information it needs to prepare the\nresource.", "zh": "所选资源与所有供应商特定配置一起被记录在 ResourceClaim 状态中，\n因此当 Pod 即将在节点上启动时，节点上的资源驱动程序具有准备资源所需的所有信息。"}
{"en": "By using structured parameters, the scheduler is able to reach a decision\nwithout communicating with any DRA resource drivers. It is also able to\nschedule multiple pods quickly by keeping information about ResourceClaim\nallocations in memory and writing this information to the ResourceClaim objects\nin the background while concurrently binding the pod to a node.", "zh": "通过使用结构化参数，调度器能够在不与 DRA 资源驱动程序通信的情况下做出决策。\n它还能够通过将 ResourceClaim 分配信息保存在内存中，并在同时将 Pod 绑定到节点的同时将此信息写入\nResourceClaim 对象中，快速调度多个 Pod。"}
{"en": "## Monitoring resources", "zh": "## 监控资源  {#monitoring-resources}"}
{"en": "The kubelet provides a gRPC service to enable discovery of dynamic resources of\nrunning Pods. For more information on the gRPC endpoints, see the\n[resource allocation reporting](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#monitoring-device-plugin-resources).", "zh": "kubelet 提供了一个 gRPC 服务，以便发现正在运行的 Pod 的动态资源。\n有关 gRPC 端点的更多信息，请参阅[资源分配报告](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#monitoring-device-plugin-resources)。"}
{"en": "## Pre-scheduled Pods\n\nWhen you - or another API client - create a Pod with `spec.nodeName` already set, the scheduler gets bypassed.\nIf some ResourceClaim needed by that Pod does not exist yet, is not allocated\nor not reserved for the Pod, then the kubelet will fail to run the Pod and\nre-check periodically because those requirements might still get fulfilled\nlater.", "zh": "## 预调度的 Pod   {#pre-scheduled-pods}\n\n当你（或别的 API 客户端）创建设置了 `spec.nodeName` 的 Pod 时，调度器将被绕过。\n如果 Pod 所需的某个 ResourceClaim 尚不存在、未被分配或未为该 Pod 保留，那么 kubelet\n将无法运行该 Pod，并会定期重新检查，因为这些要求可能在以后得到满足。"}
{"en": "Such a situation can also arise when support for dynamic resource allocation\nwas not enabled in the scheduler at the time when the Pod got scheduled\n(version skew, configuration, feature gate, etc.). kube-controller-manager\ndetects this and tries to make the Pod runnable by triggering allocation and/or\nreserving the required ResourceClaims.", "zh": "这种情况也可能发生在 Pod 被调度时调度器中未启用动态资源分配支持的时候（原因可能是版本偏差、配置、特性门控等）。\nkube-controller-manager 能够检测到这一点，并尝试通过触发分配和/或预留所需的 ResourceClaim 来使 Pod 可运行。\n\n{{< note >}}"}
{"en": "This only works with resource drivers that don't use structured parameters.", "zh": "这仅适用于不使用结构化参数的资源驱动程序。\n{{< /note >}}"}
{"en": "It is better to avoid bypassing the scheduler because a Pod that is assigned to a node\nblocks normal resources (RAM, CPU) that then cannot be used for other Pods\nwhile the Pod is stuck. To make a Pod run on a specific node while still going\nthrough the normal scheduling flow, create the Pod with a node selector that\nexactly matches the desired node:", "zh": "绕过调度器并不是一个好的选择，因为分配给节点的 Pod 会锁住一些正常的资源（RAM、CPU），\n而这些资源在 Pod 被卡住时无法用于其他 Pod。为了让一个 Pod 在特定节点上运行，\n同时仍然通过正常的调度流程进行，请在创建 Pod 时使用与期望的节点精确匹配的节点选择算符：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-cats\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: name-of-the-intended-node\n  ...\n```"}
{"en": "You may also be able to mutate the incoming Pod, at admission time, to unset\nthe `.spec.nodeName` field and to use a node selector instead.", "zh": "你还可以在准入时变更传入的 Pod，取消设置 `.spec.nodeName` 字段，并改为使用节点选择算符。"}
{"en": "## Enabling dynamic resource allocation", "zh": "## 启用动态资源分配 {#enabling-dynamic-resource-allocation}"}
{"en": "Dynamic resource allocation is an *alpha feature* and only enabled when the\n`DynamicResourceAllocation` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nand the `resource.k8s.io/v1alpha3` {{< glossary_tooltip text=\"API group\" term_id=\"api-group\" >}}\nare enabled. For details on that, see the `--feature-gates` and `--runtime-config`\n[kube-apiserver parameters](/docs/reference/command-line-tools-reference/kube-apiserver/).\nkube-scheduler, kube-controller-manager and kubelet also need the feature gate.", "zh": "动态资源分配是一个 **Alpha 特性**，只有在启用 `DynamicResourceAllocation`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)\n和 `resource.k8s.io/v1alpha3`\n{{< glossary_tooltip text=\"API 组\" term_id=\"api-group\" >}} 时才启用。\n有关详细信息，参阅 `--feature-gates` 和 `--runtime-config`\n[kube-apiserver 参数](/zh-cn/docs/reference/command-line-tools-reference/kube-apiserver/)。\nkube-scheduler、kube-controller-manager 和 kubelet 也需要设置该特性门控。"}
{"en": "When a resource driver uses a control plane controller, then the\n`DRAControlPlaneController` feature gate has to be enabled in addition to\n`DynamicResourceAllocation`.", "zh": "当资源驱动程序使用控制平面控制器时，除了需要启用 `DynamicResourceAllocation` 外，\n还必须启用 `DRAControlPlaneController` 特性门控。"}
{"en": "A quick check whether a Kubernetes cluster supports the feature is to list\nDeviceClass objects with:", "zh": "快速检查 Kubernetes 集群是否支持该特性的方法是列举 DeviceClass 对象：\n\n```shell\nkubectl get deviceclasses\n```"}
{"en": "If your cluster supports dynamic resource allocation, the response is either a\nlist of DeviceClass objects or:", "zh": "如果你的集群支持动态资源分配，则响应是 DeviceClass 对象列表或：\n\n```\nNo resources found\n```"}
{"en": "If not supported, this error is printed instead:", "zh": "如果不支持，则会输出如下错误：\n\n```\nerror: the server doesn't have a resource type \"deviceclasses\"\n```"}
{"en": "A control plane controller is supported when it is possible to create a\nResourceClaim where the `spec.controller` field is set. When the\n`DRAControlPlaneController` feature is disabled, that field automatically\ngets cleared when storing the ResourceClaim.", "zh": "当可以创建设置了 `spec.controller` 字段的 ResourceClaim 时，控制平面控制器是受支持的。\n当 `DRAControlPlaneController` 特性被禁用时，存储 ResourceClaim 时该字段会自动被清除。"}
{"en": "The default configuration of kube-scheduler enables the \"DynamicResources\"\nplugin if and only if the feature gate is enabled and when using\nthe v1 configuration API. Custom configurations may have to be modified to\ninclude it.", "zh": "kube-scheduler 的默认配置仅在启用特性门控且使用 v1 配置 API 时才启用 \"DynamicResources\" 插件。\n自定义配置可能需要被修改才能启用它。"}
{"en": "In addition to enabling the feature in the cluster, a resource driver also has to\nbe installed. Please refer to the driver's documentation for details.", "zh": "除了在集群中启用该功能外，还必须安装资源驱动程序。\n欲了解详细信息，请参阅驱动程序的文档。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- For more information on the design, see the\n  [Dynamic Resource Allocation with Structured Parameters](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/4381-dra-structured-parameters)\n  and the\n  [Dynamic Resource Allocation with Control Plane Controller](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md) KEPs.", "zh": "- 了解更多该设计的信息，\n  参阅[使用结构化参数的动态资源分配 KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/4381-dra-structured-parameters)\n  和[使用控制平面控制器的动态资源分配 KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/3063-dynamic-resource-allocation/README.md)。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "When you run a Pod on a Node, the Pod itself takes an amount of system resources. These\nresources are additional to the resources needed to run the container(s) inside the Pod.\nIn Kubernetes, _Pod Overhead_ is a way to account for the resources consumed by the Pod\ninfrastructure on top of the container requests & limits.", "zh": "在节点上运行 Pod 时，Pod 本身占用大量系统资源。这些是运行 Pod 内容器所需资源之外的资源。\n在 Kubernetes 中，_POD 开销_ 是一种方法，用于计算 Pod 基础设施在容器请求和限制之上消耗的资源。"}
{"en": "In Kubernetes, the Pod's overhead is set at\n[admission](/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks)\ntime according to the overhead associated with the Pod's\n[RuntimeClass](/docs/concepts/containers/runtime-class/).", "zh": "在 Kubernetes 中，Pod 的开销是根据与 Pod 的 [RuntimeClass](/zh-cn/docs/concepts/containers/runtime-class/)\n相关联的开销在[准入](/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks)时设置的。"}
{"en": "A pod's overhead is considered in addition to the sum of container resource requests when\nscheduling a Pod. Similarly, the kubelet will include the Pod overhead when sizing the Pod cgroup,\nand when carrying out Pod eviction ranking.", "zh": "在调度 Pod 时，除了考虑容器资源请求的总和外，还要考虑 Pod 开销。\n类似地，kubelet 将在确定 Pod cgroups 的大小和执行 Pod 驱逐排序时也会考虑 Pod 开销。"}
{"en": "## Configuring Pod overhead {#set-up}", "zh": "## 配置 Pod 开销 {#set-up}"}
{"en": "You need to make sure a `RuntimeClass` is utilized which defines the `overhead` field.", "zh": "你需要确保使用一个定义了 `overhead` 字段的 `RuntimeClass`。"}
{"en": "## Usage example", "zh": "## 使用示例 {#usage-example}"}
{"en": "To work with Pod overhead, you need a RuntimeClass that defines the `overhead` field. As\nan example, you could use the following RuntimeClass definition with a virtualization container\nruntime (in this example, Kata Containers combined with the Firecracker virtual machine monitor)\nthat uses around 120MiB per Pod for the virtual machine and the guest OS:", "zh": "要使用 Pod 开销，你需要一个定义了 `overhead` 字段的 RuntimeClass。\n例如，你可以使用以下 RuntimeClass 定义，其中使用了一个虚拟化容器运行时（在这个例子中，Kata Containers 与 Firecracker 虚拟机监视器结合使用），\n每个 Pod 使用大约 120MiB 的虚拟机和寄宿操作系统："}
{"en": "```yaml\n# You need to change this example to match the actual runtime name, and per-Pod\n# resource overhead, that the container runtime is adding in your cluster.\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: kata-fc\nhandler: kata-fc\noverhead:\n  podFixed:\n    memory: \"120Mi\"\n    cpu: \"250m\"\n```", "zh": "```yaml\n# 你需要修改这个示例以匹配实际的运行时名称，\n# 以及在你的集群中运行时在 Pod 层面增加的资源开销。\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: kata-fc\nhandler: kata-fc\noverhead:\n  podFixed:\n    memory: \"120Mi\"\n    cpu: \"250m\"\n```"}
{"en": "Workloads which are created which specify the `kata-fc` RuntimeClass handler will take the memory and\ncpu overheads into account for resource quota calculations, node scheduling, as well as Pod cgroup sizing.\n\nConsider running the given example workload, test-pod:", "zh": "通过指定 `kata-fc` RuntimeClass 处理程序创建的工作负载会将内存和 CPU\n开销计入资源配额计算、节点调度以及 Pod cgroup 尺寸确定。\n\n假设我们运行下面给出的工作负载示例 test-pod:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  runtimeClassName: kata-fc\n  containers:\n  - name: busybox-ctr\n    image: busybox:1.28\n    stdin: true\n    tty: true\n    resources:\n      limits:\n        cpu: 500m\n        memory: 100Mi\n  - name: nginx-ctr\n    image: nginx\n    resources:\n      limits:\n        cpu: 1500m\n        memory: 100Mi\n```\n\n{{< note >}}"}
{"en": "If only `limits` are specified in the pod definition, kubelet will deduce `requests` from those limits and set them to be the same as the defined `limits`.", "zh": "如果在 Pod 定义中只设置了 `limits`，kubelet 将根据 limits 推断 `requests`，并将其设置与 limits 相同的值。\n{{< /note >}}"}
{"en": "At admission time the RuntimeClass [admission controller](/docs/reference/access-authn-authz/admission-controllers/)\nupdates the workload's PodSpec to include the `overhead` as described in the RuntimeClass. If the PodSpec already has this field defined,\nthe Pod will be rejected. In the given example, since only the RuntimeClass name is specified, the admission controller mutates the Pod\nto include an `overhead`.", "zh": "在准入阶段 RuntimeClass [准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/)\n更新工作负载的 PodSpec 以包含\nRuntimeClass 中定义的 `overhead`。如果 PodSpec 中已定义该字段，该 Pod 将会被拒绝。\n在这个例子中，由于只指定了 RuntimeClass 名称，所以准入控制器更新了 Pod，使之包含 `overhead`。"}
{"en": "After the RuntimeClass admission controller has made modifications, you can check the updated\nPod overhead value:", "zh": "在 RuntimeClass 准入控制器进行修改后，你可以查看更新后的 Pod 开销值：\n```bash\nkubectl get pod test-pod -o jsonpath='{.spec.overhead}'\n```"}
{"en": "The output is:", "zh": "输出：\n```\nmap[cpu:250m memory:120Mi]\n```"}
{"en": "If a [ResourceQuota](/docs/concepts/policy/resource-quotas/) is defined, the sum of container requests as well as the\n`overhead` field are counted.", "zh": "如果定义了 [ResourceQuota](/zh-cn/docs/concepts/policy/resource-quotas/), \n则容器请求的总量以及 `overhead` 字段都将计算在内。"}
{"en": "When the kube-scheduler is deciding which node should run a new Pod, the scheduler considers that Pod's\n`overhead` as well as the sum of container requests for that Pod. For this example, the scheduler adds the\nrequests and the overhead, then looks for a node that has 2.25 CPU and 320 MiB of memory available.", "zh": "当 kube-scheduler 决定在哪一个节点调度运行新的 Pod 时，调度器会兼顾该 Pod 的\n`overhead` 以及该 Pod 的容器请求总量。在这个示例中，调度器将资源请求和开销相加，\n然后寻找具备 2.25 CPU 和 320 MiB 内存可用的节点。"}
{"en": "Once a Pod is scheduled to a node, the kubelet on that node creates a new {{< glossary_tooltip\ntext=\"cgroup\" term_id=\"cgroup\" >}} for the Pod. It is within this pod that the underlying\ncontainer runtime will create containers.", "zh": "一旦 Pod 被调度到了某个节点， 该节点上的 kubelet 将为该 Pod 新建一个 \n{{< glossary_tooltip text=\"cgroup\" term_id=\"cgroup\" >}}。 底层容器运行时将在这个\nPod 中创建容器。"}
{"en": "If the resource has a limit defined for each container (Guaranteed QoS or Burstable QoS with limits defined),\nthe kubelet will set an upper limit for the pod cgroup associated with that resource (cpu.cfs_quota_us for CPU\nand memory.limit_in_bytes memory). This upper limit is based on the sum of the container limits plus the `overhead`\ndefined in the PodSpec.", "zh": "如果该资源对每一个容器都定义了一个限制（定义了限制值的 Guaranteed QoS 或者\nBurstable QoS），kubelet 会为与该资源（CPU 的 `cpu.cfs_quota_us` 以及内存的\n`memory.limit_in_bytes`）\n相关的 Pod cgroup 设定一个上限。该上限基于 PodSpec 中定义的容器限制总量与 `overhead` 之和。"}
{"en": "For CPU, if the Pod is Guaranteed or Burstable QoS, the kubelet will set `cpu.shares` based on the\nsum of container requests plus the `overhead` defined in the PodSpec.", "zh": "对于 CPU，如果 Pod 的 QoS 是 Guaranteed 或者 Burstable，kubelet 会基于容器请求总量与\nPodSpec 中定义的 `overhead` 之和设置 `cpu.shares`。"}
{"en": "Looking at our example, verify the container requests for the workload:", "zh": "请看这个例子，验证工作负载的容器请求：\n\n```bash\nkubectl get pod test-pod -o jsonpath='{.spec.containers[*].resources.limits}'\n```"}
{"en": "The total container requests are 2000m CPU and 200MiB of memory:", "zh": "容器请求总计 2000m CPU 和 200MiB 内存：\n\n```\nmap[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]\n```"}
{"en": "Check this against what is observed by the node:", "zh": "对照从节点观察到的情况来检查一下：\n\n```bash\nkubectl describe node | grep test-pod -B2\n```"}
{"en": "The output shows requests for 2250m CPU, and for 320MiB of memory. The requests include Pod overhead:", "zh": "该输出显示请求了 2250m CPU 以及 320MiB 内存。请求包含了 Pod 开销在内：\n```\n  Namespace    Name       CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE\n  ---------    ----       ------------  ----------   ---------------  -------------  ---\n  default      test-pod   2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m\n```"}
{"en": "## Verify Pod cgroup limits", "zh": "## 验证 Pod cgroup 限制 {#verify-pod-cgroup-limits}"}
{"en": "Check the Pod's memory cgroups on the node where the workload is running. In the following example,\n[`crictl`](https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md)\nis used on the node, which provides a CLI for CRI-compatible container runtimes. This is an\nadvanced example to show Pod overhead behavior, and it is not expected that users should need to check\ncgroups directly on the node.\n\nFirst, on the particular node, determine the Pod identifier:", "zh": "在工作负载所运行的节点上检查 Pod 的内存 cgroups。在接下来的例子中，\n将在该节点上使用具备 CRI 兼容的容器运行时命令行工具\n[`crictl`](https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md)。\n这是一个显示 Pod 开销行为的高级示例， 预计用户不需要直接在节点上检查 cgroups。\n首先在特定的节点上确定该 Pod 的标识符："}
{"en": "```bash\n# Run this on the node where the Pod is scheduled", "zh": "```bash\n# 在该 Pod 被调度到的节点上执行如下命令：\nPOD_ID=\"$(sudo crictl pods --name test-pod -q)\"\n```"}
{"en": "From this, you can determine the cgroup path for the Pod:", "zh": "可以依此判断该 Pod 的 cgroup 路径："}
{"en": "```bash\n# Run this on the node where the Pod is scheduled", "zh": "```bash\n# 在该 Pod 被调度到的节点上执行如下命令：\nsudo crictl inspectp -o=json $POD_ID | grep cgroupsPath\n```"}
{"en": "The resulting cgroup path includes the Pod's `pause` container. The Pod level cgroup is one directory above.", "zh": "执行结果的 cgroup 路径中包含了该 Pod 的 `pause` 容器。Pod 级别的 cgroup 在即上一层目录。\n\n```\n  \"cgroupsPath\": \"/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a\"\n```"}
{"en": "In this specific case, the pod cgroup path is `kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2`.\nVerify the Pod level cgroup setting for memory:", "zh": "在这个例子中，该 Pod 的 cgroup 路径是 `kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2`。\n验证内存的 Pod 级别 cgroup 设置："}
{"en": "```bash\n# Run this on the node where the Pod is scheduled.\n# Also, change the name of the cgroup to match the cgroup allocated for your pod.", "zh": "```bash\n# 在该 Pod 被调度到的节点上执行这个命令。\n# 另外，修改 cgroup 的名称以匹配为该 Pod 分配的 cgroup。\n cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes\n```"}
{"en": "This is 320 MiB, as expected:", "zh": "和预期的一样，这一数值为 320 MiB。\n\n```\n335544320\n```"}
{"en": "### Observability", "zh": "### 可观察性 {#observability}"}
{"en": "Some `kube_pod_overhead_*` metrics are available in [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)\nto help identify when Pod overhead is being utilized and to help observe stability of workloads\nrunning with a defined overhead.", "zh": "在 [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) 中可以通过\n`kube_pod_overhead_*` 指标来协助确定何时使用 Pod 开销，\n以及协助观察以一个既定开销运行的工作负载的稳定性。\n该特性在 kube-state-metrics 的 1.9 发行版本中不可用，不过预计将在后续版本中发布。\n在此之前，用户需要从源代码构建 kube-state-metrics。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn more about [RuntimeClass](/docs/concepts/containers/runtime-class/)\n* Read the [PodOverhead Design](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead)\n  enhancement proposal for extra context", "zh": "* 学习更多关于 [RuntimeClass](/zh-cn/docs/concepts/containers/runtime-class/) 的信息\n* 阅读 [PodOverhead 设计](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead)增强建议以获取更多上下文"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.14\" state=\"beta\" >}}"}
{"en": "[kube-scheduler](/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler)\nis the Kubernetes default scheduler. It is responsible for placement of Pods\non Nodes in a cluster.", "zh": "作为 kubernetes 集群的默认调度器，\n[kube-scheduler](/zh-cn/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler)\n主要负责将 Pod 调度到集群的 Node 上。"}
{"en": "Nodes in a cluster that meet the scheduling requirements of a Pod are\ncalled _feasible_ Nodes for the Pod. The scheduler finds feasible Nodes\nfor a Pod and then runs a set of functions to score the feasible Nodes,\npicking a Node with the highest score among the feasible ones to run\nthe Pod. The scheduler then notifies the API server about this decision\nin a process called _Binding_.", "zh": "在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为**可调度** Node。\n调度器先在集群中找到一个 Pod 的可调度 Node，然后根据一系列函数对这些可调度 Node 打分，\n之后选出其中得分最高的 Node 来运行 Pod。\n最后，调度器将这个调度决定告知 kube-apiserver，这个过程叫做**绑定（Binding）**。"}
{"en": "This page explains performance tuning optimizations that are relevant for\nlarge Kubernetes clusters.", "zh": "这篇文章将会介绍一些在大规模 Kubernetes 集群下调度器性能优化的方式。"}
{"en": "In large clusters, you can tune the scheduler's behaviour balancing\nscheduling outcomes between latency (new Pods are placed quickly) and\naccuracy (the scheduler rarely makes poor placement decisions).\n\nYou configure this tuning setting via kube-scheduler setting\n`percentageOfNodesToScore`. This KubeSchedulerConfiguration setting determines\na threshold for scheduling nodes in your cluster.", "zh": "在大规模集群中，你可以调节调度器的表现来平衡调度的延迟（新 Pod 快速就位）\n和精度（调度器很少做出糟糕的放置决策）。\n\n你可以通过设置 kube-scheduler 的 `percentageOfNodesToScore` 来配置这个调优设置。\n这个 KubeSchedulerConfiguration 设置决定了调度集群中节点的阈值。"}
{"en": "### Setting the threshold", "zh": "### 设置阈值"}
{"en": "The `percentageOfNodesToScore` option accepts whole numeric values between 0\nand 100. The value 0 is a special number which indicates that the kube-scheduler\nshould use its compiled-in default.\nIf you set `percentageOfNodesToScore` above 100, kube-scheduler acts as if you\nhad set a value of 100.", "zh": "`percentageOfNodesToScore` 选项接受从 0 到 100 之间的整数值。\n0 值比较特殊，表示 kube-scheduler 应该使用其编译后的默认值。\n如果你设置 `percentageOfNodesToScore` 的值超过了 100，\nkube-scheduler 的表现等价于设置值为 100。"}
{"en": "To change the value, edit the\n[kube-scheduler configuration file](/docs/reference/config-api/kube-scheduler-config.v1/)\nand then restart the scheduler.\nIn many cases, the configuration file can be found at `/etc/kubernetes/config/kube-scheduler.yaml`.", "zh": "要修改这个值，先编辑\n[kube-scheduler 的配置文件](/zh-cn/docs/reference/config-api/kube-scheduler-config.v1/)然后重启调度器。\n大多数情况下，这个配置文件是 `/etc/kubernetes/config/kube-scheduler.yaml`。"}
{"en": "After you have made this change, you can run", "zh": "修改完成后，你可以执行\n\n```bash\nkubectl get pods -n kube-system | grep kube-scheduler\n```"}
{"en": "to verify that the kube-scheduler component is healthy.", "zh": "来检查该 kube-scheduler 组件是否健康。"}
{"en": "## Node scoring threshold {#percentage-of-nodes-to-score}", "zh": "## 节点打分阈值 {#percentage-of-nodes-to-score}"}
{"en": "To improve scheduling performance, the kube-scheduler can stop looking for\nfeasible nodes once it has found enough of them. In large clusters, this saves\ntime compared to a naive approach that would consider every node.", "zh": "要提升调度性能，kube-scheduler 可以在找到足够的可调度节点之后停止查找。\n在大规模集群中，比起考虑每个节点的简单方法相比可以节省时间。"}
{"en": "You specify a threshold for how many nodes are enough, as a whole number percentage\nof all the nodes in your cluster. The kube-scheduler converts this into an\ninteger number of nodes. During scheduling, if the kube-scheduler has identified\nenough feasible nodes to exceed the configured percentage, the kube-scheduler\nstops searching for more feasible nodes and moves on to the\n[scoring phase](/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation).", "zh": "你可以使用整个集群节点总数的百分比作为阈值来指定需要多少节点就足够。\nkube-scheduler 会将它转换为节点数的整数值。在调度期间，如果\nkube-scheduler 已确认的可调度节点数足以超过了配置的百分比数量，\nkube-scheduler 将停止继续查找可调度节点并继续进行\n[打分阶段](/zh-cn/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation)。"}
{"en": "[How the scheduler iterates over Nodes](#how-the-scheduler-iterates-over-nodes)\ndescribes the process in detail.", "zh": "[调度器如何遍历节点](#how-the-scheduler-iterates-over-nodes)详细介绍了这个过程。"}
{"en": "### Default threshold", "zh": "### 默认阈值"}
{"en": "If you don't specify a threshold, Kubernetes calculates a figure using a\nlinear formula that yields 50% for a 100-node cluster and yields 10%\nfor a 5000-node cluster. The lower bound for the automatic value is 5%.", "zh": "如果你不指定阈值，Kubernetes 使用线性公式计算出一个比例，在 100-节点集群\n下取 50%，在 5000-节点的集群下取 10%。这个自动设置的参数的最低值是 5%。"}
{"en": "This means that the kube-scheduler always scores at least 5% of your cluster no\nmatter how large the cluster is, unless you have explicitly set\n`percentageOfNodesToScore` to be smaller than 5.", "zh": "这意味着，调度器至少会对集群中 5% 的节点进行打分，除非用户将该参数设置的低于 5。"}
{"en": "If you want the scheduler to score all nodes in your cluster, set\n`percentageOfNodesToScore` to 100.", "zh": "如果你想让调度器对集群内所有节点进行打分，则将 `percentageOfNodesToScore` 设置为 100。"}
{"en": "## Example", "zh": "## 示例"}
{"en": "Below is an example configuration that sets `percentageOfNodesToScore` to 50%.", "zh": "下面就是一个将 `percentageOfNodesToScore` 参数设置为 50% 的例子。\n\n```yaml\napiVersion: kubescheduler.config.k8s.io/v1alpha1\nkind: KubeSchedulerConfiguration\nalgorithmSource:\n  provider: DefaultProvider\n\n...\n\npercentageOfNodesToScore: 50\n```"}
{"en": "## Tuning percentageOfNodesToScore", "zh": "## 调节 percentageOfNodesToScore 参数"}
{"en": "`percentageOfNodesToScore` must be a value between 1 and 100 with the default\nvalue being calculated based on the cluster size. There is also a hardcoded\nminimum value of 100 nodes.", "zh": "`percentageOfNodesToScore` 的值必须在 1 到 100 之间，而且其默认值是通过集群的规模计算得来的。\n另外，还有一个 100 个 Node 的最小值是硬编码在程序中。"}
{"en": "{{< note >}} In clusters with less than 100 feasible nodes, the scheduler still\nchecks all the nodes because there are not enough feasible nodes to stop\nthe scheduler's search early.\n\nIn a small cluster, if you set a low value for `percentageOfNodesToScore`, your\nchange will have no or little effect, for a similar reason.\n\nIf your cluster has several hundred Nodes or fewer, leave this configuration option\nat its default value. Making changes is unlikely to improve the\nscheduler's performance significantly.\n{{< /note >}}", "zh": "{{< note >}}\n当集群中的可调度节点少于 100 个时，调度器仍然会去检查所有的 Node，\n因为可调度节点太少，不足以停止调度器最初的过滤选择。\n\n同理，在小规模集群中，如果你将 `percentageOfNodesToScore`\n设置为一个较低的值，则没有或者只有很小的效果。\n\n如果集群只有几百个节点或者更少，请保持这个配置的默认值。\n改变基本不会对调度器的性能有明显的提升。\n{{< /note >}}"}
{"en": "An important detail to consider when setting this value is that when a smaller\nnumber of nodes in a cluster are checked for feasibility, some nodes are not\nsent to be scored for a given Pod. As a result, a Node which could possibly\nscore a higher value for running the given Pod might not even be passed to the\nscoring phase. This would result in a less than ideal placement of the Pod.\n\nYou should avoid setting `percentageOfNodesToScore` very low so that kube-scheduler\ndoes not make frequent, poor Pod placement decisions. Avoid setting the\npercentage to anything below 10%, unless the scheduler's throughput is critical\nfor your application and the score of nodes is not important. In other words, you\nprefer to run the Pod on any Node as long as it is feasible.", "zh": "值得注意的是，该参数设置后可能会导致只有集群中少数节点被选为可调度节点，\n很多节点都没有进入到打分阶段。这样就会造成一种后果，\n一个本来可以在打分阶段得分很高的节点甚至都不能进入打分阶段。\n\n由于这个原因，这个参数不应该被设置成一个很低的值。\n通常的做法是不会将这个参数的值设置的低于 10。\n很低的参数值一般在调度器的吞吐量很高且对节点的打分不重要的情况下才使用。\n换句话说，只有当你更倾向于在可调度节点中任意选择一个节点来运行这个 Pod 时，\n才使用很低的参数设置。"}
{"en": "## How the scheduler iterates over Nodes", "zh": "## 调度器做调度选择的时候如何覆盖所有的 Node {#how-the-scheduler-iterates-over-nodes}"}
{"en": "This section is intended for those who want to understand the internal details\nof this feature.", "zh": "如果你想要理解这一个特性的内部细节，那么请仔细阅读这一章节。"}
{"en": "In order to give all the Nodes in a cluster a fair chance of being considered\nfor running Pods, the scheduler iterates over the nodes in a round robin\nfashion. You can imagine that Nodes are in an array. The scheduler starts from\nthe start of the array and checks feasibility of the nodes until it finds enough\nNodes as specified by `percentageOfNodesToScore`. For the next Pod, the\nscheduler continues from the point in the Node array that it stopped at when\nchecking feasibility of Nodes for the previous Pod.", "zh": "在将 Pod 调度到节点上时，为了让集群中所有节点都有公平的机会去运行这些 Pod，\n调度器将会以轮询的方式覆盖全部的 Node。\n你可以将 Node 列表想象成一个数组。调度器从数组的头部开始筛选可调度节点，\n依次向后直到可调度节点的数量达到 `percentageOfNodesToScore` 参数的要求。\n在对下一个 Pod 进行调度的时候，前一个 Pod 调度筛选停止的 Node 列表的位置，\n将会来作为这次调度筛选 Node 开始的位置。"}
{"en": "If Nodes are in multiple zones, the scheduler iterates over Nodes in various\nzones to ensure that Nodes from different zones are considered in the\nfeasibility checks. As an example, consider six nodes in two zones:", "zh": "如果集群中的 Node 在多个区域，那么调度器将从不同的区域中轮询 Node，\n来确保不同区域的 Node 接受可调度性检查。如下例，考虑两个区域中的六个节点：\n\n```\nZone 1: Node 1, Node 2, Node 3, Node 4\nZone 2: Node 5, Node 6\n```"}
{"en": "The Scheduler evaluates feasibility of the nodes in this order:", "zh": "调度器将会按照如下的顺序去评估 Node 的可调度性：\n\n```\nNode 1, Node 5, Node 2, Node 6, Node 3, Node 4\n```"}
{"en": "After going over all the Nodes, it goes back to Node 1.", "zh": "在评估完所有 Node 后，将会返回到 Node 1，从头开始。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Check the [kube-scheduler configuration reference (v1)](/docs/reference/config-api/kube-scheduler-config.v1/)", "zh": "* 参见 [kube-scheduler 配置参考（v1）](/zh-cn/docs/reference/config-api/kube-scheduler-config.v1/)"}
{"en": "In the [scheduling-plugin](/docs/reference/scheduling/config/#scheduling-plugins) `NodeResourcesFit` of kube-scheduler, there are two\nscoring strategies that support the bin packing of resources: `MostAllocated` and `RequestedToCapacityRatio`.", "zh": "在 kube-scheduler 的[调度插件](/zh-cn/docs/reference/scheduling/config/#scheduling-plugins)\n`NodeResourcesFit` 中存在两种支持资源装箱（bin packing）的策略：`MostAllocated` 和\n`RequestedToCapacityRatio`。"}
{"en": "## Enabling bin packing using MostAllocated strategy\n\nThe `MostAllocated` strategy scores the nodes based on the utilization of resources, favoring the ones with higher allocation.\nFor each resource type, you can set a weight to modify its influence in the node score.\n\nTo set the `MostAllocated` strategy for the `NodeResourcesFit` plugin, use a\n[scheduler configuration](/docs/reference/scheduling/config) similar to the following:", "zh": "## 使用 MostAllocated 策略启用资源装箱   {#enabling-bin-packing-using-mostallocated-strategy}\n\n`MostAllocated` 策略基于资源的利用率来为节点计分，优选分配比率较高的节点。\n针对每种资源类型，你可以设置一个权重值以改变其对节点得分的影响。\n\n要为插件 `NodeResourcesFit` 设置 `MostAllocated` 策略，\n可以使用一个类似于下面这样的[调度器配置](/zh-cn/docs/reference/scheduling/config/)：\n\n```yaml\napiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n- pluginConfig:\n  - args:\n      scoringStrategy:\n        resources:\n        - name: cpu\n          weight: 1\n        - name: memory\n          weight: 1\n        - name: intel.com/foo\n          weight: 3\n        - name: intel.com/bar\n          weight: 3\n        type: MostAllocated\n    name: NodeResourcesFit\n```"}
{"en": "To learn more about other parameters and their default configuration, see the API documentation for\n[`NodeResourcesFitArgs`](/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-NodeResourcesFitArgs).", "zh": "要进一步了解其它参数及其默认配置，请参阅\n[`NodeResourcesFitArgs`](/zh-cn/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-NodeResourcesFitArgs)\n的 API 文档。"}
{"en": "## Enabling bin packing using RequestedToCapacityRatio\n\nThe `RequestedToCapacityRatio` strategy allows the users to specify the resources along with weights for\neach resource to score nodes based on the request to capacity ratio. This\nallows users to bin pack extended resources by using appropriate parameters\nto improve the utilization of scarce resources in large clusters. It favors nodes according to a\nconfigured function of the allocated resources. The behavior of the `RequestedToCapacityRatio` in\nthe `NodeResourcesFit` score function can be controlled by the\n[scoringStrategy](/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-ScoringStrategy) field.\nWithin the `scoringStrategy` field, you can configure two parameters: `requestedToCapacityRatio` and\n`resources`. The `shape` in the `requestedToCapacityRatio`\nparameter allows the user to tune the function as least requested or most\nrequested based on `utilization` and `score` values. The `resources` parameter\ncomprises both the `name` of the resource to be considered during scoring and\nits corresponding `weight`, which specifies the weight of each resource.", "zh": "## 使用 RequestedToCapacityRatio 策略来启用资源装箱 {#enabling-bin-packing-using-requestedtocapacityratio}\n\n`RequestedToCapacityRatio` 策略允许用户基于请求值与容量的比率，针对参与节点计分的每类资源设置权重。\n这一策略使得用户可以使用合适的参数来对扩展资源执行装箱操作，进而提升大规模集群中稀有资源的利用率。\n此策略根据所分配资源的一个配置函数来评价节点。\n`NodeResourcesFit` 计分函数中的 `RequestedToCapacityRatio` 可以通过\n[scoringStrategy](/zh-cn/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-ScoringStrategy)\n字段来控制。在 `scoringStrategy` 字段中，你可以配置两个参数：\n`requestedToCapacityRatio` 和 `resources`。`requestedToCapacityRatio` 参数中的 `shape`\n设置使得用户能够调整函数的算法，基于 `utilization` 和 `score` 值计算最少请求或最多请求。\n`resources` 参数中包含计分过程中需要考虑的资源的 `name`，以及对应的 `weight`，\n后者指定了每个资源的权重。"}
{"en": "Below is an example configuration that sets\nthe bin packing behavior for extended resources `intel.com/foo` and `intel.com/bar`\nusing the `requestedToCapacityRatio` field.", "zh": "下面是一个配置示例，使用 `requestedToCapacityRatio` 字段为扩展资源 `intel.com/foo`\n和 `intel.com/bar` 设置装箱行为：\n\n```yaml\napiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n- pluginConfig:\n  - args:\n      scoringStrategy:\n        resources:\n        - name: intel.com/foo\n          weight: 3\n        - name: intel.com/bar\n          weight: 3\n        requestedToCapacityRatio:\n          shape:\n          - utilization: 0\n            score: 0\n          - utilization: 100\n            score: 10\n        type: RequestedToCapacityRatio\n    name: NodeResourcesFit\n```"}
{"en": "Referencing the `KubeSchedulerConfiguration` file with the kube-scheduler\nflag `--config=/path/to/config/file` will pass the configuration to the\nscheduler.", "zh": "使用 kube-scheduler 标志 `--config=/path/to/config/file` \n引用 `KubeSchedulerConfiguration` 文件，可以将配置传递给调度器。"}
{"en": "To learn more about other parameters and their default configuration, see the API documentation for\n[`NodeResourcesFitArgs`](/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-NodeResourcesFitArgs).", "zh": "要进一步了解其它参数及其默认配置，可以参阅\n[`NodeResourcesFitArgs`](/zh-cn/docs/reference/config-api/kube-scheduler-config.v1/#kubescheduler-config-k8s-io-v1-NodeResourcesFitArgs)\n的 API 文档。"}
{"en": "### Tuning the score function\n\n`shape` is used to specify the behavior of the `RequestedToCapacityRatio` function.", "zh": "### 调整计分函数    {#tuning-the-score-function}\n\n`shape` 用于指定 `RequestedToCapacityRatio` 函数的行为。\n\n```yaml\nshape:\n  - utilization: 0\n    score: 0\n  - utilization: 100\n    score: 10\n```"}
{"en": "The above arguments give the node a `score` of 0 if `utilization` is 0% and 10 for\n`utilization` 100%, thus enabling bin packing behavior. To enable least\nrequested the score value must be reversed as follows.", "zh": "上面的参数在 `utilization` 为 0% 时给节点评分为 0，在 `utilization` 为\n100% 时给节点评分为 10，因此启用了装箱行为。\n要启用最少请求（least requested）模式，必须按如下方式反转得分值。\n\n```yaml\nshape:\n  - utilization: 0\n    score: 10\n  - utilization: 100\n    score: 0\n```"}
{"en": "`resources` is an optional parameter which defaults to:", "zh": "`resources` 是一个可选参数，默认值为：\n\n```yaml\nresources:\n  - name: cpu\n    weight: 1\n  - name: memory\n    weight: 1\n```"}
{"en": "It can be used to add extended resources as follows:", "zh": "它可以像下面这样用来添加扩展资源：\n\n```yaml\nresources:\n  - name: intel.com/foo\n    weight: 5\n  - name: cpu\n    weight: 3\n  - name: memory\n    weight: 1\n```"}
{"en": "The `weight` parameter is optional and is set to 1 if not specified. Also, the\n`weight` cannot be set to a negative value.", "zh": "`weight` 参数是可选的，如果未指定，则设置为 1。\n同时，`weight` 不能设置为负值。"}
{"en": "### Node scoring for capacity allocation\n\nThis section is intended for those who want to understand the internal details\nof this feature.\nBelow is an example of how the node score is calculated for a given set of values.", "zh": "### 节点容量分配的评分   {#node-scoring-for-capacity-allocation}\n\n本节适用于希望了解此功能的内部细节的人员。\n以下是如何针对给定的一组值来计算节点得分的示例。"}
{"en": "Requested resources:", "zh": "请求的资源：\n\n```\nintel.com/foo : 2\nmemory: 256MB\ncpu: 2\n```"}
{"en": "Resource weights:", "zh": "资源权重：\n\n```\nintel.com/foo : 5\nmemory: 1\ncpu: 3\n```\n\n```\nFunctionShapePoint {{0, 0}, {100, 10}}\n```"}
{"en": "Node 1 spec:", "zh": "节点 1 配置：\n\n```\n可用：\n  intel.com/foo: 4\n  memory: 1 GB\n  cpu: 8\n\n已用：\n  intel.com/foo: 1\n  memory: 256MB\n  cpu: 1\n```"}
{"en": "Node score:", "zh": "节点得分：\n\n```\nintel.com/foo  = resourceScoringFunction((2+1),4)\n               = (100 - ((4-3)*100/4)\n               = (100 - 25)\n               = 75                       # requested + used = 75% * available\n               = rawScoringFunction(75)\n               = 7                        # floor(75/10)\n\nmemory         = resourceScoringFunction((256+256),1024)\n               = (100 -((1024-512)*100/1024))\n               = 50                       # requested + used = 50% * available\n               = rawScoringFunction(50)\n               = 5                        # floor(50/10)\n\ncpu            = resourceScoringFunction((2+1),8)\n               = (100 -((8-3)*100/8))\n               = 37.5                     # requested + used = 37.5% * available\n               = rawScoringFunction(37.5)\n               = 3                        # floor(37.5/10)\n\nNodeScore   =  ((7 * 5) + (5 * 1) + (3 * 3)) / (5 + 1 + 3)\n            =  5\n```"}
{"en": "Node 2 spec:", "zh": "节点 2 配置：\n\n```\n可用：\n  intel.com/foo: 8\n  memory: 1GB\n  cpu: 8\n\n已用：\n  intel.com/foo: 2\n  memory: 512MB\n  cpu: 6\n```"}
{"en": "Node score:", "zh": "节点得分：\n\n```\nintel.com/foo  = resourceScoringFunction((2+2),8)\n               = (100 - ((8-4)*100/8)\n               = (100 - 50)\n               = 50\n               = rawScoringFunction(50)\n               = 5\n\nmemory         = resourceScoringFunction((256+512),1024)\n               = (100 -((1024-768)*100/1024))\n               = 75\n               = rawScoringFunction(75)\n               = 7\n\ncpu            = resourceScoringFunction((2+6),8)\n               = (100 -((8-8)*100/8))\n               = 100\n               = rawScoringFunction(100)\n               = 10\n\nNodeScore   =  ((5 * 5) + (7 * 1) + (10 * 3)) / (5 + 1 + 3)\n            =  7\n```\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Read more about the [scheduling framework](/docs/concepts/scheduling-eviction/scheduling-framework/)\n- Read more about [scheduler configuration](/docs/reference/scheduling/config/)", "zh": "- 继续阅读[调度器框架](/zh-cn/docs/concepts/scheduling-eviction/scheduling-framework/)\n- 继续阅读[调度器配置](/zh-cn/docs/reference/scheduling/config/)"}
{"en": "title: API-initiated Eviction\ncontent_type: concept\nweight: 110", "zh": "{{< glossary_definition term_id=\"api-eviction\" length=\"short\" >}} </br>"}
{"en": "You can request eviction by calling the Eviction API directly, or programmatically\nusing a client of the {{<glossary_tooltip term_id=\"kube-apiserver\" text=\"API server\">}}, like the `kubectl drain` command. This\ncreates an `Eviction` object, which causes the API server to terminate the Pod.\n\nAPI-initiated evictions respect your configured [`PodDisruptionBudgets`](/docs/tasks/run-application/configure-pdb/)\nand [`terminationGracePeriodSeconds`](/docs/concepts/workloads/pods/pod-lifecycle#pod-termination).\n\nUsing the API to create an Eviction object for a Pod is like performing a\npolicy-controlled [`DELETE` operation](/docs/reference/kubernetes-api/workload-resources/pod-v1/#delete-delete-a-pod)\non the Pod.", "zh": "你可以通过直接调用 Eviction API 发起驱逐，也可以通过编程的方式使用\n{{<glossary_tooltip term_id=\"kube-apiserver\" text=\"API 服务器\">}}的客户端来发起驱逐，\n比如 `kubectl drain` 命令。\n此操作创建一个 `Eviction` 对象，该对象再驱动 API 服务器终止选定的 Pod。\n\nAPI 发起的驱逐将遵从你的\n[`PodDisruptionBudgets`](/zh-cn/docs/tasks/run-application/configure-pdb/)\n和 [`terminationGracePeriodSeconds`](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle#pod-termination)\n配置。\n\n使用 API 创建 Eviction 对象，就像对 Pod 执行策略控制的\n[`DELETE` 操作](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#delete-delete-a-pod)"}
{"en": "## Calling the Eviction API\n\nYou can use a [Kubernetes language client](/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api)\nto access the Kubernetes API and create an `Eviction` object. To do this, you\nPOST the attempted operation, similar to the following example:", "zh": "## 调用 Eviction API   {#calling-eviction-api}\n\n你可以使用 [Kubernetes 语言客户端](/zh-cn/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api)\n来访问 Kubernetes API 并创建 `Eviction` 对象。\n要执行此操作，你应该用 POST 发出要尝试的请求，类似于下面的示例：\n\n{{< tabs name=\"Eviction_example\" >}}\n{{% tab name=\"policy/v1\" %}}\n{{< note >}}"}
{"en": "`policy/v1` Eviction is available in v1.22+. Use `policy/v1beta1` with prior releases.", "zh": "`policy/v1` 版本的 Eviction 在 v1.22 以及更高的版本中可用，之前的发行版本使用 `policy/v1beta1` 版本。\n{{< /note >}}\n\n```json\n{\n  \"apiVersion\": \"policy/v1\",\n  \"kind\": \"Eviction\",\n  \"metadata\": {\n    \"name\": \"quux\",\n    \"namespace\": \"default\"\n  }\n}\n```\n{{% /tab %}}\n{{% tab name=\"policy/v1beta1\" %}}\n{{< note >}}"}
{"en": "Deprecated in v1.22 in favor of `policy/v1`", "zh": "在 v1.22 版本废弃以支持 `policy/v1`。\n{{< /note >}}\n\n```json\n{\n  \"apiVersion\": \"policy/v1beta1\",\n  \"kind\": \"Eviction\",\n  \"metadata\": {\n    \"name\": \"quux\",\n    \"namespace\": \"default\"\n  }\n}\n```\n{{% /tab %}}\n{{< /tabs >}}"}
{"en": "Alternatively, you can attempt an eviction operation by accessing the API using\n`curl` or `wget`, similar to the following example:", "zh": "或者，你可以通过使用 `curl` 或者 `wget` 来访问 API 以尝试驱逐操作，类似于以下示例：\n\n```bash\ncurl -v -H 'Content-type: application/json' https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json\n```"}
{"en": "## How API-initiated eviction works\n\nWhen you request an eviction using the API, the API server performs admission\nchecks and responds in one of the following ways:", "zh": "## API 发起驱逐的工作原理   {#how-api-initiated-eviction-works}\n\n当你使用 API 来请求驱逐时，API 服务器将执行准入检查，并通过以下方式之一做出响应："}
{"en": "* `200 OK`: the eviction is allowed, the `Eviction` subresource is created, and\n  the Pod is deleted, similar to sending a `DELETE` request to the Pod URL.\n* `429 Too Many Requests`: the eviction is not currently allowed because of the\n  configured {{<glossary_tooltip term_id=\"pod-disruption-budget\" text=\"PodDisruptionBudget\">}}.\n  You may be able to attempt the eviction again later. You might also see this\n  response because of API rate limiting.\n* `500 Internal Server Error`: the eviction is not allowed because there is a\n  misconfiguration, like if multiple PodDisruptionBudgets reference the same Pod.", "zh": "* `200 OK`：允许驱逐，子资源 `Eviction` 被创建，并且 Pod 被删除，\n  类似于发送一个 `DELETE` 请求到 Pod 地址。\n* `429 Too Many Requests`：当前不允许驱逐，因为配置了\n  {{<glossary_tooltip term_id=\"pod-disruption-budget\" text=\"PodDisruptionBudget\">}}。\n  你可以稍后再尝试驱逐。你也可能因为 API 速率限制而看到这种响应。\n* `500 Internal Server Error`：不允许驱逐，因为存在配置错误，\n  例如存在多个 PodDisruptionBudgets 引用同一个 Pod。"}
{"en": "If the Pod you want to evict isn't part of a workload that has a\nPodDisruptionBudget, the API server always returns `200 OK` and allows the\neviction.\n\nIf the API server allows the eviction, the Pod is deleted as follows:", "zh": "如果你想驱逐的 Pod 不属于有 PodDisruptionBudget 的工作负载，\nAPI 服务器总是返回 `200 OK` 并且允许驱逐。\n\n如果 API 服务器允许驱逐，Pod 按照如下方式删除："}
{"en": "1. The `Pod` resource in the API server is updated with a deletion timestamp,\n   after which the API server considers the `Pod` resource to be terminated. The\n   `Pod` resource is also marked with the configured grace period.\n1. The {{<glossary_tooltip term_id=\"kubelet\" text=\"kubelet\">}} on the node where the local Pod is running notices that the `Pod`\n   resource is marked for termination and starts to gracefully shut down the\n   local Pod.\n1. While the kubelet is shutting the Pod down, the control plane removes the Pod\n   from {{<glossary_tooltip term_id=\"endpoint\" text=\"Endpoint\">}} and\n   {{<glossary_tooltip term_id=\"endpoint-slice\" text=\"EndpointSlice\">}}\n   objects. As a result, controllers no longer consider the Pod as a valid object.\n1. After the grace period for the Pod expires, the kubelet forcefully terminates\n   the local Pod.\n1. The kubelet tells the API server to remove the `Pod` resource.\n1. The API server deletes the `Pod` resource.", "zh": "1. API 服务器中的 `Pod` 资源会更新上删除时间戳，之后 API 服务器会认为此 `Pod` 资源将被终止。\n   此 `Pod` 资源还会标记上配置的宽限期。\n1. 本地运行状态的 Pod 所处的节点上的 {{<glossary_tooltip term_id=\"kubelet\" text=\"kubelet\">}}\n   注意到 `Pod` 资源被标记为终止，并开始优雅停止本地 Pod。\n1. 当 kubelet 停止 Pod 时，控制面从 {{<glossary_tooltip term_id=\"endpoint\" text=\"Endpoint\">}}\n   和 {{<glossary_tooltip term_id=\"endpoint-slice\" text=\"EndpointSlice\">}}\n   对象中移除该 Pod。因此，控制器不再将此 Pod 视为有用对象。\n1. Pod 的宽限期到期后，kubelet 强制终止本地 Pod。\n1. kubelet 告诉 API 服务器删除 `Pod` 资源。\n1. API 服务器删除 `Pod` 资源。"}
{"en": "## Troubleshooting stuck evictions\n\nIn some cases, your applications may enter a broken state, where the Eviction\nAPI will only return `429` or `500` responses until you intervene. This can\nhappen if, for example, a ReplicaSet creates pods for your application but new\npods do not enter a `Ready` state. You may also notice this behavior in cases\nwhere the last evicted Pod had a long termination grace period.", "zh": "## 解决驱逐被卡住的问题   {#troubleshooting-stuck-evictions}\n\n在某些情况下，你的应用可能进入中断状态，\n在你干预之前，驱逐 API 总是返回 `429` 或 `500`。\n例如，如果 ReplicaSet 为你的应用程序创建了 Pod，\n但新的 Pod 没有进入 `Ready` 状态，就会发生这种情况。\n在最后一个被驱逐的 Pod 有很长的终止宽限期的情况下，你可能也会注意到这种行为。"}
{"en": "If you notice stuck evictions, try one of the following solutions:\n\n* Abort or pause the automated operation causing the issue. Investigate the stuck\n  application before you restart the operation.\n* Wait a while, then directly delete the Pod from your cluster control plane\n  instead of using the Eviction API.", "zh": "如果你注意到驱逐被卡住，请尝试以下解决方案之一：\n\n* 终止或暂停导致问题的自动化操作，重新启动操作之前，请检查被卡住的应用程序。\n* 等待一段时间后，直接从集群控制平面删除 Pod，而不是使用 Eviction API。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn how to protect your applications with a [Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).\n* Learn about [Node-pressure Eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n* Learn about [Pod Priority and Preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).", "zh": "* 了解如何使用 [Pod 干扰预算](/zh-cn/docs/tasks/run-application/configure-pdb/)保护你的应用。\n* 了解[节点压力引发的驱逐](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)。\n* 了解 [Pod 优先级和抢占](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/)。"}
{"en": "[_Node affinity_](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)\nis a property of {{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} that *attracts* them to\na set of {{< glossary_tooltip text=\"nodes\" term_id=\"node\" >}} (either as a preference or a\nhard requirement). _Taints_ are the opposite -- they allow a node to repel a set of pods.", "zh": "[节点亲和性](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)\n是 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 的一种属性，它使 Pod\n被吸引到一类特定的{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}\n（这可能出于一种偏好，也可能是硬性要求）。\n**污点（Taint）** 则相反——它使节点能够排斥一类特定的 Pod。"}
{"en": "_Tolerations_ are applied to pods. Tolerations allow the scheduler to schedule pods with matching\ntaints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler also\n[evaluates other parameters](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\nas part of its function.\n\nTaints and tolerations work together to ensure that pods are not scheduled\nonto inappropriate nodes. One or more taints are applied to a node; this\nmarks that the node should not accept any pods that do not tolerate the taints.", "zh": "**容忍度（Toleration）** 是应用于 Pod 上的。容忍度允许调度器调度带有对应污点的 Pod。\n容忍度允许调度但并不保证调度：作为其功能的一部分，\n调度器也会[评估其他参数](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/)。\n\n污点和容忍度（Toleration）相互配合，可以用来避免 Pod 被分配到不合适的节点上。\n每个节点上都可以应用一个或多个污点，这表示对于那些不能容忍这些污点的 Pod，\n是不会被该节点接受的。"}
{"en": "## Concepts\n\nYou add a taint to a node using [kubectl taint](/docs/reference/generated/kubectl/kubectl-commands#taint).\nFor example,", "zh": "## 概念  {#concepts}\n\n你可以使用命令 [kubectl taint](/docs/reference/generated/kubectl/kubectl-commands#taint)\n给节点增加一个污点。比如：\n\n```shell\nkubectl taint nodes node1 key1=value1:NoSchedule\n```"}
{"en": "places a taint on node `node1`. The taint has key `key1`, value `value1`, and taint effect `NoSchedule`.\nThis means that no pod will be able to schedule onto `node1` unless it has a matching toleration.\n\nTo remove the taint added by the command above, you can run:", "zh": "给节点 `node1` 增加一个污点，它的键名是 `key1`，键值是 `value1`，效果是 `NoSchedule`。\n这表示只有拥有和这个污点相匹配的容忍度的 Pod 才能够被分配到 `node1` 这个节点。\n\n若要移除上述命令所添加的污点，你可以执行：\n\n```shell\nkubectl taint nodes node1 key1=value1:NoSchedule-\n```"}
{"en": "You specify a toleration for a pod in the PodSpec. Both of the following tolerations \"match\" the\ntaint created by the `kubectl taint` line above, and thus a pod with either toleration would be able\nto schedule onto `node1`:", "zh": "你可以在 Pod 规约中为 Pod 设置容忍度。\n下面两个容忍度均与上面例子中使用 `kubectl taint` 命令创建的污点相匹配，\n因此如果一个 Pod 拥有其中的任何一个容忍度，都能够被调度到 `node1`：\n\n```yaml\ntolerations:\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoSchedule\"\n```\n\n```yaml\ntolerations:\n- key: \"key1\"\n  operator: \"Exists\"\n  effect: \"NoSchedule\"\n```"}
{"en": "The default Kubernetes scheduler takes taints and tolerations into account when\nselecting a node to run a particular Pod. However, if you manually specify the\n`.spec.nodeName` for a Pod, that action bypasses the scheduler; the Pod is then\nbound onto the node where you assigned it, even if there are `NoSchedule`\ntaints on that node that you selected.\nIf this happens and the node also has a `NoExecute` taint set, the kubelet will\neject the Pod unless there is an appropriate tolerance set.", "zh": "默认的 Kubernetes 调度器在选择一个节点来运行特定的 Pod 时会考虑污点和容忍度。\n然而，如果你手动为一个 Pod 指定了 `.spec.nodeName`，那么选节点操作会绕过调度器；\n这个 Pod 将会绑定到你指定的节点上，即使你选择的节点上有 `NoSchedule` 的污点。\n如果这种情况发生，且节点上还设置了 `NoExecute` 的污点，kubelet 会将 Pod 驱逐出去，除非有适当的容忍度设置。"}
{"en": "Here's an example of a pod that has some tolerations defined:", "zh": "下面是一个定义了一些容忍度的 Pod 的例子：\n\n{{% code_sample file=\"pods/pod-with-toleration.yaml\" %}}"}
{"en": "The default value for `operator` is `Equal`.", "zh": "`operator` 的默认值是 `Equal`。"}
{"en": "A toleration \"matches\" a taint if the keys are the same and the effects are the same, and:\n\n* the `operator` is `Exists` (in which case no `value` should be specified), or\n* the `operator` is `Equal` and the values should be equal.", "zh": "一个容忍度和一个污点相“匹配”是指它们有一样的键名和效果，并且：\n\n* 如果 `operator` 是 `Exists`（此时容忍度不能指定 `value`），或者\n* 如果 `operator` 是 `Equal`，则它们的值应该相等。\n\n{{< note >}}"}
{"en": "There are two special cases:\n\nIf the `key` is empty, then the `operator` must be `Exists`, which matches all keys and values. Note that the `effect` still needs to be matched at the same time.\n\nAn empty `effect` matches all effects with key `key1`.", "zh": "存在两种特殊情况：\n\n如果 `key` 为空，那么 `operator` 必须是 `Exists`，匹配所有 key 和 value。\n注意，同时 `effect` 仍然需要匹配。\n\n如果一个容忍度的 `key` 为空且 `operator` 为 `Exists`，\n表示这个容忍度与任意的 key、value 和 effect 都匹配，即这个容忍度能容忍任何污点。\n\n如果 `effect` 为空，则可以与所有键名 `key1` 的效果相匹配。\n{{< /note >}}"}
{"en": "The above example used the `effect` of `NoSchedule`. Alternatively, you can use the `effect` of `PreferNoSchedule`.", "zh": "上述例子中 `effect` 使用的值为 `NoSchedule`，你也可以使用另外一个值 `PreferNoSchedule`。"}
{"en": "The allowed values for the `effect` field are:", "zh": "`effect` 字段的允许值包括："}
{"en": "`NoExecute`\n: This affects pods that are already running on the node as follows:\n  * Pods that do not tolerate the taint are evicted immediately\n  * Pods that tolerate the taint without specifying `tolerationSeconds` in\n    their toleration specification remain bound forever\n  * Pods that tolerate the taint with a specified `tolerationSeconds` remain\n    bound for the specified amount of time. After that time elapses, the node\n    lifecycle controller evicts the Pods from the node.", "zh": "`NoExecute`\n: 这会影响已在节点上运行的 Pod，具体影响如下：\n  * 如果 Pod 不能容忍这类污点，会马上被驱逐。\n  * 如果 Pod 能够容忍这类污点，但是在容忍度定义中没有指定 `tolerationSeconds`，\n    则 Pod 还会一直在这个节点上运行。\n  * 如果 Pod 能够容忍这类污点，而且指定了 `tolerationSeconds`，\n    则 Pod 还能在这个节点上继续运行这个指定的时间长度。\n    这段时间过去后，节点生命周期控制器从节点驱除这些 Pod。"}
{"en": "`NoSchedule`\n: No new Pods will be scheduled on the tainted node unless they have a matching\n  toleration. Pods currently running on the node are **not** evicted.", "zh": "`NoSchedule`\n: 除非具有匹配的容忍度规约，否则新的 Pod 不会被调度到带有污点的节点上。\n  当前正在节点上运行的 Pod **不会**被驱逐。"}
{"en": "`PreferNoSchedule`\n: `PreferNoSchedule` is a \"preference\" or \"soft\" version of `NoSchedule`.\n  The control plane will *try* to avoid placing a Pod that does not tolerate\n  the taint on the node, but it is not guaranteed.", "zh": "`PreferNoSchedule`\n: `PreferNoSchedule` 是“偏好”或“软性”的 `NoSchedule`。\n  控制平面将**尝试**避免将不能容忍污点的 Pod 调度到的节点上，但不能保证完全避免。"}
{"en": "You can put multiple taints on the same node and multiple tolerations on the same pod.\nThe way Kubernetes processes multiple taints and tolerations is like a filter: start\nwith all of a node's taints, then ignore the ones for which the pod has a matching toleration; the\nremaining un-ignored taints have the indicated effects on the pod. In particular,", "zh": "你可以给一个节点添加多个污点，也可以给一个 Pod 添加多个容忍度设置。\nKubernetes 处理多个污点和容忍度的过程就像一个过滤器：从一个节点的所有污点开始遍历，\n过滤掉那些 Pod 中存在与之相匹配的容忍度的污点。余下未被过滤的污点的 effect 值决定了\nPod 是否会被分配到该节点。需要注意以下情况："}
{"en": "* if there is at least one un-ignored taint with effect `NoSchedule` then Kubernetes will not schedule\nthe pod onto that node\n* if there is no un-ignored taint with effect `NoSchedule` but there is at least one un-ignored taint with\neffect `PreferNoSchedule` then Kubernetes will *try* to not schedule the pod onto the node\n* if there is at least one un-ignored taint with effect `NoExecute` then the pod will be evicted from\nthe node (if it is already running on the node), and will not be\nscheduled onto the node (if it is not yet running on the node).", "zh": "* 如果未被忽略的污点中存在至少一个 effect 值为 `NoSchedule` 的污点，\n  则 Kubernetes 不会将 Pod 调度到该节点。\n* 如果未被忽略的污点中不存在 effect 值为 `NoSchedule` 的污点，\n  但是存在至少一个 effect 值为 `PreferNoSchedule` 的污点，\n  则 Kubernetes 会**尝试**不将 Pod 调度到该节点。\n* 如果未被忽略的污点中存在至少一个 effect 值为 `NoExecute` 的污点，\n  则 Kubernetes 不会将 Pod 调度到该节点（如果 Pod 还未在节点上运行），\n  并且会将 Pod 从该节点驱逐（如果 Pod 已经在节点上运行）。"}
{"en": "For example, imagine you taint a node like this", "zh": "例如，假设你给一个节点添加了如下污点：\n\n```shell\nkubectl taint nodes node1 key1=value1:NoSchedule\nkubectl taint nodes node1 key1=value1:NoExecute\nkubectl taint nodes node1 key2=value2:NoSchedule\n```"}
{"en": "And a pod has two tolerations:", "zh": "假定某个 Pod 有两个容忍度：\n\n```yaml\ntolerations:\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoSchedule\"\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoExecute\"\n```"}
{"en": "In this case, the pod will not be able to schedule onto the node, because there is no\ntoleration matching the third taint. But it will be able to continue running if it is\nalready running on the node when the taint is added, because the third taint is the only\none of the three that is not tolerated by the pod.", "zh": "在这种情况下，上述 Pod 不会被调度到上述节点，因为其没有容忍度和第三个污点相匹配。\n但是如果在给节点添加上述污点之前，该 Pod 已经在上述节点运行，\n那么它还可以继续运行在该节点上，因为第三个污点是三个污点中唯一不能被这个 Pod 容忍的。"}
{"en": "Normally, if a taint with effect `NoExecute` is added to a node, then any pods that do\nnot tolerate the taint will be evicted immediately, and pods that do tolerate the\ntaint will never be evicted. However, a toleration with `NoExecute` effect can specify\nan optional `tolerationSeconds` field that dictates how long the pod will stay bound\nto the node after the taint is added. For example,", "zh": "通常情况下，如果给一个节点添加了一个 effect 值为 `NoExecute` 的污点，\n则任何不能容忍这个污点的 Pod 都会马上被驱逐，任何可以容忍这个污点的 Pod 都不会被驱逐。\n但是，如果 Pod 存在一个 effect 值为 `NoExecute` 的容忍度指定了可选属性\n`tolerationSeconds` 的值，则表示在给节点添加了上述污点之后，\nPod 还能继续在节点上运行的时间。例如，\n\n```yaml\ntolerations:\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoExecute\"\n  tolerationSeconds: 3600\n```"}
{"en": "means that if this pod is running and a matching taint is added to the node, then\nthe pod will stay bound to the node for 3600 seconds, and then be evicted. If the\ntaint is removed before that time, the pod will not be evicted.", "zh": "这表示如果这个 Pod 正在运行，同时一个匹配的污点被添加到其所在的节点，\n那么 Pod 还将继续在节点上运行 3600 秒，然后被驱逐。\n如果在此之前上述污点被删除了，则 Pod 不会被驱逐。"}
{"en": "## Example Use Cases\n\nTaints and tolerations are a flexible way to steer pods *away* from nodes or evict\npods that shouldn't be running. A few of the use cases are", "zh": "## 使用例子  {#example-use-cases}\n\n通过污点和容忍度，可以灵活地让 Pod **避开**某些节点或者将 Pod 从某些节点驱逐。\n下面是几个使用例子："}
{"en": "* **Dedicated Nodes**: If you want to dedicate a set of nodes for exclusive use by\na particular set of users, you can add a taint to those nodes (say,\n`kubectl taint nodes nodename dedicated=groupName:NoSchedule`) and then add a corresponding\ntoleration to their pods (this would be done most easily by writing a custom\n[admission controller](/docs/reference/access-authn-authz/admission-controllers/)).\nThe pods with the tolerations will then be allowed to use the tainted (dedicated) nodes as\nwell as any other nodes in the cluster. If you want to dedicate the nodes to them *and*\nensure they *only* use the dedicated nodes, then you should additionally add a label similar\nto the taint to the same set of nodes (e.g. `dedicated=groupName`), and the admission\ncontroller should additionally add a node affinity to require that the pods can only schedule\nonto nodes labeled with `dedicated=groupName`.", "zh": "* **专用节点**：如果想将某些节点专门分配给特定的一组用户使用，你可以给这些节点添加一个污点（即，\n  `kubectl taint nodes nodename dedicated=groupName:NoSchedule`），\n  然后给这组用户的 Pod 添加一个相对应的容忍度\n  （通过编写一个自定义的[准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/)，\n  很容易就能做到）。\n  拥有上述容忍度的 Pod 就能够被调度到上述专用节点，同时也能够被调度到集群中的其它节点。\n  如果你希望这些 Pod 只能被调度到上述专用节点，\n  那么你还需要给这些专用节点另外添加一个和上述污点类似的 label（例如：`dedicated=groupName`），\n  同时还要在上述准入控制器中给 Pod 增加节点亲和性要求，要求上述 Pod 只能被调度到添加了\n  `dedicated=groupName` 标签的节点上。"}
{"en": "* **Nodes with Special Hardware**: In a cluster where a small subset of nodes have specialized\nhardware (for example GPUs), it is desirable to keep pods that don't need the specialized\nhardware off of those nodes, thus leaving room for later-arriving pods that do need the\nspecialized hardware. This can be done by tainting the nodes that have the specialized\nhardware (e.g. `kubectl taint nodes nodename special=true:NoSchedule` or\n`kubectl taint nodes nodename special=true:PreferNoSchedule`) and adding a corresponding\ntoleration to pods that use the special hardware. As in the dedicated nodes use case,\nit is probably easiest to apply the tolerations using a custom\n[admission controller](/docs/reference/access-authn-authz/admission-controllers/).\nFor example, it is recommended to use [Extended\nResources](/docs/concepts/configuration/manage-resources-containers/#extended-resources)\nto represent the special hardware, taint your special hardware nodes with the\nextended resource name and run the\n[ExtendedResourceToleration](/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration)\nadmission controller. Now, because the nodes are tainted, no pods without the\ntoleration will schedule on them. But when you submit a pod that requests the\nextended resource, the `ExtendedResourceToleration` admission controller will\nautomatically add the correct toleration to the pod and that pod will schedule\non the special hardware nodes. This will make sure that these special hardware\nnodes are dedicated for pods requesting such hardware and you don't have to\nmanually add tolerations to your pods.", "zh": "* **配备了特殊硬件的节点**：在部分节点配备了特殊硬件（比如 GPU）的集群中，\n  我们希望不需要这类硬件的 Pod 不要被调度到这些特殊节点，以便为后继需要这类硬件的 Pod 保留资源。\n  要达到这个目的，可以先给配备了特殊硬件的节点添加污点\n  （例如 `kubectl taint nodes nodename special=true:NoSchedule` 或\n  `kubectl taint nodes nodename special=true:PreferNoSchedule`），\n  然后给使用了这类特殊硬件的 Pod 添加一个相匹配的容忍度。\n  和专用节点的例子类似，添加这个容忍度的最简单的方法是使用自定义\n  [准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/)。\n  比如，我们推荐使用[扩展资源](/zh-cn/docs/concepts/configuration/manage-resources-containers/#extended-resources)\n  来表示特殊硬件，给配置了特殊硬件的节点添加污点时包含扩展资源名称，\n  然后运行一个 [ExtendedResourceToleration](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration)\n  准入控制器。此时，因为节点已经被设置污点了，没有对应容忍度的 Pod 不会被调度到这些节点。\n  但当你创建一个使用了扩展资源的 Pod 时，`ExtendedResourceToleration` 准入控制器会自动给\n  Pod 加上正确的容忍度，这样 Pod 就会被自动调度到这些配置了特殊硬件的节点上。\n  这种方式能够确保配置了特殊硬件的节点专门用于运行需要这些硬件的 Pod，\n  并且你无需手动给这些 Pod 添加容忍度。"}
{"en": "* **Taint based Evictions**: A per-pod-configurable eviction behavior\nwhen there are node problems, which is described in the next section.", "zh": "* **基于污点的驱逐**：这是在每个 Pod 中配置的在节点出现问题时的驱逐行为，\n  接下来的章节会描述这个特性。"}
{"en": "## Taint based Evictions", "zh": "## 基于污点的驱逐   {#taint-based-evictions}\n\n{{< feature-state for_k8s_version=\"v1.18\" state=\"stable\" >}}"}
{"en": "The node controller automatically taints a Node when certain conditions\nare true. The following taints are built in:\n\n * `node.kubernetes.io/not-ready`: Node is not ready. This corresponds to\n   the NodeCondition `Ready` being \"`False`\".\n * `node.kubernetes.io/unreachable`: Node is unreachable from the node\n   controller. This corresponds to the NodeCondition `Ready` being \"`Unknown`\".\n * `node.kubernetes.io/memory-pressure`: Node has memory pressure.\n * `node.kubernetes.io/disk-pressure`: Node has disk pressure.\n * `node.kubernetes.io/pid-pressure`: Node has PID pressure.\n * `node.kubernetes.io/network-unavailable`: Node's network is unavailable.\n * `node.kubernetes.io/unschedulable`: Node is unschedulable.\n * `node.cloudprovider.kubernetes.io/uninitialized`: When the kubelet is started\n    with an \"external\" cloud provider, this taint is set on a node to mark it\n    as unusable. After a controller from the cloud-controller-manager initializes\n    this node, the kubelet removes this taint.", "zh": "当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：\n\n* `node.kubernetes.io/not-ready`：节点未准备好。这相当于节点状况 `Ready` 的值为 \"`False`\"。\n* `node.kubernetes.io/unreachable`：节点控制器访问不到节点. 这相当于节点状况 `Ready`\n  的值为 \"`Unknown`\"。\n* `node.kubernetes.io/memory-pressure`：节点存在内存压力。\n* `node.kubernetes.io/disk-pressure`：节点存在磁盘压力。\n* `node.kubernetes.io/pid-pressure`：节点的 PID 压力。\n* `node.kubernetes.io/network-unavailable`：节点网络不可用。\n* `node.kubernetes.io/unschedulable`：节点不可调度。\n* `node.cloudprovider.kubernetes.io/uninitialized`：如果 kubelet 启动时指定了一个“外部”云平台驱动，\n  它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager\n  的一个控制器初始化这个节点后，kubelet 将删除这个污点。"}
{"en": "In case a node is to be drained, the node controller or the kubelet adds relevant taints\nwith `NoExecute` effect. This effect is added by default for the\n`node.kubernetes.io/not-ready` and `node.kubernetes.io/unreachable` taints.\nIf the fault condition returns to normal, the kubelet or node\ncontroller can remove the relevant taint(s).", "zh": "在节点被排空时，节点控制器或者 kubelet 会添加带有 `NoExecute` 效果的相关污点。\n此效果被默认添加到 `node.kubernetes.io/not-ready` 和 `node.kubernetes.io/unreachable` 污点中。\n如果异常状态恢复正常，kubelet 或节点控制器能够移除相关的污点。"}
{"en": "In some cases when the node is unreachable, the API server is unable to communicate\nwith the kubelet on the node. The decision to delete the pods cannot be communicated to\nthe kubelet until communication with the API server is re-established. In the meantime,\nthe pods that are scheduled for deletion may continue to run on the partitioned node.", "zh": "在某些情况下，当节点不可达时，API 服务器无法与节点上的 kubelet 进行通信。\n在与 API 服务器的通信被重新建立之前，删除 Pod 的决定无法传递到 kubelet。\n同时，被调度进行删除的那些 Pod 可能会继续运行在分区后的节点上。\n\n{{< note >}}"}
{"en": "The control plane limits the rate of adding new taints to nodes. This rate limiting\nmanages the number of evictions that are triggered when many nodes become unreachable at\nonce (for example: if there is a network disruption).", "zh": "控制面会限制向节点添加新污点的速率。这一速率限制可以管理多个节点同时不可达时\n（例如出现网络中断的情况），可能触发的驱逐的数量。\n{{< /note >}}"}
{"en": "You can specify `tolerationSeconds` for a Pod to define how long that Pod stays bound\nto a failing or unresponsive Node.", "zh": "你可以为 Pod 设置 `tolerationSeconds`，以指定当节点失效或者不响应时，\nPod 维系与该节点间绑定关系的时长。"}
{"en": "For example, you might want to keep an application with a lot of local state\nbound to node for a long time in the event of network partition, hoping\nthat the partition will recover and thus the pod eviction can be avoided.\nThe toleration you set for that Pod might look like:", "zh": "比如，你可能希望在出现网络分裂事件时，对于一个与节点本地状态有着深度绑定的应用而言，\n仍然停留在当前节点上运行一段较长的时间，以等待网络恢复以避免被驱逐。\n你为这种 Pod 所设置的容忍度看起来可能是这样：\n\n```yaml\ntolerations:\n- key: \"node.kubernetes.io/unreachable\"\n  operator: \"Exists\"\n  effect: \"NoExecute\"\n  tolerationSeconds: 6000\n```\n\n{{< note >}}"}
{"en": "Kubernetes automatically adds a toleration for\n`node.kubernetes.io/not-ready` and `node.kubernetes.io/unreachable`\nwith `tolerationSeconds=300`,\nunless you, or a controller, set those tolerations explicitly.\n\nThese automatically-added tolerations mean that Pods remain bound to\nNodes for 5 minutes after one of these problems is detected.", "zh": "Kubernetes 会自动给 Pod 添加针对 `node.kubernetes.io/not-ready` 和\n`node.kubernetes.io/unreachable` 的容忍度，且配置 `tolerationSeconds=300`，\n除非用户自身或者某控制器显式设置此容忍度。\n\n这些自动添加的容忍度意味着 Pod 可以在检测到对应的问题之一时，在 5\n分钟内保持绑定在该节点上。\n{{< /note >}}"}
{"en": "[DaemonSet](/docs/concepts/workloads/controllers/daemonset/) pods are created with\n`NoExecute` tolerations for the following taints with no `tolerationSeconds`:\n\n* `node.kubernetes.io/unreachable`\n* `node.kubernetes.io/not-ready`\n\nThis ensures that DaemonSet pods are never evicted due to these problems.", "zh": "[DaemonSet](/zh-cn/docs/concepts/workloads/controllers/daemonset/) 中的 Pod 被创建时，\n针对以下污点自动添加的 `NoExecute` 的容忍度将不会指定 `tolerationSeconds`：\n\n* `node.kubernetes.io/unreachable`\n* `node.kubernetes.io/not-ready`\n\n这保证了出现上述问题时 DaemonSet 中的 Pod 永远不会被驱逐。"}
{"en": "## Taint Nodes by Condition\n\nThe control plane, using the node {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}},\nautomatically creates taints with a `NoSchedule` effect for\n[node conditions](/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions).", "zh": "## 基于节点状态添加污点  {#taint-nodes-by-condition}\n\n控制平面使用节点{{<glossary_tooltip text=\"控制器\" term_id=\"controller\">}}自动创建\n与[节点状况](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions)\n对应的、效果为 `NoSchedule` 的污点。"}
{"en": "The scheduler checks taints, not node conditions, when it makes scheduling\ndecisions. This ensures that node conditions don't directly affect scheduling.\nFor example, if the `DiskPressure` node condition is active, the control plane\nadds the `node.kubernetes.io/disk-pressure` taint and does not schedule new pods\nonto the affected node. If the `MemoryPressure` node condition is active, the\ncontrol plane adds the `node.kubernetes.io/memory-pressure` taint.", "zh": "调度器在进行调度时检查污点，而不是检查节点状况。这确保节点状况不会直接影响调度。\n例如，如果 `DiskPressure` 节点状况处于活跃状态，则控制平面添加\n`node.kubernetes.io/disk-pressure` 污点并且不会调度新的 Pod 到受影响的节点。\n如果 `MemoryPressure` 节点状况处于活跃状态，则控制平面添加\n`node.kubernetes.io/memory-pressure` 污点。"}
{"en": "You can ignore node conditions for newly created pods by adding the corresponding\nPod tolerations. The control plane also adds the `node.kubernetes.io/memory-pressure`\ntoleration on pods that have a {{< glossary_tooltip text=\"QoS class\" term_id=\"qos-class\" >}}\nother than `BestEffort`. This is because Kubernetes treats pods in the `Guaranteed`\nor `Burstable` QoS classes (even pods with no memory request set) as if they are\nable to cope with memory pressure, while new `BestEffort` pods are not scheduled\nonto the affected node.", "zh": "对于新创建的 Pod，可以通过添加相应的 Pod 容忍度来忽略节点状况。\n控制平面还在具有除 `BestEffort` 之外的\n{{<glossary_tooltip text=\"QoS 类\" term_id=\"qos-class\" >}}的 Pod 上添加\n`node.kubernetes.io/memory-pressure` 容忍度。\n这是因为 Kubernetes 将 `Guaranteed` 或 `Burstable` QoS 类中的 Pod（甚至没有设置内存请求的 Pod）\n视为能够应对内存压力，而新创建的 `BestEffort` Pod 不会被调度到受影响的节点上。"}
{"en": "The DaemonSet controller automatically adds the following `NoSchedule`\ntolerations to all daemons, to prevent DaemonSets from breaking.\n\n  * `node.kubernetes.io/memory-pressure`\n  * `node.kubernetes.io/disk-pressure`\n  * `node.kubernetes.io/pid-pressure` (1.14 or later)\n  * `node.kubernetes.io/unschedulable` (1.10 or later)\n  * `node.kubernetes.io/network-unavailable` (*host network only*)", "zh": "DaemonSet 控制器自动为所有守护进程添加如下 `NoSchedule` 容忍度，以防 DaemonSet 崩溃：\n\n* `node.kubernetes.io/memory-pressure`\n* `node.kubernetes.io/disk-pressure`\n* `node.kubernetes.io/pid-pressure`（1.14 或更高版本）\n* `node.kubernetes.io/unschedulable`（1.10 或更高版本）\n* `node.kubernetes.io/network-unavailable`（**只适合主机网络配置**）"}
{"en": "Adding these tolerations ensures backward compatibility. You can also add\narbitrary tolerations to DaemonSets.", "zh": "添加上述容忍度确保了向后兼容，你也可以选择自由向 DaemonSet 添加容忍度。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about [Node-pressure Eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/)\n  and how you can configure it\n* Read about [Pod Priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/)", "zh": "* 阅读[节点压力驱逐](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)，\n  以及如何配置其行为\n* 阅读 [Pod 优先级](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/)"}
{"en": "You can use _topology spread constraints_ to control how\n{{< glossary_tooltip text=\"Pods\" term_id=\"Pod\" >}} are spread across your cluster\namong failure-domains such as regions, zones, nodes, and other user-defined topology\ndomains. This can help to achieve high availability as well as efficient resource\nutilization.\n\nYou can set [cluster-level constraints](#cluster-level-default-constraints) as a default,\nor configure topology spread constraints for individual workloads.", "zh": "你可以使用 **拓扑分布约束（Topology Spread Constraints）** 来控制\n{{< glossary_tooltip text=\"Pod\" term_id=\"Pod\" >}} 在集群内故障域之间的分布，\n例如区域（Region）、可用区（Zone）、节点和其他用户自定义拓扑域。\n这样做有助于实现高可用并提升资源利用率。\n\n你可以将[集群级约束](#cluster-level-default-constraints)设为默认值，或为个别工作负载配置拓扑分布约束。"}
{"en": "## Motivation\n\nImagine that you have a cluster of up to twenty nodes, and you want to run a\n{{< glossary_tooltip text=\"workload\" term_id=\"workload\" >}}\nthat automatically scales how many replicas it uses. There could be as few as\ntwo Pods or as many as fifteen.\nWhen there are only two Pods, you'd prefer not to have both of those Pods run on the\nsame node: you would run the risk that a single node failure takes your workload\noffline.\n\nIn addition to this basic usage, there are some advanced usage examples that\nenable your workloads to benefit on high availability and cluster utilization.", "zh": "## 动机 {#motivation}\n\n假设你有一个最多包含二十个节点的集群，你想要运行一个自动扩缩的\n{{< glossary_tooltip text=\"工作负载\" term_id=\"workload\" >}}，请问要使用多少个副本？\n答案可能是最少 2 个 Pod，最多 15 个 Pod。\n当只有 2 个 Pod 时，你倾向于这 2 个 Pod 不要同时在同一个节点上运行：\n你所遭遇的风险是如果放在同一个节点上且单节点出现故障，可能会让你的工作负载下线。\n\n除了这个基本的用法之外，还有一些高级的使用案例，能够让你的工作负载受益于高可用性并提高集群利用率。"}
{"en": "As you scale up and run more Pods, a different concern becomes important. Imagine\nthat you have three nodes running five Pods each. The nodes have enough capacity\nto run that many replicas; however, the clients that interact with this workload\nare split across three different datacenters (or infrastructure zones). Now you\nhave less concern about a single node failure, but you notice that latency is\nhigher than you'd like, and you are paying for network costs associated with\nsending network traffic between the different zones.\n\nYou decide that under normal operation you'd prefer to have a similar number of replicas\n[scheduled](/docs/concepts/scheduling-eviction/) into each infrastructure zone,\nand you'd like the cluster to self-heal in the case that there is a problem.\n\nPod topology spread constraints offer you a declarative way to configure that.", "zh": "随着你的工作负载扩容，运行的 Pod 变多，将需要考虑另一个重要问题。\n假设你有 3 个节点，每个节点运行 5 个 Pod。这些节点有足够的容量能够运行许多副本；\n但与这个工作负载互动的客户端分散在三个不同的数据中心（或基础设施可用区）。\n现在你可能不太关注单节点故障问题，但你会注意到延迟高于自己的预期，\n在不同的可用区之间发送网络流量会产生一些网络成本。\n\n你决定在正常运营时倾向于将类似数量的副本[调度](/zh-cn/docs/concepts/scheduling-eviction/)\n到每个基础设施可用区，且你想要该集群在遇到问题时能够自愈。\n\nPod 拓扑分布约束使你能够以声明的方式进行配置。"}
{"en": "## `topologySpreadConstraints` field\n\nThe Pod API includes a field, `spec.topologySpreadConstraints`. The usage of this field looks like\nthe following:", "zh": "## `topologySpreadConstraints` 字段   {#topologyspreadconstraints-field}\n\nPod API 包括一个 `spec.topologySpreadConstraints` 字段。这个字段的用法如下所示："}
{"en": "```yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  # Configure a topology spread constraint\n  topologySpreadConstraints:\n    - maxSkew: <integer>\n      minDomains: <integer> # optional\n      topologyKey: <string>\n      whenUnsatisfiable: <string>\n      labelSelector: <object>\n      matchLabelKeys: <list> # optional; beta since v1.27\n      nodeAffinityPolicy: [Honor|Ignore] # optional; beta since v1.26\n      nodeTaintsPolicy: [Honor|Ignore] # optional; beta since v1.26\n  ### other Pod fields go here\n```", "zh": "```yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  # 配置一个拓扑分布约束\n  topologySpreadConstraints:\n    - maxSkew: <integer>\n      minDomains: <integer> # 可选\n      topologyKey: <string>\n      whenUnsatisfiable: <string>\n      labelSelector: <object>\n      matchLabelKeys: <list> # 可选；自从 v1.27 开始成为 Beta\n      nodeAffinityPolicy: [Honor|Ignore] # 可选；自从 v1.26 开始成为 Beta\n      nodeTaintsPolicy: [Honor|Ignore] # 可选；自从 v1.26 开始成为 Beta\n  ### 其他 Pod 字段置于此处\n```"}
{"en": "You can read more about this field by running `kubectl explain Pod.spec.topologySpreadConstraints` or\nrefer to [scheduling](/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling) section of the API reference for Pod.", "zh": "你可以运行 `kubectl explain Pod.spec.topologySpreadConstraints` 或参阅 Pod API\n参考的[调度](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling)一节，\n了解有关此字段的更多信息。"}
{"en": "### Spread constraint definition\n\nYou can define one or multiple `topologySpreadConstraints` entries to instruct the\nkube-scheduler how to place each incoming Pod in relation to the existing Pods across\nyour cluster. Those fields are:", "zh": "### 分布约束定义   {#spread-constraint-definition}\n\n你可以定义一个或多个 `topologySpreadConstraints` 条目以指导 kube-scheduler\n如何将每个新来的 Pod 与跨集群的现有 Pod 相关联。这些字段包括："}
{"en": "- **maxSkew** describes the degree to which Pods may be unevenly distributed. You must\n  specify this field and the number must be greater than zero. Its semantics differ\n  according to the value of `whenUnsatisfiable`:\n\n  - if you select `whenUnsatisfiable: DoNotSchedule`, then `maxSkew` defines the\n    maximum permitted difference between the number of matching pods in the target\n    topology and the _global minimum_\n    (the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains).\n    For example, if you have 3 zones with 2, 2 and 1 matching pods respectively,\n    `MaxSkew` is set to 1 then the global minimum is 1.\n  - if you select `whenUnsatisfiable: ScheduleAnyway`, the scheduler gives higher\n    precedence to topologies that would help reduce the skew.", "zh": "- **maxSkew** 描述这些 Pod 可能被不均匀分布的程度。你必须指定此字段且该数值必须大于零。\n  其语义将随着 `whenUnsatisfiable` 的值发生变化：\n\n  - 如果你选择 `whenUnsatisfiable: DoNotSchedule`，则 `maxSkew` 定义目标拓扑中匹配 Pod\n    的数量与**全局最小值**（符合条件的域中匹配的最小 Pod 数量，如果符合条件的域数量小于 MinDomains 则为零）\n    之间的最大允许差值。例如，如果你有 3 个可用区，分别有 2、2 和 1 个匹配的 Pod，则 `MaxSkew` 设为 1，\n    且全局最小值为 1。\n  - 如果你选择 `whenUnsatisfiable: ScheduleAnyway`，则该调度器会更为偏向能够降低偏差值的拓扑域。"}
{"en": "- **minDomains** indicates a minimum number of eligible domains. This field is optional.\n  A domain is a particular instance of a topology. An eligible domain is a domain whose\n  nodes match the node selector.", "zh": "- **minDomains** 表示符合条件的域的最小数量。此字段是可选的。域是拓扑的一个特定实例。\n  符合条件的域是其节点与节点选择器匹配的域。\n\n  {{< note >}}"}
{"en": "Before Kubernetes v1.30, the `minDomains` field was only available if the\n  `MinDomainsInPodTopologySpread` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n  was enabled (default since v1.28). In older Kubernetes clusters it might be explicitly\n  disabled or the field might not be available.", "zh": "在 Kubernetes v1.30 之前，`minDomains` 字段只有在启用 `MinDomainsInPodTopologySpread`\n  [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)时才可用（自 v1.28 起默认启用）\n  在早期的 Kubernetes 集群中，此特性门控可能被显式禁用或此字段可能不可用。\n  {{< /note >}}"}
{"en": "- The value of `minDomains` must be greater than 0, when specified.\n    You can only specify `minDomains` in conjunction with `whenUnsatisfiable: DoNotSchedule`.\n  - When the number of eligible domains with match topology keys is less than `minDomains`,\n    Pod topology spread treats global minimum as 0, and then the calculation of `skew` is performed.\n    The global minimum is the minimum number of matching Pods in an eligible domain,\n    or zero if the number of eligible domains is less than `minDomains`.\n  - When the number of eligible domains with matching topology keys equals or is greater than\n    `minDomains`, this value has no effect on scheduling.\n  - If you do not specify `minDomains`, the constraint behaves as if `minDomains` is 1.", "zh": "- 指定的 `minDomains` 值必须大于 0。你可以结合 `whenUnsatisfiable: DoNotSchedule` 仅指定 `minDomains`。\n  - 当符合条件的、拓扑键匹配的域的数量小于 `minDomains` 时，拓扑分布将“全局最小值”（global minimum）设为 0，\n    然后进行 `skew` 计算。“全局最小值”是一个符合条件的域中匹配 Pod 的最小数量，\n    如果符合条件的域的数量小于 `minDomains`，则全局最小值为零。\n  - 当符合条件的拓扑键匹配域的个数等于或大于 `minDomains` 时，该值对调度没有影响。\n  - 如果你未指定 `minDomains`，则约束行为类似于 `minDomains` 等于 1。"}
{"en": "- **topologyKey** is the key of [node labels](#node-labels). Nodes that have a label with this key\n\tand identical values are considered to be in the same topology.\n  We call each instance of a topology (in other words, a <key, value> pair) a domain. The scheduler\n  will try to put a balanced number of pods into each domain.\n\tAlso, we define an eligible domain as a domain whose nodes meet the requirements of\n\tnodeAffinityPolicy and nodeTaintsPolicy.\n\n- **whenUnsatisfiable** indicates how to deal with a Pod if it doesn't satisfy the spread constraint:\n  - `DoNotSchedule` (default) tells the scheduler not to schedule it.\n  - `ScheduleAnyway` tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.\n\n- **labelSelector** is used to find matching Pods. Pods\n  that match this label selector are counted to determine the\n  number of Pods in their corresponding topology domain.\n  See [Label Selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors)\n  for more details.", "zh": "- **topologyKey** 是[节点标签](#node-labels)的键。如果节点使用此键标记并且具有相同的标签值，\n  则将这些节点视为处于同一拓扑域中。我们将拓扑域中（即键值对）的每个实例称为一个域。\n  调度器将尝试在每个拓扑域中放置数量均衡的 Pod。\n  另外，我们将符合条件的域定义为其节点满足 nodeAffinityPolicy 和 nodeTaintsPolicy 要求的域。\n\n- **whenUnsatisfiable** 指示如果 Pod 不满足分布约束时如何处理：\n  - `DoNotSchedule`（默认）告诉调度器不要调度。\n  - `ScheduleAnyway` 告诉调度器仍然继续调度，只是根据如何能将偏差最小化来对节点进行排序。\n\n- **labelSelector** 用于查找匹配的 Pod。匹配此标签的 Pod 将被统计，以确定相应拓扑域中 Pod 的数量。\n  有关详细信息，请参考[标签选择算符](/zh-cn/docs/concepts/overview/working-with-objects/labels/#label-selectors)。"}
{"en": "- **matchLabelKeys** is a list of pod label keys to select the pods over which\n  spreading will be calculated. The keys are used to lookup values from the pod labels,\n  those key-value labels are ANDed with `labelSelector` to select the group of existing\n  pods over which spreading will be calculated for the incoming pod. The same key is\n  forbidden to exist in both `matchLabelKeys` and `labelSelector`. `matchLabelKeys` cannot\n  be set when `labelSelector` isn't set. Keys that don't exist in the pod labels will be\n  ignored. A null or empty list means only match against the `labelSelector`.\n\n  With `matchLabelKeys`, you don't need to update the `pod.spec` between different revisions.\n  The controller/operator just needs to set different values to the same label key for different\n  revisions. The scheduler will assume the values automatically based on `matchLabelKeys`. For\n  example, if you are configuring a Deployment, you can use the label keyed with\n  [pod-template-hash](/docs/concepts/workloads/controllers/deployment/#pod-template-hash-label), which\n  is added automatically by the Deployment controller, to distinguish between different revisions\n  in a single Deployment.", "zh": "- **matchLabelKeys** 是一个 Pod 标签键的列表，用于选择需要计算分布方式的 Pod 集合。\n  这些键用于从 Pod 标签中查找值，这些键值标签与 `labelSelector` 进行逻辑与运算，以选择一组已有的 Pod，\n  通过这些 Pod 计算新来 Pod 的分布方式。`matchLabelKeys` 和 `labelSelector` 中禁止存在相同的键。\n  未设置 `labelSelector` 时无法设置 `matchLabelKeys`。Pod 标签中不存在的键将被忽略。\n  null 或空列表意味着仅与 `labelSelector` 匹配。\n\n  借助 `matchLabelKeys`，你无需在变更 Pod 修订版本时更新 `pod.spec`。\n  控制器或 Operator 只需要将不同修订版的标签键设为不同的值。\n  调度器将根据 `matchLabelKeys` 自动确定取值。例如，如果你正在配置一个 Deployment，\n  则你可以使用由 Deployment 控制器自动添加的、以\n  [pod-template-hash](/zh-cn/docs/concepts/workloads/controllers/deployment/#pod-template-hash-label)\n  为键的标签来区分同一个 Deployment 的不同修订版。\n\n  ```yaml\n      topologySpreadConstraints:\n          - maxSkew: 1\n            topologyKey: kubernetes.io/hostname\n            whenUnsatisfiable: DoNotSchedule\n            labelSelector:\n              matchLabels:\n                app: foo\n            matchLabelKeys:\n              - pod-template-hash\n  ```\n\n  {{< note >}}"}
{"en": "The `matchLabelKeys` field is a beta-level field and enabled by default in 1.27. You can disable it by disabling the\n  `MatchLabelKeysInPodTopologySpread` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).", "zh": "`matchLabelKeys` 字段是 1.27 中默认启用的一个 Beta 级别字段。\n  你可以通过禁用 `MatchLabelKeysInPodTopologySpread`\n  [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)来禁用此字段。\n  {{< /note >}}"}
{"en": "- **nodeAffinityPolicy** indicates how we will treat Pod's nodeAffinity/nodeSelector\n  when calculating pod topology spread skew. Options are:\n  - Honor: only nodes matching nodeAffinity/nodeSelector are included in the calculations.\n  - Ignore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations.\n\n  If this value is null, the behavior is equivalent to the Honor policy.", "zh": "- **nodeAffinityPolicy** 表示我们在计算 Pod 拓扑分布偏差时将如何处理 Pod 的 nodeAffinity/nodeSelector。\n  选项为：\n  - Honor：只有与 nodeAffinity/nodeSelector 匹配的节点才会包括到计算中。\n  - Ignore：nodeAffinity/nodeSelector 被忽略。所有节点均包括到计算中。\n\n  如果此值为 nil，此行为等同于 Honor 策略。\n\n  {{< note >}}"}
{"en": "The `nodeAffinityPolicy` is a beta-level field and enabled by default in 1.26. You can disable it by disabling the\n  `NodeInclusionPolicyInPodTopologySpread` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).", "zh": "`nodeAffinityPolicy` 是 1.26 中默认启用的一个 Beta 级别字段。\n  你可以通过禁用 `NodeInclusionPolicyInPodTopologySpread`\n  [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)来禁用此字段。\n  {{< /note >}}"}
{"en": "- **nodeTaintsPolicy** indicates how we will treat node taints when calculating\n  pod topology spread skew. Options are:\n  - Honor: nodes without taints, along with tainted nodes for which the incoming pod\n    has a toleration, are included.\n  - Ignore: node taints are ignored. All nodes are included.\n\n  If this value is null, the behavior is equivalent to the Ignore policy.", "zh": "- **nodeTaintsPolicy** 表示我们在计算 Pod 拓扑分布偏差时将如何处理节点污点。选项为：\n  - Honor：包括不带污点的节点以及污点被新 Pod 所容忍的节点。\n  - Ignore：节点污点被忽略。包括所有节点。\n\n  如果此值为 null，此行为等同于 Ignore 策略。\n\n  {{< note >}}"}
{"en": "The `nodeTaintsPolicy` is a beta-level field and enabled by default in 1.26. You can disable it by disabling the\n  `NodeInclusionPolicyInPodTopologySpread` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).", "zh": "`nodeTaintsPolicy` 是一个 Beta 级别字段，在 1.26 版本默认启用。\n  你可以通过禁用 `NodeInclusionPolicyInPodTopologySpread`\n  [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)来禁用此字段。\n  {{< /note >}}"}
{"en": "When a Pod defines more than one `topologySpreadConstraint`, those constraints are\ncombined using a logical AND operation: the kube-scheduler looks for a node for the incoming Pod\nthat satisfies all the configured constraints.", "zh": "当 Pod 定义了不止一个 `topologySpreadConstraint`，这些约束之间是逻辑与的关系。\nkube-scheduler 会为新的 Pod 寻找一个能够满足所有约束的节点。"}
{"en": "### Node labels\n\nTopology spread constraints rely on node labels to identify the topology\ndomain(s) that each {{< glossary_tooltip text=\"node\" term_id=\"node\" >}} is in.\nFor example, a node might have labels:", "zh": "### 节点标签 {#node-labels}\n\n拓扑分布约束依赖于节点标签来标识每个{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}所在的拓扑域。\n例如，某节点可能具有标签：\n\n```yaml\n  region: us-east-1\n  zone: us-east-1a\n```\n\n{{< note >}}"}
{"en": "For brevity, this example doesn't use the\n[well-known](/docs/reference/labels-annotations-taints/) label keys\n`topology.kubernetes.io/zone` and `topology.kubernetes.io/region`. However,\nthose registered label keys are nonetheless recommended rather than the private\n(unqualified) label keys `region` and `zone` that are used here.\n\nYou can't make a reliable assumption about the meaning of a private label key\nbetween different contexts.", "zh": "为了简便，此示例未使用[众所周知](/zh-cn/docs/reference/labels-annotations-taints/)的标签键\n`topology.kubernetes.io/zone` 和 `topology.kubernetes.io/region`。\n但是，建议使用那些已注册的标签键，而不是此处使用的私有（不合格）标签键 `region` 和 `zone`。\n\n你无法对不同上下文之间的私有标签键的含义做出可靠的假设。\n{{< /note >}}"}
{"en": "Suppose you have a 4-node cluster with the following labels:", "zh": "假设你有一个 4 节点的集群且带有以下标签：\n\n```\nNAME    STATUS   ROLES    AGE     VERSION   LABELS\nnode1   Ready    <none>   4m26s   v1.16.0   node=node1,zone=zoneA\nnode2   Ready    <none>   3m58s   v1.16.0   node=node2,zone=zoneA\nnode3   Ready    <none>   3m17s   v1.16.0   node=node3,zone=zoneB\nnode4   Ready    <none>   2m43s   v1.16.0   node=node4,zone=zoneB\n```"}
{"en": "Then the cluster is logically viewed as below:", "zh": "那么，从逻辑上看集群如下：\n\n{{<mermaid>}}\ngraph TB\n    subgraph \"zoneB\"\n        n3(Node3)\n        n4(Node4)\n    end\n    subgraph \"zoneA\"\n        n1(Node1)\n        n2(Node2)\n    end\n\n    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n    class n1,n2,n3,n4 k8s;\n    class zoneA,zoneB cluster;\n{{< /mermaid >}}"}
{"en": "## Consistency\n\nYou should set the same Pod topology spread constraints on all pods in a group.\n\nUsually, if you are using a workload controller such as a Deployment, the pod template\ntakes care of this for you. If you mix different spread constraints then Kubernetes\nfollows the API definition of the field; however, the behavior is more likely to become\nconfusing and troubleshooting is less straightforward.\n\nYou need a mechanism to ensure that all the nodes in a topology domain (such as a\ncloud provider region) are labelled consistently.\nTo avoid you needing to manually label nodes, most clusters automatically\npopulate well-known labels such as `kubernetes.io/hostname`. Check whether\nyour cluster supports this.", "zh": "## 一致性 {#Consistency}\n\n你应该为一个组中的所有 Pod 设置相同的 Pod 拓扑分布约束。\n\n通常，如果你正使用一个工作负载控制器，例如 Deployment，则 Pod 模板会帮你解决这个问题。\n如果你混合不同的分布约束，则 Kubernetes 会遵循该字段的 API 定义；\n但是，该行为可能更令人困惑，并且故障排除也没那么简单。\n\n你需要一种机制来确保拓扑域（例如云提供商区域）中的所有节点具有一致的标签。\n为了避免你需要手动为节点打标签，大多数集群会自动填充知名的标签，\n例如 `kubernetes.io/hostname`。检查你的集群是否支持此功能。"}
{"en": "## Topology spread constraint examples\n\n### Example: one topology spread constraint {#example-one-topologyspreadconstraint}\n\nSuppose you have a 4-node cluster where 3 Pods labelled `foo: bar` are located in\nnode1, node2 and node3 respectively:", "zh": "## 拓扑分布约束示例 {#topology-spread-constraint-examples}\n\n### 示例：一个拓扑分布约束 {#example-one-topologyspreadconstraint}\n\n假设你拥有一个 4 节点集群，其中标记为 `foo: bar` 的 3 个 Pod 分别位于 node1、node2 和 node3 中：\n\n{{<mermaid>}}\ngraph BT\n    subgraph \"zoneB\"\n        p3(Pod) --> n3(Node3)\n        n4(Node4)\n    end\n    subgraph \"zoneA\"\n        p1(Pod) --> n1(Node1)\n        p2(Pod) --> n2(Node2)\n    end\n\n    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n    class n1,n2,n3,n4,p1,p2,p3 k8s;\n    class zoneA,zoneB cluster;\n{{< /mermaid >}}"}
{"en": "If you want an incoming Pod to be evenly spread with existing Pods across zones, you\ncan use a manifest similar to:", "zh": "如果你希望新来的 Pod 均匀分布在现有的可用区域，则可以按如下设置其清单：\n\n{{% code_sample file=\"pods/topology-spread-constraints/one-constraint.yaml\" %}}"}
{"en": "From that manifest, `topologyKey: zone` implies the even distribution will only be applied\nto nodes that are labeled `zone: <any value>` (nodes that don't have a `zone` label\nare skipped). The field `whenUnsatisfiable: DoNotSchedule` tells the scheduler to let the\nincoming Pod stay pending if the scheduler can't find a way to satisfy the constraint.\n\nIf the scheduler placed this incoming Pod into zone `A`, the distribution of Pods would\nbecome `[3, 1]`. That means the actual skew is then 2 (calculated as `3 - 1`), which\nviolates `maxSkew: 1`. To satisfy the constraints and context for this example, the\nincoming Pod can only be placed onto a node in zone `B`:", "zh": "从此清单看，`topologyKey: zone` 意味着均匀分布将只应用于存在标签键值对为 `zone: <any value>` 的节点\n（没有 `zone` 标签的节点将被跳过）。如果调度器找不到一种方式来满足此约束，\n则 `whenUnsatisfiable: DoNotSchedule` 字段告诉该调度器将新来的 Pod 保持在 pending 状态。\n\n如果该调度器将这个新来的 Pod 放到可用区 `A`，则 Pod 的分布将成为 `[3, 1]`。\n这意味着实际偏差是 2（计算公式为 `3 - 1`），这违反了 `maxSkew: 1` 的约定。\n为了满足这个示例的约束和上下文，新来的 Pod 只能放到可用区 `B` 中的一个节点上：\n\n{{<mermaid>}}\ngraph BT\n    subgraph \"zoneB\"\n        p3(Pod) --> n3(Node3)\n        p4(mypod) --> n4(Node4)\n    end\n    subgraph \"zoneA\"\n        p1(Pod) --> n1(Node1)\n        p2(Pod) --> n2(Node2)\n    end\n\n    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n    class n1,n2,n3,n4,p1,p2,p3 k8s;\n    class p4 plain;\n    class zoneA,zoneB cluster;\n{{< /mermaid >}}\n\n或者\n\n{{<mermaid>}}\ngraph BT\n    subgraph \"zoneB\"\n        p3(Pod) --> n3(Node3)\n        p4(mypod) --> n3\n        n4(Node4)\n    end\n    subgraph \"zoneA\"\n        p1(Pod) --> n1(Node1)\n        p2(Pod) --> n2(Node2)\n    end\n\n    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n    class n1,n2,n3,n4,p1,p2,p3 k8s;\n    class p4 plain;\n    class zoneA,zoneB cluster;\n{{< /mermaid >}}"}
{"en": "You can tweak the Pod spec to meet various kinds of requirements:\n\n- Change `maxSkew` to a bigger value - such as `2` -  so that the incoming Pod can\n  be placed into zone `A` as well.\n- Change `topologyKey` to `node` so as to distribute the Pods evenly across nodes\n  instead of zones. In the above example, if `maxSkew` remains `1`, the incoming\n  Pod can only be placed onto the node `node4`.\n- Change `whenUnsatisfiable: DoNotSchedule` to `whenUnsatisfiable: ScheduleAnyway`\n  to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs\n  are satisfied). However, it's preferred to be placed into the topology domain which\n  has fewer matching Pods. (Be aware that this preference is jointly normalized\n  with other internal scheduling priorities such as resource usage ratio).", "zh": "你可以调整 Pod 规约以满足各种要求：\n\n- 将 `maxSkew` 更改为更大的值，例如 `2`，这样新来的 Pod 也可以放在可用区 `A` 中。\n- 将 `topologyKey` 更改为 `node`，以便将 Pod 均匀分布在节点上而不是可用区中。\n  在上面的例子中，如果 `maxSkew` 保持为 `1`，则新来的 Pod 只能放到 `node4` 节点上。\n- 将 `whenUnsatisfiable: DoNotSchedule` 更改为 `whenUnsatisfiable: ScheduleAnyway`，\n  以确保新来的 Pod 始终可以被调度（假设满足其他的调度 API）。但是，最好将其放置在匹配 Pod 数量较少的拓扑域中。\n  请注意，这一优先判定会与其他内部调度优先级（如资源使用率等）排序准则一起进行标准化。"}
{"en": "### Example: multiple topology spread constraints {#example-multiple-topologyspreadconstraints}\n\nThis builds upon the previous example. Suppose you have a 4-node cluster where 3\nexisting Pods labeled `foo: bar` are located on node1, node2 and node3 respectively:", "zh": "### 示例：多个拓扑分布约束 {#example-multiple-topologyspreadconstraints}\n\n下面的例子建立在前面例子的基础上。假设你拥有一个 4 节点集群，\n其中 3 个标记为 `foo: bar` 的 Pod 分别位于 node1、node2 和 node3 上：\n\n{{<mermaid>}}\ngraph BT\n    subgraph \"zoneB\"\n        p3(Pod) --> n3(Node3)\n        n4(Node4)\n    end\n    subgraph \"zoneA\"\n        p1(Pod) --> n1(Node1)\n        p2(Pod) --> n2(Node2)\n    end\n\n    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n    class n1,n2,n3,n4,p1,p2,p3 k8s;\n    class p4 plain;\n    class zoneA,zoneB cluster;\n{{< /mermaid >}}"}
{"en": "You can combine two topology spread constraints to control the spread of Pods both\nby node and by zone:", "zh": "可以组合使用 2 个拓扑分布约束来控制 Pod 在节点和可用区两个维度上的分布：\n\n{{% code_sample file=\"pods/topology-spread-constraints/two-constraints.yaml\" %}}"}
{"en": "In this case, to match the first constraint, the incoming Pod can only be placed onto\nnodes in zone `B`; while in terms of the second constraint, the incoming Pod can only be\nscheduled to the node `node4`. The scheduler only considers options that satisfy all\ndefined constraints, so the only valid placement is onto node `node4`.", "zh": "在这种情况下，为了匹配第一个约束，新的 Pod 只能放置在可用区 `B` 中；\n而在第二个约束中，新来的 Pod 只能调度到节点 `node4` 上。\n该调度器仅考虑满足所有已定义约束的选项，因此唯一可行的选择是放置在节点 `node4` 上。"}
{"en": "### Example: conflicting topology spread constraints {#example-conflicting-topologyspreadconstraints}\n\nMultiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:", "zh": "### 示例：有冲突的拓扑分布约束 {#example-conflicting-topologyspreadconstraints}\n\n多个约束可能导致冲突。假设有一个跨 2 个可用区的 3 节点集群：\n\n{{<mermaid>}}\ngraph BT\n    subgraph \"zoneB\"\n        p4(Pod) --> n3(Node3)\n        p5(Pod) --> n3\n    end\n    subgraph \"zoneA\"\n        p1(Pod) --> n1(Node1)\n        p2(Pod) --> n1\n        p3(Pod) --> n2(Node2)\n    end\n\n    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n    class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s;\n    class zoneA,zoneB cluster;\n{{< /mermaid >}}"}
{"en": "If you were to apply\n[`two-constraints.yaml`](https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml)\n(the manifest from the previous example)\nto **this** cluster, you would see that the Pod `mypod` stays in the `Pending` state.\nThis happens because: to satisfy the first constraint, the Pod `mypod` can only\nbe placed into zone `B`; while in terms of the second constraint, the Pod `mypod`\ncan only schedule to node `node2`. The intersection of the two constraints returns\nan empty set, and the scheduler cannot place the Pod.\n\nTo overcome this situation, you can either increase the value of `maxSkew` or modify\none of the constraints to use `whenUnsatisfiable: ScheduleAnyway`. Depending on\ncircumstances, you might also decide to delete an existing Pod manually - for example,\nif you are troubleshooting why a bug-fix rollout is not making progress.", "zh": "如果你将 [`two-constraints.yaml`](https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml)\n（来自上一个示例的清单）应用到**这个**集群，你将看到 Pod `mypod` 保持在 `Pending` 状态。\n出现这种情况的原因为：为了满足第一个约束，Pod `mypod` 只能放置在可用区 `B` 中；\n而在第二个约束中，Pod `mypod` 只能调度到节点 `node2` 上。\n两个约束的交集将返回一个空集，且调度器无法放置该 Pod。\n\n为了应对这种情形，你可以提高 `maxSkew` 的值或修改其中一个约束才能使用 `whenUnsatisfiable: ScheduleAnyway`。\n根据实际情形，例如若你在故障排查时发现某个漏洞修复工作毫无进展，你还可能决定手动删除一个现有的 Pod。"}
{"en": "#### Interaction with node affinity and node selectors\n\nThe scheduler will skip the non-matching nodes from the skew calculations if the\nincoming Pod has `spec.nodeSelector` or `spec.affinity.nodeAffinity` defined.", "zh": "#### 与节点亲和性和节点选择算符的相互作用 {#interaction-with-node-affinity-and-node-selectors}\n\n如果 Pod 定义了 `spec.nodeSelector` 或 `spec.affinity.nodeAffinity`，\n调度器将在偏差计算中跳过不匹配的节点。"}
{"en": "### Example: topology spread constraints with node affinity {#example-topologyspreadconstraints-with-nodeaffinity}\n\nSuppose you have a 5-node cluster ranging across zones A to C:", "zh": "### 示例：带节点亲和性的拓扑分布约束 {#example-topologyspreadconstraints-with-nodeaffinity}\n\n假设你有一个跨可用区 A 到 C 的 5 节点集群：\n\n{{<mermaid>}}\ngraph BT\n    subgraph \"zoneB\"\n        p3(Pod) --> n3(Node3)\n        n4(Node4)\n    end\n    subgraph \"zoneA\"\n        p1(Pod) --> n1(Node1)\n        p2(Pod) --> n2(Node2)\n    end\n\nclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\nclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\nclassDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\nclass n1,n2,n3,n4,p1,p2,p3 k8s;\nclass p4 plain;\nclass zoneA,zoneB cluster;\n{{< /mermaid >}}\n\n{{<mermaid>}}\ngraph BT\n    subgraph \"zoneC\"\n        n5(Node5)\n    end\n\nclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\nclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\nclassDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\nclass n5 k8s;\nclass zoneC cluster;\n{{< /mermaid >}}"}
{"en": "and you know that zone `C` must be excluded. In this case, you can compose a manifest\nas below, so that Pod `mypod` will be placed into zone `B` instead of zone `C`.\nSimilarly, Kubernetes also respects `spec.nodeSelector`.", "zh": "而且你知道可用区 `C` 必须被排除在外。在这种情况下，可以按如下方式编写清单，\n以便将 Pod `mypod` 放置在可用区 `B` 上，而不是可用区 `C` 上。\n同样，Kubernetes 也会一样处理 `spec.nodeSelector`。\n\n{{% code_sample file=\"pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml\" %}}"}
{"en": "## Implicit conventions\n\nThere are some implicit conventions worth noting here:\n\n- Only the Pods holding the same namespace as the incoming Pod can be matching candidates.\n\n- The scheduler only considers nodes that have all `topologySpreadConstraints[*].topologyKey` present at the same time.\n  Nodes missing any of these `topologyKeys` are bypassed. This implies that:\n\n  1. any Pods located on those bypassed nodes do not impact `maxSkew` calculation - in the\n     above [example](#example-conflicting-topologyspreadconstraints), suppose the node `node1`\n     does not have a label \"zone\", then the 2 Pods will\n     be disregarded, hence the incoming Pod will be scheduled into zone `A`.\n  2. the incoming Pod has no chances to be scheduled onto this kind of nodes -\n     in the above example, suppose a node `node5` has the **mistyped** label `zone-typo: zoneC`\n     (and no `zone` label set). After node `node5` joins the cluster, it will be bypassed and\n     Pods for this workload aren't scheduled there.", "zh": "## 隐式约定 {#implicit-conventions}\n\n这里有一些值得注意的隐式约定：\n\n- 只有与新来的 Pod 具有相同命名空间的 Pod 才能作为匹配候选者。\n\n- 调度器只会考虑同时具有全部 `topologySpreadConstraints[*].topologyKey` 的节点。\n  缺少任一 `topologyKey` 的节点将被忽略。这意味着：\n\n  1. 位于这些节点上的 Pod 不影响 `maxSkew` 计算，在上面的[例子](#example-conflicting-topologyspreadconstraints)中，\n     假设节点 `node1` 没有标签 \"zone\"，则 2 个 Pod 将被忽略，因此新来的\n     Pod 将被调度到可用区 `A` 中。\n  2. 新的 Pod 没有机会被调度到这类节点上。在上面的例子中，\n     假设节点 `node5` 带有**拼写错误的**标签 `zone-typo: zoneC`（且没有设置 `zone` 标签）。\n     节点 `node5` 接入集群之后，该节点将被忽略且针对该工作负载的 Pod 不会被调度到那里。"}
{"en": "- Be aware of what will happen if the incoming Pod's\n  `topologySpreadConstraints[*].labelSelector` doesn't match its own labels. In the\n  above example, if you remove the incoming Pod's labels, it can still be placed onto\n  nodes in zone `B`, since the constraints are still satisfied. However, after that\n  placement, the degree of imbalance of the cluster remains unchanged - it's still zone `A`\n  having 2 Pods labeled as `foo: bar`, and zone `B` having 1 Pod labeled as\n  `foo: bar`. If this is not what you expect, update the workload's\n  `topologySpreadConstraints[*].labelSelector` to match the labels in the pod template.", "zh": "- 注意，如果新 Pod 的 `topologySpreadConstraints[*].labelSelector` 与自身的标签不匹配，将会发生什么。\n  在上面的例子中，如果移除新 Pod 的标签，则 Pod 仍然可以放置到可用区 `B` 中的节点上，因为这些约束仍然满足。\n  然而，在放置之后，集群的不平衡程度保持不变。可用区 `A` 仍然有 2 个 Pod 带有标签 `foo: bar`，\n  而可用区 `B` 有 1 个 Pod 带有标签 `foo: bar`。如果这不是你所期望的，\n  更新工作负载的 `topologySpreadConstraints[*].labelSelector` 以匹配 Pod 模板中的标签。"}
{"en": "## Cluster-level default constraints\n\nIt is possible to set default topology spread constraints for a cluster. Default\ntopology spread constraints are applied to a Pod if, and only if:\n\n- It doesn't define any constraints in its `.spec.topologySpreadConstraints`.\n- It belongs to a Service, ReplicaSet, StatefulSet or ReplicationController.\n\nDefault constraints can be set as part of the `PodTopologySpread` plugin\narguments in a [scheduling profile](/docs/reference/scheduling/config/#profiles).\nThe constraints are specified with the same [API above](#topologyspreadconstraints-field), except that\n`labelSelector` must be empty. The selectors are calculated from the Services,\nReplicaSets, StatefulSets or ReplicationControllers that the Pod belongs to.\n\nAn example configuration might look like follows:", "zh": "## 集群级别的默认约束 {#cluster-level-default-constraints}\n\n为集群设置默认的拓扑分布约束也是可能的。默认拓扑分布约束在且仅在以下条件满足时才会被应用到 Pod 上：\n\n- Pod 没有在其 `.spec.topologySpreadConstraints` 中定义任何约束。\n- Pod 隶属于某个 Service、ReplicaSet、StatefulSet 或 ReplicationController。\n\n默认约束可以设置为[调度方案](/zh-cn/docs/reference/scheduling/config/#profiles)中\n`PodTopologySpread` 插件参数的一部分。约束的设置采用[如前所述的 API](#topologyspreadconstraints-field)，\n只是 `labelSelector` 必须为空。\n选择算符是根据 Pod 所属的 Service、ReplicaSet、StatefulSet 或 ReplicationController 来设置的。\n\n配置的示例可能看起来像下面这个样子：\n\n```yaml\napiVersion: kubescheduler.config.k8s.io/v1beta3\nkind: KubeSchedulerConfiguration\n\nprofiles:\n  - schedulerName: default-scheduler\n    pluginConfig:\n      - name: PodTopologySpread\n        args:\n          defaultConstraints:\n            - maxSkew: 1\n              topologyKey: topology.kubernetes.io/zone\n              whenUnsatisfiable: ScheduleAnyway\n          defaultingType: List\n```"}
{"en": "### Built-in default constraints {#internal-default-constraints}", "zh": "### 内置默认约束 {#internal-default-constraints}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "If you don't configure any cluster-level default constraints for pod topology spreading,\nthen kube-scheduler acts as if you specified the following default topology constraints:", "zh": "如果你没有为 Pod 拓扑分布配置任何集群级别的默认约束，\nkube-scheduler 的行为就像你指定了以下默认拓扑约束一样：\n\n```yaml\ndefaultConstraints:\n  - maxSkew: 3\n    topologyKey: \"kubernetes.io/hostname\"\n    whenUnsatisfiable: ScheduleAnyway\n  - maxSkew: 5\n    topologyKey: \"topology.kubernetes.io/zone\"\n    whenUnsatisfiable: ScheduleAnyway\n```"}
{"en": "Also, the legacy `SelectorSpread` plugin, which provides an equivalent behavior,\nis disabled by default.", "zh": "此外，原来用于提供等同行为的 `SelectorSpread` 插件默认被禁用。\n\n{{< note >}}"}
{"en": "The `PodTopologySpread` plugin does not score the nodes that don't have\nthe topology keys specified in the spreading constraints. This might result\nin a different default behavior compared to the legacy `SelectorSpread` plugin when\nusing the default topology constraints.\n\nIf your nodes are not expected to have **both** `kubernetes.io/hostname` and\n`topology.kubernetes.io/zone` labels set, define your own constraints\ninstead of using the Kubernetes defaults.", "zh": "对于分布约束中所指定的拓扑键而言，`PodTopologySpread` 插件不会为不包含这些拓扑键的节点评分。\n这可能导致在使用默认拓扑约束时，其行为与原来的 `SelectorSpread` 插件的默认行为不同。\n\n如果你的节点不会**同时**设置 `kubernetes.io/hostname` 和 `topology.kubernetes.io/zone` 标签，\n你应该定义自己的约束而不是使用 Kubernetes 的默认约束。\n{{< /note >}}"}
{"en": "If you don't want to use the default Pod spreading constraints for your cluster,\nyou can disable those defaults by setting `defaultingType` to `List` and leaving\nempty `defaultConstraints` in the `PodTopologySpread` plugin configuration:", "zh": "如果你不想为集群使用默认的 Pod 分布约束，你可以通过设置 `defaultingType` 参数为 `List`，\n并将 `PodTopologySpread` 插件配置中的 `defaultConstraints` 参数置空来禁用默认 Pod 分布约束：\n\n```yaml\napiVersion: kubescheduler.config.k8s.io/v1beta3\nkind: KubeSchedulerConfiguration\n\nprofiles:\n  - schedulerName: default-scheduler\n    pluginConfig:\n      - name: PodTopologySpread\n        args:\n          defaultConstraints: []\n          defaultingType: List\n```"}
{"en": "## Comparison with podAffinity and podAntiAffinity {#comparison-with-podaffinity-podantiaffinity}\n\nIn Kubernetes, [inter-Pod affinity and anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)\ncontrol how Pods are scheduled in relation to one another - either more packed\nor more scattered.\n\n`podAffinity`\n: attracts Pods; you can try to pack any number of Pods into qualifying\n  topology domain(s).\n\n`podAntiAffinity`\n: repels Pods. If you set this to `requiredDuringSchedulingIgnoredDuringExecution` mode then\n  only a single Pod can be scheduled into a single topology domain; if you choose\n  `preferredDuringSchedulingIgnoredDuringExecution` then you lose the ability to enforce the\n  constraint.", "zh": "## 比较 podAffinity 和 podAntiAffinity {#comparison-with-podaffinity-podantiaffinity}\n\n在 Kubernetes 中，\n[Pod 间亲和性和反亲和性](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)控制\nPod 彼此的调度方式（更密集或更分散）。\n\n`podAffinity`\n: 吸引 Pod；你可以尝试将任意数量的 Pod 集中到符合条件的拓扑域中。\n\n`podAntiAffinity`\n: 驱逐 Pod。如果将此设为 `requiredDuringSchedulingIgnoredDuringExecution` 模式，\n  则只有单个 Pod 可以调度到单个拓扑域；如果你选择 `preferredDuringSchedulingIgnoredDuringExecution`，\n  则你将丢失强制执行此约束的能力。"}
{"en": "For finer control, you can specify topology spread constraints to distribute\nPods across different topology domains - to achieve either high availability or\ncost-saving. This can also help on rolling update workloads and scaling out\nreplicas smoothly.\n\nFor more context, see the\n[Motivation](https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation)\nsection of the enhancement proposal about Pod topology spread constraints.", "zh": "要实现更细粒度的控制，你可以设置拓扑分布约束来将 Pod 分布到不同的拓扑域下，从而实现高可用性或节省成本。\n这也有助于工作负载的滚动更新和平稳地扩展副本规模。\n\n有关详细信息，请参阅有关 Pod 拓扑分布约束的增强倡议的\n[动机](https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation)一节。"}
{"en": "## Known limitations\n\n- There's no guarantee that the constraints remain satisfied when Pods are removed. For\n  example, scaling down a Deployment may result in imbalanced Pods distribution.\n\n  You can use a tool such as the [Descheduler](https://github.com/kubernetes-sigs/descheduler)\n  to rebalance the Pods distribution.\n- Pods matched on tainted nodes are respected.\n  See [Issue 80921](https://github.com/kubernetes/kubernetes/issues/80921).", "zh": "## 已知局限性 {#known-limitations}\n\n- 当 Pod 被移除时，无法保证约束仍被满足。例如，缩减某 Deployment 的规模时，Pod 的分布可能不再均衡。\n\n  你可以使用 [Descheduler](https://github.com/kubernetes-sigs/descheduler) 来重新实现 Pod 分布的均衡。\n\n- 具有污点的节点上匹配的 Pod 也会被统计。\n  参考 [Issue 80921](https://github.com/kubernetes/kubernetes/issues/80921)。"}
{"en": "- The scheduler doesn't have prior knowledge of all the zones or other topology\n  domains that a cluster has. They are determined from the existing nodes in the\n  cluster. This could lead to a problem in autoscaled clusters, when a node pool (or\n  node group) is scaled to zero nodes, and you're expecting the cluster to scale up,\n  because, in this case, those topology domains won't be considered until there is\n  at least one node in them.\n\n  You can work around this by using a cluster autoscaling tool that is aware of\n  Pod topology spread constraints and is also aware of the overall set of topology\n  domains.", "zh": "- 该调度器不会预先知道集群拥有的所有可用区和其他拓扑域。\n  拓扑域由集群中存在的节点确定。在自动扩缩的集群中，如果一个节点池（或节点组）的节点数量缩减为零，\n  而用户正期望其扩容时，可能会导致调度出现问题。\n  因为在这种情况下，调度器不会考虑这些拓扑域，直至这些拓扑域中至少包含有一个节点。\n\n  你可以通过使用感知 Pod 拓扑分布约束并感知整个拓扑域集的集群自动扩缩工具来解决此问题。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- The blog article [Introducing PodTopologySpread](/blog/2020/05/introducing-podtopologyspread/)\n  explains `maxSkew` in some detail, as well as covering some advanced usage examples.\n- Read the [scheduling](/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling) section of\n  the API reference for Pod.", "zh": "- 博客：[PodTopologySpread 介绍](/blog/2020/05/introducing-podtopologyspread/)详细解释了 `maxSkew`，\n  并给出了一些进阶的使用示例。\n- 阅读针对 Pod 的 API\n  参考的[调度](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling)一节。"}
{"en": "In Kubernetes, _scheduling_ refers to making sure that {{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}}\nare matched to {{< glossary_tooltip text=\"Nodes\" term_id=\"node\" >}} so that\n{{< glossary_tooltip term_id=\"kubelet\" >}} can run them.", "zh": "在 Kubernetes 中，**调度**是指将 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}\n放置到合适的{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}上，以便对应节点上的\n{{< glossary_tooltip term_id=\"kubelet\" >}} 能够运行这些 Pod。"}
{"en": "## Scheduling overview {#scheduling}", "zh": "## 调度概览 {#scheduling}"}
{"en": "A scheduler watches for newly created Pods that have no Node assigned. For\nevery Pod that the scheduler discovers, the scheduler becomes responsible\nfor finding the best Node for that Pod to run on. The scheduler reaches\nthis placement decision taking into account the scheduling principles\ndescribed below.", "zh": "调度器通过 Kubernetes 的监测（Watch）机制来发现集群中新创建且尚未被调度到节点上的 Pod。\n调度器会将所发现的每一个未调度的 Pod 调度到一个合适的节点上来运行。\n调度器会依据下文的调度原则来做出调度选择。"}
{"en": "If you want to understand why Pods are placed onto a particular Node,\nor if you're planning to implement a custom scheduler yourself, this\npage will help you learn about scheduling.", "zh": "如果你想要理解 Pod 为什么会被调度到特定的节点上，\n或者你想要尝试实现一个自定义的调度器，这篇文章将帮助你了解调度。"}
{"en": "## kube-scheduler", "zh": "## kube-scheduler"}
{"en": "[kube-scheduler](/docs/reference/command-line-tools-reference/kube-scheduler/)\nis the default scheduler for Kubernetes and runs as part of the\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}}.\nkube-scheduler is designed so that, if you want and need to, you can\nwrite your own scheduling component and use that instead.", "zh": "[kube-scheduler](/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/)\n是 Kubernetes 集群的默认调度器，并且是集群\n{{< glossary_tooltip text=\"控制面\" term_id=\"control-plane\" >}} 的一部分。\n如果你真得希望或者有这方面的需求，kube-scheduler\n在设计上允许你自己编写一个调度组件并替换原有的 kube-scheduler。"}
{"en": "Kube-scheduler selects an optimal node to run newly created or not yet\nscheduled (unscheduled) pods. Since containers in pods - and pods themselves -\ncan have different requirements, the scheduler filters out any nodes that\ndon't meet a Pod's specific scheduling needs. Alternatively, the API lets\nyou specify a node for a Pod when you create it, but this is unusual\nand is only done in special cases.", "zh": "Kube-scheduler 选择一个最佳节点来运行新创建的或尚未调度（unscheduled）的 Pod。\n由于 Pod 中的容器和 Pod 本身可能有不同的要求，调度程序会过滤掉任何不满足 Pod 特定调度需求的节点。\n或者，API 允许你在创建 Pod 时为它指定一个节点，但这并不常见，并且仅在特殊情况下才会这样做。"}
{"en": "In a cluster, Nodes that meet the scheduling requirements for a Pod\nare called _feasible_ nodes. If none of the nodes are suitable, the pod\nremains unscheduled until the scheduler is able to place it.", "zh": "在一个集群中，满足一个 Pod 调度请求的所有节点称之为**可调度节点**。\n如果没有任何一个节点能满足 Pod 的资源请求，\n那么这个 Pod 将一直停留在未调度状态直到调度器能够找到合适的 Node。"}
{"en": "The scheduler finds feasible Nodes for a Pod and then runs a set of\nfunctions to score the feasible Nodes and picks a Node with the highest\nscore among the feasible ones to run the Pod. The scheduler then notifies\nthe API server about this decision in a process called _binding_.", "zh": "调度器先在集群中找到一个 Pod 的所有可调度节点，然后根据一系列函数对这些可调度节点打分，\n选出其中得分最高的节点来运行 Pod。之后，调度器将这个调度决定通知给\nkube-apiserver，这个过程叫做**绑定**。"}
{"en": "Factors that need to be taken into account for scheduling decisions include\nindividual and collective resource requirements, hardware / software /\npolicy constraints, affinity and anti-affinity specifications, data\nlocality, inter-workload interference, and so on.", "zh": "在做调度决定时需要考虑的因素包括：单独和整体的资源请求、硬件/软件/策略限制、\n亲和以及反亲和要求、数据局部性、负载间的干扰等等。"}
{"en": "### Node selection in kube-scheduler {#kube-scheduler-implementation}", "zh": "### kube-scheduler 中的节点选择 {#kube-scheduler-implementation}"}
{"en": "kube-scheduler selects a node for the pod in a 2-step operation:\n\n1. Filtering\n1. Scoring", "zh": "kube-scheduler 给一个 Pod 做调度选择时包含两个步骤：\n\n1. 过滤\n2. 打分"}
{"en": "The _filtering_ step finds the set of Nodes where it's feasible to\nschedule the Pod. For example, the PodFitsResources filter checks whether a\ncandidate Node has enough available resources to meet a Pod's specific\nresource requests. After this step, the node list contains any suitable\nNodes; often, there will be more than one. If the list is empty, that\nPod isn't (yet) schedulable.", "zh": "过滤阶段会将所有满足 Pod 调度需求的节点选出来。\n例如，PodFitsResources 过滤函数会检查候选节点的可用资源能否满足 Pod 的资源请求。\n在过滤之后，得出一个节点列表，里面包含了所有可调度节点；通常情况下，\n这个节点列表包含不止一个节点。如果这个列表是空的，代表这个 Pod 不可调度。"}
{"en": "In the _scoring_ step, the scheduler ranks the remaining nodes to choose\nthe most suitable Pod placement. The scheduler assigns a score to each Node\nthat survived filtering, basing this score on the active scoring rules.", "zh": "在打分阶段，调度器会为 Pod 从所有可调度节点中选取一个最合适的节点。\n根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。"}
{"en": "Finally, kube-scheduler assigns the Pod to the Node with the highest ranking.\nIf there is more than one node with equal scores, kube-scheduler selects\none of these at random.", "zh": "最后，kube-scheduler 会将 Pod 调度到得分最高的节点上。\n如果存在多个得分最高的节点，kube-scheduler 会从中随机选取一个。"}
{"en": "There are two supported ways to configure the filtering and scoring behavior\nof the scheduler:", "zh": "支持以下两种方式配置调度器的过滤和打分行为："}
{"en": "1. [Scheduling Policies](/docs/reference/scheduling/policies) allow you to\n  configure _Predicates_ for filtering and _Priorities_ for scoring.\n1. [Scheduling Profiles](/docs/reference/scheduling/config/#profiles) allow you to\n  configure Plugins that implement different scheduling stages, including:\n  `QueueSort`, `Filter`, `Score`, `Bind`, `Reserve`, `Permit`, and others. You\n  can also configure the kube-scheduler to run different profiles.", "zh": "1. [调度策略](/zh-cn/docs/reference/scheduling/policies)\n   允许你配置过滤所用的 **断言（Predicates）** 和打分所用的 **优先级（Priorities）**。\n2. [调度配置](/zh-cn/docs/reference/scheduling/config/#profiles) 允许你配置实现不同调度阶段的插件，\n   包括：`QueueSort`、`Filter`、`Score`、`Bind`、`Reserve`、`Permit` 等等。\n   你也可以配置 kube-scheduler 运行不同的配置文件。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about [scheduler performance tuning](/docs/concepts/scheduling-eviction/scheduler-perf-tuning/)\n* Read about [Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n* Read the [reference documentation](/docs/reference/command-line-tools-reference/kube-scheduler/) for kube-scheduler\n* Read the [kube-scheduler config (v1)](/docs/reference/config-api/kube-scheduler-config.v1/) reference\n* Learn about [configuring multiple schedulers](/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)\n* Learn about [topology management policies](/docs/tasks/administer-cluster/topology-manager/)\n* Learn about [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/)\n* Learn about scheduling of Pods that use volumes in:\n  * [Volume Topology Support](/docs/concepts/storage/storage-classes/#volume-binding-mode)\n  * [Storage Capacity Tracking](/docs/concepts/storage/storage-capacity/)\n  * [Node-specific Volume Limits](/docs/concepts/storage/storage-limits/)", "zh": "* 阅读关于[调度器性能调优](/zh-cn/docs/concepts/scheduling-eviction/scheduler-perf-tuning/)\n* 阅读关于 [Pod 拓扑分布约束](/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n* 阅读关于 kube-scheduler 的[参考文档](/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/)\n* 阅读 [kube-scheduler 配置参考（v1）](/zh-cn/docs/reference/config-api/kube-scheduler-config.v1/)\n* 了解关于[配置多个调度器](/zh-cn/docs/tasks/extend-kubernetes/configure-multiple-schedulers/) 的方式\n* 了解关于[拓扑结构管理策略](/zh-cn/docs/tasks/administer-cluster/topology-manager/)\n* 了解关于 [Pod 开销](/zh-cn/docs/concepts/scheduling-eviction/pod-overhead/)\n* 了解关于如何在以下情形使用卷来调度 Pod：\n  * [卷拓扑支持](/zh-cn/docs/concepts/storage/storage-classes/#volume-binding-mode)\n  * [存储容量跟踪](/zh-cn/docs/concepts/storage/storage-capacity/)\n  * [特定于节点的卷数限制](/zh-cn/docs/concepts/storage/storage-limits/)"}
{"en": "In Kubernetes, scheduling refers to making sure that {{<glossary_tooltip text=\"Pods\" term_id=\"pod\">}}\nare matched to {{<glossary_tooltip text=\"Nodes\" term_id=\"node\">}} so that the\n{{<glossary_tooltip text=\"kubelet\" term_id=\"kubelet\">}} can run them. Preemption\nis the process of terminating Pods with lower {{<glossary_tooltip text=\"Priority\" term_id=\"pod-priority\">}}\nso that Pods with higher Priority can schedule on Nodes. Eviction is the process\nof terminating one or more Pods on Nodes.", "zh": "在 Kubernetes 中，调度（scheduling）指的是确保 {{<glossary_tooltip text=\"Pod\" term_id=\"pod\">}}\n匹配到合适的{{<glossary_tooltip text=\"节点\" term_id=\"node\">}}，\n以便 {{<glossary_tooltip text=\"kubelet\" term_id=\"kubelet\">}} 能够运行它们。\n抢占（Preemption）指的是终止低{{<glossary_tooltip text=\"优先级\" term_id=\"pod-priority\">}}的\nPod 以便高优先级的 Pod 可以调度到 Node 上的过程。\n驱逐（Eviction）是在资源匮乏的节点上，主动让一个或多个 Pod 失效的过程。"}
{"en": "## Scheduling\n\n* [Kubernetes Scheduler](/docs/concepts/scheduling-eviction/kube-scheduler/)\n* [Assigning Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/)\n* [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/)\n* [Pod Topology Spread Constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n* [Taints and Tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration/)\n* [Scheduling Framework](/docs/concepts/scheduling-eviction/scheduling-framework)\n* [Dynamic Resource Allocation](/docs/concepts/scheduling-eviction/dynamic-resource-allocation)\n* [Scheduler Performance Tuning](/docs/concepts/scheduling-eviction/scheduler-perf-tuning/)\n* [Resource Bin Packing for Extended Resources](/docs/concepts/scheduling-eviction/resource-bin-packing/)\n* [Pod Scheduling Readiness](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/)\n* [Descheduler](https://github.com/kubernetes-sigs/descheduler#descheduler-for-kubernetes)", "zh": "## 调度   {#scheduling}\n\n* [Kubernetes 调度器](/zh-cn/docs/concepts/scheduling-eviction/kube-scheduler/)\n* [将 Pod 指派到节点](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/)\n* [Pod 开销](/zh-cn/docs/concepts/scheduling-eviction/pod-overhead/)\n* [Pod 拓扑分布约束](/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n* [污点和容忍度](/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/)\n* [动态资源分配](/zh-cn/docs/concepts/scheduling-eviction/dynamic-resource-allocation)\n* [调度器性能调试](/zh-cn/docs/concepts/scheduling-eviction/scheduler-perf-tuning/)\n* [扩展资源的资源装箱](/zh-cn/docs/concepts/scheduling-eviction/resource-bin-packing/)\n* [Pod 调度就绪](/zh-cn/docs/concepts/scheduling-eviction/pod-scheduling-readiness/)\n* [Descheduler](https://github.com/kubernetes-sigs/descheduler#descheduler-for-kubernetes)"}
{"en": "## Pod Disruption\n\n* [Pod Priority and Preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\n* [Node-pressure Eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/)\n* [API-initiated Eviction](/docs/concepts/scheduling-eviction/api-eviction/)", "zh": "## Pod 干扰   {#pod-disruption}\n\n{{<glossary_definition term_id=\"pod-disruption\" length=\"all\">}}\n\n* [Pod 优先级和抢占](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/)\n* [节点压力驱逐](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)\n* [API 发起的驱逐](/zh-cn/docs/concepts/scheduling-eviction/api-eviction/)"}
{"en": "This document describes _ephemeral volumes_ in Kubernetes. Familiarity\nwith [volumes](/docs/concepts/storage/volumes/) is suggested, in\nparticular PersistentVolumeClaim and PersistentVolume.", "zh": "本文档描述 Kubernetes 中的 **临时卷（Ephemeral Volume）**。\n建议先了解[卷](/zh-cn/docs/concepts/storage/volumes/)，特别是 PersistentVolumeClaim 和 PersistentVolume。"}
{"en": "Some applications need additional storage but don't care whether that\ndata is stored persistently across restarts. For example, caching\nservices are often limited by memory size and can move infrequently\nused data into storage that is slower than memory with little impact\non overall performance.", "zh": "有些应用程序需要额外的存储，但并不关心数据在重启后是否仍然可用。\n例如，缓存服务经常受限于内存大小，而且可以将不常用的数据转移到比内存慢的存储中，对总体性能的影响并不大。"}
{"en": "Other applications expect some read-only input data to be present in\nfiles, like configuration data or secret keys.", "zh": "另有些应用程序需要以文件形式注入的只读数据，比如配置数据或密钥。"}
{"en": "_Ephemeral volumes_ are designed for these use cases. Because volumes\nfollow the Pod's lifetime and get created and deleted along with the\nPod, Pods can be stopped and restarted without being limited to where\nsome persistent volume is available.", "zh": "**临时卷** 就是为此类用例设计的。因为卷会遵从 Pod 的生命周期，与 Pod 一起创建和删除，\n所以停止和重新启动 Pod 时，不会受持久卷在何处可用的限制。"}
{"en": "Ephemeral volumes are specified _inline_ in the Pod spec, which\nsimplifies application deployment and management.", "zh": "临时卷在 Pod 规约中以 **内联** 方式定义，这简化了应用程序的部署和管理。"}
{"en": "### Types of ephemeral volumes", "zh": "### 临时卷的类型 {#types-of-ephemeral-volumes}"}
{"en": "Kubernetes supports several different kinds of ephemeral volumes for\ndifferent purposes:\n- [emptyDir](/docs/concepts/storage/volumes/#emptydir): empty at Pod startup,\n  with storage coming locally from the kubelet base directory (usually\n  the root disk) or RAM\n- [configMap](/docs/concepts/storage/volumes/#configmap),\n  [downwardAPI](/docs/concepts/storage/volumes/#downwardapi),\n  [secret](/docs/concepts/storage/volumes/#secret): inject different\n  kinds of Kubernetes data into a Pod\n- [image](/docs/concepts/storage/volumes/#image): allows mounting container image files or artifacts,\n  directly to a Pod.\n- [CSI ephemeral volumes](#csi-ephemeral-volumes):\n  similar to the previous volume kinds, but provided by special {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} drivers\n  which specifically [support this feature](https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html)\n- [generic ephemeral volumes](#generic-ephemeral-volumes), which\n  can be provided by all storage drivers that also support persistent volumes", "zh": "Kubernetes 为了不同的用途，支持几种不同类型的临时卷：\n- [emptyDir](/zh-cn/docs/concepts/storage/volumes/#emptydir)：\n  Pod 启动时为空，存储空间来自本地的 kubelet 根目录（通常是根磁盘）或内存\n- [configMap](/zh-cn/docs/concepts/storage/volumes/#configmap)、\n  [downwardAPI](/zh-cn/docs/concepts/storage/volumes/#downwardapi)、\n  [secret](/zh-cn/docs/concepts/storage/volumes/#secret)：\n  将不同类型的 Kubernetes 数据注入到 Pod 中\n- [镜像](/zh-cn/docs/concepts/storage/volumes/#image)：\n  允许将容器镜像文件或制品直接挂载到 Pod。\n- [CSI 临时卷](#csi-ephemeral-volumes)：\n  类似于前面的卷类型，但由专门[支持此特性](https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html)\n  的指定 {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} 驱动程序提供\n- [通用临时卷](#generic-ephemeral-volumes)：\n  它可以由所有支持持久卷的存储驱动程序提供"}
{"en": "`emptyDir`, `configMap`, `downwardAPI`, `secret` are provided as\n[local ephemeral\nstorage](/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage).\nThey are managed by kubelet on each node.\n\nCSI ephemeral volumes *must* be provided by third-party CSI storage\ndrivers.", "zh": "`emptyDir`、`configMap`、`downwardAPI`、`secret` 是作为\n[本地临时存储](/zh-cn/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage)\n提供的。它们由各个节点上的 kubelet 管理。\n\nCSI 临时卷 **必须** 由第三方 CSI 存储驱动程序提供。"}
{"en": "Generic ephemeral volumes *can* be provided by third-party CSI storage\ndrivers, but also by any other storage driver that supports dynamic\nprovisioning. Some CSI drivers are written specifically for CSI\nephemeral volumes and do not support dynamic provisioning: those then\ncannot be used for generic ephemeral volumes.", "zh": "通用临时卷 **可以** 由第三方 CSI 存储驱动程序提供，也可以由支持动态制备的任何其他存储驱动程序提供。\n一些专门为 CSI 临时卷编写的 CSI 驱动程序，不支持动态制备：因此这些驱动程序不能用于通用临时卷。"}
{"en": "The advantage of using third-party drivers is that they can offer\nfunctionality that Kubernetes itself does not support, for example\nstorage with different performance characteristics than the disk that\nis managed by kubelet, or injecting different data.", "zh": "使用第三方驱动程序的优势在于，它们可以提供 Kubernetes 本身不支持的功能，\n例如，与 kubelet 管理的磁盘具有不同性能特征的存储，或者用来注入不同的数据。"}
{"en": "### CSI ephemeral volumes", "zh": "### CSI 临时卷 {#csi-ephemeral-volumes}\n\n{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}"}
{"en": "CSI ephemeral volumes are only supported by a subset of CSI drivers.\nThe Kubernetes CSI [Drivers list](https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html)\nshows which drivers support ephemeral volumes.", "zh": "{{< note >}}\n只有一部分 CSI 驱动程序支持 CSI 临时卷。Kubernetes CSI\n[驱动程序列表](https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html)\n显示了支持临时卷的驱动程序。\n{{< /note >}}"}
{"en": "Conceptually, CSI ephemeral volumes are similar to `configMap`,\n`downwardAPI` and `secret` volume types: the storage is managed locally on each\n node and is created together with other local resources after a Pod has been\nscheduled onto a node. Kubernetes has no concept of rescheduling Pods\nanymore at this stage. Volume creation has to be unlikely to fail,\notherwise Pod startup gets stuck. In particular, [storage capacity\naware Pod scheduling](/docs/concepts/storage/storage-capacity/) is *not*\nsupported for these volumes. They are currently also not covered by\nthe storage resource usage limits of a Pod, because that is something\nthat kubelet can only enforce for storage that it manages itself.\n\n\nHere's an example manifest for a Pod that uses CSI ephemeral storage:", "zh": "从概念上讲，CSI 临时卷类似于 `configMap`、`downwardAPI` 和 `secret` 类型的卷：\n在各个本地节点管理卷的存储，并在 Pod 调度到节点后与其他本地资源一起创建。\n在这个阶段，Kubernetes 没有重新调度 Pod 的概念。卷创建不太可能失败，否则 Pod 启动将会受阻。\n特别是，这些卷 **不** 支持[感知存储容量的 Pod 调度](/zh-cn/docs/concepts/storage/storage-capacity/)。\n它们目前也没包括在 Pod 的存储资源使用限制中，因为 kubelet 只能对它自己管理的存储强制执行。\n\n下面是使用 CSI 临时存储的 Pod 的示例清单：\n\n```yaml\nkind: Pod\napiVersion: v1\nmetadata:\n  name: my-csi-app\nspec:\n  containers:\n    - name: my-frontend\n      image: busybox:1.28\n      volumeMounts:\n      - mountPath: \"/data\"\n        name: my-csi-inline-vol\n      command: [ \"sleep\", \"1000000\" ]\n  volumes:\n    - name: my-csi-inline-vol\n      csi:\n        driver: inline.storage.kubernetes.io\n        volumeAttributes:\n          foo: bar\n```"}
{"en": "The `volumeAttributes` determine what volume is prepared by the\ndriver. These attributes are specific to each driver and not\nstandardized. See the documentation of each CSI driver for further\ninstructions.", "zh": "`volumeAttributes` 决定驱动程序准备什么样的卷。每个驱动程序的属性不尽相同，没有实现标准化。\n有关进一步的说明，请参阅每个 CSI 驱动程序的文档。"}
{"en": "### CSI driver restrictions\n\nCSI ephemeral volumes allow users to provide `volumeAttributes`\ndirectly to the CSI driver as part of the Pod spec. A CSI driver\nallowing `volumeAttributes` that are typically restricted to\nadministrators is NOT suitable for use in an inline ephemeral volume.\nFor example, parameters that are normally defined in the StorageClass\nshould not be exposed to users through the use of inline ephemeral volumes.", "zh": "### CSI 驱动程序限制 {#csi-driver-restrictions}\n\nCSI 临时卷允许用户直接向 CSI 驱动程序提供 `volumeAttributes`，它会作为 Pod 规约的一部分。\n有些 `volumeAttributes` 通常仅限于管理员使用，允许这一类 `volumeAttributes` 的 CSI 驱动程序不适合在内联临时卷中使用。\n例如，通常在 StorageClass 中定义的参数不应通过使用内联临时卷向用户公开。"}
{"en": "Cluster administrators who need to restrict the CSI drivers that are\nallowed to be used as inline volumes within a Pod spec may do so by:\n\n- Removing `Ephemeral` from `volumeLifecycleModes` in the CSIDriver spec, which prevents the\n  driver from being used as an inline ephemeral volume.\n- Using an [admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/)\n  to restrict how this driver is used.", "zh": "如果集群管理员需要限制在 Pod 规约中作为内联卷使用的 CSI 驱动程序，可以这样做：\n\n- 从 CSIDriver 规约的 `volumeLifecycleModes` 中删除 `Ephemeral`，这可以防止驱动程序被用作内联临时卷。\n- 使用[准入 Webhook](/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/)\n  来限制如何使用此驱动程序。"}
{"en": "### Generic ephemeral volumes", "zh": "### 通用临时卷 {#generic-ephemeral-volumes}\n\n{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}"}
{"en": "Generic ephemeral volumes are similar to `emptyDir` volumes in the\nsense that they provide a per-pod directory for scratch data that is\nusually empty after provisioning. But they may also have additional\nfeatures:", "zh": "通用临时卷类似于 `emptyDir` 卷，因为它为每个 Pod 提供临时数据存放目录，\n在最初制备完毕时一般为空。不过通用临时卷也有一些额外的功能特性："}
{"en": "- Storage can be local or network-attached.\n- Volumes can have a fixed size that Pods are not able to exceed.\n- Volumes may have some initial data, depending on the driver and\n  parameters.", "zh": "- 存储可以是本地的，也可以是网络连接的。\n- 卷可以有固定的大小，Pod 不能超量使用。\n- 卷可能有一些初始数据，这取决于驱动程序和参数。"}
{"en": "- Typical operations on volumes are supported assuming that the driver\n  supports them, including\n  [snapshotting](/docs/concepts/storage/volume-snapshots/),\n  [cloning](/docs/concepts/storage/volume-pvc-datasource/),\n  [resizing](/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims),\n  and [storage capacity tracking](/docs/concepts/storage/storage-capacity/).\n\nExample:", "zh": "- 支持典型的卷操作，前提是相关的驱动程序也支持该操作，包括\n  [快照](/zh-cn/docs/concepts/storage/volume-snapshots/)、\n  [克隆](/zh-cn/docs/concepts/storage/volume-pvc-datasource/)、\n  [调整大小](/zh-cn/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims)和\n  [存储容量跟踪](/zh-cn/docs/concepts/storage/storage-capacity/)）。\n\n示例：\n\n```yaml\nkind: Pod\napiVersion: v1\nmetadata:\n  name: my-app\nspec:\n  containers:\n    - name: my-frontend\n      image: busybox:1.28\n      volumeMounts:\n      - mountPath: \"/scratch\"\n        name: scratch-volume\n      command: [ \"sleep\", \"1000000\" ]\n  volumes:\n    - name: scratch-volume\n      ephemeral:\n        volumeClaimTemplate:\n          metadata:\n            labels:\n              type: my-frontend-volume\n          spec:\n            accessModes: [ \"ReadWriteOnce\" ]\n            storageClassName: \"scratch-storage-class\"\n            resources:\n              requests:\n                storage: 1Gi\n```"}
{"en": "### Lifecycle and PersistentVolumeClaim", "zh": "### 生命周期和 PersistentVolumeClaim {#lifecycle-and-persistentvolumeclaim}"}
{"en": "The key design idea is that the\n[parameters for a volume claim](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#ephemeralvolumesource-v1-core)\nare allowed inside a volume source of the Pod. Labels, annotations and\nthe whole set of fields for a PersistentVolumeClaim are supported. When such a Pod gets\ncreated, the ephemeral volume controller then creates an actual PersistentVolumeClaim\nobject in the same namespace as the Pod and ensures that the PersistentVolumeClaim\ngets deleted when the Pod gets deleted.", "zh": "关键的设计思想是在 Pod 的卷来源中允许使用\n[卷申领的参数](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#ephemeralvolumesource-v1-core)。\nPersistentVolumeClaim 的标签、注解和整套字段集均被支持。\n创建这样一个 Pod 后，临时卷控制器在 Pod 所属的命名空间中创建一个实际的\nPersistentVolumeClaim 对象，并确保删除 Pod 时，同步删除 PersistentVolumeClaim。"}
{"en": "That triggers volume binding and/or provisioning, either immediately if\nthe {{< glossary_tooltip text=\"StorageClass\" term_id=\"storage-class\" >}} uses immediate volume binding or when the Pod is\ntentatively scheduled onto a node (`WaitForFirstConsumer` volume\nbinding mode). The latter is recommended for generic ephemeral volumes\nbecause then the scheduler is free to choose a suitable node for\nthe Pod. With immediate binding, the scheduler is forced to select a node that has\naccess to the volume once it is available.", "zh": "如上设置将触发卷的绑定与/或制备，相应动作或者在\n{{< glossary_tooltip text=\"StorageClass\" term_id=\"storage-class\" >}}\n使用即时卷绑定时立即执行，或者当 Pod 被暂时性调度到某节点时执行 (`WaitForFirstConsumer` 卷绑定模式)。\n对于通用的临时卷，建议采用后者，这样调度器就可以自由地为 Pod 选择合适的节点。\n对于即时绑定，调度器则必须选出一个节点，使得在卷可用时，能立即访问该卷。"}
{"en": "In terms of [resource ownership](/docs/concepts/architecture/garbage-collection/#owners-dependents),\na Pod that has generic ephemeral storage is the owner of the PersistentVolumeClaim(s)\nthat provide that ephemeral storage. When the Pod is deleted,\nthe Kubernetes garbage collector deletes the PVC, which then usually\ntriggers deletion of the volume because the default reclaim policy of\nstorage classes is to delete volumes. You can create quasi-ephemeral local storage\nusing a StorageClass with a reclaim policy of `retain`: the storage outlives the Pod,\nand in this case you need to ensure that volume clean up happens separately.", "zh": "就[资源所有权](/zh-cn/docs/concepts/architecture/garbage-collection/#owners-dependents)而言，\n拥有通用临时存储的 Pod 是提供临时存储 (ephemeral storage) 的 PersistentVolumeClaim 的所有者。\n当 Pod 被删除时，Kubernetes 垃圾收集器会删除 PVC，\n然后 PVC 通常会触发卷的删除，因为存储类的默认回收策略是删除卷。\n你可以使用带有 `retain` 回收策略的 StorageClass 创建准临时 (Quasi-Ephemeral) 本地存储：\n该存储比 Pod 寿命长，所以在这种情况下，你需要确保单独进行卷清理。"}
{"en": "While these PVCs exist, they can be used like any other PVC. In\nparticular, they can be referenced as data source in volume cloning or\nsnapshotting. The PVC object also holds the current status of the\nvolume.", "zh": "当这些 PVC 存在时，它们可以像其他 PVC 一样使用。\n特别是，它们可以被引用作为批量克隆或快照的数据源。\nPVC 对象还保持着卷的当前状态。"}
{"en": "### PersistentVolumeClaim naming", "zh": "### PersistentVolumeClaim 的命名 {#persistentvolumeclaim-naming}"}
{"en": "Naming of the automatically created PVCs is deterministic: the name is\na combination of the Pod name and volume name, with a hyphen (`-`) in the\nmiddle. In the example above, the PVC name will be\n`my-app-scratch-volume`.  This deterministic naming makes it easier to\ninteract with the PVC because one does not have to search for it once\nthe Pod name and volume name are known.", "zh": "自动创建的 PVC 采取确定性的命名机制：名称是 Pod 名称和卷名称的组合，中间由连字符(`-`)连接。\n在上面的示例中，PVC 将被命名为 `my-app-scratch-volume` 。\n这种确定性的命名机制使得与 PVC 交互变得更容易，因为一旦知道 Pod 名称和卷名，就不必搜索它。"}
{"en": "The deterministic naming also introduces a potential conflict between different\nPods (a Pod \"pod-a\" with volume \"scratch\" and another Pod with name\n\"pod\" and volume \"a-scratch\" both end up with the same PVC name\n\"pod-a-scratch\") and between Pods and manually created PVCs.", "zh": "这种命名机制也引入了潜在的冲突，不同的 Pod 之间（名为 “Pod-a” 的\nPod 挂载名为 \"scratch\" 的卷，和名为 \"pod\" 的 Pod 挂载名为 “a-scratch” 的卷，\n这两者均会生成名为 \"pod-a-scratch\" 的 PVC），或者在 Pod 和手工创建的\nPVC 之间可能出现冲突。"}
{"en": "Such conflicts are detected: a PVC is only used for an ephemeral\nvolume if it was created for the Pod. This check is based on the\nownership relationship. An existing PVC is not overwritten or\nmodified. But this does not resolve the conflict because without the\nright PVC, the Pod cannot start.", "zh": "这类冲突会被检测到：如果 PVC 是为 Pod 创建的，那么它只用于临时卷。\n此检测基于所有权关系。现有的 PVC 不会被覆盖或修改。\n但这并不能解决冲突，因为如果没有正确的 PVC，Pod 就无法启动。\n\n{{< caution >}}"}
{"en": "Take care when naming Pods and volumes inside the\nsame namespace, so that these conflicts can't occur.", "zh": "当同一个命名空间中命名 Pod 和卷时，要小心，以防止发生此类冲突。\n{{< /caution >}}"}
{"en": "### Security", "zh": "### 安全 {#security}"}
{"en": "Using generic ephemeral volumes allows users to create PVCs indirectly\nif they can create Pods, even if they do not have permission to create PVCs directly.\nCluster administrators must be aware of this. If this does not fit their security model,\nthey should use an [admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/)", "zh": "只要用户有权限创建 Pod，就可以使用通用的临时卷间接地创建持久卷申领（PVCs），\n即使他们没有权限直接创建 PVCs。集群管理员必须注意这一点。如果这与他们的安全模型相悖，\n他们应该使用[准入 Webhook](/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/)。"}
{"en": "The normal [namespace quota for PVCs](/docs/concepts/policy/resource-quotas/#storage-resource-quota)\nstill applies, so even if users are allowed to use this new mechanism, they cannot use\nit to circumvent other policies.", "zh": "正常的 [PVC 的名字空间配额](/zh-cn/docs/concepts/policy/resource-quotas/#storage-resource-quota)\n仍然有效，因此即使允许用户使用这种新机制，他们也不能使用它来规避其他策略。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "### Ephemeral volumes managed by kubelet\n\nSee [local ephemeral storage](/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage).", "zh": "### kubelet 管理的临时卷 {#ephemeral-volumes-managed-by-kubelet}\n\n参阅[本地临时存储](/zh-cn/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage)。"}
{"en": "### CSI ephemeral volumes\n\n- For more information on the design, see the\n  [Ephemeral Inline CSI volumes KEP](https://github.com/kubernetes/enhancements/blob/ad6021b3d61a49040a3f835e12c8bb5424db2bbb/keps/sig-storage/20190122-csi-inline-volumes.md).\n- For more information on further development of this feature, see the\n  [enhancement tracking issue #596](https://github.com/kubernetes/enhancements/issues/596).", "zh": "### CSI 临时卷 {#csi-ephemeral-volumes}\n\n- 有关设计的更多信息，参阅\n  [Ephemeral Inline CSI volumes KEP](https://github.com/kubernetes/enhancements/blob/ad6021b3d61a49040a3f835e12c8bb5424db2bbb/keps/sig-storage/20190122-csi-inline-volumes.md)。\n- 关于本特性下一步开发的更多信息，参阅\n  [enhancement tracking issue #596](https://github.com/kubernetes/enhancements/issues/596)。"}
{"en": "### Generic ephemeral volumes\n\n- For more information on the design, see the\n[Generic ephemeral inline volumes KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1698-generic-ephemeral-volumes/README.md).", "zh": "### 通用临时卷 {#generic-ephemeral-volumes}\n\n- 有关设计的更多信息，参阅\n  [Generic ephemeral inline volumes KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1698-generic-ephemeral-volumes/README.md)。"}
{"en": "Storage capacity is limited and may vary depending on the node on\nwhich a pod runs: network-attached storage might not be accessible by\nall nodes, or storage is local to a node to begin with.", "zh": "存储容量是有限的，并且会因为运行 Pod 的节点不同而变化：\n网络存储可能并非所有节点都能够访问，或者对于某个节点存储是本地的。\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "This page describes how Kubernetes keeps track of storage capacity and\nhow the scheduler uses that information to [schedule Pods](/docs/concepts/scheduling-eviction/) onto nodes\nthat have access to enough storage capacity for the remaining missing\nvolumes. Without storage capacity tracking, the scheduler may choose a\nnode that doesn't have enough capacity to provision a volume and\nmultiple scheduling retries will be needed.", "zh": "本页面描述了 Kubernetes 如何跟踪存储容量以及调度程序如何为了余下的尚未挂载的卷使用该信息将\n[Pod 调度](/zh-cn/docs/concepts/scheduling-eviction/)到能够访问到足够存储容量的节点上。\n如果没有跟踪存储容量，调度程序可能会选择一个没有足够容量来提供卷的节点，并且需要多次调度重试。\n\n## {{% heading \"prerequisites\" %}}"}
{"en": "Kubernetes v{{< skew currentVersion >}} includes cluster-level API support for\nstorage capacity tracking. To use this you must also be using a CSI driver that\nsupports capacity tracking. Consult the documentation for the CSI drivers that\nyou use to find out whether this support is available and, if so, how to use\nit. If you are not running Kubernetes v{{< skew currentVersion >}}, check the\ndocumentation for that version of Kubernetes.", "zh": "Kubernetes v{{< skew currentVersion >}} 包含了对存储容量跟踪的集群级 API 支持。\n要使用它，你还必须使用支持容量跟踪的 CSI 驱动程序。请查阅你使用的 CSI 驱动程序的文档，\n以了解此支持是否可用，如果可用，该如何使用它。如果你运行的不是\nKubernetes v{{< skew currentVersion >}}，请查看对应版本的 Kubernetes 文档。"}
{"en": "## API\n\nThere are two API extensions for this feature:\n- [CSIStorageCapacity](/docs/reference/kubernetes-api/config-and-storage-resources/csi-storage-capacity-v1/) objects:\n  these get produced by a CSI driver in the namespace\n  where the driver is installed. Each object contains capacity\n  information for one storage class and defines which nodes have\n  access to that storage.\n- [The `CSIDriverSpec.StorageCapacity` field](/docs/reference/kubernetes-api/config-and-storage-resources/csi-driver-v1/#CSIDriverSpec):\n  when set to `true`, the Kubernetes scheduler will consider storage\n  capacity for volumes that use the CSI driver.", "zh": "## API\n\n这个特性有两个 API 扩展接口：\n- [CSIStorageCapacity](/zh-cn/docs/reference/kubernetes-api/config-and-storage-resources/csi-storage-capacity-v1/) 对象：这些对象由\n  CSI 驱动程序在安装驱动程序的命名空间中产生。\n  每个对象都包含一个存储类的容量信息，并定义哪些节点可以访问该存储。\n- [`CSIDriverSpec.StorageCapacity` 字段](/zh-cn/docs/reference/kubernetes-api/config-and-storage-resources/csi-driver-v1/#CSIDriverSpec)：\n  设置为 true 时，Kubernetes 调度程序将考虑使用 CSI 驱动程序的卷的存储容量。"}
{"en": "## Scheduling\n\nStorage capacity information is used by the Kubernetes scheduler if:\n- a Pod uses a volume that has not been created yet,\n- that volume uses a {{< glossary_tooltip text=\"StorageClass\" term_id=\"storage-class\" >}} which references a CSI driver and\n  uses `WaitForFirstConsumer` [volume binding\n  mode](/docs/concepts/storage/storage-classes/#volume-binding-mode),\n  and\n- the `CSIDriver` object for the driver has `StorageCapacity` set to\n  true.\n\nIn that case, the scheduler only considers nodes for the Pod which\nhave enough storage available to them. This check is very\nsimplistic and only compares the size of the volume against the\ncapacity listed in `CSIStorageCapacity` objects with a topology that\nincludes the node.\n\nFor volumes with `Immediate` volume binding mode, the storage driver\ndecides where to create the volume, independently of Pods that will\nuse the volume. The scheduler then schedules Pods onto nodes where the\nvolume is available after the volume has been created.\n\nFor [CSI ephemeral volumes](/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes),\nscheduling always happens without considering storage capacity. This\nis based on the assumption that this volume type is only used by\nspecial CSI drivers which are local to a node and do not need\nsignificant resources there.", "zh": "## 调度  {#scheduling}\n\n如果有以下情况，存储容量信息将会被 Kubernetes 调度程序使用：\n- Pod 使用的卷还没有被创建，\n- 卷使用引用了 CSI 驱动的 {{< glossary_tooltip text=\"StorageClass\" term_id=\"storage-class\" >}}，\n  并且使用了 `WaitForFirstConsumer` [卷绑定模式](/zh-cn/docs/concepts/storage/storage-classes/#volume-binding-mode)，\n- 驱动程序的 `CSIDriver` 对象的 `StorageCapacity` 被设置为 true。\n\n在这种情况下，调度程序仅考虑将 Pod 调度到有足够存储容量的节点上。这个检测非常简单，\n仅将卷的大小与 `CSIStorageCapacity` 对象中列出的容量进行比较，并使用包含该节点的拓扑。\n\n对于具有 `Immediate` 卷绑定模式的卷，存储驱动程序将决定在何处创建该卷，而不取决于将使用该卷的 Pod。\n然后，调度程序将 Pod 调度到创建卷后可使用该卷的节点上。\n\n对于 [CSI 临时卷](/zh-cn/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes)，\n调度总是在不考虑存储容量的情况下进行。\n这是基于这样的假设：该卷类型仅由节点本地的特殊 CSI 驱动程序使用，并且不需要大量资源。"}
{"en": "## Rescheduling\n\nWhen a node has been selected for a Pod with `WaitForFirstConsumer`\nvolumes, that decision is still tentative. The next step is that the\nCSI storage driver gets asked to create the volume with a hint that the\nvolume is supposed to be available on the selected node.\n\nBecause Kubernetes might have chosen a node based on out-dated\ncapacity information, it is possible that the volume cannot really be\ncreated. The node selection is then reset and the Kubernetes scheduler\ntries again to find a node for the Pod.", "zh": "## 重新调度  {#rescheduling}\n\n当为带有 `WaitForFirstConsumer` 的卷的 Pod 来选择节点时，该决定仍然是暂定的。\n下一步是要求 CSI 存储驱动程序创建卷，并提示该卷在被选择的节点上可用。\n\n因为 Kubernetes 可能会根据已经过时的存储容量信息来选择一个节点，因此可能无法真正创建卷。\n然后就会重置节点选择，Kubernetes 调度器会再次尝试为 Pod 查找节点。"}
{"en": "## Limitations\n\nStorage capacity tracking increases the chance that scheduling works\non the first try, but cannot guarantee this because the scheduler has\nto decide based on potentially out-dated information. Usually, the\nsame retry mechanism as for scheduling without any storage capacity\ninformation handles scheduling failures.\n\nOne situation where scheduling can fail permanently is when a Pod uses\nmultiple volumes: one volume might have been created already in a\ntopology segment which then does not have enough capacity left for\nanother volume. Manual intervention is necessary to recover from this,\nfor example by increasing capacity or deleting the volume that was\nalready created.", "zh": "## 限制  {#limitations}\n\n存储容量跟踪增加了调度器第一次尝试即成功的机会，但是并不能保证这一点，因为调度器必须根据可能过期的信息来进行决策。\n通常，与没有任何存储容量信息的调度相同的重试机制可以处理调度失败。\n\n当 Pod 使用多个卷时，调度可能会永久失败：一个卷可能已经在拓扑段中创建，而该卷又没有足够的容量来创建另一个卷，\n要想从中恢复，必须要进行手动干预，比如通过增加存储容量或者删除已经创建的卷。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- For more information on the design, see the\n[Storage Capacity Constraints for Pod Scheduling KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md).", "zh": "- 想要获得更多该设计的信息，查看\n  [Storage Capacity Constraints for Pod Scheduling KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md)。"}
{"en": "This page provides an storage overview specific to the Windows operating system.", "zh": "此页面提供特定于 Windows 操作系统的存储概述。"}
{"en": "## Persistent storage {#storage}\n\nWindows has a layered filesystem driver to mount container layers and create a copy\nfilesystem based on NTFS. All file paths in the container are resolved only within\nthe context of that container.", "zh": "## 持久存储 {#storage}\nWindows 有一个分层文件系统驱动程序用来挂载容器层和创建基于 NTFS 的文件系统拷贝。\n容器中的所有文件路径仅在该容器的上下文中解析。"}
{"en": "* With Docker, volume mounts can only target a directory in the container, and not\n  an individual file. This limitation does not apply to containerd.\n* Volume mounts cannot project files or directories back to the host filesystem.\n* Read-only filesystems are not supported because write access is always required\n  for the Windows registry and SAM database. However, read-only volumes are supported.\n* Volume user-masks and permissions are not available. Because the SAM is not shared\n  between the host & container, there's no mapping between them. All permissions are\n  resolved within the context of the container.", "zh": "* 使用 Docker 时，卷挂载只能是容器中的目录，而不能是单个文件。此限制不适用于 containerd。\n* 卷挂载不能将文件或目录映射回宿主文件系统。\n* 不支持只读文件系统，因为 Windows 注册表和 SAM 数据库始终需要写访问权限。不过，Windows 支持只读的卷。\n* 不支持卷的用户掩码和访问许可，因为宿主与容器之间并不共享 SAM，二者之间不存在映射关系。\n  所有访问许可都是在容器上下文中解析的。"}
{"en": "As a result, the following storage functionality is not supported on Windows nodes:", "zh": "因此，Windows 节点不支持以下存储功能："}
{"en": "* Volume subpath mounts: only the entire volume can be mounted in a Windows container\n* Subpath volume mounting for Secrets\n* Host mount projection\n* Read-only root filesystem (mapped volumes still support `readOnly`)\n* Block device mapping\n* Memory as the storage medium (for example, `emptyDir.medium` set to `Memory`)\n* File system features like uid/gid; per-user Linux filesystem permissions\n* Setting [secret permissions with DefaultMode](/docs/tasks/inject-data-application/distribute-credentials-secure/#set-posix-permissions-for-secret-keys) (due to UID/GID dependency)\n* NFS based storage/volume support\n* Expanding the mounted volume (resizefs)", "zh": "* 卷子路径挂载：只能在 Windows 容器上挂载整个卷\n* Secret 的子路径挂载\n* 宿主挂载映射\n* 只读的根文件系统（映射的卷仍然支持 `readOnly`）\n* 块设备映射\n* 内存作为存储介质（例如 `emptyDir.medium` 设置为 `Memory`）\n* 类似 UID/GID、各用户不同的 Linux 文件系统访问许可等文件系统特性\n* 使用 [DefaultMode 设置 Secret 权限](/zh-cn/docs/tasks/inject-data-application/distribute-credentials-secure/#set-posix-permissions-for-secret-keys)\n  （因为该特性依赖 UID/GID）\n* 基于 NFS 的存储和卷支持\n* 扩展已挂载卷（resizefs）"}
{"en": "Kubernetes {{< glossary_tooltip text=\"volumes\" term_id=\"volume\" >}} enable complex\napplications, with data persistence and Pod volume sharing requirements, to be deployed\non Kubernetes. Management of persistent volumes associated with a specific storage\nback-end or protocol includes actions such as provisioning/de-provisioning/resizing\nof volumes, attaching/detaching a volume to/from a Kubernetes node and\nmounting/dismounting a volume to/from individual containers in a pod that needs to\npersist data.", "zh": "使用 Kubernetes {{< glossary_tooltip text=\"卷\" term_id=\"volume\" >}}，\n对数据持久性和 Pod 卷共享有需求的复杂应用也可以部署到 Kubernetes 上。\n管理与特定存储后端或协议相关的持久卷时，相关的操作包括：对卷的制备（Provisioning）、\n去配（De-provisioning）和调整大小，将卷挂接到 Kubernetes 节点或从节点上解除挂接，\n将卷挂载到需要持久数据的 Pod 中的某容器上或从容器上卸载。"}
{"en": "Volume management components are shipped as Kubernetes volume\n[plugin](/docs/concepts/storage/volumes/#volume-types).\nThe following broad classes of Kubernetes volume plugins are supported on Windows:", "zh": "卷管理组件作为 Kubernetes 卷[插件](/zh-cn/docs/concepts/storage/volumes/#volume-types)发布。\nWindows 支持以下类型的 Kubernetes 卷插件："}
{"en": "* [`FlexVolume plugins`](/docs/concepts/storage/volumes/#flexvolume)\n  * Please note that FlexVolumes have been deprecated as of 1.23\n* [`CSI Plugins`](/docs/concepts/storage/volumes/#csi)", "zh": "* [`FlexVolume plugins`](/zh-cn/docs/concepts/storage/volumes/#flexvolume)\n  * 请注意自 1.23 版本起，FlexVolume 已被弃用\n* [`CSI Plugins`](/zh-cn/docs/concepts/storage/volumes/#csi)"}
{"en": "##### In-tree volume plugins\n\nThe following in-tree plugins support persistent storage on Windows nodes:", "zh": "##### 树内（In-Tree）卷插件  {#in-tree-volume-plugins}\n\n以下树内（In-Tree）插件支持 Windows 节点上的持久存储："}
{"en": "* [`azureFile`](/docs/concepts/storage/volumes/#azurefile)\n* [`vsphereVolume`](/docs/concepts/storage/volumes/#vspherevolume)", "zh": "* [`azureFile`](/zh-cn/docs/concepts/storage/volumes/#azurefile)\n* [`vsphereVolume`](/zh-cn/docs/concepts/storage/volumes/#vspherevolume)"}
{"en": "This document describes _projected volumes_ in Kubernetes. Familiarity with [volumes](/docs/concepts/storage/volumes/) is suggested.", "zh": "本文档描述 Kubernetes 中的**投射卷（Projected Volumes）**。\n建议先熟悉[卷](/zh-cn/docs/concepts/storage/volumes/)概念。"}
{"en": "## Introduction\n\nA `projected` volume maps several existing volume sources into the same directory.\n\nCurrently, the following types of volume sources can be projected:\n\n* [`secret`](/docs/concepts/storage/volumes/#secret)\n* [`downwardAPI`](/docs/concepts/storage/volumes/#downwardapi)\n* [`configMap`](/docs/concepts/storage/volumes/#configmap)\n* [`serviceAccountToken`](#serviceaccounttoken)\n* [`clusterTrustBundle`](#clustertrustbundle)", "zh": "## 介绍    {#introduction}\n\n一个 `projected` 卷可以将若干现有的卷源映射到同一个目录之上。\n\n目前，以下类型的卷源可以被投射：\n\n* [`secret`](/zh-cn/docs/concepts/storage/volumes/#secret)\n* [`downwardAPI`](/zh-cn/docs/concepts/storage/volumes/#downwardapi)\n* [`configMap`](/zh-cn/docs/concepts/storage/volumes/#configmap)\n* [`serviceAccountToken`](#serviceaccounttoken)\n* [`clusterTrustBundle`](#clustertrustbundle)"}
{"en": "All sources are required to be in the same namespace as the Pod. For more details,\nsee the [all-in-one volume](https://git.k8s.io/design-proposals-archive/node/all-in-one-volume.md) design document.", "zh": "所有的卷源都要求处于 Pod 所在的同一个名字空间内。更多详细信息，\n可参考[一体化卷](https://git.k8s.io/design-proposals-archive/node/all-in-one-volume.md)设计文档。"}
{"en": "### Example configuration with a secret, a downwardAPI, and a configMap {#example-configuration-secret-downwardapi-configmap}", "zh": "### 带有 Secret、DownwardAPI 和 ConfigMap 的配置示例 {#example-configuration-secret-downwardapi-configmap}\n\n{{% code_sample file=\"pods/storage/projected-secret-downwardapi-configmap.yaml\" %}}"}
{"en": "### Example configuration: secrets with a non-default permission mode set {#example-configuration-secrets-nondefault-permission-mode}", "zh": "### 带有非默认权限模式设置的 Secret 的配置示例 {#example-configuration-secrets-nondefault-permission-mode}\n\n{{% code_sample file=\"pods/storage/projected-secrets-nondefault-permission-mode.yaml\" %}}"}
{"en": "Each projected volume source is listed in the spec under `sources`. The\nparameters are nearly the same with two exceptions:\n\n* For secrets, the `secretName` field has been changed to `name` to be consistent\n  with ConfigMap naming.\n* The `defaultMode` can only be specified at the projected level and not for each\n  volume source. However, as illustrated above, you can explicitly set the `mode`\n  for each individual projection.", "zh": "每个被投射的卷源都列举在规约中的 `sources` 下面。参数几乎相同，只有两个例外：\n\n* 对于 Secret，`secretName` 字段被改为 `name` 以便于 ConfigMap 的命名一致；\n* `defaultMode` 只能在投射层级设置，不能在卷源层级设置。不过，正如上面所展示的，\n  你可以显式地为每个投射单独设置 `mode` 属性。"}
{"en": "## serviceAccountToken projected volumes {#serviceaccounttoken}\nYou can inject the token for the current [service account](/docs/reference/access-authn-authz/authentication/#service-account-tokens)\ninto a Pod at a specified path. For example:", "zh": "## serviceAccountToken 投射卷 {#serviceaccounttoken}\n\n你可以将当前[服务账号](/zh-cn/docs/reference/access-authn-authz/authentication/#service-account-tokens)的令牌注入到\nPod 中特定路径下。例如：\n\n{{% code_sample file=\"pods/storage/projected-service-account-token.yaml\" %}}"}
{"en": "The example Pod has a projected volume containing the injected service account\ntoken. Containers in this Pod can use that token to access the Kubernetes API\nserver, authenticating with the identity of [the pod's ServiceAccount](/docs/tasks/configure-pod-container/configure-service-account/).\nThe `audience` field contains the intended audience of the\ntoken. A recipient of the token must identify itself with an identifier specified\nin the audience of the token, and otherwise should reject the token. This field\nis optional and it defaults to the identifier of the API server.", "zh": "示例 Pod 中包含一个投射卷，其中包含注入的服务账号令牌。\n此 Pod 中的容器可以使用该令牌访问 Kubernetes API 服务器， 使用\n[Pod 的 ServiceAccount](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/)\n进行身份验证。`audience` 字段包含令牌所针对的受众。\n收到令牌的主体必须使用令牌受众中所指定的某个标识符来标识自身，否则应该拒绝该令牌。\n此字段是可选的，默认值为 API 服务器的标识。"}
{"en": "The `expirationSeconds` is the expected duration of validity of the service account\ntoken. It defaults to 1 hour and must be at least 10 minutes (600 seconds). An administrator\ncan also limit its maximum value by specifying the `--service-account-max-token-expiration`\noption for the API server. The `path` field specifies a relative path to the mount point\nof the projected volume.", "zh": "字段 `expirationSeconds` 是服务账号令牌预期的生命期长度。默认值为 1 小时，\n必须至少为 10 分钟（600 秒）。管理员也可以通过设置 API 服务器的命令行参数\n`--service-account-max-token-expiration` 来为其设置最大值上限。\n`path` 字段给出与投射卷挂载点之间的相对路径。\n\n{{< note >}}"}
{"en": "A container using a projected volume source as a [`subPath`](/docs/concepts/storage/volumes/#using-subpath)\nvolume mount will not receive updates for those volume sources.", "zh": "以 [`subPath`](/zh-cn/docs/concepts/storage/volumes/#using-subpath)\n形式使用投射卷源的容器无法收到对应卷源的更新。\n{{< /note >}}"}
{"en": "## clusterTrustBundle projected volumes {#clustertrustbundle}", "zh": "## clusterTrustBundle 投射卷    {#clustertrustbundle}\n\n{{<feature-state for_k8s_version=\"v1.29\" state=\"alpha\" >}}\n\n{{< note >}}"}
{"en": "To use this feature in Kubernetes {{< skew currentVersion >}}, you must enable support for ClusterTrustBundle objects with the `ClusterTrustBundle` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) and `--runtime-config=certificates.k8s.io/v1alpha1/clustertrustbundles=true` kube-apiserver flag, then enable the `ClusterTrustBundleProjection` feature gate.", "zh": "要在 Kubernetes {{< skew currentVersion >}} 中使用此特性，你必须通过 `ClusterTrustBundle`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)和\n`--runtime-config=certificates.k8s.io/v1alpha1/clustertrustbundles=true` kube-apiserver\n标志启用对 ClusterTrustBundle 对象的支持，然后才能启用 `ClusterTrustBundleProjection` 特性门控。\n{{< /note >}}"}
{"en": "The `clusterTrustBundle` projected volume source injects the contents of one or more [ClusterTrustBundle](/docs/reference/access-authn-authz/certificate-signing-requests#cluster-trust-bundles) objects as an automatically-updating file in the container filesystem.", "zh": "`clusterTrustBundle` 投射卷源将一个或多个\n[ClusterTrustBundle](/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests#cluster-trust-bundles)\n对象的内容作为一个自动更新的文件注入到容器文件系统中。"}
{"en": "ClusterTrustBundles can be selected either by [name](/docs/reference/access-authn-authz/certificate-signing-requests#ctb-signer-unlinked) or by [signer name](/docs/reference/access-authn-authz/certificate-signing-requests#ctb-signer-linked).", "zh": "ClusterTrustBundle 可以通过[名称](/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests#ctb-signer-unlinked)\n或[签名者名称](/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests#ctb-signer-linked)被选中。"}
{"en": "To select by name, use the `name` field to designate a single ClusterTrustBundle object.\n\nTo select by signer name, use the `signerName` field (and optionally the\n`labelSelector` field) to designate a set of ClusterTrustBundle objects that use\nthe given signer name. If `labelSelector` is not present, then all\nClusterTrustBundles for that signer are selected.", "zh": "要按名称选择，可以使用 `name` 字段指定单个 ClusterTrustBundle 对象。\n\n要按签名者名称选择，可以使用 `signerName` 字段（也可选用 `labelSelector` 字段）\n指定一组使用给定签名者名称的 ClusterTrustBundle 对象。\n如果 `labelSelector` 不存在，则针对该签名者的所有 ClusterTrustBundles 将被选中。"}
{"en": "The kubelet deduplicates the certificates in the selected ClusterTrustBundle objects, normalizes the PEM representations (discarding comments and headers), reorders the certificates, and writes them into the file named by `path`. As the set of selected ClusterTrustBundles or their content changes, kubelet keeps the file up-to-date.", "zh": "kubelet 会对所选 ClusterTrustBundle 对象中的证书进行去重，规范化 PEM 表示（丢弃注释和头部），\n重新排序证书，并将这些证书写入由 `path` 指定的文件中。\n随着所选 ClusterTrustBundles 的集合或其内容发生变化，kubelet 会保持更新此文件。"}
{"en": "By default, the kubelet will prevent the pod from starting if the named ClusterTrustBundle is not found, or if `signerName` / `labelSelector` do not match any ClusterTrustBundles.  If this behavior is not what you want, then set the `optional` field to `true`, and the pod will start up with an empty file at `path`.", "zh": "默认情况下，如果找不到指定的 ClusterTrustBundle，或者 `signerName` / `labelSelector`\n与所有 ClusterTrustBundle 都不匹配，kubelet 将阻止 Pod 启动。如果这不是你想要的行为，\n可以将 `optional` 字段设置为 `true`，Pod 将使用 `path` 处的空白文件启动。\n\n{{% code_sample file=\"pods/storage/projected-clustertrustbundle.yaml\" %}}"}
{"en": "## SecurityContext interactions", "zh": "## 与 SecurityContext 间的关系    {#securitycontext-interactions}"}
{"en": "The [proposal](https://git.k8s.io/enhancements/keps/sig-storage/2451-service-account-token-volumes#proposal) for file permission handling in projected service account volume enhancement introduced the projected files having the correct owner permissions set.", "zh": "关于在投射的服务账号卷中处理文件访问权限的[提案](https://git.k8s.io/enhancements/keps/sig-storage/2451-service-account-token-volumes#proposal)\n介绍了如何使得所投射的文件具有合适的属主访问权限。\n\n### Linux"}
{"en": "In Linux pods that have a projected volume and `RunAsUser` set in the Pod\n[`SecurityContext`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context),\nthe projected files have the correct ownership set including container user\nownership.", "zh": "在包含了投射卷并在\n[`SecurityContext`](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context)\n中设置了 `RunAsUser` 属性的 Linux Pod 中，投射文件具有正确的属主属性设置，\n其中包含了容器用户属主。"}
{"en": "When all containers in a pod have the same `runAsUser` set in their\n[`PodSecurityContext`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context)\nor container\n[`SecurityContext`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1),\nthen the kubelet ensures that the contents of the `serviceAccountToken` volume are owned by that user,\nand the token file has its permission mode set to `0600`.", "zh": "当 Pod 中的所有容器在其\n[`PodSecurityContext`](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context)\n或容器\n[`SecurityContext`](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1)\n中设置了相同的 `runAsUser` 时，kubelet 将确保 `serviceAccountToken`\n卷的内容归该用户所有，并且令牌文件的权限模式会被设置为 `0600`。\n\n{{< note >}}"}
{"en": "{{< glossary_tooltip text=\"Ephemeral containers\" term_id=\"ephemeral-container\" >}}\nadded to a Pod after it is created do *not* change volume permissions that were\nset when the pod was created.\n\nIf a Pod's `serviceAccountToken` volume permissions were set to `0600` because\nall other containers in the Pod have the same `runAsUser`, ephemeral\ncontainers must use the same `runAsUser` to be able to read the token.", "zh": "在某 Pod 被创建后为其添加的{{< glossary_tooltip text=\"临时容器\" term_id=\"ephemeral-container\" >}}**不会**更改创建该\nPod 时设置的卷权限。\n\n如果 Pod 的 `serviceAccountToken` 卷权限被设为 `0600`\n是因为 Pod 中的其他所有容器都具有相同的 `runAsUser`，\n则临时容器必须使用相同的 `runAsUser` 才能读取令牌。\n{{< /note >}}\n\n### Windows"}
{"en": "In Windows pods that have a projected volume and `RunAsUsername` set in the\nPod `SecurityContext`, the ownership is not enforced due to the way user\naccounts are managed in Windows. Windows stores and manages local user and group\naccounts in a database file called Security Account Manager (SAM). Each\ncontainer maintains its own instance of the SAM database, to which the host has\nno visibility into while the container is running. Windows containers are\ndesigned to run the user mode portion of the OS in isolation from the host,\nhence the maintenance of a virtual SAM database. As a result, the kubelet running\non the host does not have the ability to dynamically configure host file\nownership for virtualized container accounts. It is recommended that if files on\nthe host machine are to be shared with the container then they should be placed\ninto their own volume mount outside of `C:\\`.", "zh": "在包含了投射卷并在 `SecurityContext` 中设置了 `RunAsUsername` 的 Windows Pod 中,\n由于 Windows 中用户账号的管理方式问题，文件的属主无法正确设置。\nWindows 在名为安全账号管理器（Security Account Manager，SAM）\n的数据库中保存本地用户和组信息。每个容器会维护其自身的 SAM 数据库实例，\n宿主系统无法窥视到容器运行期间数据库内容。Windows 容器被设计用来运行操作系统的用户态部分，\n与宿主系统之间隔离，因此维护了一个虚拟的 SAM 数据库。\n所以，在宿主系统上运行的 kubelet 无法动态为虚拟的容器账号配置宿主文件的属主。\n如果需要将宿主机器上的文件与容器共享，建议将它们放到挂载于 `C:\\` 之外的独立卷中。"}
{"en": "By default, the projected files will have the following ownership as shown for\nan example projected volume file:", "zh": "默认情况下，所投射的文件会具有如下例所示的属主属性设置：\n\n```powershell\nPS C:\\> Get-Acl C:\\var\\run\\secrets\\kubernetes.io\\serviceaccount\\..2021_08_31_22_22_18.318230061\\ca.crt | Format-List\n\nPath   : Microsoft.PowerShell.Core\\FileSystem::C:\\var\\run\\secrets\\kubernetes.io\\serviceaccount\\..2021_08_31_22_22_18.318230061\\ca.crt\nOwner  : BUILTIN\\Administrators\nGroup  : NT AUTHORITY\\SYSTEM\nAccess : NT AUTHORITY\\SYSTEM Allow  FullControl\n         BUILTIN\\Administrators Allow  FullControl\n         BUILTIN\\Users Allow  ReadAndExecute, Synchronize\nAudit  :\nSddl   : O:BAG:SYD:AI(A;ID;FA;;;SY)(A;ID;FA;;;BA)(A;ID;0x1200a9;;;BU)\n```"}
{"en": "This implies all administrator users like `ContainerAdministrator` will have\nread, write and execute access while, non-administrator users will have read and\nexecute access.", "zh": "这意味着，所有类似 `ContainerAdministrator` 的管理员用户都具有读、写和执行访问权限，\n而非管理员用户将具有读和执行访问权限。\n\n{{< note >}}"}
{"en": "In general, granting the container access to the host is discouraged as it can\nopen the door for potential security exploits.\n\nCreating a Windows Pod with `RunAsUser` in it's `SecurityContext` will result in\nthe Pod being stuck at `ContainerCreating` forever. So it is advised to not use\nthe Linux only `RunAsUser` option with Windows Pods.", "zh": "总体而言，为容器授予访问宿主系统的权限这种做法是不推荐的，因为这样做可能会打开潜在的安全性攻击之门。\n\n在创建 Windows Pod 时，如果在其 `SecurityContext` 中设置了 `RunAsUser`，\nPod 会一直阻塞在 `ContainerCreating` 状态。因此，建议不要在 Windows\n节点上使用仅针对 Linux 的 `RunAsUser` 选项。\n{{< /note >}}"}
{"en": "In Kubernetes, a _VolumeSnapshot_ represents a snapshot of a volume on a storage\nsystem. This document assumes that you are already familiar with Kubernetes\n[persistent volumes](/docs/concepts/storage/persistent-volumes/).", "zh": "在 Kubernetes 中，**卷快照** 是一个存储系统上卷的快照，本文假设你已经熟悉了 Kubernetes\n的[持久卷](/zh-cn/docs/concepts/storage/persistent-volumes/)。"}
{"en": "## Introduction", "zh": "## 介绍 {#introduction}"}
{"en": "Similar to how API resources `PersistentVolume` and `PersistentVolumeClaim` are\nused to provision volumes for users and administrators, `VolumeSnapshotContent`\nand `VolumeSnapshot` API resources are provided to create volume snapshots for\nusers and administrators.", "zh": "与 `PersistentVolume` 和 `PersistentVolumeClaim` 这两个 API 资源用于给用户和管理员制备卷类似，\n`VolumeSnapshotContent` 和 `VolumeSnapshot` 这两个 API 资源用于给用户和管理员创建卷快照。"}
{"en": "A `VolumeSnapshotContent` is a snapshot taken from a volume in the cluster that\nhas been provisioned by an administrator. It is a resource in the cluster just\nlike a PersistentVolume is a cluster resource.", "zh": "`VolumeSnapshotContent` 是从一个卷获取的一种快照，该卷由管理员在集群中进行制备。\n就像持久卷（PersistentVolume）是集群的资源一样，它也是集群中的资源。"}
{"en": "A `VolumeSnapshot` is a request for snapshot of a volume by a user. It is similar\nto a PersistentVolumeClaim.", "zh": "`VolumeSnapshot` 是用户对于卷的快照的请求。它类似于持久卷声明（PersistentVolumeClaim）。"}
{"en": "`VolumeSnapshotClass` allows you to specify different attributes belonging to a\n`VolumeSnapshot`. These attributes may differ among snapshots taken from the same\nvolume on the storage system and therefore cannot be expressed by using the same\n`StorageClass` of a `PersistentVolumeClaim`.", "zh": "`VolumeSnapshotClass` 允许指定属于 `VolumeSnapshot` 的不同属性。在从存储系统的相同卷上获取的快照之间，\n这些属性可能有所不同，因此不能通过使用与 `PersistentVolumeClaim` 相同的 `StorageClass` 来表示。"}
{"en": "Volume snapshots provide Kubernetes users with a standardized way to copy a volume's\ncontents at a particular point in time without creating an entirely new volume. This\nfunctionality enables, for example, database administrators to backup databases before\nperforming edit or delete modifications.", "zh": "卷快照能力为 Kubernetes 用户提供了一种标准的方式来在指定时间点复制卷的内容，并且不需要创建全新的卷。\n例如，这一功能使得数据库管理员能够在执行编辑或删除之类的修改之前对数据库执行备份。"}
{"en": "Users need to be aware of the following when using this feature:", "zh": "当使用该功能时，用户需要注意以下几点："}
{"en": "- API Objects `VolumeSnapshot`, `VolumeSnapshotContent`, and `VolumeSnapshotClass`\n  are {{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CRDs\" >}}, not\n  part of the core API.\n- `VolumeSnapshot` support is only available for CSI drivers.\n- As part of the deployment process of `VolumeSnapshot`, the Kubernetes team provides\n  a snapshot controller to be deployed into the control plane, and a sidecar helper\n  container called csi-snapshotter to be deployed together with the CSI driver.\n  The snapshot controller watches `VolumeSnapshot` and `VolumeSnapshotContent` objects\n  and is responsible for the creation and deletion of `VolumeSnapshotContent` object.\n  The sidecar csi-snapshotter watches `VolumeSnapshotContent` objects and triggers\n  `CreateSnapshot` and `DeleteSnapshot` operations against a CSI endpoint.\n- There is also a validating webhook server which provides tightened validation on\n  snapshot objects. This should be installed by the Kubernetes distros along with\n  the snapshot controller and CRDs, not CSI drivers. It should be installed in all\n  Kubernetes clusters that has the snapshot feature enabled.\n- CSI drivers may or may not have implemented the volume snapshot functionality.\n  The CSI drivers that have provided support for volume snapshot will likely use\n  the csi-snapshotter. See [CSI Driver documentation](https://kubernetes-csi.github.io/docs/) for details.\n- The CRDs and snapshot controller installations are the responsibility of the Kubernetes distribution.", "zh": "- API 对象 `VolumeSnapshot`，`VolumeSnapshotContent` 和 `VolumeSnapshotClass`\n  是 {{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CRD\" >}}，\n  不属于核心 API。\n- `VolumeSnapshot` 支持仅可用于 CSI 驱动。\n- 作为 `VolumeSnapshot` 部署过程的一部分，Kubernetes 团队提供了一个部署于控制平面的快照控制器，\n  并且提供了一个叫做 `csi-snapshotter` 的边车（Sidecar）辅助容器，和 CSI 驱动程序一起部署。\n  快照控制器监视 `VolumeSnapshot` 和 `VolumeSnapshotContent` 对象，\n  并且负责创建和删除 `VolumeSnapshotContent` 对象。\n  边车 csi-snapshotter 监视 `VolumeSnapshotContent` 对象，\n  并且触发针对 CSI 端点的 `CreateSnapshot` 和 `DeleteSnapshot` 的操作。\n- 还有一个验证性质的 Webhook 服务器，可以对快照对象进行更严格的验证。\n  Kubernetes 发行版应将其与快照控制器和 CRD（而非 CSI 驱动程序）一起安装。\n  此服务器应该安装在所有启用了快照功能的 Kubernetes 集群中。\n- CSI 驱动可能实现，也可能没有实现卷快照功能。CSI 驱动可能会使用 csi-snapshotter\n  来提供对卷快照的支持。详见 [CSI 驱动程序文档](https://kubernetes-csi.github.io/docs/)\n- Kubernetes 负责 CRD 和快照控制器的安装。"}
{"en": "## Lifecycle of a volume snapshot and volume snapshot content\n\n`VolumeSnapshotContents` are resources in the cluster. `VolumeSnapshots` are requests\nfor those resources. The interaction between `VolumeSnapshotContents` and `VolumeSnapshots`\nfollow this lifecycle:", "zh": "## 卷快照和卷快照内容的生命周期 {#lifecycle-of-a-volume-snapshot-and-volume-snapshot-content}\n\n`VolumeSnapshotContents` 是集群中的资源。`VolumeSnapshots` 是对于这些资源的请求。\n`VolumeSnapshotContents` 和 `VolumeSnapshots` 之间的交互遵循以下生命周期："}
{"en": "### Provisioning Volume Snapshot\n\nThere are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.", "zh": "### 制备卷快照 {#provisioning-volume-snapshot}\n\n快照可以通过两种方式进行制备：预制备或动态制备。"}
{"en": "#### Pre-provisioned {#static}\n\nA cluster administrator creates a number of `VolumeSnapshotContents`. They carry the details\nof the real volume snapshot on the storage system which is available for use by cluster users.\nThey exist in the Kubernetes API and are available for consumption.", "zh": "#### 预制备 {#static}\n\n集群管理员创建多个 `VolumeSnapshotContents`。它们带有存储系统上实际卷快照的详细信息，可以供集群用户使用。\n它们存在于 Kubernetes API 中，并且能够被使用。"}
{"en": "#### Dynamic\n\nInstead of using a pre-existing snapshot, you can request that a snapshot to be dynamically\ntaken from a PersistentVolumeClaim. The [VolumeSnapshotClass](/docs/concepts/storage/volume-snapshot-classes/)\nspecifies storage provider-specific parameters to use when taking a snapshot.", "zh": "#### 动态制备 {#dynamic}\n\n可以从 `PersistentVolumeClaim` 中动态获取快照，而不用使用已经存在的快照。\n在获取快照时，[卷快照类](/zh-cn/docs/concepts/storage/volume-snapshot-classes/)\n指定要用的特定于存储提供程序的参数。"}
{"en": "### Binding\n\nThe snapshot controller handles the binding of a `VolumeSnapshot` object with an appropriate\n`VolumeSnapshotContent` object, in both pre-provisioned and dynamically provisioned scenarios.\nThe binding is a one-to-one mapping.", "zh": "### 绑定 {#binding}\n\n在预制备和动态制备场景下，快照控制器处理绑定 `VolumeSnapshot` 对象和其合适的 `VolumeSnapshotContent` 对象。\n绑定关系是一对一的。"}
{"en": "In the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the\nrequested VolumeSnapshotContent object is created.", "zh": "在预制备快照绑定场景下，`VolumeSnapshotContent` 对象创建之后，才会和 `VolumeSnapshot` 进行绑定。"}
{"en": "### Persistent Volume Claim as Snapshot Source Protection\n\nThe purpose of this protection is to ensure that in-use\n{{< glossary_tooltip text=\"PersistentVolumeClaim\" term_id=\"persistent-volume-claim\" >}}\nAPI objects are not removed from the system while a snapshot is being taken from it\n(as this may result in data loss).", "zh": "### 快照源的持久性卷声明保护   {#persistent-volume-claim-as-snapshot-source-protection}\n\n这种保护的目的是确保在从系统中获取快照时，不会将正在使用的\n{{< glossary_tooltip text=\"PersistentVolumeClaim\" term_id=\"persistent-volume-claim\" >}}\nAPI 对象从系统中删除（因为这可能会导致数据丢失）。"}
{"en": "While a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim\nis in-use. If you delete a PersistentVolumeClaim API object in active use as a snapshot\nsource, the PersistentVolumeClaim object is not removed immediately. Instead, removal of\nthe PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.", "zh": "在为某 `PersistentVolumeClaim` 生成快照时，该 `PersistentVolumeClaim` 处于被使用状态。\n如果删除一个正作为快照源使用的 `PersistentVolumeClaim` API 对象，该 `PersistentVolumeClaim` 对象不会立即被移除。\n相反，移除 `PersistentVolumeClaim` 对象的动作会被推迟，直到快照状态变为 ReadyToUse 或快照操作被中止时再执行。"}
{"en": "### Delete\n\nDeletion is triggered by deleting the `VolumeSnapshot` object, and the `DeletionPolicy`\nwill be followed. If the `DeletionPolicy` is `Delete`, then the underlying storage snapshot\nwill be deleted along with the `VolumeSnapshotContent` object. If the `DeletionPolicy` is\n`Retain`, then both the underlying snapshot and `VolumeSnapshotContent` remain.", "zh": "### 删除 {#delete}\n\n删除 `VolumeSnapshot` 对象触发删除 `VolumeSnapshotContent` 操作，并且 `DeletionPolicy` 会紧跟着执行。\n如果 `DeletionPolicy` 是 `Delete`，那么底层存储快照会和 `VolumeSnapshotContent` 一起被删除。\n如果 `DeletionPolicy` 是 `Retain`，那么底层快照和 `VolumeSnapshotContent` 都会被保留。"}
{"en": "## VolumeSnapshots\n\nEach VolumeSnapshot contains a spec and a status.", "zh": "## 卷快照 {#volume-snapshots}\n\n每个 `VolumeSnapshot` 包含一个 spec 和一个 status。\n\n```yaml\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: new-snapshot-test\nspec:\n  volumeSnapshotClassName: csi-hostpath-snapclass\n  source:\n    persistentVolumeClaimName: pvc-test\n```"}
{"en": "`persistentVolumeClaimName` is the name of the PersistentVolumeClaim data source\nfor the snapshot. This field is required for dynamically provisioning a snapshot.\n\nA volume snapshot can request a particular class by specifying the name of a\n[VolumeSnapshotClass](/docs/concepts/storage/volume-snapshot-classes/)\nusing the attribute `volumeSnapshotClassName`. If nothing is set, then the\ndefault class is used if available.", "zh": "`persistentVolumeClaimName` 是 `PersistentVolumeClaim` 数据源对快照的名称。\n这个字段是动态制备快照中的必填字段。\n\n卷快照可以通过指定 [VolumeSnapshotClass](/zh-cn/docs/concepts/storage/volume-snapshot-classes/)\n使用 `volumeSnapshotClassName` 属性来请求特定类。如果没有设置，那么使用默认类（如果有）。"}
{"en": "For pre-provisioned snapshots, you need to specify a `volumeSnapshotContentName`\nas the source for the snapshot as shown in the following example. The\n`volumeSnapshotContentName` source field is required for pre-provisioned snapshots.", "zh": "如下面例子所示，对于预制备的快照，需要给快照指定 `volumeSnapshotContentName` 作为来源。\n对于预制备的快照 `source` 中的`volumeSnapshotContentName` 字段是必填的。\n\n```yaml\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: test-snapshot\nspec:\n  source:\n    volumeSnapshotContentName: test-content\n```"}
{"en": "## Volume Snapshot Contents\n\nEach VolumeSnapshotContent contains a spec and status. In dynamic provisioning,\nthe snapshot common controller creates `VolumeSnapshotContent` objects. Here is an example:", "zh": "## 卷快照内容   {#volume-snapshot-contents}\n\n每个 VolumeSnapshotContent 对象包含 spec 和 status。\n在动态制备时，快照通用控制器创建 `VolumeSnapshotContent` 对象。下面是例子：\n\n```yaml\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotContent\nmetadata:\n  name: snapcontent-72d9a349-aacd-42d2-a240-d775650d2455\nspec:\n  deletionPolicy: Delete\n  driver: hostpath.csi.k8s.io\n  source:\n    volumeHandle: ee0cfb94-f8d4-11e9-b2d8-0242ac110002\n  sourceVolumeMode: Filesystem\n  volumeSnapshotClassName: csi-hostpath-snapclass\n  volumeSnapshotRef:\n    name: new-snapshot-test\n    namespace: default\n    uid: 72d9a349-aacd-42d2-a240-d775650d2455\n```"}
{"en": "`volumeHandle` is the unique identifier of the volume created on the storage\nbackend and returned by the CSI driver during the volume creation. This field\nis required for dynamically provisioning a snapshot.\nIt specifies the volume source of the snapshot.\n\nFor pre-provisioned snapshots, you (as cluster administrator) are responsible\nfor creating the `VolumeSnapshotContent` object as follows.", "zh": "`volumeHandle` 是存储后端创建卷的唯一标识符，在卷创建期间由 CSI 驱动程序返回。\n动态设置快照需要此字段。它指出了快照的卷源。\n\n对于预制备快照，你（作为集群管理员）要按如下命令来创建 `VolumeSnapshotContent` 对象。\n\n```yaml\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotContent\nmetadata:\n  name: new-snapshot-content-test\nspec:\n  deletionPolicy: Delete\n  driver: hostpath.csi.k8s.io\n  source:\n    snapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002\n  sourceVolumeMode: Filesystem\n  volumeSnapshotRef:\n    name: new-snapshot-test\n    namespace: default\n```"}
{"en": "`snapshotHandle` is the unique identifier of the volume snapshot created on\nthe storage backend. This field is required for the pre-provisioned snapshots.\nIt specifies the CSI snapshot id on the storage system that this\n`VolumeSnapshotContent` represents.", "zh": "`snapshotHandle` 是存储后端创建卷的唯一标识符。对于预制备的快照，这个字段是必需的。\n它指定此 `VolumeSnapshotContent` 表示的存储系统上的 CSI 快照 ID。"}
{"en": "`sourceVolumeMode` is the mode of the volume whose snapshot is taken. The value\nof the `sourceVolumeMode` field can be either `Filesystem` or `Block`. If the\nsource volume mode is not specified, Kubernetes treats the snapshot as if the\nsource volume's mode is unknown.", "zh": "`sourceVolumeMode` 是创建快照的卷的模式。`sourceVolumeMode` 字段的值可以是\n`Filesystem` 或 `Block`。如果没有指定源卷模式，Kubernetes 会将快照视为未知的源卷模式。"}
{"en": "`volumeSnapshotRef` is the reference of the corresponding `VolumeSnapshot`. Note that\nwhen the `VolumeSnapshotContent` is being created as a pre-provisioned snapshot, the\n`VolumeSnapshot` referenced in `volumeSnapshotRef` might not exist yet.", "zh": "`volumeSnapshotRef` 字段是对相应的 `VolumeSnapshot` 的引用。\n请注意，当 `VolumeSnapshotContent` 被创建为预配置快照时。\n`volumeSnapshotRef` 中引用的 `VolumeSnapshot` 可能还不存在。"}
{"en": "## Converting the volume mode of a Snapshot {#convert-volume-mode}\n\nIf the `VolumeSnapshots` API installed on your cluster supports the `sourceVolumeMode`\nfield, then the API has the capability to prevent unauthorized users from converting\nthe mode of a volume.\n\nTo check if your cluster has capability for this feature, run the following command:", "zh": "## 转换快照的卷模式 {#convert-volume-mode}\n\n如果在你的集群上安装的 `VolumeSnapshots` API 支持 `sourceVolumeMode`\n字段，则该 API 可以防止未经授权的用户转换卷的模式。\n\n要检查你的集群是否具有此特性的能力，可以运行如下命令：\n\n```shell\nkubectl get crd volumesnapshotcontent -o yaml\n```"}
{"en": "If you want to allow users to create a `PersistentVolumeClaim` from an existing\n`VolumeSnapshot`, but with a different volume mode than the source, the annotation\n`snapshot.storage.kubernetes.io/allow-volume-mode-change: \"true\"`needs to be added to\nthe `VolumeSnapshotContent` that corresponds to the `VolumeSnapshot`.", "zh": "如果你希望允许用户从现有的 `VolumeSnapshot` 创建 `PersistentVolumeClaim`，\n但是使用与源卷不同的卷模式，则需要添加注解\n`snapshot.storage.kubernetes.io/allow-volume-mode-change: \"true\"`\n到对应 `VolumeSnapshot` 的 `VolumeSnapshotContent` 中。"}
{"en": "For pre-provisioned snapshots, `spec.sourceVolumeMode` needs to be populated\nby the cluster administrator.\n\nAn example `VolumeSnapshotContent` resource with this feature enabled would look like:", "zh": "对于预制备的快照，`spec.sourceVolumeMode` 需要由集群管理员填充。\n\n启用此特性的 `VolumeSnapshotContent` 资源示例如下所示：\n\n```yaml\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotContent\nmetadata:\n  name: new-snapshot-content-test\n  annotations:\n    - snapshot.storage.kubernetes.io/allow-volume-mode-change: \"true\"\nspec:\n  deletionPolicy: Delete\n  driver: hostpath.csi.k8s.io\n  source:\n    snapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002\n  sourceVolumeMode: Filesystem\n  volumeSnapshotRef:\n    name: new-snapshot-test\n    namespace: default\n```"}
{"en": "## Provisioning Volumes from Snapshots", "zh": "## 从快照制备卷 {#provisioning-volumes-from-snapshots}"}
{"en": "You can provision a new volume, pre-populated with data from a snapshot, by using\nthe _dataSource_ field in the `PersistentVolumeClaim` object.", "zh": "你可以制备一个新卷，该卷预填充了快照中的数据，在 `PersistentVolumeClaim` 对象中使用 **dataSource** 字段。"}
{"en": "For more details, see\n[Volume Snapshot and Restore Volume from Snapshot](/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support).", "zh": "更多详细信息，\n请参阅[卷快照和从快照还原卷](/zh-cn/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support)。"}
{"en": "This document describes the concept of cloning existing CSI Volumes in Kubernetes.  \nFamiliarity with [Volumes](/docs/concepts/storage/volumes) is suggested.", "zh": "本文档介绍 Kubernetes 中克隆现有 CSI 卷的概念。阅读前建议先熟悉\n[卷](/zh-cn/docs/concepts/storage/volumes)。"}
{"en": "## Introduction\n\nThe {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} Volume Cloning feature adds \nsupport for specifying existing {{< glossary_tooltip text=\"PVC\" term_id=\"persistent-volume-claim\" >}}s \nin the `dataSource` field to indicate a user would like to clone a {{< glossary_tooltip term_id=\"volume\" >}}.", "zh": "## 介绍\n\n{{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} 卷克隆功能增加了通过在\n`dataSource` 字段中指定存在的\n{{< glossary_tooltip text=\"PVC\" term_id=\"persistent-volume-claim\" >}}，\n来表示用户想要克隆的 {{< glossary_tooltip term_id=\"volume\" >}}。"}
{"en": "A Clone is defined as a duplicate of an existing Kubernetes Volume that can be \nconsumed as any standard Volume would be.  The only difference is that upon \nprovisioning, rather than creating a \"new\" empty Volume, the back end device \ncreates an exact duplicate of the specified Volume.", "zh": "克隆（Clone），意思是为已有的 Kubernetes 卷创建副本，它可以像任何其它标准卷一样被使用。\n唯一的区别就是配置后，后端设备将创建指定完全相同的副本，而不是创建一个“新的”空卷。"}
{"en": "The implementation of cloning, from the perspective of the Kubernetes API, adds \nthe ability to specify an existing PVC as a dataSource during new PVC creation. \nThe source PVC must be bound and available (not in use).\n\nUsers need to be aware of the following when using this feature:", "zh": "从 Kubernetes API 的角度看，克隆的实现只是在创建新的 PVC 时，\n增加了指定一个现有 PVC 作为数据源的能力。源 PVC 必须是 bound\n状态且可用的（不在使用中）。\n\n用户在使用该功能时，需要注意以下事项："}
{"en": "* Cloning support (`VolumePVCDataSource`) is only available for CSI drivers.\n* Cloning support is only available for dynamic provisioners.\n* CSI drivers may or may not have implemented the volume cloning functionality.\n* You can only clone a PVC when it exists in the same namespace as the destination PVC \n  (source and destination must be in the same namespace).\n* Cloning is supported with a different Storage Class.\n    - Destination volume can be the same or a different storage class as the source.\n    - Default storage class can be used and storageClassName omitted in the spec.\n* Cloning can only be performed between two volumes that use the same VolumeMode setting \n  (if you request a block mode volume, the source MUST also be block mode)", "zh": "* 克隆支持（`VolumePVCDataSource`）仅适用于 CSI 驱动。\n* 克隆支持仅适用于 动态供应器。\n* CSI 驱动可能实现，也可能未实现卷克隆功能。\n* 仅当 PVC 与目标 PVC 存在于同一命名空间（源和目标 PVC 必须在相同的命名空间）时，才可以克隆 PVC。\n* 支持用一个不同存储类进行克隆。\n    - 目标卷和源卷可以是相同的存储类，也可以不同。\n    - 可以使用默认的存储类，也可以在 spec 中省略 storageClassName 字段。\n* 克隆只能在两个使用相同 VolumeMode 设置的卷中进行\n  （如果请求克隆一个块存储模式的卷，源卷必须也是块存储模式）。"}
{"en": "## Provisioning\n\nClones are provisioned like any other PVC with the exception of adding a dataSource that references an existing PVC in the same namespace.", "zh": "## 制备\n\n克隆卷与其他任何 PVC 一样配置，除了需要增加 dataSource 来引用同一命名空间中现有的 PVC。\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: clone-of-pvc-1\n    namespace: myns\nspec:\n  accessModes:\n  - ReadWriteOnce\n  storageClassName: cloning\n  resources:\n    requests:\n      storage: 5Gi\n  dataSource:\n    kind: PersistentVolumeClaim\n    name: pvc-1\n```\n\n{{< note >}}"}
{"en": "You must specify a capacity value for `spec.resources.requests.storage`, and the \nvalue you specify must be the same or larger than the capacity of the source volume.", "zh": "你必须为 `spec.resources.requests.storage` 指定一个值，并且你指定的值必须大于或等于源卷的值。\n{{< /note >}}"}
{"en": "The result is a new PVC with the name `clone-of-pvc-1` that has the exact same \ncontent as the specified source `pvc-1`.", "zh": "结果是一个名称为 `clone-of-pvc-1` 的新 PVC 与指定的源 `pvc-1` 拥有相同的内容。"}
{"en": "## Usage\n\nUpon availability of the new PVC, the cloned PVC is consumed the same as other PVC.  \nIt's also expected at this point that the newly created PVC is an independent object.  \nIt can be consumed, cloned, snapshotted, or deleted independently and without \nconsideration for it's original dataSource PVC.  This also implies that the source \nis not linked in any way to the newly created clone, it may also be modified or \ndeleted without affecting the newly created clone.", "zh": "## 使用\n\n一旦新的 PVC 可用，被克隆的 PVC 像其他 PVC 一样被使用。\n可以预期的是，新创建的 PVC 是一个独立的对象。\n可以独立使用、克隆、快照或删除它，而不需要考虑它的原始数据源 PVC。\n这也意味着，源没有以任何方式链接到新创建的 PVC，它也可以被修改或删除，而不会影响到新创建的克隆。"}
{"en": "This document describes the concept of a StorageClass in Kubernetes. Familiarity\nwith [volumes](/docs/concepts/storage/volumes/) and\n[persistent volumes](/docs/concepts/storage/persistent-volumes) is suggested.", "zh": "本文描述了 Kubernetes 中 StorageClass 的概念。\n建议先熟悉[卷](/zh-cn/docs/concepts/storage/volumes/)和[持久卷](/zh-cn/docs/concepts/storage/persistent-volumes)的概念。"}
{"en": "A StorageClass provides a way for administrators to describe the _classes_ of\nstorage they offer. Different classes might map to quality-of-service levels,\nor to backup policies, or to arbitrary policies determined by the cluster\nadministrators. Kubernetes itself is unopinionated about what classes\nrepresent.\n\nThe Kubernetes concept of a storage class is similar to “profiles” in some other\nstorage system designs.", "zh": "StorageClass 为管理员提供了描述存储**类**的方法。\n不同的类型可能会映射到不同的服务质量等级或备份策略，或是由集群管理员制定的任意策略。\nKubernetes 本身并不清楚各种类代表的什么。\n\nKubernetes 存储类的概念类似于一些其他存储系统设计中的\"配置文件\"。"}
{"en": "## StorageClass objects\n\nEach StorageClass contains the fields `provisioner`, `parameters`, and\n`reclaimPolicy`, which are used when a PersistentVolume belonging to the\nclass needs to be dynamically provisioned to satisfy a PersistentVolumeClaim (PVC).", "zh": "## StorageClass 对象   {#storageclass-objects}\n\n每个 StorageClass 都包含 `provisioner`、`parameters` 和 `reclaimPolicy` 字段，\n这些字段会在 StorageClass 需要动态制备 PersistentVolume 以满足 PersistentVolumeClaim (PVC) 时使用到。"}
{"en": "The name of a StorageClass object is significant, and is how users can\nrequest a particular class. Administrators set the name and other parameters\nof a class when first creating StorageClass objects.", "zh": "StorageClass 对象的命名很重要，用户使用这个命名来请求生成一个特定的类。\n当创建 StorageClass 对象时，管理员设置 StorageClass 对象的命名和其他参数。"}
{"en": "As an administrator, you can specify a default StorageClass that applies to any PVCs that\ndon't request a specific class. For more details, see the\n[PersistentVolumeClaim concept](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims).\n\nHere's an example of a StorageClass:", "zh": "作为管理员，你可以为没有申请绑定到特定 StorageClass 的 PVC 指定一个默认的存储类：\n更多详情请参阅\n[PersistentVolumeClaim 概念](/zh-cn/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)。\n\n{{% code_sample file=\"storage/storageclass-low-latency.yaml\" %}}"}
{"en": "## Default StorageClass\n\nYou can mark a StorageClass as the default for your cluster.\nFor instructions on setting the default StorageClass, see\n[Change the default StorageClass](/docs/tasks/administer-cluster/change-default-storage-class/).\n\nWhen a PVC does not specify a `storageClassName`, the default StorageClass is\nused.", "zh": "### 默认 StorageClass  {#default-storageclass}\n\n你可以将某个 StorageClass 标记为集群的默认存储类。\n关于如何设置默认的 StorageClass，\n请参见[更改默认 StorageClass](/zh-cn/docs/tasks/administer-cluster/change-default-storage-class/)。\n\n当一个 PVC 没有指定 `storageClassName` 时，会使用默认的 StorageClass。"}
{"en": "If you set the\n[`storageclass.kubernetes.io/is-default-class`](/docs/reference/labels-annotations-taints/#storageclass-kubernetes-io-is-default-class)\nannotation to true on more than one StorageClass in your cluster, and you then\ncreate a PersistentVolumeClaim with no `storageClassName` set, Kubernetes\nuses the most recently created default StorageClass.", "zh": "如果你在集群中的多个 StorageClass 上将\n[`storageclass.kubernetes.io/is-default-class`](/zh-cn/docs/reference/labels-annotations-taints/#storageclass-kubernetes-io-is-default-class)\n注解设置为 true，然后创建一个未设置 `storageClassName` 的 PersistentVolumeClaim (PVC)，\nKubernetes 将使用最近创建的默认 StorageClass。\n\n{{< note >}}"}
{"en": "You should try to only have one StorageClass in your cluster that is\nmarked as the default. The reason that Kubernetes allows you to have\nmultiple default StorageClasses is to allow for seamless migration.", "zh": "你应该尝试在集群中只将一个 StorageClass 标记为默认的存储类。\nKubernetes 允许你拥有多个默认 StorageClass 的原因是为了无缝迁移。\n{{< /note >}}"}
{"en": "You can create a PersistentVolumeClaim without specifying a `storageClassName`\nfor the new PVC, and you can do so even when no default StorageClass exists\nin your cluster. In this case, the new PVC creates as you defined it, and the\n`storageClassName` of that PVC remains unset until a default becomes available.\n\nYou can have a cluster without any default StorageClass. If you don't mark any\nStorageClass as default (and one hasn't been set for you by, for example, a cloud provider),\nthen Kubernetes cannot apply that defaulting for PersistentVolumeClaims that need\nit.", "zh": "你可以在创建新的 PVC 时不指定 `storageClassName`，即使在集群中没有默认 StorageClass 的情况下也可以这样做。\n在这种情况下，新的 PVC 会按照你定义的方式进行创建，并且该 PVC 的 `storageClassName` 将保持不设置，\n直到有可用的默认 StorageClass 为止。\n\n你可以拥有一个没有任何默认 StorageClass 的集群。\n如果你没有将任何 StorageClass 标记为默认（例如，云服务提供商还没有为你设置默认值），那么\nKubernetes 将无法为需要 StorageClass 的 PersistentVolumeClaim 应用默认值。"}
{"en": "If or when a default StorageClass becomes available, the control plane identifies any\nexisting PVCs without `storageClassName`. For the PVCs that either have an empty\nvalue for `storageClassName` or do not have this key, the control plane then\nupdates those PVCs to set `storageClassName` to match the new default StorageClass.\nIf you have an existing PVC where the `storageClassName` is `\"\"`, and you configure\na default StorageClass, then this PVC will not get updated.", "zh": "当默认 StorageClass 变得可用时，控制平面会查找所有未设置 `storageClassName` 的现有 PVC。\n对于那些 `storageClassName` 值为空或没有此键的 PVC，控制平面将更新它们，\n将 `storageClassName` 设置为匹配新的默认 StorageClass。如果你有一个现成的 PVC，其 `storageClassName` 为 `\"\"`，\n而你配置了默认的 StorageClass，那么该 PVC 将不会被更新。"}
{"en": "In order to keep binding to PVs with `storageClassName` set to `\"\"`\n(while a default StorageClass is present), you need to set the `storageClassName`\nof the associated PVC to `\"\"`.", "zh": "（当默认的 StorageClass 存在时）为了继续绑定到 `storageClassName` 为 `\"\"` 的 PV，\n你需要将关联 PVC 的 `storageClassName` 设置为 `\"\"`。"}
{"en": "### Provisioner\n\nEach StorageClass has a provisioner that determines what volume plugin is used\nfor provisioning PVs. This field must be specified.", "zh": "### 存储制备器  {#provisioner}\n\n每个 StorageClass 都有一个制备器（Provisioner），用来决定使用哪个卷插件制备 PV。\n该字段必须指定。"}
{"en": "| Volume Plugin        | Internal Provisioner |            Config Example             |", "zh": "| 卷插件               | 内置制备器 |               配置示例                |\n| :------------------- | :--------: | :-----------------------------------: |\n| AzureFile            |  &#x2713;  |       [Azure File](#azure-file)       |\n| CephFS               |     -      |                   -                   |\n| FC                   |     -      |                   -                   |\n| FlexVolume           |     -      |                   -                   |\n| iSCSI                |     -      |                   -                   |\n| Local                |     -      |            [Local](#local)            |\n| NFS                  |     -      |              [NFS](#nfs)              |\n| PortworxVolume       |  &#x2713;  |  [Portworx Volume](#portworx-volume)  |\n| RBD                  |  &#x2713;  |         [Ceph RBD](#ceph-rbd)         |\n| VsphereVolume        |  &#x2713;  |          [vSphere](#vsphere)          |"}
{"en": "You are not restricted to specifying the \"internal\" provisioners\nlisted here (whose names are prefixed with \"kubernetes.io\" and shipped\nalongside Kubernetes). You can also run and specify external provisioners,\nwhich are independent programs that follow a [specification](https://git.k8s.io/design-proposals-archive/storage/volume-provisioning.md)\ndefined by Kubernetes. Authors of external provisioners have full discretion\nover where their code lives, how the provisioner is shipped, how it needs to be\nrun, what volume plugin it uses (including Flex), etc. The repository\n[kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner)\nhouses a library for writing external provisioners that implements the bulk of\nthe specification. Some external provisioners are listed under the repository\n[kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner).", "zh": "你不限于指定此处列出的 \"内置\" 制备器（其名称前缀为 \"kubernetes.io\" 并打包在 Kubernetes 中）。\n你还可以运行和指定外部制备器，这些独立的程序遵循由 Kubernetes\n定义的[规范](https://git.k8s.io/design-proposals-archive/storage/volume-provisioning.md)。\n外部供应商的作者完全可以自由决定他们的代码保存于何处、打包方式、运行方式、使用的插件（包括 Flex）等。\n代码仓库 [kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner)\n包含一个用于为外部制备器编写功能实现的类库。你可以访问代码仓库\n[kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner)\n了解外部驱动列表。"}
{"en": "For example, NFS doesn't provide an internal provisioner, but an external\nprovisioner can be used. There are also cases when 3rd party storage\nvendors provide their own external provisioner.", "zh": "例如，NFS 没有内部制备器，但可以使用外部制备器。\n也有第三方存储供应商提供自己的外部制备器。"}
{"en": "## Reclaim policy\n\nPersistentVolumes that are dynamically created by a StorageClass will have the\n[reclaim policy](/docs/concepts/storage/persistent-volumes/#reclaiming)\nspecified in the `reclaimPolicy` field of the class, which can be\neither `Delete` or `Retain`. If no `reclaimPolicy` is specified when a\nStorageClass object is created, it will default to `Delete`.\n\nPersistentVolumes that are created manually and managed via a StorageClass will have\nwhatever reclaim policy they were assigned at creation.", "zh": "## 回收策略 {#reclaim-policy}\n\n由 StorageClass 动态创建的 PersistentVolume 会在类的\n[reclaimPolicy](/zh-cn/docs/concepts/storage/persistent-volumes/#reclaiming)\n字段中指定回收策略，可以是 `Delete` 或者 `Retain`。\n如果 StorageClass 对象被创建时没有指定 `reclaimPolicy`，它将默认为 `Delete`。\n\n通过 StorageClass 手动创建并管理的 PersistentVolume 会使用它们被创建时指定的回收策略。"}
{"en": "## Volume expansion {#allow-volume-expansion}\n\nPersistentVolumes can be configured to be expandable. This allows you to resize the\nvolume by editing the corresponding PVC object, requesting a new larger amount of\nstorage.\n\nThe following types of volumes support volume expansion, when the underlying\nStorageClass has the field `allowVolumeExpansion` set to true.", "zh": "## 卷扩展   {#allow-volume-expansion}\n\nPersistentVolume 可以配置为可扩展。\n这允许你通过编辑相应的 PVC 对象来调整卷大小，申请一个新的、更大的存储容量。\n\n当下层 StorageClass 的 `allowVolumeExpansion` 字段设置为 true 时，以下类型的卷支持卷扩展。"}
{"en": "\"Table of Volume types and the version of Kubernetes they require\"", "zh": "{{< table caption = \"卷类型及其 Kubernetes 版本要求\"  >}}"}
{"en": "Volume type | Required Kubernetes version for volume expansion", "zh": "| 卷类型               | 卷扩展的 Kubernetes 版本要求  |\n| :------------------- | :------------------------ |\n| Azure File           | 1.11                      |\n| CSI                  | 1.24                      |\n| FlexVolume           | 1.13                      |\n| Portworx             | 1.11                      |\n| rbd                  | 1.11                      |\n\n{{< /table >}}\n\n{{< note >}}"}
{"en": "You can only use the volume expansion feature to grow a Volume, not to shrink it.", "zh": "此功能仅可用于扩容卷，不能用于缩小卷。\n{{< /note >}}"}
{"en": "## Mount options\n\nPersistentVolumes that are dynamically created by a StorageClass will have the\nmount options specified in the `mountOptions` field of the class.\n\nIf the volume plugin does not support mount options but mount options are\nspecified, provisioning will fail. Mount options are **not** validated on either\nthe class or PV. If a mount option is invalid, the PV mount fails.", "zh": "## 挂载选项 {#mount-options}\n\n由 StorageClass 动态创建的 PersistentVolume 将使用类中 `mountOptions` 字段指定的挂载选项。\n\n如果卷插件不支持挂载选项，却指定了挂载选项，则制备操作会失败。\n挂载选项在 StorageClass 和 PV 上都**不**会做验证。如果其中一个挂载选项无效，那么这个 PV 挂载操作就会失败。"}
{"en": "## Volume binding mode", "zh": "## 卷绑定模式 {#volume-binding-mode}"}
{"en": "The `volumeBindingMode` field controls when\n[volume binding and dynamic provisioning](/docs/concepts/storage/persistent-volumes/#provisioning)\nshould occur. When unset, `Immediate` mode is used by default.", "zh": "`volumeBindingMode`\n字段控制了[卷绑定和动态制备](/zh-cn/docs/concepts/storage/persistent-volumes/#provisioning)应该发生在什么时候。\n当未设置时，默认使用 `Immediate` 模式。"}
{"en": "The `Immediate` mode indicates that volume binding and dynamic\nprovisioning occurs once the PersistentVolumeClaim is created. For storage\nbackends that are topology-constrained and not globally accessible from all Nodes\nin the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling\nrequirements. This may result in unschedulable Pods.", "zh": "`Immediate` 模式表示一旦创建了 PersistentVolumeClaim 也就完成了卷绑定和动态制备。\n对于由于拓扑限制而非集群所有节点可达的存储后端，PersistentVolume\n会在不知道 Pod 调度要求的情况下绑定或者制备。"}
{"en": "A cluster administrator can address this issue by specifying the `WaitForFirstConsumer` mode which\nwill delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.\nPersistentVolumes will be selected or provisioned conforming to the topology that is\nspecified by the Pod's scheduling constraints. These include, but are not limited to, [resource\nrequirements](/docs/concepts/configuration/manage-resources-containers/),\n[node selectors](/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector),\n[pod affinity and\nanti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity),\nand [taints and tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration).", "zh": "集群管理员可以通过指定 `WaitForFirstConsumer` 模式来解决此问题。\n该模式将延迟 PersistentVolume 的绑定和制备，直到使用该 PersistentVolumeClaim 的 Pod 被创建。\nPersistentVolume 会根据 Pod 调度约束指定的拓扑来选择或制备。\n这些包括但不限于[资源需求](/zh-cn/docs/concepts/configuration/manage-resources-containers/)、\n[节点筛选器](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)、\n[Pod 亲和性和互斥性](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity/)、\n以及[污点和容忍度](/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration)。"}
{"en": "The following plugins support `WaitForFirstConsumer` with dynamic provisioning:\n\n- CSI volumes, provided that the specific CSI driver supports this\n\nThe following plugins support `WaitForFirstConsumer` with pre-created PersistentVolume binding:\n\n- CSI volumes, provided that the specific CSI driver supports this\n- [`local`](#local)", "zh": "以下插件支持使用动态制备的 `WaitForFirstConsumer`：\n\n- CSI 卷，前提是特定的 CSI 驱动程序支持此卷\n\n以下插件支持预创建绑定 PersistentVolume 的 `WaitForFirstConsumer` 模式：\n\n- CSI 卷，前提是特定的 CSI 驱动程序支持此卷\n- [`local`](#local)\n\n{{< note >}}"}
{"en": "If you choose to use `WaitForFirstConsumer`, do not use `nodeName` in the Pod spec\nto specify node affinity.\nIf `nodeName` is used in this case, the scheduler will be bypassed and PVC will remain in `pending` state.\n\nInstead, you can use node selector for `kubernetes.io/hostname`:", "zh": "如果你选择使用 `WaitForFirstConsumer`，请不要在 Pod 规约中使用 `nodeName` 来指定节点亲和性。\n如果在这种情况下使用 `nodeName`，Pod 将会绕过调度程序，PVC 将停留在 `pending` 状态。\n\n相反，你可以为 `kubernetes.io/hostname` 使用节点选择器：\n\n{{< /note >}}\n\n{{% code_sample language=\"yaml\" file=\"storage/storageclass/pod-volume-binding.yaml\" %}}"}
{"en": "## Allowed topologies", "zh": "## 允许的拓扑结构  {#allowed-topologies}"}
{"en": "When a cluster operator specifies the `WaitForFirstConsumer` volume binding mode, it is no longer necessary\nto restrict provisioning to specific topologies in most situations. However,\nif still required, `allowedTopologies` can be specified.", "zh": "当集群操作人员使用了 `WaitForFirstConsumer` 的卷绑定模式，\n在大部分情况下就没有必要将制备限制为特定的拓扑结构。\n然而，如果还有需要的话，可以使用 `allowedTopologies`。"}
{"en": "This example demonstrates how to restrict the topology of provisioned volumes to specific\nzones and should be used as a replacement for the `zone` and `zones` parameters for the\nsupported plugins.", "zh": "这个例子描述了如何将制备卷的拓扑限制在特定的区域，\n在使用时应该根据插件支持情况替换 `zone` 和 `zones` 参数。\n\n{{% code_sample language=\"yaml\" file=\"storage/storageclass/storageclass-topology.yaml\" %}}"}
{"en": "## Parameters\n\nStorageClasses have parameters that describe volumes belonging to the storage\nclass. Different parameters may be accepted depending on the `provisioner`.\nWhen a parameter is omitted, some default is used.\n\nThere can be at most 512 parameters defined for a StorageClass.\nThe total length of the parameters object including its keys and values cannot\nexceed 256 KiB.", "zh": "## 参数 {#parameters}\n\nStorageClass 的参数描述了存储类的卷。取决于制备器，可以接受不同的参数。\n当参数被省略时，会使用默认值。\n\n一个 StorageClass 最多可以定义 512 个参数。这些参数对象的总长度不能超过 256 KiB，包括参数的键和值。\n\n### AWS EBS"}
{"en": "Kubernetes {{< skew currentVersion >}} does not include a `awsElasticBlockStore` volume type.\n\nThe AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release\nand then removed entirely in the v1.27 release.", "zh": "Kubernetes {{< skew currentVersion >}} 不包含 `awsElasticBlockStore` 卷类型。\n\nAWSElasticBlockStore 树内存储驱动程序在 Kubernetes v1.19 版本中被弃用，并在 v1.27 版本中被完全移除。"}
{"en": "The Kubernetes project suggests that you use the [AWS EBS](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)\nout-of-tree storage driver instead.\n\nHere is an example StorageClass for the AWS EBS CSI driver:", "zh": "Kubernetes 项目建议你转为使用 [AWS EBS](https://github.com/kubernetes-sigs/aws-ebs-csi-driver) 树外存储驱动程序。\n\n以下是一个针对 AWS EBS CSI 驱动程序的 StorageClass 示例：\n\n{{% code_sample language=\"yaml\" file=\"storage/storageclass/storageclass-aws-ebs.yaml\" %}}\n\n### AWS EFS"}
{"en": "To configure AWS EFS storage, you can use the out-of-tree [AWS_EFS_CSI_DRIVER](https://github.com/kubernetes-sigs/aws-efs-csi-driver).", "zh": "要配置 AWS EFS 存储，你可以使用树外\n[AWS_EFS_CSI_DRIVER](https://github.com/kubernetes-sigs/aws-efs-csi-driver)。\n\n{{% code_sample language=\"yaml\" file=\"storage/storageclass/storageclass-aws-efs.yaml\" %}}"}
{"en": "- `provisioningMode`: The type of volume to be provisioned by Amazon EFS. Currently, only access point based provisioning is supported (`efs-ap`).\n- `fileSystemId`: The file system under which the access point is created.\n- `directoryPerms`: The directory permissions of the root directory created by the access point.", "zh": "- `provisioningMode`：由 Amazon EFS 制备的卷类型。目前，仅支持基于访问点的制备（`efs-ap`）。\n- `fileSystemId`：在此文件系统下创建访问点。\n- `directoryPerms`：由访问点所创建的根目录的目录权限。"}
{"en": "For more details, refer to the [AWS_EFS_CSI_Driver Dynamic Provisioning](https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/dynamic_provisioning/README.md) documentation.", "zh": "有关细节参阅\n[AWS_EFS_CSI_Driver 动态制备](https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/dynamic_provisioning/README.md)文档。\n\n### NFS"}
{"en": "To configure NFS storage, you can use the in-tree driver or the\n[NFS CSI driver for Kubernetes](https://github.com/kubernetes-csi/csi-driver-nfs#readme)\n(recommended).", "zh": "要配置 NFS 存储，\n你可以使用树内驱动程序或[针对 Kubernetes 的 NFS CSI 驱动程序](https://github.com/kubernetes-csi/csi-driver-nfs#readme)（推荐）。\n\n{{% code_sample language=\"yaml\" file=\"storage/storageclass/storageclass-nfs.yaml\" %}}"}
{"en": "- `server`: Server is the hostname or IP address of the NFS server.\n- `path`: Path that is exported by the NFS server.\n- `readOnly`: A flag indicating whether the storage will be mounted as read only (default false).", "zh": "- `server`：NFS 服务器的主机名或 IP 地址。\n- `path`：NFS 服务器导出的路径。\n- `readOnly`：是否将存储挂载为只读的标志（默认为 false）。"}
{"en": "Kubernetes doesn't include an internal NFS provisioner.\nYou need to use an external provisioner to create a StorageClass for NFS.\nHere are some examples:\n\n- [NFS Ganesha server and external provisioner](https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner)\n- [NFS subdir external provisioner](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)", "zh": "Kubernetes 不包含内部 NFS 驱动。你需要使用外部驱动为 NFS 创建 StorageClass。\n这里有些例子：\n\n- [NFS Ganesha 服务器和外部驱动](https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner)\n- [NFS subdir 外部驱动](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)\n\n### vSphere"}
{"en": "There are two types of provisioners for vSphere storage classes:\n\n- [CSI provisioner](#vsphere-provisioner-csi): `csi.vsphere.vmware.com`\n- [vCP provisioner](#vcp-provisioner): `kubernetes.io/vsphere-volume`\n\nIn-tree provisioners are [deprecated](/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi).\nFor more information on the CSI provisioner, see\n[Kubernetes vSphere CSI Driver](https://vsphere-csi-driver.sigs.k8s.io/) and\n[vSphereVolume CSI migration](/docs/concepts/storage/volumes/#vsphere-csi-migration).", "zh": "vSphere 存储类有两种制备器：\n\n- [CSI 制备器](#vsphere-provisioner-csi)：`csi.vsphere.vmware.com`\n- [vCP 制备器](#vcp-provisioner)：`kubernetes.io/vsphere-volume`\n\n树内制备器已经被\n[弃用](/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi)。\n更多关于 CSI 制备器的详情，请参阅\n[Kubernetes vSphere CSI 驱动](https://vsphere-csi-driver.sigs.k8s.io/)\n和 [vSphereVolume CSI 迁移](/zh-cn/docs/concepts/storage/volumes/#vsphere-csi-migration)。"}
{"en": "#### CSI Provisioner {#vsphere-provisioner-csi}\n\nThe vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters.\nFor an example, refer to the [vSphere CSI repository](https://github.com/kubernetes-sigs/vsphere-csi-driver/blob/master/example/vanilla-k8s-RWM-filesystem-volumes/example-sc.yaml).", "zh": "#### CSI 制备器 {#vsphere-provisioner-csi}\n\nvSphere CSI StorageClass 制备器在 Tanzu Kubernetes 集群下运行。示例请参阅\n[vSphere CSI 仓库](https://github.com/kubernetes-sigs/vsphere-csi-driver/blob/master/example/vanilla-k8s-RWM-filesystem-volumes/example-sc.yaml)。"}
{"en": "#### vCP Provisioner\n\nThe following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.", "zh": "#### vCP 制备器 {#vcp-provisioner}\n\n以下示例使用 VMware Cloud Provider（vCP）StorageClass 制备器。"}
{"en": "1. Create a StorageClass with a user specified disk format.", "zh": "1. 使用用户指定的磁盘格式创建一个 StorageClass。\n\n   ```yaml\n   apiVersion: storage.k8s.io/v1\n   kind: StorageClass\n   metadata:\n     name: fast\n   provisioner: kubernetes.io/vsphere-volume\n   parameters:\n     diskformat: zeroedthick\n   ```"}
{"en": "`diskformat`: `thin`, `zeroedthick` and `eagerzeroedthick`. Default: `\"thin\"`.", "zh": "`diskformat`：`thin`、`zeroedthick` 和 `eagerzeroedthick`。默认值：`\"thin\"`。"}
{"en": "2. Create a StorageClass with a disk format on a user specified datastore.", "zh": "2. 在用户指定的数据存储上创建磁盘格式的 StorageClass。\n\n   ```yaml\n   apiVersion: storage.k8s.io/v1\n   kind: StorageClass\n   metadata:\n     name: fast\n   provisioner: kubernetes.io/vsphere-volume\n   parameters:\n     diskformat: zeroedthick\n     datastore: VSANDatastore\n   ```"}
{"en": "`datastore`: The user can also specify the datastore in the StorageClass.\n   The volume will be created on the datastore specified in the StorageClass,\n   which in this case is `VSANDatastore`. This field is optional. If the\n   datastore is not specified, then the volume will be created on the datastore\n   specified in the vSphere config file used to initialize the vSphere Cloud\n   Provider.", "zh": "`datastore`：用户也可以在 StorageClass 中指定数据存储。\n   卷将在 StorageClass 中指定的数据存储上创建，在这种情况下是 `VSANDatastore`。\n   该字段是可选的。\n   如果未指定数据存储，则将在用于初始化 vSphere Cloud Provider 的 vSphere\n   配置文件中指定的数据存储上创建该卷。"}
{"en": "3. Storage Policy Management inside kubernetes", "zh": "3. Kubernetes 中的存储策略管理"}
{"en": "- Using existing vCenter SPBM policy\n\n     One of the most important features of vSphere for Storage Management is\n     policy based Management. Storage Policy Based Management (SPBM) is a\n     storage policy framework that provides a single unified control plane\n     across a broad range of data services and storage solutions. SPBM enables\n     vSphere administrators to overcome upfront storage provisioning challenges,\n     such as capacity planning, differentiated service levels and managing\n     capacity headroom.\n\n     The SPBM policies can be specified in the StorageClass using the\n     `storagePolicyName` parameter.", "zh": "- 使用现有的 vCenter SPBM 策略\n\n     vSphere 用于存储管理的最重要特性之一是基于策略的管理。\n     基于存储策略的管理（SPBM）是一个存储策略框架，提供单一的统一控制平面的跨越广泛的数据服务和存储解决方案。\n     SPBM 使得 vSphere 管理员能够克服先期的存储配置挑战，如容量规划、差异化服务等级和管理容量空间。\n\n     SPBM 策略可以在 StorageClass 中使用 `storagePolicyName` 参数声明。"}
{"en": "- Virtual SAN policy support inside Kubernetes\n\n     Vsphere Infrastructure (VI) Admins will have the ability to specify custom\n     Virtual SAN Storage Capabilities during dynamic volume provisioning. You\n     can now define storage requirements, such as performance and availability,\n     in the form of storage capabilities during dynamic volume provisioning.\n     The storage capability requirements are converted into a Virtual SAN\n     policy which are then pushed down to the Virtual SAN layer when a\n     persistent volume (virtual disk) is being created. The virtual disk is\n     distributed across the Virtual SAN datastore to meet the requirements.\n\n     You can see [Storage Policy Based Management for dynamic provisioning of volumes](https://github.com/vmware-archive/vsphere-storage-for-kubernetes/blob/fa4c8b8ad46a85b6555d715dd9d27ff69839df53/documentation/policy-based-mgmt.md)\n     for more details on how to use storage policies for persistent volumes\n     management.", "zh": "- Kubernetes 内的 Virtual SAN 策略支持\n\n     Vsphere Infrastructure（VI）管理员将能够在动态卷配置期间指定自定义 Virtual SAN\n     存储功能。你现在可以在动态制备卷期间以存储能力的形式定义存储需求，例如性能和可用性。\n     存储能力需求会转换为 Virtual SAN 策略，之后当持久卷（虚拟磁盘）被创建时，\n     会将其推送到 Virtual SAN 层。虚拟磁盘分布在 Virtual SAN 数据存储中以满足要求。\n\n     你可以参考[基于存储策略的动态制备卷管理](https://github.com/vmware-archive/vsphere-storage-for-kubernetes/blob/fa4c8b8ad46a85b6555d715dd9d27ff69839df53/documentation/policy-based-mgmt.md)，\n     进一步了解有关持久卷管理的存储策略的详细信息。"}
{"en": "There are few\n[vSphere examples](https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere)\nwhich you try out for persistent volume management inside Kubernetes for vSphere.", "zh": "有几个 [vSphere 例子](https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere)供你在\nKubernetes for vSphere 中尝试进行持久卷管理。"}
{"en": "### Ceph RBD (deprecated) {#ceph-rbd}", "zh": "### Ceph RBD（已弃用）  {#ceph-rbd}\n\n{{< note >}}\n{{< feature-state state=\"deprecated\" for_k8s_version=\"v1.28\" >}}"}
{"en": "This internal provisioner of Ceph RBD is deprecated. Please use\n[CephFS RBD CSI driver](https://github.com/ceph/ceph-csi).", "zh": "Ceph RBD 的内部驱动程序已被弃用。请使用 [CephFS RBD CSI驱动程序](https://github.com/ceph/ceph-csi)。\n{{< /note >}}\n\n{{% code_sample language=\"yaml\" file=\"storage/storageclass/storageclass-ceph-rbd.yaml\" %}}"}
{"en": "- `monitors`: Ceph monitors, comma delimited. This parameter is required.\n- `adminId`: Ceph client ID that is capable of creating images in the pool.\n  Default is \"admin\".\n- `adminSecretName`: Secret Name for `adminId`. This parameter is required.\n  The provided secret must have type \"kubernetes.io/rbd\".\n- `adminSecretNamespace`: The namespace for `adminSecretName`. Default is \"default\".\n- `pool`: Ceph RBD pool. Default is \"rbd\".\n- `userId`: Ceph client ID that is used to map the RBD image. Default is the\n  same as `adminId`.", "zh": "- `monitors`：Ceph monitor，逗号分隔。该参数是必需的。\n- `adminId`：Ceph 客户端 ID，用于在池 ceph 池中创建映像。默认是 \"admin\"。\n- `adminSecret`：`adminId` 的 Secret 名称。该参数是必需的。\n  提供的 secret 必须有值为 \"kubernetes.io/rbd\" 的 type 参数。\n- `adminSecretNamespace`：`adminSecret` 的命名空间。默认是 \"default\"。\n- `pool`：Ceph RBD 池。默认是 \"rbd\"。\n- `userId`：Ceph 客户端 ID，用于映射 RBD 镜像。默认与 `adminId` 相同。"}
{"en": "- `userSecretName`: The name of Ceph Secret for `userId` to map RBD image. It\n  must exist in the same namespace as PVCs. This parameter is required.\n  The provided secret must have type \"kubernetes.io/rbd\", for example created in this\n  way:", "zh": "- `userSecretName`：用于映射 RBD 镜像的 `userId` 的 Ceph Secret 的名字。\n  它必须与 PVC 存在于相同的 namespace 中。该参数是必需的。\n  提供的 secret 必须具有值为 \"kubernetes.io/rbd\" 的 type 参数，例如以这样的方式创建：\n\n  ```shell\n  kubectl create secret generic ceph-secret --type=\"kubernetes.io/rbd\" \\\n    --from-literal=key='QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==' \\\n    --namespace=kube-system\n  ```"}
{"en": "- `userSecretNamespace`: The namespace for `userSecretName`.\n- `fsType`: fsType that is supported by kubernetes. Default: `\"ext4\"`.\n- `imageFormat`: Ceph RBD image format, \"1\" or \"2\". Default is \"2\".\n- `imageFeatures`: This parameter is optional and should only be used if you\n  set `imageFormat` to \"2\". Currently supported features are `layering` only.\n  Default is \"\", and no features are turned on.", "zh": "- `userSecretNamespace`：`userSecretName` 的命名空间。\n- `fsType`：Kubernetes 支持的 fsType。默认：`\"ext4\"`。\n- `imageFormat`：Ceph RBD 镜像格式，\"1\" 或者 \"2\"。默认值是 \"1\"。\n- `imageFeatures`：这个参数是可选的，只能在你将 `imageFormat` 设置为 \"2\" 才使用。\n  目前支持的功能只是 `layering`。默认是 \"\"，没有功能打开。"}
{"en": "### Azure Disk", "zh": "### Azure 磁盘 {#azure-disk}"}
{"en": "Kubernetes {{< skew currentVersion >}} does not include a `azureDisk` volume type.\n\nThe `azureDisk` in-tree storage driver was deprecated in the Kubernetes v1.19 release\nand then removed entirely in the v1.27 release.", "zh": "Kubernetes {{< skew currentVersion >}} 不包含 `azureDisk` 卷类型。\n\n`azureDisk` 树内存储驱动程序在 Kubernetes v1.19 版本中被弃用，并在 v1.27 版本中被完全移除。"}
{"en": "The Kubernetes project suggests that you use the [Azure Disk](https://github.com/kubernetes-sigs/azuredisk-csi-driver) third party\nstorage driver instead.", "zh": "Kubernetes 项目建议你转为使用\n[Azure Disk](https://github.com/kubernetes-sigs/azuredisk-csi-driver) 第三方存储驱动程序。"}
{"en": "### Azure File (deprecated) {#azure-file}", "zh": "### Azure 文件（已弃用）  {#azure-file}\n\n{{% code_sample language=\"yaml\" file=\"storage/storageclass/storageclass-azure-file.yaml\" %}}"}
{"en": "- `skuName`: Azure storage account SKU tier. Default is empty.\n- `location`: Azure storage account location. Default is empty.\n- `storageAccount`: Azure storage account name. Default is empty. If a storage\n  account is not provided, all storage accounts associated with the resource\n  group are searched to find one that matches `skuName` and `location`. If a\n  storage account is provided, it must reside in the same resource group as the\n  cluster, and `skuName` and `location` are ignored.\n- `secretNamespace`: the namespace of the secret that contains the Azure Storage\n  Account Name and Key. Default is the same as the Pod.\n- `secretName`: the name of the secret that contains the Azure Storage Account Name and\n  Key. Default is `azure-storage-account-<accountName>-secret`\n- `readOnly`: a flag indicating whether the storage will be mounted as read only.\n  Defaults to false which means a read/write mount. This setting will impact the\n  `ReadOnly` setting in VolumeMounts as well.", "zh": "- `skuName`：Azure 存储帐户 SKU 层。默认为空。\n- `location`：Azure 存储帐户位置。默认为空。\n- `storageAccount`：Azure 存储帐户名称。默认为空。\n  如果不提供存储帐户，会搜索所有与资源相关的存储帐户，以找到一个匹配\n  `skuName` 和 `location` 的账号。\n  如果提供存储帐户，它必须存在于与集群相同的资源组中，`skuName` 和 `location` 会被忽略。\n- `secretNamespace`：包含 Azure 存储帐户名称和密钥的密钥的名字空间。\n  默认值与 Pod 相同。\n- `secretName`：包含 Azure 存储帐户名称和密钥的密钥的名称。\n  默认值为 `azure-storage-account-<accountName>-secret`\n- `readOnly`：指示是否将存储安装为只读的标志。默认为 false，表示\"读/写\"挂载。\n  该设置也会影响 VolumeMounts 中的 `ReadOnly` 设置。"}
{"en": "During storage provisioning, a secret named by `secretName` is created for the\nmounting credentials. If the cluster has enabled both\n[RBAC](/docs/reference/access-authn-authz/rbac/) and\n[Controller Roles](/docs/reference/access-authn-authz/rbac/#controller-roles),\nadd the `create` permission of resource `secret` for clusterrole\n`system:controller:persistent-volume-binder`.", "zh": "在存储制备期间，为挂载凭证创建一个名为 `secretName` 的 Secret。如果集群同时启用了\n[RBAC](/zh-cn/docs/reference/access-authn-authz/rbac/)\n和[控制器角色](/zh-cn/docs/reference/access-authn-authz/rbac/#controller-roles)，\n为 `system:controller:persistent-volume-binder` 的 clusterrole 添加\n`Secret` 资源的 `create` 权限。"}
{"en": "In a multi-tenancy context, it is strongly recommended to set the value for\n`secretNamespace` explicitly, otherwise the storage account credentials may\nbe read by other users.", "zh": "在多租户上下文中，强烈建议显式设置 `secretNamespace` 的值，否则其他用户可能会读取存储帐户凭据。"}
{"en": "### Portworx volume (deprecated) {#portworx-volume}", "zh": "### Portworx 卷（已弃用）    {#portworx-volume}\n\n{{% code_sample language=\"yaml\" file=\"storage/storageclass/storageclass-portworx-volume.yaml\" %}}"}
{"en": "- `fs`: filesystem to be laid out: `none/xfs/ext4` (default: `ext4`).\n- `block_size`: block size in Kbytes (default: `32`).\n- `repl`: number of synchronous replicas to be provided in the form of\n  replication factor `1..3` (default: `1`) A string is expected here i.e.\n  `\"1\"` and not `1`.\n- `priority_io`: determines whether the volume will be created from higher\n  performance or a lower priority storage `high/medium/low` (default: `low`).\n- `snap_interval`: clock/time interval in minutes for when to trigger snapshots.\n  Snapshots are incremental based on difference with the prior snapshot, 0\n  disables snaps (default: `0`). A string is expected here i.e.\n  `\"70\"` and not `70`.\n- `aggregation_level`: specifies the number of chunks the volume would be\n  distributed into, 0 indicates a non-aggregated volume (default: `0`). A string\n  is expected here i.e. `\"0\"` and not `0`\n- `ephemeral`: specifies whether the volume should be cleaned-up after unmount\n  or should be persistent. `emptyDir` use case can set this value to true and\n  `persistent volumes` use case such as for databases like Cassandra should set\n  to false, `true/false` (default `false`). A string is expected here i.e.\n  `\"true\"` and not `true`.", "zh": "- `fs`：选择的文件系统：`none/xfs/ext4`（默认：`ext4`）。\n- `block_size`：以 Kbytes 为单位的块大小（默认值：`32`）。\n- `repl`：同步副本数量，以复制因子 `1..3`（默认值：`1`）的形式提供。\n  这里需要填写字符串，即，`\"1\"` 而不是 `1`。\n- `io_priority`：决定是否从更高性能或者较低优先级存储创建卷\n  `high/medium/low`（默认值：`low`）。\n- `snap_interval`：触发快照的时钟/时间间隔（分钟）。\n  快照是基于与先前快照的增量变化，0 是禁用快照（默认：`0`）。\n  这里需要填写字符串，即，是 `\"70\"` 而不是 `70`。\n- `aggregation_level`：指定卷分配到的块数量，0 表示一个非聚合卷（默认：`0`）。\n  这里需要填写字符串，即，是 `\"0\"` 而不是 `0`。\n- `ephemeral`：指定卷在卸载后进行清理还是持久化。\n  `emptyDir` 的使用场景可以将这个值设置为 true，\n  `persistent volumes` 的使用场景可以将这个值设置为 false\n  （例如 Cassandra 这样的数据库）\n  `true/false`（默认为 `false`）。这里需要填写字符串，即，\n  是 `\"true\"` 而不是 `true`。"}
{"en": "### Local", "zh": "### 本地 {#local}\n\n{{% code_sample language=\"yaml\" file=\"storage/storageclass/storageclass-local.yaml\" %}}"}
{"en": "Local volumes do not support dynamic provisioning in Kubernetes {{< skew currentVersion >}};\nhowever a StorageClass should still be created to delay volume binding until a Pod is actually\nscheduled to the appropriate node. This is specified by the `WaitForFirstConsumer` volume\nbinding mode.", "zh": "在 Kubernetes {{< skew currentVersion >}} 中，本地卷还不支持动态制备；\n然而还是需要创建 StorageClass 以延迟卷绑定，直到 Pod 被实际调度到合适的节点。\n这是由 `WaitForFirstConsumer` 卷绑定模式指定的。"}
{"en": "Delaying volume binding allows the scheduler to consider all of a Pod's\nscheduling constraints when choosing an appropriate PersistentVolume for a\nPersistentVolumeClaim.", "zh": "延迟卷绑定使得调度器在为 PersistentVolumeClaim 选择一个合适的\nPersistentVolume 时能考虑到所有 Pod 的调度限制。"}
{"en": "This document describes the concept of VolumeSnapshotClass in Kubernetes. Familiarity\nwith [volume snapshots](/docs/concepts/storage/volume-snapshots/) and\n[storage classes](/docs/concepts/storage/storage-classes) is suggested.", "zh": "本文档描述了 Kubernetes 中 VolumeSnapshotClass 的概念。建议熟悉\n[卷快照（Volume Snapshots）](/zh-cn/docs/concepts/storage/volume-snapshots/)和\n[存储类（Storage Class）](/zh-cn/docs/concepts/storage/storage-classes)。"}
{"en": "## Introduction\n\nJust like StorageClass provides a way for administrators to describe the \"classes\"\nof storage they offer when provisioning a volume, VolumeSnapshotClass provides a\nway to describe the \"classes\" of storage when provisioning a volume snapshot.", "zh": "## 介绍 {#introduction}\n\n就像 StorageClass 为管理员提供了一种在配置卷时描述存储“类”的方法，\nVolumeSnapshotClass 提供了一种在配置卷快照时描述存储“类”的方法。"}
{"en": "## The VolumeSnapshotClass Resource\n\nEach VolumeSnapshotClass contains the fields `driver`, `deletionPolicy`, and `parameters`,\nwhich are used when a VolumeSnapshot belonging to the class needs to be\ndynamically provisioned.\n\nThe name of a VolumeSnapshotClass object is significant, and is how users can\nrequest a particular class. Administrators set the name and other parameters\nof a class when first creating VolumeSnapshotClass objects, and the objects cannot\nbe updated once they are created.\n\n{{< note >}}\nInstallation of the CRDs is the responsibility of the Kubernetes distribution. \nWithout the required CRDs present, the creation of a VolumeSnapshotClass fails.\n{{< /note >}}", "zh": "## VolumeSnapshotClass 资源  {#the-volumesnapshortclass-resource}\n\n每个 VolumeSnapshotClass 都包含 `driver`、`deletionPolicy` 和 `parameters` 字段，\n在需要动态配置属于该类的 VolumeSnapshot 时使用。\n\nVolumeSnapshotClass 对象的名称很重要，是用户可以请求特定类的方式。\n管理员在首次创建 VolumeSnapshotClass 对象时设置类的名称和其他参数，\n对象一旦创建就无法更新。\n\n{{< note >}}\nCRD 的安装是 Kubernetes 发行版的责任。 如果不存在所需的 CRD，则 VolumeSnapshotClass 的创建将失败。\n{{< /note >}}\n\n```yaml\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: csi-hostpath-snapclass\ndriver: hostpath.csi.k8s.io\ndeletionPolicy: Delete\nparameters:\n```"}
{"en": "Administrators can specify a default VolumeSnapshotClass for VolumeSnapshots\nthat don't request any particular class to bind to by adding the\n`snapshot.storage.kubernetes.io/is-default-class: \"true\"` annotation:", "zh": "管理员可以为未请求任何特定类绑定的 VolumeSnapshots 指定默认的 VolumeSnapshotClass，\n方法是设置注解 `snapshot.storage.kubernetes.io/is-default-class: \"true\"`：\n\n```yaml\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: csi-hostpath-snapclass\n  annotations:\n    snapshot.storage.kubernetes.io/is-default-class: \"true\"\ndriver: hostpath.csi.k8s.io\ndeletionPolicy: Delete\nparameters:\n```"}
{"en": "### Driver\n\nVolume snapshot classes have a driver that determines what CSI volume plugin is\nused for provisioning VolumeSnapshots. This field must be specified.", "zh": "### 驱动程序 {#driver}\n\n卷快照类有一个驱动程序，用于确定配置 VolumeSnapshot 的 CSI 卷插件。\n此字段必须指定。"}
{"en": "### DeletionPolicy\n\nVolume snapshot classes have a [deletionPolicy](/docs/concepts/storage/volume-snapshots/#delete).\nIt enables you to configure what happens to a VolumeSnapshotContent when the VolumeSnapshot\nobject it is bound to is to be deleted. The deletionPolicy of a volume snapshot class can\neither be `Retain` or `Delete`. This field must be specified.\n\nIf the deletionPolicy is `Delete`, then the underlying storage snapshot will be \ndeleted along with the VolumeSnapshotContent object. If the deletionPolicy is `Retain`, \nthen both the underlying snapshot and VolumeSnapshotContent remain.", "zh": "### 删除策略 {#deletion-policy}\n\n卷快照类具有 [deletionPolicy](/zh-cn/docs/concepts/storage/volume-snapshots/#delete) 属性。\n用户可以配置当所绑定的 VolumeSnapshot 对象将被删除时，如何处理 VolumeSnapshotContent 对象。\n卷快照类的这个策略可以是 `Retain` 或者 `Delete`。这个策略字段必须指定。\n\n如果删除策略是 `Delete`，那么底层的存储快照会和 VolumeSnapshotContent 对象\n一起删除。如果删除策略是 `Retain`，那么底层快照和 VolumeSnapshotContent\n对象都会被保留。"}
{"en": "## Parameters\n\nVolume snapshot classes have parameters that describe volume snapshots belonging to\nthe volume snapshot class. Different parameters may be accepted depending on the\n`driver`.", "zh": "## 参数 {#parameters}\n\n卷快照类具有描述属于该卷快照类的卷快照的参数，可根据 `driver` 接受不同的参数。"}
{"en": "This document describes _persistent volumes_ in Kubernetes. Familiarity with\n[volumes](/docs/concepts/storage/volumes/), [StorageClasses](/docs/concepts/storage/storage-classes/)\nand [VolumeAttributesClasses](/docs/concepts/storage/volume-attributes-classes/) is suggested.", "zh": "本文描述 Kubernetes 中的**持久卷（Persistent Volumes）**。\n建议先熟悉[卷（volume）](/zh-cn/docs/concepts/storage/volumes/)、\n[存储类（StorageClass）](/zh-cn/docs/concepts/storage/storage-classes/)和\n[卷属性类（VolumeAttributesClass）](/zh-cn/docs/concepts/storage/volume-attributes-classes/)。"}
{"en": "## Introduction\n\nManaging storage is a distinct problem from managing compute instances.\nThe PersistentVolume subsystem provides an API for users and administrators\nthat abstracts details of how storage is provided from how it is consumed.\nTo do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.", "zh": "## 介绍  {#introduction}\n\n存储的管理是一个与计算实例的管理完全不同的问题。\nPersistentVolume 子系统为用户和管理员提供了一组 API，\n将存储如何制备的细节从其如何被使用中抽象出来。\n为了实现这点，我们引入了两个新的 API 资源：PersistentVolume 和\nPersistentVolumeClaim。"}
{"en": "A _PersistentVolume_ (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using [Storage Classes](/docs/concepts/storage/storage-classes/). It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.", "zh": "**持久卷（PersistentVolume，PV）** 是集群中的一块存储，可以由管理员事先制备，\n或者使用[存储类（Storage Class）](/zh-cn/docs/concepts/storage/storage-classes/)来动态制备。\n持久卷是集群资源，就像节点也是集群资源一样。PV 持久卷和普通的 Volume 一样，\n也是使用卷插件来实现的，只是它们拥有独立于任何使用 PV 的 Pod 的生命周期。\n此 API 对象中记述了存储的实现细节，无论其背后是 NFS、iSCSI 还是特定于云平台的存储系统。"}
{"en": "A _PersistentVolumeClaim_ (PVC) is a request for storage by a user. It is similar\nto a Pod. Pods consume node resources and PVCs consume PV resources. Pods can\nrequest specific levels of resources (CPU and Memory). Claims can request specific\nsize and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany,\nReadWriteMany, or ReadWriteOncePod, see [AccessModes](#access-modes)).", "zh": "**持久卷申领（PersistentVolumeClaim，PVC）** 表达的是用户对存储的请求。概念上与 Pod 类似。\nPod 会耗用节点资源，而 PVC 申领会耗用 PV 资源。Pod 可以请求特定数量的资源（CPU\n和内存）。同样 PVC 申领也可以请求特定的大小和访问模式\n（例如，可以挂载为 ReadWriteOnce、ReadOnlyMany、ReadWriteMany 或 ReadWriteOncePod，\n请参阅[访问模式](#access-modes)）。"}
{"en": "While PersistentVolumeClaims allow a user to consume abstract storage resources,\nit is common that users need PersistentVolumes with varying properties, such as\nperformance, for different problems. Cluster administrators need to be able to\noffer a variety of PersistentVolumes that differ in more ways than size and access\nmodes, without exposing users to the details of how those volumes are implemented.\nFor these needs, there is the _StorageClass_ resource.\n\nSee the [detailed walkthrough with working examples](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).", "zh": "尽管 PersistentVolumeClaim 允许用户消耗抽象的存储资源，\n常见的情况是针对不同的问题用户需要的是具有不同属性（如，性能）的 PersistentVolume 卷。\n集群管理员需要能够提供不同性质的 PersistentVolume，\n并且这些 PV 卷之间的差别不仅限于卷大小和访问模式，同时又不能将卷是如何实现的这些细节暴露给用户。\n为了满足这类需求，就有了**存储类（StorageClass）** 资源。\n\n参见[基于运行示例的详细演练](/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/)。"}
{"en": "## Lifecycle of a volume and claim\n\nPVs are resources in the cluster. PVCs are requests for those resources and also act\nas claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:\n\n### Provisioning\n\nThere are two ways PVs may be provisioned: statically or dynamically.", "zh": "## 卷和申领的生命周期   {#lifecycle-of-a-volume-and-claim}\n\nPV 卷是集群中的资源。PVC 申领是对这些资源的请求，也被用来执行对资源的申领检查。\nPV 卷和 PVC 申领之间的互动遵循如下生命周期：\n\n### 制备   {#provisioning}\n\nPV 卷的制备有两种方式：静态制备或动态制备。"}
{"en": "#### Static\n\nA cluster administrator creates a number of PVs. They carry the details of the\nreal storage, which is available for use by cluster users. They exist in the\nKubernetes API and are available for consumption.", "zh": "#### 静态制备  {#static}\n\n集群管理员创建若干 PV 卷。这些卷对象带有真实存储的细节信息，\n并且对集群用户可用（可见）。PV 卷对象存在于 Kubernetes API 中，可供用户消费（使用）。"}
{"en": "#### Dynamic\n\nWhen none of the static PVs the administrator created match a user's PersistentVolumeClaim,\nthe cluster may try to dynamically provision a volume specially for the PVC.\nThis provisioning is based on StorageClasses: the PVC must request a\n[storage class](/docs/concepts/storage/storage-classes/) and\nthe administrator must have created and configured that class for dynamic\nprovisioning to occur. Claims that request the class `\"\"` effectively disable\ndynamic provisioning for themselves.", "zh": "#### 动态制备     {#dynamic}\n\n如果管理员所创建的所有静态 PV 卷都无法与用户的 PersistentVolumeClaim 匹配，\n集群可以尝试为该 PVC 申领动态制备一个存储卷。\n这一制备操作是基于 StorageClass 来实现的：PVC 申领必须请求某个\n[存储类](/zh-cn/docs/concepts/storage/storage-classes/)，\n同时集群管理员必须已经创建并配置了该类，这样动态制备卷的动作才会发生。\n如果 PVC 申领指定存储类为 `\"\"`，则相当于为自身禁止使用动态制备的卷。"}
{"en": "To enable dynamic storage provisioning based on storage class, the cluster administrator\nneeds to enable the `DefaultStorageClass`\n[admission controller](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)\non the API server. This can be done, for example, by ensuring that `DefaultStorageClass` is\namong the comma-delimited, ordered list of values for the `--enable-admission-plugins` flag of\nthe API server component. For more information on API server command-line flags,\ncheck [kube-apiserver](/docs/reference/command-line-tools-reference/kube-apiserver/) documentation.", "zh": "为了基于存储类完成动态的存储制备，集群管理员需要在 API 服务器上启用 `DefaultStorageClass`\n[准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)。\n举例而言，可以通过保证 `DefaultStorageClass` 出现在 API 服务器组件的\n`--enable-admission-plugins` 标志值中实现这点；该标志的值可以是逗号分隔的有序列表。\n关于 API 服务器标志的更多信息，可以参考\n[kube-apiserver](/zh-cn/docs/reference/command-line-tools-reference/kube-apiserver/)\n文档。"}
{"en": "### Binding\n\nA user creates, or in the case of dynamic provisioning, has already created,\na PersistentVolumeClaim with a specific amount of storage requested and with\ncertain access modes. A control loop in the control plane watches for new PVCs, finds\na matching PV (if possible), and binds them together. If a PV was dynamically\nprovisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise,\nthe user will always get at least what they asked for, but the volume may be in\nexcess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive,\nregardless of how they were bound. A PVC to PV binding is a one-to-one mapping,\nusing a ClaimRef which is a bi-directional binding between the PersistentVolume\nand the PersistentVolumeClaim.", "zh": "### 绑定     {#binding}\n\n用户创建一个带有特定存储容量和特定访问模式需求的 PersistentVolumeClaim 对象；\n在动态制备场景下，这个 PVC 对象可能已经创建完毕。\n控制平面中的控制回路监测新的 PVC 对象，寻找与之匹配的 PV 卷（如果可能的话），\n并将二者绑定到一起。\n如果为了新的 PVC 申领动态制备了 PV 卷，则控制回路总是将该 PV 卷绑定到这一 PVC 申领。\n否则，用户总是能够获得他们所请求的资源，只是所获得的 PV 卷可能会超出所请求的配置。\n一旦绑定关系建立，则 PersistentVolumeClaim 绑定就是排他性的，\n无论该 PVC 申领是如何与 PV 卷建立的绑定关系。\nPVC 申领与 PV 卷之间的绑定是一种一对一的映射，实现上使用 ClaimRef 来记述\nPV 卷与 PVC 申领间的双向绑定关系。"}
{"en": "Claims will remain unbound indefinitely if a matching volume does not exist.\nClaims will be bound as matching volumes become available. For example, a\ncluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi.\nThe PVC can be bound when a 100Gi PV is added to the cluster.", "zh": "如果找不到匹配的 PV 卷，PVC 申领会无限期地处于未绑定状态。\n当与之匹配的 PV 卷可用时，PVC 申领会被绑定。\n例如，即使某集群上制备了很多 50 Gi 大小的 PV 卷，也无法与请求\n100 Gi 大小的存储的 PVC 匹配。当新的 100 Gi PV 卷被加入到集群时，\n该 PVC 才有可能被绑定。"}
{"en": "### Using\n\nPods use claims as volumes. The cluster inspects the claim to find the bound\nvolume and mounts that volume for a Pod. For volumes that support multiple\naccess modes, the user specifies which mode is desired when using their claim\nas a volume in a Pod.", "zh": "### 使用    {#using}\n\nPod 将 PVC 申领当做存储卷来使用。集群会检视 PVC 申领，找到所绑定的卷，\n并为 Pod 挂载该卷。对于支持多种访问模式的卷，\n用户要在 Pod 中以卷的形式使用申领时指定期望的访问模式。"}
{"en": "Once a user has a claim and that claim is bound, the bound PV belongs to the\nuser for as long as they need it. Users schedule Pods and access their claimed\nPVs by including a `persistentVolumeClaim` section in a Pod's `volumes` block.\nSee [Claims As Volumes](#claims-as-volumes) for more details on this.", "zh": "一旦用户有了申领对象并且该申领已经被绑定，\n则所绑定的 PV 卷在用户仍然需要它期间一直属于该用户。\n用户通过在 Pod 的 `volumes` 块中包含 `persistentVolumeClaim`\n节区来调度 Pod，访问所申领的 PV 卷。\n相关细节可参阅[使用申领作为卷](#claims-as-volumes)。"}
{"en": "### Storage Object in Use Protection\n\nThe purpose of the Storage Object in Use Protection feature is to ensure that\nPersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs)\nthat are bound to PVCs are not removed from the system, as this may result in data loss.", "zh": "### 保护使用中的存储对象   {#storage-object-in-use-protection}\n\n保护使用中的存储对象（Storage Object in Use Protection）\n这一功能特性的目的是确保仍被 Pod 使用的 PersistentVolumeClaim（PVC）\n对象及其所绑定的 PersistentVolume（PV）对象在系统中不会被删除，因为这样做可能会引起数据丢失。\n\n{{< note >}}"}
{"en": "PVC is in active use by a Pod when a Pod object exists that is using the PVC.", "zh": "当使用某 PVC 的 Pod 对象仍然存在时，认为该 PVC 仍被此 Pod 使用。\n{{< /note >}}"}
{"en": "If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately.\nPVC removal is postponed until the PVC is no longer actively used by any Pods. Also,\nif an admin deletes a PV that is bound to a PVC, the PV is not removed immediately.\nPV removal is postponed until the PV is no longer bound to a PVC.\n\nYou can see that a PVC is protected when the PVC's status is `Terminating` and the\n`Finalizers` list includes `kubernetes.io/pvc-protection`:", "zh": "如果用户删除被某 Pod 使用的 PVC 对象，该 PVC 申领不会被立即移除。\nPVC 对象的移除会被推迟，直至其不再被任何 Pod 使用。\n此外，如果管理员删除已绑定到某 PVC 申领的 PV 卷，该 PV 卷也不会被立即移除。\nPV 对象的移除也要推迟到该 PV 不再绑定到 PVC。\n\n你可以看到当 PVC 的状态为 `Terminating` 且其 `Finalizers` 列表中包含\n`kubernetes.io/pvc-protection` 时，PVC 对象是处于被保护状态的。\n\n```shell\nkubectl describe pvc hostpath\n```\n\n```\nName:          hostpath\nNamespace:     default\nStorageClass:  example-hostpath\nStatus:        Terminating\nVolume:\nLabels:        <none>\nAnnotations:   volume.beta.kubernetes.io/storage-class=example-hostpath\n               volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath\nFinalizers:    [kubernetes.io/pvc-protection]\n...\n```"}
{"en": "You can see that a PV is protected when the PV's status is `Terminating` and\nthe `Finalizers` list includes `kubernetes.io/pv-protection` too:", "zh": "你也可以看到当 PV 对象的状态为 `Terminating` 且其 `Finalizers` 列表中包含\n`kubernetes.io/pv-protection` 时，PV 对象是处于被保护状态的。\n\n```shell\nkubectl describe pv task-pv-volume\n```\n\n```\nName:            task-pv-volume\nLabels:          type=local\nAnnotations:     <none>\nFinalizers:      [kubernetes.io/pv-protection]\nStorageClass:    standard\nStatus:          Terminating\nClaim:\nReclaim Policy:  Delete\nAccess Modes:    RWO\nCapacity:        1Gi\nMessage:\nSource:\n    Type:          HostPath (bare host directory volume)\n    Path:          /tmp/data\n    HostPathType:\nEvents:            <none>\n```"}
{"en": "### Reclaiming\n\nWhen a user is done with their volume, they can delete the PVC objects from the\nAPI that allows reclamation of the resource. The reclaim policy for a PersistentVolume\ntells the cluster what to do with the volume after it has been released of its claim.\nCurrently, volumes can either be Retained, Recycled, or Deleted.", "zh": "### 回收（Reclaiming）   {#reclaiming}\n\n当用户不再使用其存储卷时，他们可以从 API 中将 PVC 对象删除，\n从而允许该资源被回收再利用。PersistentVolume 对象的回收策略告诉集群，\n当其被从申领中释放时如何处理该数据卷。\n目前，数据卷可以被 Retained（保留）、Recycled（回收）或 Deleted（删除）。"}
{"en": "#### Retain\n\nThe `Retain` reclaim policy allows for manual reclamation of the resource.\nWhen the PersistentVolumeClaim is deleted, the PersistentVolume still exists\nand the volume is considered \"released\". But it is not yet available for\nanother claim because the previous claimant's data remains on the volume.\nAn administrator can manually reclaim the volume with the following steps.", "zh": "#### 保留（Retain）    {#retain}\n\n回收策略 `Retain` 使得用户可以手动回收资源。当 PersistentVolumeClaim\n对象被删除时，PersistentVolume 卷仍然存在，对应的数据卷被视为\"已释放（released）\"。\n由于卷上仍然存在这前一申领人的数据，该卷还不能用于其他申领。\n管理员可以通过下面的步骤来手动回收该卷："}
{"en": "1. Delete the PersistentVolume. The associated storage asset in external infrastructure\n   still exists after the PV is deleted.\n1. Manually clean up the data on the associated storage asset accordingly.\n1. Manually delete the associated storage asset.\n\nIf you want to reuse the same storage asset, create a new PersistentVolume with\nthe same storage asset definition.", "zh": "1. 删除 PersistentVolume 对象。与之相关的、位于外部基础设施中的存储资产在\n   PV 删除之后仍然存在。\n1. 根据情况，手动清除所关联的存储资产上的数据。\n1. 手动删除所关联的存储资产。\n\n如果你希望重用该存储资产，可以基于存储资产的定义创建新的 PersistentVolume 卷对象。"}
{"en": "#### Delete\n\nFor volume plugins that support the `Delete` reclaim policy, deletion removes\nboth the PersistentVolume object from Kubernetes, as well as the associated\nstorage asset in the external infrastructure. Volumes that were dynamically provisioned\ninherit the [reclaim policy of their StorageClass](#reclaim-policy), which\ndefaults to `Delete`. The administrator should configure the StorageClass\naccording to users' expectations; otherwise, the PV must be edited or\npatched after it is created. See\n[Change the Reclaim Policy of a PersistentVolume](/docs/tasks/administer-cluster/change-pv-reclaim-policy/).", "zh": "#### 删除（Delete）    {#delete}\n\n对于支持 `Delete` 回收策略的卷插件，删除动作会将 PersistentVolume 对象从\nKubernetes 中移除，同时也会从外部基础设施中移除所关联的存储资产。\n动态制备的卷会继承[其 StorageClass 中设置的回收策略](#reclaim-policy)，\n该策略默认为 `Delete`。管理员需要根据用户的期望来配置 StorageClass；\n否则 PV 卷被创建之后必须要被编辑或者修补。\n参阅[更改 PV 卷的回收策略](/zh-cn/docs/tasks/administer-cluster/change-pv-reclaim-policy/)。"}
{"en": "#### Recycle", "zh": "#### 回收（Recycle）     {#recycle}\n\n{{< warning >}}"}
{"en": "The `Recycle` reclaim policy is deprecated. Instead, the recommended approach\nis to use dynamic provisioning.", "zh": "回收策略 `Recycle` 已被废弃。取而代之的建议方案是使用动态制备。\n{{< /warning >}}"}
{"en": "If supported by the underlying volume plugin, the `Recycle` reclaim policy performs\na basic scrub (`rm -rf /thevolume/*`) on the volume and makes it available again for a new claim.", "zh": "如果下层的卷插件支持，回收策略 `Recycle` 会在卷上执行一些基本的擦除\n（`rm -rf /thevolume/*`）操作，之后允许该卷用于新的 PVC 申领。"}
{"en": "However, an administrator can configure a custom recycler Pod template using\nthe Kubernetes controller manager command line arguments as described in the\n[reference](/docs/reference/command-line-tools-reference/kube-controller-manager/).\nThe custom recycler Pod template must contain a `volumes` specification, as\nshown in the example below:", "zh": "不过，管理员可以按[参考资料](/zh-cn/docs/reference/command-line-tools-reference/kube-controller-manager/)中所述，\n使用 Kubernetes 控制器管理器命令行参数来配置一个定制的回收器（Recycler）\nPod 模板。此定制的回收器 Pod 模板必须包含一个 `volumes` 规约，如下例所示：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pv-recycler\n  namespace: default\nspec:\n  restartPolicy: Never\n  volumes:\n  - name: vol\n    hostPath:\n      path: /any/path/it/will/be/replaced\n  containers:\n  - name: pv-recycler\n    image: \"registry.k8s.io/busybox\"\n    command: [\"/bin/sh\", \"-c\", \"test -e /scrub && rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  && test -z \\\"$(ls -A /scrub)\\\" || exit 1\"]\n    volumeMounts:\n    - name: vol\n      mountPath: /scrub\n```"}
{"en": "However, the particular path specified in the custom recycler Pod template in the\n`volumes` part is replaced with the particular path of the volume that is being recycled.", "zh": "定制回收器 Pod 模板中在 `volumes` 部分所指定的特定路径要替换为正被回收的卷的路径。"}
{"en": "### PersistentVolume deletion protection finalizer", "zh": "### PersistentVolume 删除保护 finalizer  {#persistentvolume-deletion-protection-finalizer}\n\n{{< feature-state feature_gate_name=\"HonorPVReclaimPolicy\" >}}"}
{"en": "Finalizers can be added on a PersistentVolume to ensure that PersistentVolumes\nhaving `Delete` reclaim policy are deleted only after the backing storage are deleted.", "zh": "可以在 PersistentVolume 上添加终结器（Finalizer），\n以确保只有在删除对应的存储后才删除具有 `Delete` 回收策略的 PersistentVolume。"}
{"en": "The finalizer `external-provisioner.volume.kubernetes.io/finalizer`(introduced\nin  v1.31) is added to both dynamically provisioned and statically provisioned\nCSI volumes.\n\nThe finalizer `kubernetes.io/pv-controller`(introduced in v1.31) is added to \ndynamically provisioned in-tree plugin volumes and skipped for statically \nprovisioned in-tree plugin volumes.\n\nThe following is an example of dynamically provisioned in-tree plugin volume:", "zh": "（在 v1.31 中引入的）终结器 `external-provisioner.volume.kubernetes.io/finalizer`\n被同时添加到动态制备和静态制备的 CSI 卷上。\n\n（在 v1.31 中引入的）终结器 `kubernetes.io/pv-controller`\n被添加到动态制备的树内插件卷上，而对于静态制备的树内插件卷，此终结器将被忽略。\n\n以下是动态制备的树内插件卷的示例：\n\n```shell\nkubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78\n```\n\n```none\nName:            pvc-74a498d6-3929-47e8-8c02-078c1ece4d78\nLabels:          <none>\nAnnotations:     kubernetes.io/createdby: vsphere-volume-dynamic-provisioner\n                 pv.kubernetes.io/bound-by-controller: yes\n                 pv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volume\nFinalizers:      [kubernetes.io/pv-protection kubernetes.io/pv-controller]\nStorageClass:    vcp-sc\nStatus:          Bound\nClaim:           default/vcp-pvc-1\nReclaim Policy:  Delete\nAccess Modes:    RWO\nVolumeMode:      Filesystem\nCapacity:        1Gi\nNode Affinity:   <none>\nMessage:\nSource:\n    Type:               vSphereVolume (a Persistent Disk resource in vSphere)\n    VolumePath:         [vsanDatastore] d49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-dynamic-pvc-74a498d6-3929-47e8-8c02-078c1ece4d78.vmdk\n    FSType:             ext4\n    StoragePolicyName:  vSAN Default Storage Policy\nEvents:                 <none>\n```"}
{"en": "The finalizer `external-provisioner.volume.kubernetes.io/finalizer` is added for CSI volumes.\nThe following is an example:", "zh": "终结器 `external-provisioner.volume.kubernetes.io/finalizer` 会被添加到 CSI 卷上。下面是一个例子：\n\n```none\nName:            pvc-2f0bab97-85a8-4552-8044-eb8be45cf48d\nLabels:          <none>\nAnnotations:     pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com\nFinalizers:      [kubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer]\nStorageClass:    fast\nStatus:          Bound\nClaim:           demo-app/nginx-logs\nReclaim Policy:  Delete\nAccess Modes:    RWO\nVolumeMode:      Filesystem\nCapacity:        200Mi\nNode Affinity:   <none>\nMessage:\nSource:\n    Type:              CSI (a Container Storage Interface (CSI) volume source)\n    Driver:            csi.vsphere.vmware.com\n    FSType:            ext4\n    VolumeHandle:      44830fa8-79b4-406b-8b58-621ba25353fd\n    ReadOnly:          false\n    VolumeAttributes:      storage.kubernetes.io/csiProvisionerIdentity=1648442357185-8081-csi.vsphere.vmware.com\n                           type=vSphere CNS Block Volume\nEvents:                <none>\n```"}
{"en": "When the `CSIMigration{provider}` feature flag is enabled for a specific in-tree volume plugin,\nthe `kubernetes.io/pv-controller` finalizer is replaced by the\n`external-provisioner.volume.kubernetes.io/finalizer` finalizer.", "zh": "当为特定的树内卷插件启用了 `CSIMigration{provider}` 特性标志时，`kubernetes.io/pv-controller`\n终结器将被替换为 `external-provisioner.volume.kubernetes.io/finalizer` 终结器。"}
{"en": "The finalizers ensure that the PV object is removed only after the volume is deleted\nfrom the storage backend provided the reclaim policy of the PV is `Delete`. This\nalso ensures that the volume is deleted from storage backend irrespective of the\norder of deletion of PV and PVC.", "zh": "这些终结器确保只有在从存储后端删除卷后，PV 对象才会被移除，\n前提是 PV 的回收策略为 `Delete`。\n这也确保了无论 PV 和 PVC 的删除顺序如何，此卷都会从存储后端被删除。"}
{"en": "### Reserving a PersistentVolume\n\nThe control plane can [bind PersistentVolumeClaims to matching PersistentVolumes](#binding)\nin the cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.", "zh": "### 预留 PersistentVolume  {#reserving-a-persistentvolume}\n\n控制平面可以在集群中[将 PersistentVolumeClaims 绑定到匹配的 PersistentVolumes](#binding)。\n但是，如果你希望 PVC 绑定到特定 PV，则需要预先绑定它们。"}
{"en": "By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding\nbetween that specific PV and PVC. If the PersistentVolume exists and has not reserved\nPersistentVolumeClaims through its `claimRef` field, then the PersistentVolume and\nPersistentVolumeClaim will be bound.", "zh": "通过在 PersistentVolumeClaim 中指定 PersistentVolume，你可以声明该特定\nPV 与 PVC 之间的绑定关系。如果该 PersistentVolume 存在且未被通过其\n`claimRef` 字段预留给 PersistentVolumeClaim，则该 PersistentVolume\n会和该 PersistentVolumeClaim 绑定到一起。"}
{"en": "The binding happens regardless of some volume matching criteria, including node affinity.\nThe control plane still checks that [storage class](/docs/concepts/storage/storage-classes/),\naccess modes, and requested storage size are valid.", "zh": "绑定操作不会考虑某些卷匹配条件是否满足，包括节点亲和性等等。\n控制面仍然会检查[存储类](/zh-cn/docs/concepts/storage/storage-classes/)、\n访问模式和所请求的存储尺寸都是合法的。"}
{"en": "```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: foo-pvc\n  namespace: foo\nspec:\n  storageClassName: \"\" # Empty string must be explicitly set otherwise default StorageClass will be set\n  volumeName: foo-pv\n  ...\n```", "zh": "```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: foo-pvc\n  namespace: foo\nspec:\n  storageClassName: \"\" # 此处须显式设置空字符串，否则会被设置为默认的 StorageClass\n  volumeName: foo-pv\n  ...\n```"}
{"en": "This method does not guarantee any binding privileges to the PersistentVolume.\nIf other PersistentVolumeClaims could use the PV that you specify, you first\nneed to reserve that storage volume. Specify the relevant PersistentVolumeClaim\nin the `claimRef` field of the PV so that other PVCs can not bind to it.", "zh": "此方法无法对 PersistentVolume 的绑定特权做出任何形式的保证。\n如果有其他 PersistentVolumeClaim 可以使用你所指定的 PV，\n则你应该首先预留该存储卷。你可以将 PV 的 `claimRef` 字段设置为相关的\nPersistentVolumeClaim 以确保其他 PVC 不会绑定到该 PV 卷。\n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: foo-pv\nspec:\n  storageClassName: \"\"\n  claimRef:\n    name: foo-pvc\n    namespace: foo\n  ...\n```"}
{"en": "This is useful if you want to consume PersistentVolumes that have their `persistentVolumeReclaimPolicy` set\nto `Retain`, including cases where you are reusing an existing PV.", "zh": "如果你想要使用 `persistentVolumeReclaimPolicy` 属性设置为 `Retain` 的 PersistentVolume 卷时，\n包括你希望复用现有的 PV 卷时，这点是很有用的"}
{"en": "### Expanding Persistent Volumes Claims", "zh": "### 扩充 PVC 申领   {#expanding-persistent-volumes-claims}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "Support for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand\nthe following types of volumes:", "zh": "现在，对扩充 PVC 申领的支持默认处于被启用状态。你可以扩充以下类型的卷："}
{"en": "* azureFile (deprecated)\n* {{< glossary_tooltip text=\"csi\" term_id=\"csi\" >}}\n* flexVolume (deprecated)\n* rbd (deprecated)\n* portworxVolume (deprecated)", "zh": "* azureFile（已弃用）\n* {{< glossary_tooltip text=\"csi\" term_id=\"csi\" >}}\n* flexVolume（已弃用）\n* rbd（已弃用）\n* portworxVolume（已弃用）"}
{"en": "You can only expand a PVC if its storage class's `allowVolumeExpansion` field is set to true.", "zh": "只有当 PVC 的存储类中将 `allowVolumeExpansion` 设置为 true 时，你才可以扩充该 PVC 申领。\n\n```yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: example-vol-default\nprovisioner: vendor-name.example/magicstorage\nparameters:\n  resturl: \"http://192.168.10.100:8080\"\n  restuser: \"\"\n  secretNamespace: \"\"\n  secretName: \"\"\nallowVolumeExpansion: true\n```"}
{"en": "To request a larger volume for a PVC, edit the PVC object and specify a larger\nsize. This triggers expansion of the volume that backs the underlying PersistentVolume. A\nnew PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.", "zh": "如果要为某 PVC 请求较大的存储卷，可以编辑 PVC 对象，设置一个更大的尺寸值。\n这一编辑操作会触发为下层 PersistentVolume 提供存储的卷的扩充。\nKubernetes 不会创建新的 PV 卷来满足此申领的请求。\n与之相反，现有的卷会被调整大小。\n\n{{< warning >}}"}
{"en": "Directly editing the size of a PersistentVolume can prevent an automatic resize of that volume.\nIf you edit the capacity of a PersistentVolume, and then edit the `.spec` of a matching\nPersistentVolumeClaim to make the size of the PersistentVolumeClaim match the PersistentVolume,\nthen no storage resize happens.\nThe Kubernetes control plane will see that the desired state of both resources matches,\nconclude that the backing volume size has been manually\nincreased and that no resize is necessary.", "zh": "直接编辑 PersistentVolume 的大小可以阻止该卷自动调整大小。\n如果对 PersistentVolume 的容量进行编辑，然后又将其所对应的\nPersistentVolumeClaim 的 `.spec` 进行编辑，使该 PersistentVolumeClaim\n的大小匹配 PersistentVolume 的话，则不会发生存储大小的调整。\nKubernetes 控制平面将看到两个资源的所需状态匹配，\n并认为其后备卷的大小已被手动增加，无需调整。\n{{< /warning >}}"}
{"en": "#### CSI Volume expansion", "zh": "#### CSI 卷的扩充     {#csi-volume-expansion}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "Support for expanding CSI volumes is enabled by default but it also requires a\nspecific CSI driver to support volume expansion. Refer to documentation of the\nspecific CSI driver for more information.", "zh": "对 CSI 卷的扩充能力默认是被启用的，不过扩充 CSI 卷要求 CSI\n驱动支持卷扩充操作。可参阅特定 CSI 驱动的文档了解更多信息。"}
{"en": "#### Resizing a volume containing a file system\n\nYou can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.", "zh": "#### 重设包含文件系统的卷的大小 {#resizing-a-volume-containing-a-file-system}\n\n只有卷中包含的文件系统是 XFS、Ext3 或者 Ext4 时，你才可以重设卷的大小。"}
{"en": "When a volume contains a file system, the file system is only resized when a new Pod is using\nthe PersistentVolumeClaim in `ReadWrite` mode. File system expansion is either done when a Pod is starting up\nor when a Pod is running and the underlying file system supports online expansion.\n\nFlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with the\n`RequiresFSResize` capability to `true`. The FlexVolume can be resized on Pod restart.", "zh": "当卷中包含文件系统时，只有在 Pod 使用 `ReadWrite` 模式来使用 PVC\n申领的情况下才能重设其文件系统的大小。文件系统扩充的操作或者是在 Pod\n启动期间完成，或者在下层文件系统支持在线扩充的前提下在 Pod 运行期间完成。\n\n如果 FlexVolumes 的驱动将 `RequiresFSResize` 能力设置为 `true`，\n则该 FlexVolume 卷（于 Kubernetes v1.23 弃用）可以在 Pod 重启期间调整大小。"}
{"en": "#### Resizing an in-use PersistentVolumeClaim", "zh": "#### 重设使用中 PVC 申领的大小    {#resizing-an-in-use-persistentvolumevlaim}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "In this case, you don't need to delete and recreate a Pod or deployment that is using an existing PVC.\nAny in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded.\nThis feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that\nuses the PVC before the expansion can complete.", "zh": "在这种情况下，你不需要删除和重建正在使用某现有 PVC 的 Pod 或 Deployment。\n所有使用中的 PVC 在其文件系统被扩充之后，立即可供其 Pod 使用。\n此功能特性对于没有被 Pod 或 Deployment 使用的 PVC 而言没有效果。\n你必须在执行扩展操作之前创建一个使用该 PVC 的 Pod。"}
{"en": "Similar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.", "zh": "与其他卷类型类似，FlexVolume 卷也可以在被 Pod 使用期间执行扩充操作。\n\n{{< note >}}"}
{"en": "FlexVolume resize is possible only when the underlying driver supports resize.", "zh": "FlexVolume 卷的重设大小只能在下层驱动支持重设大小的时候才可进行。\n{{< /note >}}"}
{"en": "#### Recovering from Failure when Expanding Volumes\n\nIf a user specifies a new size that is too big to be satisfied by underlying\nstorage system, expansion of PVC will be continuously retried until user or\ncluster administrator takes some action. This can be undesirable and hence\nKubernetes provides following methods of recovering from such failures.", "zh": "#### 处理扩充卷过程中的失败      {#recovering-from-failure-when-expanding-volumes}\n\n如果用户指定的新大小过大，底层存储系统无法满足，PVC 的扩展将不断重试，\n直到用户或集群管理员采取一些措施。这种情况是不希望发生的，因此 Kubernetes\n提供了以下从此类故障中恢复的方法。\n\n{{< tabs name=\"recovery_methods\" >}}\n{{% tab name=\"集群管理员手动处理\" %}}"}
{"en": "If expanding underlying storage fails, the cluster administrator can manually\nrecover the Persistent Volume Claim (PVC) state and cancel the resize requests.\nOtherwise, the resize requests are continuously retried by the controller without\nadministrator intervention.", "zh": "如果扩充下层存储的操作失败，集群管理员可以手动地恢复 PVC\n申领的状态并取消重设大小的请求。否则，在没有管理员干预的情况下，\n控制器会反复重试重设大小的操作。"}
{"en": "1. Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC)\n   with `Retain` reclaim policy.\n2. Delete the PVC. Since PV has `Retain` reclaim policy - we will not lose any data\n   when we recreate the PVC.\n3. Delete the `claimRef` entry from PV specs, so as new PVC can bind to it.\n   This should make the PV `Available`.\n4. Re-create the PVC with smaller size than PV and set `volumeName` field of the\n   PVC to the name of the PV. This should bind new PVC to existing PV.\n5. Don't forget to restore the reclaim policy of the PV.", "zh": "1. 将绑定到 PVC 申领的 PV 卷标记为 `Retain` 回收策略。\n2. 删除 PVC 对象。由于 PV 的回收策略为 `Retain`，我们不会在重建 PVC 时丢失数据。\n3. 删除 PV 规约中的 `claimRef` 项，这样新的 PVC 可以绑定到该卷。\n   这一操作会使得 PV 卷变为 \"可用（Available）\"。\n4. 使用小于 PV 卷大小的尺寸重建 PVC，设置 PVC 的 `volumeName` 字段为 PV 卷的名称。\n   这一操作将把新的 PVC 对象绑定到现有的 PV 卷。\n5. 不要忘记恢复 PV 卷上设置的回收策略。\n\n{{% /tab %}}\n{{% tab name=\"通过请求扩展为更小尺寸\" %}}\n{{% feature-state for_k8s_version=\"v1.23\" state=\"alpha\" %}}\n\n{{< note >}}"}
{"en": "Recovery from failing PVC expansion by users is available as an alpha feature\nsince Kubernetes 1.23. The `RecoverVolumeExpansionFailure` feature must be\nenabled for this feature to work. Refer to the\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\ndocumentation for more information.", "zh": "Kubernetes 从 1.23 版本开始将允许用户恢复失败的 PVC 扩展这一能力作为\nAlpha 特性支持。`RecoverVolumeExpansionFailure` 必须被启用以允许使用此特性。\n可参考[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)\n文档了解更多信息。\n{{< /note >}}"}
{"en": "If the feature gates `RecoverVolumeExpansionFailure` is\nenabled in your cluster, and expansion has failed for a PVC, you can retry expansion with a\nsmaller size than the previously requested value. To request a new expansion attempt with a\nsmaller proposed size, edit `.spec.resources` for that PVC and choose a value that is less than the\nvalue you previously tried.\nThis is useful if expansion to a higher value did not succeed because of capacity constraint.\nIf that has happened, or you suspect that it might have, you can retry expansion by specifying a\nsize that is within the capacity limits of underlying storage provider. You can monitor status of\nresize operation by watching `.status.allocatedResourceStatuses` and events on the PVC.", "zh": "如果集群中的特性门控 `RecoverVolumeExpansionFailure`\n已启用，在 PVC 的扩展发生失败时，你可以使用比先前请求的值更小的尺寸来重试扩展。\n要使用一个更小的尺寸尝试请求新的扩展，请编辑该 PVC 的 `.spec.resources`\n并选择一个比你之前所尝试的值更小的值。\n如果由于容量限制而无法成功扩展至更高的值，这将很有用。\n如果发生了这种情况，或者你怀疑可能发生了这种情况，\n你可以通过指定一个在底层存储制备容量限制内的尺寸来重试扩展。\n你可以通过查看 `.status.allocatedResourceStatuses` 以及 PVC 上的事件来监控调整大小操作的状态。"}
{"en": "Note that,\nalthough you can specify a lower amount of storage than what was requested previously,\nthe new value must still be higher than `.status.capacity`.\nKubernetes does not support shrinking a PVC to less than its current size.", "zh": "请注意，\n尽管你可以指定比之前的请求更低的存储量，新值必须仍然高于 `.status.capacity`。\nKubernetes 不支持将 PVC 缩小到小于其当前的尺寸。\n\n{{% /tab %}}\n{{% /tabs %}}"}
{"en": "## Types of Persistent Volumes\n\nPersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:", "zh": "## 持久卷的类型     {#types-of-persistent-volumes}\n\nPV 持久卷是用插件的形式来实现的。Kubernetes 目前支持以下插件："}
{"en": "* [`csi`](/docs/concepts/storage/volumes/#csi) - Container Storage Interface (CSI)\n* [`fc`](/docs/concepts/storage/volumes/#fc) - Fibre Channel (FC) storage\n* [`hostPath`](/docs/concepts/storage/volumes/#hostpath) - HostPath volume\n  (for single node testing only; WILL NOT WORK in a multi-node cluster;\n  consider using `local` volume instead)\n* [`iscsi`](/docs/concepts/storage/volumes/#iscsi) - iSCSI (SCSI over IP) storage\n* [`local`](/docs/concepts/storage/volumes/#local) - local storage devices\n  mounted on nodes.\n* [`nfs`](/docs/concepts/storage/volumes/#nfs) - Network File System (NFS) storage", "zh": "* [`csi`](/zh-cn/docs/concepts/storage/volumes/#csi) - 容器存储接口（CSI）\n* [`fc`](/zh-cn/docs/concepts/storage/volumes/#fc) - Fibre Channel（FC）存储\n* [`hostPath`](/zh-cn/docs/concepts/storage/volumes/#hostpath) - HostPath 卷\n  （仅供单节点测试使用；不适用于多节点集群；请尝试使用 `local` 卷作为替代）\n* [`iscsi`](/zh-cn/docs/concepts/storage/volumes/#iscsi) - iSCSI（IP 上的 SCSI）存储\n* [`local`](/zh-cn/docs/concepts/storage/volumes/#local) - 节点上挂载的本地存储设备\n* [`nfs`](/zh-cn/docs/concepts/storage/volumes/#nfs) - 网络文件系统（NFS）存储"}
{"en": "The following types of PersistentVolume are deprecated but still available.\nIf you are using these volume types except for `flexVolume`, `cephfs` and `rbd`,\nplease install corresponding CSI drivers.", "zh": "以下的持久卷已被弃用但仍然可用。\n如果你使用除 `flexVolume`、`cephfs` 和 `rbd` 之外的卷类型，请安装相应的 CSI 驱动程序。"}
{"en": "* [`awsElasticBlockStore`](/docs/concepts/storage/volumes/#awselasticblockstore) - AWS Elastic Block Store (EBS)\n  (**migration on by default** starting v1.23)\n* [`azureDisk`](/docs/concepts/storage/volumes/#azuredisk) - Azure Disk\n  (**migration on by default** starting v1.23)\n* [`azureFile`](/docs/concepts/storage/volumes/#azurefile) - Azure File\n  (**migration on by default** starting v1.24)\n* [`cinder`](/docs/concepts/storage/volumes/#cinder) - Cinder (OpenStack block storage)\n  (**migration on by default** starting v1.21)\n* [`flexVolume`](/docs/concepts/storage/volumes/#flexvolume) - FlexVolume\n  (**deprecated** starting v1.23, no migration plan and no plan to remove support)\n* [`gcePersistentDisk`](/docs/concepts/storage/volumes/#gcePersistentDisk) - GCE Persistent Disk\n  (**migration on by default** starting v1.23)\n* [`portworxVolume`](/docs/concepts/storage/volumes/#portworxvolume) - Portworx volume\n  (**migration on by default** starting v1.31)\n* [`vsphereVolume`](/docs/concepts/storage/volumes/#vspherevolume) - vSphere VMDK volume\n  (**migration on by default** starting v1.25)", "zh": "* [`awsElasticBlockStore`](/zh-cn/docs/concepts/storage/volumes/#awselasticblockstore) - AWS Elastic 块存储（EBS）\n  （从 v1.23 开始**默认启用迁移**）\n* [`azureDisk`](/zh-cn/docs/concepts/storage/volumes/#azuredisk) - Azure 磁盘\n  （从 v1.23 开始**默认启用迁移**）\n* [`azureFile`](/zh-cn/docs/concepts/storage/volumes/#azurefile) - Azure 文件\n  （从 v1.24 开始**默认启用迁移**）\n* [`cinder`](/zh-cn/docs/concepts/storage/volumes/#cinder) - Cinder（OpenStack 块存储）\n  （从 v1.21 开始**默认启用迁移**）\n* [`flexVolume`](/zh-cn/docs/concepts/storage/volumes/#flexVolume) - FlexVolume\n  （从 v1.23 开始**弃用**，没有迁移计划，没有移除支持的计划）\n* [`gcePersistentDisk`](/zh-cn/docs/concepts/storage/volumes/#gcePersistentDisk) - GCE 持久磁盘\n  （从 v1.23 开始**默认启用迁移**）\n* [`portworxVolume`](/zh-cn/docs/concepts/storage/volumes/#portworxvolume) - Portworx 卷\n  （从 v1.31 开始**默认启用迁移**）\n* [`vsphereVolume`](/zh-cn/docs/concepts/storage/volumes/#vspherevolume) - vSphere VMDK 卷\n （从 v1.25 开始**默认启用迁移**）"}
{"en": "Older versions of Kubernetes also supported the following in-tree PersistentVolume types:\n\n* [`cephfs`](/docs/concepts/storage/volumes/#cephfs)\n  (**not available** starting v1.31)\n* `flocker` - Flocker storage.\n  (**not available** starting v1.25)\n* `photonPersistentDisk` - Photon controller persistent disk.\n  (**not available** starting v1.15)\n* `quobyte` - Quobyte volume.\n  (**not available** starting v1.25)\n* [`rbd`](/docs/concepts/storage/volumes/#rbd) - Rados Block Device (RBD) volume \n  (**not available** starting v1.31)\n* `scaleIO` - ScaleIO volume.\n  (**not available** starting v1.21)\n* `storageos` - StorageOS volume.\n  (**not available** starting v1.25)", "zh": "旧版本的 Kubernetes 仍支持这些“树内（In-Tree）”持久卷类型：\n\n* [`cephfs`](/zh-cn/docs/concepts/storage/volumes/#cephfs)\n  （v1.31 之后**不可用**）\n* `flocker` - Flocker 存储。\n  （v1.25 之后**不可用**）\n* `photonPersistentDisk` - Photon 控制器持久化盘\n  （v1.15 之后**不可用**）\n* `quobyte` - Quobyte 卷。\n  （v1.25 之后**不可用**）\n* [`rbd`](/zh-cn/docs/concepts/storage/volumes/#rbd) - Rados 块设备 (RBD) 卷\n  （v1.31 之后**不可用**）\n* `scaleIO` - ScaleIO 卷\n  （v1.21 之后**不可用**）\n* `storageos` - StorageOS 卷\n  （v1.25 之后**不可用**）"}
{"en": "## Persistent Volumes\n\nEach PV contains a spec and status, which is the specification and status of the volume.\nThe name of a PersistentVolume object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).", "zh": "## 持久卷    {#persistent-volumes}\n\n每个 PV 对象都包含 `spec` 部分和 `status` 部分，分别对应卷的规约和状态。\nPersistentVolume 对象的名称必须是合法的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。\n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv0003\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Recycle\n  storageClassName: slow\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  nfs:\n    path: /tmp\n    server: 172.17.0.2\n```\n\n{{< note >}}"}
{"en": "Helper programs relating to the volume type may be required for consumption of\na PersistentVolume within a cluster.  In this example, the PersistentVolume is\nof type NFS and the helper program /sbin/mount.nfs is required to support the\nmounting of NFS filesystems.", "zh": "在集群中使用持久卷存储通常需要一些特定于具体卷类型的辅助程序。\n在这个例子中，PersistentVolume 是 NFS 类型的，因此需要辅助程序 `/sbin/mount.nfs`\n来支持挂载 NFS 文件系统。\n{{< /note >}}"}
{"en": "### Capacity\n\nGenerally, a PV will have a specific storage capacity. This is set using the PV's\n`capacity` attribute which is a {{< glossary_tooltip term_id=\"quantity\" >}} value.\n\nCurrently, storage size is the only resource that can be set or requested.\nFuture attributes may include IOPS, throughput, etc.", "zh": "### 容量    {#capacity}\n\n一般而言，每个 PV 卷都有确定的存储容量。\n这是通过 PV 的 `capacity` 属性设置的，\n该属性是一个{{< glossary_tooltip text=\"量纲（Quantity）\" term_id=\"quantity\" >}}。\n\n目前存储大小是可以设置和请求的唯一资源，\n未来可能会包含 IOPS、吞吐量等属性。"}
{"en": "### Volume Mode", "zh": "### 卷模式     {#volume-mode}\n\n{{< feature-state for_k8s_version=\"v1.18\" state=\"stable\" >}}"}
{"en": "Kubernetes supports two `volumeModes` of PersistentVolumes: `Filesystem` and `Block`.\n\n`volumeMode` is an optional API parameter.\n`Filesystem` is the default mode used when `volumeMode` parameter is omitted.\n\nA volume with `volumeMode: Filesystem` is *mounted* into Pods into a directory. If the volume\nis backed by a block device and the device is empty, Kubernetes creates a filesystem\non the device before mounting it for the first time.", "zh": "针对 PV 持久卷，Kubernetes\n支持两种卷模式（`volumeModes`）：`Filesystem（文件系统）` 和 `Block（块）`。\n`volumeMode` 是一个可选的 API 参数。\n如果该参数被省略，默认的卷模式是 `Filesystem`。\n\n`volumeMode` 属性设置为 `Filesystem` 的卷会被 Pod **挂载（Mount）** 到某个目录。\n如果卷的存储来自某块设备而该设备目前为空，Kuberneretes 会在第一次挂载卷之前在设备上创建文件系统。"}
{"en": "You can set the value of `volumeMode` to `Block` to use a volume as a raw block device.\nSuch volume is presented into a Pod as a block device, without any filesystem on it.\nThis mode is useful to provide a Pod the fastest possible way to access a volume, without\nany filesystem layer between the Pod and the volume. On the other hand, the application\nrunning in the Pod must know how to handle a raw block device.\nSee [Raw Block Volume Support](#raw-block-volume-support)\nfor an example on how to use a volume with `volumeMode: Block` in a Pod.", "zh": "你可以将 `volumeMode` 设置为 `Block`，以便将卷作为原始块设备来使用。\n这类卷以块设备的方式交给 Pod 使用，其上没有任何文件系统。\n这种模式对于为 Pod 提供一种使用最快可能方式来访问卷而言很有帮助，\nPod 和卷之间不存在文件系统层。另外，Pod 中运行的应用必须知道如何处理原始块设备。\n关于如何在 Pod 中使用 `volumeMode: Block` 的卷，\n可参阅[原始块卷支持](#raw-block-volume-support)。"}
{"en": "### Access Modes\n\nA PersistentVolume can be mounted on a host in any way supported by the resource\nprovider. As shown in the table below, providers will have different capabilities\nand each PV's access modes are set to the specific modes supported by that particular\nvolume. For example, NFS can support multiple read/write clients, but a specific\nNFS PV might be exported on the server as read-only. Each PV gets its own set of\naccess modes describing that specific PV's capabilities.", "zh": "### 访问模式   {#access-modes}\n\nPersistentVolume 卷可以用资源提供者所支持的任何方式挂载到宿主系统上。\n如下表所示，提供者（驱动）的能力不同，每个 PV 卷的访问模式都会设置为对应卷所支持的模式值。\n例如，NFS 可以支持多个读写客户，但是某个特定的 NFS PV 卷可能在服务器上以只读的方式导出。\n每个 PV 卷都会获得自身的访问模式集合，描述的是特定 PV 卷的能力。"}
{"en": "The access modes are:\n\n`ReadWriteOnce`\n: the volume can be mounted as read-write by a single node. ReadWriteOnce access\n  mode still can allow multiple pods to access the volume when the pods are\n  running on the same node. For single pod access, please see ReadWriteOncePod.\n\n`ReadOnlyMany`\n: the volume can be mounted as read-only by many nodes.\n\n`ReadWriteMany`\n: the volume can be mounted as read-write by many nodes.\n\n `ReadWriteOncePod`\n: {{< feature-state for_k8s_version=\"v1.29\" state=\"stable\" >}}\n  the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod\n  access mode if you want to ensure that only one pod across the whole cluster can\n  read that PVC or write to it.", "zh": "访问模式有：\n\n`ReadWriteOnce`\n: 卷可以被一个节点以读写方式挂载。\n  ReadWriteOnce 访问模式仍然可以在同一节点上运行的多个 Pod 访问该卷。\n  对于单个 Pod 的访问，请参考 ReadWriteOncePod 访问模式。\n\n`ReadOnlyMany`\n: 卷可以被多个节点以只读方式挂载。\n\n`ReadWriteMany`\n: 卷可以被多个节点以读写方式挂载。\n\n`ReadWriteOncePod`\n: {{< feature-state for_k8s_version=\"v1.29\" state=\"stable\" >}}\n  卷可以被单个 Pod 以读写方式挂载。\n  如果你想确保整个集群中只有一个 Pod 可以读取或写入该 PVC，\n  请使用 ReadWriteOncePod 访问模式。\n\n{{< note >}}"}
{"en": "The `ReadWriteOncePod` access mode is only supported for\n{{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} volumes and Kubernetes version\n1.22+. To use this feature you will need to update the following\n[CSI sidecars](https://kubernetes-csi.github.io/docs/sidecar-containers.html)\nto these versions or greater:\n\n* [csi-provisioner:v3.0.0+](https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0)\n* [csi-attacher:v3.3.0+](https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0)\n* [csi-resizer:v1.3.0+](https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0)", "zh": "`ReadWriteOncePod` 访问模式仅适用于 {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} 卷和 Kubernetes v1.22+。\n要使用此特性，你需要将以下\n[CSI 边车](https://kubernetes-csi.github.io/docs/sidecar-containers.html)更新为下列或更高版本：\n\n- [csi-provisioner:v3.0.0+](https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0)\n- [csi-attacher:v3.3.0+](https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0)\n- [csi-resizer:v1.3.0+](https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0)\n{{< /note >}}"}
{"en": "In the CLI, the access modes are abbreviated to:\n\n* RWO - ReadWriteOnce\n* ROX - ReadOnlyMany\n* RWX - ReadWriteMany\n* RWOP - ReadWriteOncePod", "zh": "在命令行接口（CLI）中，访问模式也使用以下缩写形式：\n\n* RWO - ReadWriteOnce\n* ROX - ReadOnlyMany\n* RWX - ReadWriteMany\n* RWOP - ReadWriteOncePod\n\n{{< note >}}"}
{"en": "Kubernetes uses volume access modes to match PersistentVolumeClaims and PersistentVolumes.\nIn some cases, the volume access modes also constrain where the PersistentVolume can be mounted.\nVolume access modes do **not** enforce write protection once the storage has been mounted.\nEven if the access modes are specified as ReadWriteOnce, ReadOnlyMany, or ReadWriteMany,\nthey don't set any constraints on the volume. For example, even if a PersistentVolume is\ncreated as ReadOnlyMany, it is no guarantee that it will be read-only. If the access modes\nare specified as ReadWriteOncePod, the volume is constrained and can be mounted on only a single Pod.", "zh": "Kubernetes 使用卷访问模式来匹配 PersistentVolumeClaim 和 PersistentVolume。\n在某些场合下，卷访问模式也会限制 PersistentVolume 可以挂载的位置。\n卷访问模式并**不会**在存储已经被挂载的情况下为其实施写保护。\n即使访问模式设置为 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany，它们也不会对卷形成限制。\n例如，即使某个卷创建时设置为 ReadOnlyMany，也无法保证该卷是只读的。\n如果访问模式设置为 ReadWriteOncePod，则卷会被限制起来并且只能挂载到一个 Pod 上。\n{{< /note >}}"}
{"en": "> __Important!__ A volume can only be mounted using one access mode at a time,\n> even if it supports many.", "zh": "> **重要提醒！** 每个卷同一时刻只能以一种访问模式挂载，即使该卷能够支持多种访问模式。"}
{"en": "| Volume Plugin        | ReadWriteOnce          | ReadOnlyMany          | ReadWriteMany | ReadWriteOncePod       |\n| :---                 | :---:                  | :---:                 | :---:         | -                      |\n| AzureFile            | &#x2713;               | &#x2713;              | &#x2713;      | -                      |\n| CephFS               | &#x2713;               | &#x2713;              | &#x2713;      | -                      |\n| CSI                  | depends on the driver  | depends on the driver | depends on the driver | depends on the driver |\n| FC                   | &#x2713;               | &#x2713;              | -             | -                      |\n| FlexVolume           | &#x2713;               | &#x2713;              | depends on the driver | -              |\n| HostPath             | &#x2713;               | -                     | -             | -                      |\n| iSCSI                | &#x2713;               | &#x2713;              | -             | -                      |\n| NFS                  | &#x2713;               | &#x2713;              | &#x2713;      | -                      |\n| RBD                  | &#x2713;               | &#x2713;              | -             | -                      |\n| VsphereVolume        | &#x2713;               | -                     | - (works when Pods are collocated) | - |\n| PortworxVolume       | &#x2713;               | -                     | &#x2713;      | -                  | - |", "zh": "| 卷插件               | ReadWriteOnce          | ReadOnlyMany          | ReadWriteMany | ReadWriteOncePod       |\n| :---                 | :---:                  | :---:                 | :---:         | -                      |\n| AzureFile            | &#x2713;               | &#x2713;              | &#x2713;      | -                      |\n| CephFS               | &#x2713;               | &#x2713;              | &#x2713;      | -                      |\n| CSI                  | 取决于驱动              | 取决于驱动            | 取决于驱动      | 取决于驱动    |\n| FC                   | &#x2713;               | &#x2713;              | -             | -                      |\n| FlexVolume           | &#x2713;               | &#x2713;              | 取决于驱动   | -              |\n| GCEPersistentDisk    | &#x2713;               | &#x2713;              | -             | -                      |\n| Glusterfs            | &#x2713;               | &#x2713;              | &#x2713;      | -                      |\n| HostPath             | &#x2713;               | -                     | -             | -                      |\n| iSCSI                | &#x2713;               | &#x2713;              | -             | -                      |\n| NFS                  | &#x2713;               | &#x2713;              | &#x2713;      | -                      |\n| RBD                  | &#x2713;               | &#x2713;              | -             | -                      |\n| VsphereVolume        | &#x2713;               | -                     | -（Pod 运行于同一节点上时可行） | - |\n| PortworxVolume       | &#x2713;               | -                     | &#x2713;      | -                  | - |"}
{"en": "### Class\n\nA PV can have a class, which is specified by setting the\n`storageClassName` attribute to the name of a\n[StorageClass](/docs/concepts/storage/storage-classes/).\nA PV of a particular class can only be bound to PVCs requesting\nthat class. A PV with no `storageClassName` has no class and can only be bound\nto PVCs that request no particular class.", "zh": "### 类    {#class}\n\n每个 PV 可以属于某个类（Class），通过将其 `storageClassName` 属性设置为某个\n[StorageClass](/zh-cn/docs/concepts/storage/storage-classes/) 的名称来指定。\n特定类的 PV 卷只能绑定到请求该类存储卷的 PVC 申领。\n未设置 `storageClassName` 的 PV 卷没有类设定，只能绑定到那些没有指定特定存储类的 PVC 申领。"}
{"en": "In the past, the annotation `volume.beta.kubernetes.io/storage-class` was used instead\nof the `storageClassName` attribute. This annotation is still working; however,\nit will become fully deprecated in a future Kubernetes release.", "zh": "早前，Kubernetes 使用注解 `volume.beta.kubernetes.io/storage-class` 而不是\n`storageClassName` 属性。这一注解目前仍然起作用，不过在将来的 Kubernetes\n发布版本中该注解会被彻底废弃。"}
{"en": "### Reclaim Policy\n\nCurrent reclaim policies are:\n\n* Retain -- manual reclamation\n* Recycle -- basic scrub (`rm -rf /thevolume/*`)\n* Delete -- delete the volume\n\nFor Kubernetes {{< skew currentVersion >}}, only `nfs` and `hostPath` volume types support recycling.", "zh": "### 回收策略   {#reclaim-policy}\n\n目前的回收策略有：\n\n* Retain -- 手动回收\n* Recycle -- 简单擦除（`rm -rf /thevolume/*`）\n* Delete -- 删除存储卷\n\n对于 Kubernetes {{< skew currentVersion >}} 来说，只有\n`nfs` 和 `hostPath` 卷类型支持回收（Recycle）。"}
{"en": "### Mount Options\n\nA Kubernetes administrator can specify additional mount options for when a\nPersistent Volume is mounted on a node.", "zh": "### 挂载选项    {#mount-options}\n\nKubernetes 管理员可以指定持久卷被挂载到节点上时使用的附加挂载选项。\n\n{{< note >}}"}
{"en": "Not all Persistent Volume types support mount options.", "zh": "并非所有持久卷类型都支持挂载选项。\n{{< /note >}}"}
{"en": "The following volume types support mount options:\n\n* `azureFile`\n* `cephfs` (**deprecated** in v1.28)\n* `cinder` (**deprecated** in v1.18)\n* `iscsi`\n* `nfs`\n* `rbd` (**deprecated** in v1.28)\n* `vsphereVolume`", "zh": "以下卷类型支持挂载选项：\n\n* `azureFile`\n* `cephfs`（于 v1.28 中**弃用**）\n* `cinder`（于 v1.18 中**弃用**）\n* `iscsi`\n* `nfs`\n* `rbd`（于 v1.28 中**弃用**）\n* `vsphereVolume`"}
{"en": "Mount options are not validated. If a mount option is invalid, the mount fails.", "zh": "Kubernetes 不对挂载选项执行合法性检查。如果挂载选项是非法的，挂载就会失败。"}
{"en": "In the past, the annotation `volume.beta.kubernetes.io/mount-options` was used instead\nof the `mountOptions` attribute. This annotation is still working; however,\nit will become fully deprecated in a future Kubernetes release.", "zh": "早前，Kubernetes 使用注解 `volume.beta.kubernetes.io/mount-options` 而不是\n`mountOptions` 属性。这一注解目前仍然起作用，不过在将来的 Kubernetes\n发布版本中该注解会被彻底废弃。"}
{"en": "### Node Affinity", "zh": "### 节点亲和性   {#node-affinity}\n\n{{< note >}}"}
{"en": "For most volume types, you do not need to set this field.\nYou need to explicitly set this for [local](/docs/concepts/storage/volumes/#local) volumes.", "zh": "对大多数卷类型而言，你不需要设置节点亲和性字段。\n你需要为 [local](/zh-cn/docs/concepts/storage/volumes/#local)\n卷显式地设置此属性。\n{{< /note >}}"}
{"en": "A PV can specify node affinity to define constraints that limit what nodes this\nvolume can be accessed from. Pods that use a PV will only be scheduled to nodes\nthat are selected by the node affinity. To specify node affinity, set\n`nodeAffinity` in the `.spec` of a PV. The\n[PersistentVolume](/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/#PersistentVolumeSpec)\nAPI reference has more details on this field.", "zh": "每个 PV 卷可以通过设置节点亲和性来定义一些约束，进而限制从哪些节点上可以访问此卷。\n使用这些卷的 Pod 只会被调度到节点亲和性规则所选择的节点上执行。\n要设置节点亲和性，配置 PV 卷 `.spec` 中的 `nodeAffinity`。\n[持久卷](/zh-cn/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/#PersistentVolumeSpec)\nAPI 参考关于该字段的更多细节。"}
{"en": "### Phase\n\nA PersistentVolume will be in one of the following phases:\n\n`Available`\n: a free resource that is not yet bound to a claim\n\n`Bound`\n: the volume is bound to a claim\n\n`Released`\n: the claim has been deleted, but the associated storage resource is not yet reclaimed by the cluster\n\n`Failed`\n: the volume has failed its (automated) reclamation", "zh": "### 阶段   {#phase}\n\n每个持久卷会处于以下阶段（Phase）之一：\n\n`Available`\n: 卷是一个空闲资源，尚未绑定到任何申领\n\n`Bound`\n: 该卷已经绑定到某申领\n\n`Released`\n: 所绑定的申领已被删除，但是关联存储资源尚未被集群回收\n\n`Failed`\n: 卷的自动回收操作失败"}
{"en": "You can see the name of the PVC bound to the PV using `kubectl describe persistentvolume <name>`.", "zh": "你可以使用 `kubectl describe persistentvolume <name>` 查看已绑定到 PV 的 PVC 的名称。"}
{"en": "#### Phase transition timestamp", "zh": "#### 阶段转换时间戳\n\n{{< feature-state feature_gate_name=\"PersistentVolumeLastPhaseTransitionTime\" >}}"}
{"en": "The `.status` field for a PersistentVolume can include an alpha `lastPhaseTransitionTime` field. This field records\nthe timestamp of when the volume last transitioned its phase. For newly created\nvolumes the phase is set to `Pending` and `lastPhaseTransitionTime` is set to\nthe current time.", "zh": "持久卷的 `.status` 字段可以包含 Alpha 状态的 `lastPhaseTransitionTime` 字段。\n该字段保存的是卷上次转换阶段的时间戳。\n对于新创建的卷，阶段被设置为 `Pending`，`lastPhaseTransitionTime` 被设置为当前时间。\n\n## PersistentVolumeClaims"}
{"en": "Each PVC contains a spec and status, which is the specification and status of the claim.\nThe name of a PersistentVolumeClaim object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).", "zh": "每个 PVC 对象都有 `spec` 和 `status` 部分，分别对应申领的规约和状态。\nPersistentVolumeClaim 对象的名称必须是合法的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: myclaim\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 8Gi\n  storageClassName: slow\n  selector:\n    matchLabels:\n      release: \"stable\"\n    matchExpressions:\n      - {key: environment, operator: In, values: [dev]}\n```"}
{"en": "### Access Modes\n\nClaims use [the same conventions as volumes](#access-modes) when requesting\nstorage with specific access modes.", "zh": "### 访问模式 {#access-modes}\n\n申领在请求具有特定访问模式的存储时，使用与卷相同的[访问模式约定](#access-modes)。"}
{"en": "### Volume Modes\n\nClaims use [the same convention as volumes](#volume-mode) to indicate the\nconsumption of the volume as either a filesystem or block device.", "zh": "### 卷模式 {#volume-modes}\n\n申领使用[与卷相同的约定](#volume-mode)来表明是将卷作为文件系统还是块设备来使用。"}
{"en": "### Resources\n\nClaims, like Pods, can request specific quantities of a resource. In this case,\nthe request is for storage. The same\n[resource model](https://git.k8s.io/design-proposals-archive/scheduling/resources.md)\napplies to both volumes and claims.", "zh": "### 资源    {#resources}\n\n申领和 Pod 一样，也可以请求特定数量的资源。在这个上下文中，请求的资源是存储。\n卷和申领都使用相同的[资源模型](https://git.k8s.io/design-proposals-archive/scheduling/resources.md)。"}
{"en": "### Selector\n\nClaims can specify a\n[label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors)\nto further filter the set of volumes. Only the volumes whose labels match the selector\ncan be bound to the claim. The selector can consist of two fields:", "zh": "### 选择算符    {#selector}\n\n申领可以设置[标签选择算符](/zh-cn/docs/concepts/overview/working-with-objects/labels/#label-selectors)\n来进一步过滤卷集合。只有标签与选择算符相匹配的卷能够绑定到申领上。\n选择算符包含两个字段："}
{"en": "* `matchLabels` - the volume must have a label with this value\n* `matchExpressions` - a list of requirements made by specifying key, list of values,\n  and operator that relates the key and values. Valid operators include In, NotIn,\n  Exists, and DoesNotExist.\n\nAll of the requirements, from both `matchLabels` and `matchExpressions`, are\nANDed together – they must all be satisfied in order to match.", "zh": "* `matchLabels` - 卷必须包含带有此值的标签\n* `matchExpressions` - 通过设定键（key）、值列表和操作符（operator）\n  来构造的需求。合法的操作符有 In、NotIn、Exists 和 DoesNotExist。\n\n来自 `matchLabels` 和 `matchExpressions` 的所有需求都按逻辑与的方式组合在一起。\n这些需求都必须被满足才被视为匹配。"}
{"en": "### Class\n\nA claim can request a particular class by specifying the name of a\n[StorageClass](/docs/concepts/storage/storage-classes/)\nusing the attribute `storageClassName`.\nOnly PVs of the requested class, ones with the same `storageClassName` as the PVC, can\nbe bound to the PVC.", "zh": "### 类      {#class}\n\n申领可以通过为 `storageClassName` 属性设置\n[StorageClass](/zh-cn/docs/concepts/storage/storage-classes/) 的名称来请求特定的存储类。\n只有所请求的类的 PV 卷，即 `storageClassName` 值与 PVC 设置相同的 PV 卷，\n才能绑定到 PVC 申领。"}
{"en": "PVCs don't necessarily have to request a class. A PVC with its `storageClassName` set\nequal to `\"\"` is always interpreted to be requesting a PV with no class, so it\ncan only be bound to PVs with no class (no annotation or one set equal to\n`\"\"`). A PVC with no `storageClassName` is not quite the same and is treated differently\nby the cluster, depending on whether the\n[`DefaultStorageClass` admission plugin](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)\nis turned on.", "zh": "PVC 申领不必一定要请求某个类。如果 PVC 的 `storageClassName` 属性值设置为 `\"\"`，\n则被视为要请求的是没有设置存储类的 PV 卷，因此这一 PVC 申领只能绑定到未设置存储类的\nPV 卷（未设置注解或者注解值为 `\"\"` 的 PersistentVolume（PV）对象在系统中不会被删除，\n因为这样做可能会引起数据丢失）。未设置 `storageClassName` 的 PVC 与此大不相同，\n也会被集群作不同处理。具体筛查方式取决于\n[`DefaultStorageClass` 准入控制器插件](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)\n是否被启用。"}
{"en": "* If the admission plugin is turned on, the administrator may specify a\n  default StorageClass. All PVCs that have no `storageClassName` can be bound only to\n  PVs of that default. Specifying a default StorageClass is done by setting the\n  annotation `storageclass.kubernetes.io/is-default-class` equal to `true` in\n  a StorageClass object. If the administrator does not specify a default, the\n  cluster responds to PVC creation as if the admission plugin were turned off. If more than one\n  default StorageClass is specified, the newest default is used when the\n  PVC is dynamically provisioned.\n* If the admission plugin is turned off, there is no notion of a default\n  StorageClass. All PVCs that have `storageClassName` set to `\"\"` can be\n  bound only to PVs that have `storageClassName` also set to `\"\"`.\n  However, PVCs with missing `storageClassName` can be updated later once\n  default StorageClass becomes available. If the PVC gets updated it will no\n  longer bind to PVs that have `storageClassName` also set to `\"\"`.", "zh": "* 如果准入控制器插件被启用，则管理员可以设置一个默认的 StorageClass。\n  所有未设置 `storageClassName` 的 PVC 都只能绑定到隶属于默认存储类的 PV 卷。\n  设置默认 StorageClass 的工作是通过将对应 StorageClass 对象的注解\n  `storageclass.kubernetes.io/is-default-class` 赋值为 `true` 来完成的。\n  如果管理员未设置默认存储类，集群对 PVC 创建的处理方式与未启用准入控制器插件时相同。\n  如果设定的默认存储类不止一个，当 PVC 被动态制备时将使用最新的默认存储类。\n* 如果准入控制器插件被关闭，则不存在默认 StorageClass 的说法。\n  所有将 `storageClassName` 设为 `\"\"` 的 PVC 只能被绑定到也将 `storageClassName` 设为 `\"\"` 的 PV。\n  不过，只要默认的 StorageClass 可用，就可以稍后更新缺少 `storageClassName` 的 PVC。\n  如果这个 PVC 更新了，它将不再绑定到也将 `storageClassName` 设为 `\"\"` 的 PV。"}
{"en": "See [retroactive default StorageClass assignment](#retroactive-default-storageclass-assignment) for more details.", "zh": "参阅[可追溯的默认 StorageClass 赋值](#retroactive-default-storageclass-assignment)了解更多详细信息。"}
{"en": "Depending on installation method, a default StorageClass may be deployed\nto a Kubernetes cluster by addon manager during installation.\n\nWhen a PVC specifies a `selector` in addition to requesting a StorageClass,\nthe requirements are ANDed together: only a PV of the requested class and with\nthe requested labels may be bound to the PVC.", "zh": "取决于安装方法，默认的 StorageClass 可能在集群安装期间由插件管理器（Addon\nManager）部署到集群中。\n\n当某 PVC 除了请求 StorageClass 之外还设置了 `selector`，则这两种需求会按逻辑与关系处理：\n只有隶属于所请求类且带有所请求标签的 PV 才能绑定到 PVC。\n\n{{< note >}}"}
{"en": "Currently, a PVC with a non-empty `selector` can't have a PV dynamically provisioned for it.", "zh": "目前，设置了非空 `selector` 的 PVC 对象无法让集群为其动态制备 PV 卷。\n{{< /note >}}"}
{"en": "In the past, the annotation `volume.beta.kubernetes.io/storage-class` was used instead\nof `storageClassName` attribute. This annotation is still working; however,\nit won't be supported in a future Kubernetes release.", "zh": "早前，Kubernetes 使用注解 `volume.beta.kubernetes.io/storage-class` 而不是\n`storageClassName` 属性。这一注解目前仍然起作用，不过在将来的 Kubernetes\n发布版本中该注解会被彻底废弃。"}
{"en": "#### Retroactive default StorageClass assignment", "zh": "#### 可追溯的默认 StorageClass 赋值   {#retroactive-default-storageclass-assignment}\n\n{{< feature-state for_k8s_version=\"v1.28\" state=\"stable\" >}}"}
{"en": "You can create a PersistentVolumeClaim without specifying a `storageClassName`\nfor the new PVC, and you can do so even when no default StorageClass exists\nin your cluster. In this case, the new PVC creates as you defined it, and the\n`storageClassName` of that PVC remains unset until default becomes available.", "zh": "你可以创建 PersistentVolumeClaim，而无需为新 PVC 指定 `storageClassName`。\n即使你的集群中不存在默认 StorageClass，你也可以这样做。\n在这种情况下，新的 PVC 会按照你的定义进行创建，并且在默认值可用之前，该 PVC 的 `storageClassName` 保持不设置。"}
{"en": "When a default StorageClass becomes available, the control plane identifies any\nexisting PVCs without `storageClassName`. For the PVCs that either have an empty\nvalue for `storageClassName` or do not have this key, the control plane then\nupdates those PVCs to set `storageClassName` to match the new default StorageClass.\nIf you have an existing PVC where the `storageClassName` is `\"\"`, and you configure\na default StorageClass, then this PVC will not get updated.", "zh": "当一个默认的 StorageClass 变得可用时，控制平面会识别所有未设置 `storageClassName` 的现有 PVC。\n对于 `storageClassName` 为空值或没有此主键的 PVC，\n控制平面会更新这些 PVC 以设置其 `storageClassName` 与新的默认 StorageClass 匹配。\n如果你有一个现有的 PVC，其中 `storageClassName` 是 `\"\"`，\n并且你配置了默认 StorageClass，则此 PVC 将不会得到更新。"}
{"en": "In order to keep binding to PVs with `storageClassName` set to `\"\"`\n(while a default StorageClass is present), you need to set the `storageClassName`\nof the associated PVC to `\"\"`.\n\nThis behavior helps administrators change default StorageClass by removing the\nold one first and then creating or setting another one. This brief window while\nthere is no default causes PVCs without `storageClassName` created at that time\nto not have any default, but due to the retroactive default StorageClass\nassignment this way of changing defaults is safe.", "zh": "为了保持绑定到 `storageClassName` 设为 `\"\"` 的 PV（当存在默认 StorageClass 时），\n你需要将关联 PVC 的 `storageClassName` 设置为 `\"\"`。\n\n此行为可帮助管理员更改默认 StorageClass，方法是先移除旧的 PVC，然后再创建或设置另一个 PVC。\n这一时间窗口内因为没有指定默认值，会导致所创建的未设置 `storageClassName` 的 PVC 也没有默认值设置，\n但由于默认 StorageClass 赋值是可追溯的，这种更改默认值的方式是安全的。"}
{"en": "## Claims As Volumes\n\nPods access storage by using the claim as a volume. Claims must exist in the\nsame namespace as the Pod using the claim. The cluster finds the claim in the\nPod's namespace and uses it to get the PersistentVolume backing the claim.\nThe volume is then mounted to the host and into the Pod.", "zh": "## 使用申领作为卷     {#claims-as-volumes}\n\nPod 将申领作为卷来使用，并藉此访问存储资源。\n申领必须位于使用它的 Pod 所在的同一名字空间内。\n集群在 Pod 的名字空间中查找申领，并使用它来获得申领所使用的 PV 卷。\n之后，卷会被挂载到宿主上并挂载到 Pod 中。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: myfrontend\n      image: nginx\n      volumeMounts:\n      - mountPath: \"/var/www/html\"\n        name: mypd\n  volumes:\n    - name: mypd\n      persistentVolumeClaim:\n        claimName: myclaim\n```"}
{"en": "### A Note on Namespaces\n\nPersistentVolumes binds are exclusive, and since PersistentVolumeClaims are\nnamespaced objects, mounting claims with \"Many\" modes (`ROX`, `RWX`) is only\npossible within one namespace.", "zh": "### 关于名字空间的说明    {#a-note-on-namespaces}\n\nPersistentVolume 卷的绑定是排他性的。\n由于 PersistentVolumeClaim 是名字空间作用域的对象，使用\n\"Many\" 模式（`ROX`、`RWX`）来挂载申领的操作只能在同一名字空间内进行。"}
{"en": "### PersistentVolumes typed `hostPath`\n\nA `hostPath` PersistentVolume uses a file or directory on the Node to emulate\nnetwork-attached storage. See\n[an example of `hostPath` typed volume](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume).", "zh": "### 类型为 `hostpath` 的 PersistentVolume  {#persistentvolumes-typed-hostpath}\n\n`hostPath` PersistentVolume 使用节点上的文件或目录来模拟网络附加（network-attached）存储。\n相关细节可参阅 [`hostPath` 卷示例](/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume)。"}
{"en": "## Raw Block Volume Support", "zh": "## 原始块卷支持   {#raw-block-volume-support}\n\n{{< feature-state for_k8s_version=\"v1.18\" state=\"stable\" >}}"}
{"en": "The following volume plugins support raw block volumes, including dynamic provisioning where\napplicable:", "zh": "以下卷插件支持原始块卷，包括其动态制备（如果支持的话）的卷："}
{"en": "* CSI\n* FC (Fibre Channel)\n* iSCSI\n* Local volume\n* OpenStack Cinder\n* RBD (deprecated)\n* RBD (Ceph Block Device; deprecated)\n* VsphereVolume", "zh": "* CSI\n* FC（光纤通道）\n* iSCSI\n* Local 卷\n* OpenStack Cinder\n* RBD（已弃用）\n* RBD（Ceph 块设备，已弃用）\n* VsphereVolume"}
{"en": "### PersistentVolume using a Raw Block Volume {#persistent-volume-using-a-raw-block-volume}", "zh": "### 使用原始块卷的持久卷      {#persistent-volume-using-a-raw-block-volume}\n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: block-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Block\n  persistentVolumeReclaimPolicy: Retain\n  fc:\n    targetWWNs: [\"50060e801049cfd1\"]\n    lun: 0\n    readOnly: false\n```"}
{"en": "### PersistentVolumeClaim requesting a Raw Block Volume {#persistent-volume-claim-requesting-a-raw-block-volume}", "zh": "### 申请原始块卷的 PVC 申领      {#persistent-volume-claim-requesting-a-raw-block-volume}\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: block-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Block\n  resources:\n    requests:\n      storage: 10Gi\n```"}
{"en": "### Pod specification adding Raw Block Device path in container", "zh": "### 在容器中添加原始块设备路径的 Pod 规约  {#pod-spec-adding-raw-block-device-path-in-container}\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-block-volume\nspec:\n  containers:\n    - name: fc-container\n      image: fedora:26\n      command: [\"/bin/sh\", \"-c\"]\n      args: [ \"tail -f /dev/null\" ]\n      volumeDevices:\n        - name: data\n          devicePath: /dev/xvda\n  volumes:\n    - name: data\n      persistentVolumeClaim:\n        claimName: block-pvc\n```\n\n{{< note >}}"}
{"en": "When adding a raw block device for a Pod, you specify the device path in the\ncontainer instead of a mount path.", "zh": "向 Pod 中添加原始块设备时，你要在容器内设置设备路径而不是挂载路径。\n{{< /note >}}"}
{"en": "### Binding Block Volumes\n\nIf a user requests a raw block volume by indicating this using the `volumeMode`\nfield in the PersistentVolumeClaim spec, the binding rules differ slightly from\nprevious releases that didn't consider this mode as part of the spec.\nListed is a table of possible combinations the user and admin might specify for\nrequesting a raw block device. The table indicates if the volume will be bound or\nnot given the combinations: Volume binding matrix for statically provisioned volumes:", "zh": "### 绑定块卷     {#binding-block-volumes}\n\n如果用户通过 PersistentVolumeClaim 规约的 `volumeMode` 字段来表明对原始块设备的请求，\n绑定规则与之前版本中未在规约中考虑此模式的实现略有不同。\n下面列举的表格是用户和管理员可以为请求原始块设备所作设置的组合。\n此表格表明在不同的组合下卷是否会被绑定。\n\n静态制备卷的卷绑定矩阵："}
{"en": "| PV volumeMode | PVC volumeMode  | Result           |\n| --------------|:---------------:| ----------------:|\n|   unspecified | unspecified     | BIND             |\n|   unspecified | Block           | NO BIND          |\n|   unspecified | Filesystem      | BIND             |\n|   Block       | unspecified     | NO BIND          |\n|   Block       | Block           | BIND             |\n|   Block       | Filesystem      | NO BIND          |\n|   Filesystem  | Filesystem      | BIND             |\n|   Filesystem  | Block           | NO BIND          |\n|   Filesystem  | unspecified     | BIND             |", "zh": "| PV volumeMode | PVC volumeMode  | Result           |\n| --------------|:---------------:| ----------------:|\n|   未指定      | 未指定          | 绑定             |\n|   未指定      | Block           | 不绑定           |\n|   未指定      | Filesystem      | 绑定             |\n|   Block       | 未指定          | 不绑定           |\n|   Block       | Block           | 绑定             |\n|   Block       | Filesystem      | 不绑定           |\n|   Filesystem  | Filesystem      | 绑定             |\n|   Filesystem  | Block           | 不绑定           |\n|   Filesystem  | 未指定          | 绑定             |\n\n{{< note >}}"}
{"en": "Only statically provisioned volumes are supported for alpha release. Administrators\nshould take care to consider these values when working with raw block devices.", "zh": "Alpha 发行版本中仅支持静态制备的卷。\n管理员需要在处理原始块设备时小心处理这些值。\n{{< /note >}}"}
{"en": "## Volume Snapshot and Restore Volume from Snapshot Support", "zh": "## 对卷快照及从卷快照中恢复卷的支持   {#volume-snapshot-and-restore-volume-from-snapshot-support}\n\n{{< feature-state for_k8s_version=\"v1.20\" state=\"stable\" >}}"}
{"en": "Volume snapshots only support the out-of-tree CSI volume plugins.\nFor details, see [Volume Snapshots](/docs/concepts/storage/volume-snapshots/).\nIn-tree volume plugins are deprecated. You can read about the deprecated volume\nplugins in the\n[Volume Plugin FAQ](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md).", "zh": "卷快照（Volume Snapshot）仅支持树外 CSI 卷插件。\n有关细节可参阅[卷快照](/zh-cn/docs/concepts/storage/volume-snapshots/)文档。\n树内卷插件被弃用。你可以查阅[卷插件 FAQ](https://git.k8s.io/community/sig-storage/volume-plugin-faq.md)\n了解已弃用的卷插件。"}
{"en": "### Create a PersistentVolumeClaim from a Volume Snapshot {#create-persistent-volume-claim-from-volume-snapshot}", "zh": "### 基于卷快照创建 PVC 申领     {#create-persistent-volume-claim-from-volume-snapshot}\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: restore-pvc\nspec:\n  storageClassName: csi-hostpath-sc\n  dataSource:\n    name: new-snapshot-test\n    kind: VolumeSnapshot\n    apiGroup: snapshot.storage.k8s.io\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n```"}
{"en": "## Volume Cloning\n\n[Volume Cloning](/docs/concepts/storage/volume-pvc-datasource/)\nonly available for CSI volume plugins.", "zh": "## 卷克隆     {#volume-cloning}\n\n[卷克隆](/zh-cn/docs/concepts/storage/volume-pvc-datasource/)功能特性仅适用于 CSI 卷插件。"}
{"en": "### Create PersistentVolumeClaim from an existing PVC {#create-persistent-volume-claim-from-an-existing-pvc}", "zh": "### 基于现有 PVC 创建新的 PVC 申领    {#create-persistent-volume-claim-from-an-existing-pvc}\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cloned-pvc\nspec:\n  storageClassName: my-csi-plugin\n  dataSource:\n    name: existing-src-pvc-name\n    kind: PersistentVolumeClaim\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n```"}
{"en": "## Volume populators and data sources", "zh": "## 卷填充器（Populator）与数据源      {#volume-populators-and-data-sources}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"beta\" >}}"}
{"en": "Kubernetes supports custom volume populators.\nTo use custom volume populators, you must enable the `AnyVolumeDataSource`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for\nthe kube-apiserver and kube-controller-manager.\n\nVolume populators take advantage of a PVC spec field called `dataSourceRef`. Unlike the\n`dataSource` field, which can only contain either a reference to another PersistentVolumeClaim\nor to a VolumeSnapshot, the `dataSourceRef` field can contain a reference to any object in the\nsame namespace, except for core objects other than PVCs. For clusters that have the feature\ngate enabled, use of the `dataSourceRef` is preferred over `dataSource`.", "zh": "Kubernetes 支持自定义的卷填充器。要使用自定义的卷填充器，你必须为\nkube-apiserver 和 kube-controller-manager 启用 `AnyVolumeDataSource`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)。\n\n卷填充器利用了 PVC 规约字段 `dataSourceRef`。\n不像 `dataSource` 字段只能包含对另一个持久卷申领或卷快照的引用，\n`dataSourceRef` 字段可以包含对同一命名空间中任何对象的引用（不包含除 PVC 以外的核心资源）。\n对于启用了特性门控的集群，使用 `dataSourceRef` 比 `dataSource` 更好。"}
{"en": "## Cross namespace data sources", "zh": "## 跨名字空间数据源   {#cross-namespace-data-sources}\n\n{{< feature-state for_k8s_version=\"v1.26\" state=\"alpha\" >}}"}
{"en": "Kubernetes supports cross namespace volume data sources.\nTo use cross namespace volume data sources, you must enable the `AnyVolumeDataSource`\nand `CrossNamespaceVolumeDataSource`\n[feature gates](/docs/reference/command-line-tools-reference/feature-gates/) for\nthe kube-apiserver, kube-controller-manager.\nAlso, you must enable the `CrossNamespaceVolumeDataSource` feature gate for the csi-provisioner.\n\nEnabling the `CrossNamespaceVolumeDataSource` feature gate allows you to specify\na namespace in the dataSourceRef field.", "zh": "Kubernetes 支持跨名字空间卷数据源。\n要使用跨名字空间卷数据源，你必须为 kube-apiserver、kube-controller 管理器启用\n`AnyVolumeDataSource` 和 `CrossNamespaceVolumeDataSource`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)。\n此外，你必须为 csi-provisioner 启用 `CrossNamespaceVolumeDataSource` 特性门控。\n\n启用 `CrossNamespaceVolumeDataSource` 特性门控允许你在 dataSourceRef 字段中指定名字空间。\n\n{{< note >}}"}
{"en": "When you specify a namespace for a volume data source, Kubernetes checks for a\nReferenceGrant in the other namespace before accepting the reference.\nReferenceGrant is part of the `gateway.networking.k8s.io` extension APIs.\nSee [ReferenceGrant](https://gateway-api.sigs.k8s.io/api-types/referencegrant/)\nin the Gateway API documentation for details.\nThis means that you must extend your Kubernetes cluster with at least ReferenceGrant from the\nGateway API before you can use this mechanism.", "zh": "当你为卷数据源指定名字空间时，Kubernetes 在接受此引用之前在另一个名字空间中检查 ReferenceGrant。\nReferenceGrant 是 `gateway.networking.k8s.io` 扩展 API 的一部分。更多细节请参见 Gateway API 文档中的\n[ReferenceGrant](https://gateway-api.sigs.k8s.io/api-types/referencegrant/)。\n这意味着你必须在使用此机制之前至少使用 Gateway API 的 ReferenceGrant 来扩展 Kubernetes 集群。\n{{< /note >}}"}
{"en": "## Data source references\n\nThe `dataSourceRef` field behaves almost the same as the `dataSource` field. If one is\nspecified while the other is not, the API server will give both fields the same value. Neither\nfield can be changed after creation, and attempting to specify different values for the two\nfields will result in a validation error. Therefore the two fields will always have the same\ncontents.", "zh": "## 数据源引用   {#data-source-references}\n\n`dataSourceRef` 字段的行为与 `dataSource` 字段几乎相同。\n如果其中一个字段被指定而另一个字段没有被指定，API 服务器将给两个字段相同的值。\n这两个字段都不能在创建后改变，如果试图为这两个字段指定不同的值，将导致验证错误。\n因此，这两个字段将总是有相同的内容。"}
{"en": "There are two differences between the `dataSourceRef` field and the `dataSource` field that\nusers should be aware of:\n\n* The `dataSource` field ignores invalid values (as if the field was blank) while the\n  `dataSourceRef` field never ignores values and will cause an error if an invalid value is\n  used. Invalid values are any core object (objects with no apiGroup) except for PVCs.\n* The `dataSourceRef` field may contain different types of objects, while the `dataSource` field\n  only allows PVCs and VolumeSnapshots.", "zh": "在 `dataSourceRef` 字段和 `dataSource` 字段之间有两个用户应该注意的区别：\n\n* `dataSource` 字段会忽略无效的值（如同是空值），\n   而 `dataSourceRef` 字段永远不会忽略值，并且若填入一个无效的值，会导致错误。\n   无效值指的是 PVC 之外的核心对象（没有 apiGroup 的对象）。\n* `dataSourceRef` 字段可以包含不同类型的对象，而 `dataSource` 字段只允许 PVC 和卷快照。"}
{"en": "When the `CrossNamespaceVolumeDataSource` feature is enabled, there are additional differences:\n\n* The `dataSource` field only allows local objects, while the `dataSourceRef` field allows\n  objects in any namespaces.  \n* When namespace is specified, `dataSource` and `dataSourceRef` are not synced.", "zh": "当 `CrossNamespaceVolumeDataSource` 特性被启用时，存在其他区别：\n\n* `dataSource` 字段仅允许本地对象，而 `dataSourceRef` 字段允许任何名字空间中的对象。\n* 若指定了 namespace，则 `dataSource` 和 `dataSourceRef` 不会被同步。"}
{"en": "Users should always use `dataSourceRef` on clusters that have the feature gate enabled, and\nfall back to `dataSource` on clusters that do not. It is not necessary to look at both fields\nunder any circumstance. The duplicated values with slightly different semantics exist only for\nbackwards compatibility. In particular, a mixture of older and newer controllers are able to\ninteroperate because the fields are the same.", "zh": "用户始终应该在启用了此特性门控的集群上使用 `dataSourceRef`，\n在没有启用该特性门控的集群上使用 `dataSource`。\n在任何情况下都没有必要查看这两个字段。\n这两个字段的值看似相同但是语义稍微不一样，是为了向后兼容。\n特别是混用旧版本和新版本的控制器时，它们能够互通。"}
{"en": "### Using volume populators\n\nVolume populators are {{< glossary_tooltip text=\"controllers\" term_id=\"controller\" >}} that can\ncreate non-empty volumes, where the contents of the volume are determined by a Custom Resource.\nUsers create a populated volume by referring to a Custom Resource using the `dataSourceRef` field:", "zh": "## 使用卷填充器   {#using-volume-populators}\n\n卷填充器是能创建非空卷的{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}，\n其卷的内容通过一个自定义资源决定。\n用户通过使用 `dataSourceRef` 字段引用自定义资源来创建一个被填充的卷：\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: populated-pvc\nspec:\n  dataSourceRef:\n    name: example-name\n    kind: ExampleDataSource\n    apiGroup: example.storage.k8s.io\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n```"}
{"en": "Because volume populators are external components, attempts to create a PVC that uses one\ncan fail if not all the correct components are installed. External controllers should generate\nevents on the PVC to provide feedback on the status of the creation, including warnings if\nthe PVC cannot be created due to some missing component.\n\nYou can install the alpha [volume data source validator](https://github.com/kubernetes-csi/volume-data-source-validator)\ncontroller into your cluster. That controller generates warning Events on a PVC in the case that no populator\nis registered to handle that kind of data source. When a suitable populator is installed for a PVC, it's the\nresponsibility of that populator controller to report Events that relate to volume creation and issues during\nthe process.", "zh": "因为卷填充器是外部组件，如果没有安装所有正确的组件，试图创建一个使用卷填充器的 PVC 就会失败。\n外部控制器应该在 PVC 上产生事件，以提供创建状态的反馈，包括在由于缺少某些组件而无法创建 PVC 的情况下发出警告。\n\n你可以把 alpha 版本的[卷数据源验证器](https://github.com/kubernetes-csi/volume-data-source-validator)\n控制器安装到你的集群中。\n如果没有填充器处理该数据源的情况下，该控制器会在 PVC 上产生警告事件。\n当一个合适的填充器被安装到 PVC 上时，该控制器的职责是上报与卷创建有关的事件，以及在该过程中发生的问题。"}
{"en": "### Using a cross-namespace volume data source", "zh": "### 使用跨名字空间的卷数据源   {#using-a-cross-namespace-volume-data-source}\n\n{{< feature-state for_k8s_version=\"v1.26\" state=\"alpha\" >}}"}
{"en": "Create a ReferenceGrant to allow the namespace owner to accept the reference.\nYou define a populated volume by specifying a cross namespace volume data source\nusing the `dataSourceRef` field. You must already have a valid ReferenceGrant\nin the source namespace:", "zh": "创建 ReferenceGrant 以允许名字空间属主接受引用。\n你通过使用 `dataSourceRef` 字段指定跨名字空间卷数据源，定义填充的卷。\n你必须在源名字空间中已经有一个有效的 ReferenceGrant：\n\n```yaml\napiVersion: gateway.networking.k8s.io/v1beta1\nkind: ReferenceGrant\nmetadata:\n  name: allow-ns1-pvc\n  namespace: default\nspec:\n  from:\n  - group: \"\"\n    kind: PersistentVolumeClaim\n    namespace: ns1\n  to:\n  - group: snapshot.storage.k8s.io\n    kind: VolumeSnapshot\n    name: new-snapshot-demo\n```\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: foo-pvc\n  namespace: ns1\nspec:\n  storageClassName: example\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  dataSourceRef:\n    apiGroup: snapshot.storage.k8s.io\n    kind: VolumeSnapshot\n    name: new-snapshot-demo\n    namespace: default\n  volumeMode: Filesystem\n```"}
{"en": "## Writing Portable Configuration\n\nIf you're writing configuration templates or examples that run on a wide range of clusters\nand need persistent storage, it is recommended that you use the following pattern:", "zh": "## 编写可移植的配置   {#writing-portable-configuration}\n\n如果你要编写配置模板和示例用来在很多集群上运行并且需要持久性存储，建议你使用以下模式："}
{"en": "- Include PersistentVolumeClaim objects in your bundle of config (alongside\n  Deployments, ConfigMaps, etc).\n- Do not include PersistentVolume objects in the config, since the user instantiating\n  the config may not have permission to create PersistentVolumes.", "zh": "- 将 PersistentVolumeClaim 对象包含到你的配置包（Bundle）中，和 Deployment\n  以及 ConfigMap 等放在一起。\n- 不要在配置中包含 PersistentVolume 对象，因为对配置进行实例化的用户很可能\n  没有创建 PersistentVolume 的权限。"}
{"en": "- Give the user the option of providing a storage class name when instantiating\n  the template.\n  - If the user provides a storage class name, put that value into the\n    `persistentVolumeClaim.storageClassName` field.\n    This will cause the PVC to match the right storage\n    class if the cluster has StorageClasses enabled by the admin.\n  - If the user does not provide a storage class name, leave the\n    `persistentVolumeClaim.storageClassName` field as nil. This will cause a\n    PV to be automatically provisioned for the user with the default StorageClass\n    in the cluster. Many cluster environments have a default StorageClass installed,\n    or administrators can create their own default StorageClass.", "zh": "- 为用户提供在实例化模板时指定存储类名称的能力。\n  - 仍按用户提供存储类名称，将该名称放到 `persistentVolumeClaim.storageClassName` 字段中。\n    这样会使得 PVC 在集群被管理员启用了存储类支持时能够匹配到正确的存储类，\n  - 如果用户未指定存储类名称，将 `persistentVolumeClaim.storageClassName` 留空（nil）。\n    这样，集群会使用默认 `StorageClass` 为用户自动制备一个存储卷。\n    很多集群环境都配置了默认的 `StorageClass`，或者管理员也可以自行创建默认的\n    `StorageClass`。"}
{"en": "- In your tooling, watch for PVCs that are not getting bound after some time\n  and surface this to the user, as this may indicate that the cluster has no\n  dynamic storage support (in which case the user should create a matching PV)\n  or the cluster has no storage system (in which case the user cannot deploy\n  config requiring PVCs).", "zh": "- 在你的工具链中，监测经过一段时间后仍未被绑定的 PVC 对象，要让用户知道这些对象，\n  因为这可能意味着集群不支持动态存储（因而用户必须先创建一个匹配的 PV），或者\n  集群没有配置存储系统（因而用户无法配置需要 PVC 的工作负载配置）。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn more about [Creating a PersistentVolume](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume).\n* Learn more about [Creating a PersistentVolumeClaim](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim).\n* Read the [Persistent Storage design document](https://git.k8s.io/design-proposals-archive/storage/persistent-storage.md).", "zh": "* 进一步了解[创建持久卷](/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume)。\n* 进一步学习[创建 PVC 申领](/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim)。\n* 阅读[持久存储的设计文档](https://git.k8s.io/design-proposals-archive/storage/persistent-storage.md)。"}
{"en": "### API references {#reference}\n\nRead about the APIs described in this page:\n\n* [`PersistentVolume`](/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/)\n* [`PersistentVolumeClaim`](/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/)", "zh": "### API 参考    {#reference}\n\n阅读以下页面中描述的 API：\n\n* [`PersistentVolume`](/zh-cn/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/)\n* [`PersistentVolumeClaim`](/zh-cn/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/)"}
{"en": "This page describes the maximum number of volumes that can be attached\nto a Node for various cloud providers.", "zh": "此页面描述了各个云供应商可关联至一个节点的最大卷数。"}
{"en": "Cloud providers like Google, Amazon, and Microsoft typically have a limit on\nhow many volumes can be attached to a Node. It is important for Kubernetes to\nrespect those limits. Otherwise, Pods scheduled on a Node could get stuck\nwaiting for volumes to attach.", "zh": "谷歌、亚马逊和微软等云供应商通常对可以关联到节点的卷数量进行限制。\nKubernetes 需要尊重这些限制。否则，在节点上调度的 Pod 可能会卡住去等待卷的关联。"}
{"en": "## Kubernetes default limits\n\nThe Kubernetes scheduler has default limits on the number of volumes\nthat can be attached to a Node:", "zh": "## Kubernetes 的默认限制\n\nThe Kubernetes 调度器对关联于一个节点的卷数有默认限制："}
{"en": "<table>\n  <tr><th>Cloud service</th><th>Maximum volumes per Node</th></tr>\n  <tr><td><a href=\"https://aws.amazon.com/ebs/\">Amazon Elastic Block Store (EBS)</a></td><td>39</td></tr>\n  <tr><td><a href=\"https://cloud.google.com/persistent-disk/\">Google Persistent Disk</a></td><td>16</td></tr>\n  <tr><td><a href=\"https://azure.microsoft.com/en-us/services/storage/main-disks/\">Microsoft Azure Disk Storage</a></td><td>16</td></tr>\n</table>", "zh": "<table>\n  <tr><th>云服务</th><th>每节点最大卷数</th></tr>\n  <tr><td><a href=\"https://aws.amazon.com/ebs/\">Amazon Elastic Block Store (EBS)</a></td><td>39</td></tr>\n  <tr><td><a href=\"https://cloud.google.com/persistent-disk/\">Google Persistent Disk</a></td><td>16</td></tr>\n  <tr><td><a href=\"https://azure.microsoft.com/en-us/services/storage/main-disks/\">Microsoft Azure Disk Storage</a></td><td>16</td></tr>\n</table>"}
{"en": "## Custom limits\n\nYou can change these limits by setting the value of the\n`KUBE_MAX_PD_VOLS` environment variable, and then starting the scheduler.\nCSI drivers might have a different procedure, see their documentation\non how to customize their limits.\n\nUse caution if you set a limit that is higher than the default limit. Consult\nthe cloud provider's documentation to make sure that Nodes can actually support\nthe limit you set.\n\nThe limit applies to the entire cluster, so it affects all Nodes.", "zh": "## 自定义限制\n\n你可以通过设置 `KUBE_MAX_PD_VOLS` 环境变量的值来设置这些限制，然后再启动调度器。\nCSI 驱动程序可能具有不同的过程，关于如何自定义其限制请参阅相关文档。\n\n如果设置的限制高于默认限制，请谨慎使用。请参阅云提供商的文档以确保节点可支持你设置的限制。\n\n此限制应用于整个集群，所以它会影响所有节点。"}
{"en": "## Dynamic volume limits", "zh": "## 动态卷限制\n\n{{< feature-state state=\"stable\" for_k8s_version=\"v1.17\" >}}"}
{"en": "Dynamic volume limits are supported for following volume types.\n\n- Amazon EBS\n- Google Persistent Disk\n- Azure Disk\n- CSI", "zh": "以下卷类型支持动态卷限制。\n\n- Amazon EBS\n- Google Persistent Disk\n- Azure Disk\n- CSI"}
{"en": "For volumes managed by in-tree volume plugins, Kubernetes automatically determines the Node\ntype and enforces the appropriate maximum number of volumes for the node. For example:", "zh": "对于由内建插件管理的卷，Kubernetes 会自动确定节点类型并确保节点上可关联的卷数目合规。例如："}
{"en": "* On\n<a href=\"https://cloud.google.com/compute/\">Google Compute Engine</a>,\nup to 127 volumes can be attached to a node, [depending on the node\ntype](https://cloud.google.com/compute/docs/disks/#pdnumberlimits).\n\n* For Amazon EBS disks on M5,C5,R5,T3 and Z1D instance types, Kubernetes allows only 25\nvolumes to be attached to a Node. For other instance types on\n<a href=\"https://aws.amazon.com/ec2/\">Amazon Elastic Compute Cloud (EC2)</a>,\nKubernetes allows 39 volumes to be attached to a Node.\n\n* On Azure, up to 64 disks can be attached to a node, depending on the node type. For more details, refer to [Sizes for virtual machines in Azure](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes).\n\n* If a CSI storage driver advertises a maximum number of volumes for a Node (using `NodeGetInfo`), the {{< glossary_tooltip text=\"kube-scheduler\" term_id=\"kube-scheduler\" >}} honors that limit.\nRefer to the [CSI specifications](https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo) for details.\n\n* For volumes managed by in-tree plugins that have been migrated to a CSI driver, the maximum number of volumes will be the one reported by the CSI driver.", "zh": "* 在\n  <a href=\"https://cloud.google.com/compute/\">Google Compute Engine</a>环境中，\n  [根据节点类型](https://cloud.google.com/compute/docs/disks/#pdnumberlimits)最多可以将 127 个卷关联到节点。\n\n* 对于 M5、C5、R5、T3 和 Z1D 类型实例的 Amazon EBS 磁盘，Kubernetes 仅允许 25 个卷关联到节点。\n  对于 ec2 上的其他实例类型\n  <a href=\"https://aws.amazon.com/ec2/\">Amazon Elastic Compute Cloud (EC2)</a>，\n  Kubernetes 允许 39 个卷关联至节点。\n\n* 在 Azure 环境中, 根据节点类型，最多 64 个磁盘可以关联至一个节点。\n  更多详细信息，请参阅 [Azure 虚拟机的数量大小](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes)。\n\n* 如果 CSI 存储驱动程序（使用 `NodeGetInfo` ）为节点通告卷数上限，则 {{< glossary_tooltip text=\"kube-scheduler\" term_id=\"kube-scheduler\" >}} 将遵守该限制值。\n  参考 [CSI 规范](https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo) 获取更多详细信息。\n\n* 对于由已迁移到 CSI 驱动程序的树内插件管理的卷，最大卷数将是 CSI 驱动程序报告的卷数。"}
{"en": "On-disk files in a container are ephemeral, which presents some problems for\nnon-trivial applications when running in containers. One problem occurs when \na container crashes or is stopped. Container state is not saved so all of the \nfiles that were created or modified during the lifetime of the container are lost. \nDuring a crash, kubelet restarts the container with a clean state. \nAnother problem occurs when multiple containers are running in a `Pod` and \nneed to share files. It can be challenging to setup \nand access a shared filesystem across all of the containers.\nThe Kubernetes {{< glossary_tooltip text=\"volume\" term_id=\"volume\" >}} abstraction\nsolves both of these problems.\nFamiliarity with [Pods](/docs/concepts/workloads/pods/) is suggested.", "zh": "容器中的文件在磁盘上是临时存放的，这给在容器中运行较重要的应用带来一些问题。\n当容器崩溃或停止时会出现一个问题。此时容器状态未保存，\n因此在容器生命周期内创建或修改的所有文件都将丢失。\n在崩溃期间，kubelet 会以干净的状态重新启动容器。\n当多个容器在一个 Pod 中运行并且需要共享文件时，会出现另一个问题。\n跨所有容器设置和访问共享文件系统具有一定的挑战性。\n\nKubernetes {{< glossary_tooltip text=\"卷（Volume）\" term_id=\"volume\" >}}\n这一抽象概念能够解决这两个问题。\n\n阅读本文前建议你熟悉一下 [Pod](/zh-cn/docs/concepts/workloads/pods)。"}
{"en": "## Background", "zh": "## 背景  {#background}"}
{"en": "Kubernetes supports many types of volumes. A {{< glossary_tooltip term_id=\"pod\" text=\"Pod\" >}}\ncan use any number of volume types simultaneously.\n[Ephemeral volume](/docs/concepts/storage/ephemeral-volumes/) types have a lifetime of a pod,\nbut [persistent volumes](/docs/concepts/storage/persistent-volumes/) exist beyond\nthe lifetime of a pod. When a pod ceases to exist, Kubernetes destroys ephemeral volumes;\nhowever, Kubernetes does not destroy persistent volumes.\nFor any kind of volume in a given pod, data is preserved across container restarts.", "zh": "Kubernetes 支持很多类型的卷。\n{{< glossary_tooltip term_id=\"pod\" text=\"Pod\" >}} 可以同时使用任意数目的卷类型。\n[临时卷](/zh-cn/docs/concepts/storage/ephemeral-volumes/)类型的生命周期与 Pod 相同，\n但[持久卷](/zh-cn/docs/concepts/storage/persistent-volumes/)可以比 Pod 的存活期长。\n当 Pod 不再存在时，Kubernetes 也会销毁临时卷；不过 Kubernetes 不会销毁持久卷。\n对于给定 Pod 中任何类型的卷，在容器重启期间数据都不会丢失。"}
{"en": "At its core, a volume is a directory, possibly with some data in it, which\nis accessible to the containers in a pod. How that directory comes to be, the\nmedium that backs it, and the contents of it are determined by the particular\nvolume type used.", "zh": "卷的核心是一个目录，其中可能存有数据，Pod 中的容器可以访问该目录中的数据。\n所采用的特定的卷类型将决定该目录如何形成的、使用何种介质保存数据以及目录中存放的内容。"}
{"en": "To use a volume, specify the volumes to provide for the Pod in `.spec.volumes`\nand declare where to mount those volumes into containers in `.spec.containers[*].volumeMounts`.\nA process in a container sees a filesystem view composed from the initial contents of\nthe {{< glossary_tooltip text=\"container image\" term_id=\"image\" >}}, plus volumes\n(if defined) mounted inside the container.\nThe process sees a root filesystem that initially matches the contents of the container\nimage.\nAny writes to within that filesystem hierarchy, if allowed, affect what that process views\nwhen it performs a subsequent filesystem access.", "zh": "使用卷时, 在 `.spec.volumes` 字段中设置为 Pod 提供的卷，并在\n`.spec.containers[*].volumeMounts` 字段中声明卷在容器中的挂载位置。\n容器中的进程看到的文件系统视图是由它们的{{< glossary_tooltip text=\"容器镜像\" term_id=\"image\" >}}\n的初始内容以及挂载在容器中的卷（如果定义了的话）所组成的。\n其中根文件系统同容器镜像的内容相吻合。\n任何在该文件系统下的写入操作，如果被允许的话，都会影响接下来容器中进程访问文件系统时所看到的内容。"}
{"en": "Volumes mount at the [specified paths](#using-subpath) within\nthe image.\nFor each container defined within a Pod, you must independently specify where\nto mount each volume that the container uses.\n\nVolumes cannot mount within other volumes (but see [Using subPath](#using-subpath)\nfor a related mechanism). Also, a volume cannot contain a hard link to anything in\na different volume.", "zh": "卷挂载在镜像中的[指定路径](#using-subpath)下。\nPod 配置中的每个容器必须独立指定各个卷的挂载位置。\n\n卷不能挂载到其他卷之上（不过存在一种[使用 subPath](#using-subpath) 的相关机制），也不能与其他卷有硬链接。"}
{"en": "## Types of volumes {#volume-types}\n\nKubernetes supports several types of volumes.", "zh": "## 卷类型  {#volume-types}\n\nKubernetes 支持下列类型的卷："}
{"en": "### awsElasticBlockStore (deprecated) {#awselasticblockstore}", "zh": "### awsElasticBlockStore （已弃用）   {#awselasticblockstore}"}
{"en": "In Kubernetes {{< skew currentVersion >}}, all operations for the in-tree `awsElasticBlockStore` type\nare redirected to the `ebs.csi.aws.com` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} driver.\n\nThe AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release\nand then removed entirely in the v1.27 release.\n\nThe Kubernetes project suggests that you use the [AWS EBS](https://github.com/kubernetes-sigs/aws-ebs-csi-driver) third party\nstorage driver instead.", "zh": "在 Kubernetes {{< skew currentVersion >}} 中，所有针对树内 `awsElasticBlockStore`\n类型的操作都会被重定向到 `ebs.csi.aws.com` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} 驱动。\n\nAWSElasticBlockStore 树内存储驱动已在 Kubernetes v1.19 版本中废弃，\n并在 v1.27 版本中被完全移除。\n\nKubernetes 项目建议你转为使用 [AWS EBS](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)\n第三方存储驱动插件。"}
{"en": "### azureDisk (deprecated) {#azuredisk}", "zh": "### azureDisk （已弃用）   {#azuredisk}"}
{"en": "In Kubernetes {{< skew currentVersion >}}, all operations for the in-tree `azureDisk` type\nare redirected to the `disk.csi.azure.com` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} driver.\n\nThe AzureDisk in-tree storage driver was deprecated in the Kubernetes v1.19 release\nand then removed entirely in the v1.27 release.\n\nThe Kubernetes project suggests that you use the [Azure Disk](https://github.com/kubernetes-sigs/azuredisk-csi-driver) third party\nstorage driver instead.", "zh": "在 Kubernetes {{< skew currentVersion >}} 中，所有针对树内 `azureDisk`\n类型的操作都会被重定向到 `disk.csi.azure.com` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} 驱动。\n\nAzureDisk 树内存储驱动已在 Kubernetes v1.19 版本中废弃，并在 v1.27 版本中被完全移除。\n\nKubernetes 项目建议你转为使用 [Azure Disk](https://github.com/kubernetes-sigs/azuredisk-csi-driver)\n第三方存储驱动插件。"}
{"en": "### azureFile (deprecated) {#azurefile}", "zh": "### azureFile （已弃用）    {#azurefile}\n\n{{< feature-state for_k8s_version=\"v1.21\" state=\"deprecated\" >}}"}
{"en": "The `azureFile` volume type mounts a Microsoft Azure File volume (SMB 2.1 and 3.0)\ninto a pod.\n\nFor more details, see the [`azureFile` volume plugin](https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file/README.md).", "zh": "`azureFile` 卷类型用来在 Pod 上挂载 Microsoft Azure 文件卷（File Volume）（SMB 2.1 和 3.0）。\n\n更多详情请参考 [`azureFile` 卷插件](https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file/README.md)。"}
{"en": "#### azureFile CSI migration", "zh": "#### azureFile CSI 迁移  {#azurefile-csi-migration}\n\n{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}"}
{"en": "The `CSIMigration` feature for `azureFile`, when enabled, redirects all plugin operations\nfrom the existing in-tree plugin to the `file.csi.azure.com` Container\nStorage Interface (CSI) Driver. In order to use this feature, the [Azure File CSI\nDriver](https://github.com/kubernetes-sigs/azurefile-csi-driver)\nmust be installed on the cluster and the `CSIMigrationAzureFile`\n[feature gates](/docs/reference/command-line-tools-reference/feature-gates/) must be enabled.", "zh": "启用 `azureFile` 的 `CSIMigration` 特性后，所有插件操作将从现有的树内插件重定向到\n`file.csi.azure.com` 容器存储接口（CSI）驱动程序。要使用此特性，必须在集群中安装\n[Azure 文件 CSI 驱动程序](https://github.com/kubernetes-sigs/azurefile-csi-driver)，\n并且 `CSIMigrationAzureFile`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)\n必须被启用。"}
{"en": "Azure File CSI driver does not support using same volume with different fsgroups. If\n`CSIMigrationAzureFile` is enabled, using same volume with different fsgroups won't be supported at all.", "zh": "Azure 文件 CSI 驱动尚不支持为同一卷设置不同的 fsgroup。\n如果 `CSIMigrationAzureFile` 特性被启用，用不同的 fsgroup 来使用同一卷也是不被支持的。"}
{"en": "#### azureFile CSI migration complete", "zh": "#### azureFile CSI 迁移完成\n\n{{< feature-state for_k8s_version=\"v1.21\" state=\"alpha\" >}}"}
{"en": "To disable the `azureFile` storage plugin from being loaded by the controller manager\nand the kubelet, set the `InTreePluginAzureFileUnregister` flag to `true`.", "zh": "要禁止控制器管理器和 kubelet 加载 `azureFile` 存储插件，\n请将 `InTreePluginAzureFileUnregister` 标志设置为 `true`。"}
{"en": "### cephfs (removed) {#cephfs}", "zh": "### cephfs（已移除）  {#cephfs}"}
{"en": "Kubernetes {{< skew currentVersion >}} does not include a `cephfs` volume type.\n\nThe `cephfs` in-tree storage driver was deprecated in the Kubernetes v1.28 release and then removed entirely in the v1.31 release.", "zh": "Kubernetes {{< skew currentVersion >}} 不包括 `cephfs` 卷类型。\n\n`cephfs` 树内存储驱动在 Kubernetes v1.28 版本中被弃用，并在 v1.31 版本中被完全移除。"}
{"en": "### cinder (deprecated) {#cinder}", "zh": "### cinder（已弃用）   {#cinder}"}
{"en": "In Kubernetes {{< skew currentVersion >}}, all operations for the in-tree `cinder` type\nare redirected to the `cinder.csi.openstack.org` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} driver.\n\nThe OpenStack Cinder in-tree storage driver was deprecated in the Kubernetes v1.11 release\nand then removed entirely in the v1.26 release.\n\nThe Kubernetes project suggests that you use the \n[OpenStack Cinder](https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md)\nthird party storage driver instead.", "zh": "在 Kubernetes {{< skew currentVersion >}} 中，所有针对树内 `cinder`\n类型的操作都会被重定向到 `cinder.csi.openstack.org` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} 驱动。\n\nOpenStack Cinder 树内存储驱动已在 Kubernetes v1.11 版本中废弃，\n并在 v1.26 版本中被完全移除。\n\nKubernetes 项目建议你转为使用\n[OpenStack Cinder](https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md)\n第三方存储驱动插件。\n\n### configMap"}
{"en": "A [ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/)\nprovides a way to inject configuration data into pods.\nThe data stored in a ConfigMap can be referenced in a volume of type\n`configMap` and then consumed by containerized applications running in a pod.", "zh": "[`configMap`](/zh-cn/docs/tasks/configure-pod-container/configure-pod-configmap/)\n卷提供了向 Pod 注入配置数据的方法。\nConfigMap 对象中存储的数据可以被 `configMap` 类型的卷引用，然后被 Pod 中运行的容器化应用使用。"}
{"en": "When referencing a ConfigMap, you provide the name of the ConfigMap in the\nvolume. You can customize the path to use for a specific\nentry in the ConfigMap. The following configuration shows how to mount\nthe `log-config` ConfigMap onto a Pod called `configmap-pod`:", "zh": "引用 configMap 对象时，你可以在卷中通过它的名称来引用。\n你可以自定义 ConfigMap 中特定条目所要使用的路径。\n下面的配置显示了如何将名为 `log-config` 的 ConfigMap 挂载到名为 `configmap-pod`\n的 Pod 中：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-pod\nspec:\n  containers:\n    - name: test\n      image: busybox:1.28\n      command: ['sh', '-c', 'echo \"The app is running!\" && tail -f /dev/null']\n      volumeMounts:\n        - name: config-vol\n          mountPath: /etc/config\n  volumes:\n    - name: config-vol\n      configMap:\n        name: log-config\n        items:\n          - key: log_level\n            path: log_level\n```"}
{"en": "The `log-config` ConfigMap is mounted as a volume, and all contents stored in\nits `log_level` entry are mounted into the Pod at path `/etc/config/log_level`.\nNote that this path is derived from the volume's `mountPath` and the `path`\nkeyed with `log_level`.", "zh": "`log-config` ConfigMap 以卷的形式挂载，并且存储在 `log_level`\n条目中的所有内容都被挂载到 Pod 的 `/etc/config/log_level` 路径下。\n请注意，这个路径来源于卷的 `mountPath` 和 `log_level` 键对应的 `path`。\n\n{{< note >}}"}
{"en": "* You must [create a ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/#create-a-configmap)\n  before you can use it.\n\n* A ConfigMap is always mounted as `readOnly`.\n\n* A container using a ConfigMap as a [`subPath`](#using-subpath) volume mount will not\n  receive ConfigMap updates.\n  \n* Text data is exposed as files using the UTF-8 character encoding. For other character encodings, use `binaryData`.", "zh": "* 你必须先[创建 ConfigMap](/zh-cn/docs/tasks/configure-pod-container/configure-pod-configmap/#create-a-configmap)，\n  才能使用它。\n* ConfigMap 总是以 `readOnly` 的模式挂载。\n* 容器以 [`subPath`](#using-subpath) 卷挂载方式使用 ConfigMap 时，将无法接收 ConfigMap 的更新。\n* 文本数据挂载成文件时采用 UTF-8 字符编码。如果使用其他字符编码形式，可使用\n  `binaryData` 字段。\n{{< /note >}}\n\n### downwardAPI {#downwardapi}"}
{"en": "A `downwardAPI` volume makes {{< glossary_tooltip term_id=\"downward-api\" text=\"downward API\" >}}\ndata available to applications. Within the volume, you can find the exposed\ndata as read-only files in plain text format.", "zh": "`downwardAPI` 卷用于为应用提供 {{< glossary_tooltip term_id=\"downward-api\" text=\"downward API\" >}} 数据。\n在这类卷中，所公开的数据以纯文本格式的只读文件形式存在。"}
{"en": "A container using the downward API as a [`subPath`](#using-subpath) volume mount does not\nreceive updates when field values change.", "zh": "{{< note >}}\n容器以 [subPath](#using-subpath) 卷挂载方式使用 downward API 时，在字段值更改时将不能接收到它的更新。\n{{< /note >}}"}
{"en": "See [Expose Pod Information to Containers Through Files](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)\nto learn more.", "zh": "更多详细信息请参考[通过文件将 Pod 信息呈现给容器](/zh-cn/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)。\n\n### emptyDir {#emptydir}"}
{"en": "For a Pod that defines an `emptyDir` volume, the volume is created when the Pod is assigned to a node.\nAs the name says, the `emptyDir` volume is initially empty. All containers in the Pod can read and write the same\nfiles in the `emptyDir` volume, though that volume can be mounted at the same\nor different paths in each container. When a Pod is removed from a node for\nany reason, the data in the `emptyDir` is deleted permanently.", "zh": "对于定义了 `emptyDir` 卷的 Pod，在 Pod 被指派到某节点时此卷会被创建。\n就像其名称所表示的那样，`emptyDir` 卷最初是空的。尽管 Pod 中的容器挂载 `emptyDir`\n卷的路径可能相同也可能不同，但这些容器都可以读写 `emptyDir` 卷中相同的文件。\n当 Pod 因为某些原因被从节点上删除时，`emptyDir` 卷中的数据也会被永久删除。\n\n{{< note >}}"}
{"en": "A container crashing does *not* remove a Pod from a node. The data in an `emptyDir` volume\nis safe across container crashes.", "zh": "容器崩溃并**不**会导致 Pod 被从节点上移除，因此容器崩溃期间 `emptyDir` 卷中的数据是安全的。\n{{< /note >}}"}
{"en": "Some uses for an `emptyDir` are:\n\n* scratch space, such as for a disk-based merge sort\n* checkpointing a long computation for recovery from crashes\n* holding files that a content-manager container fetches while a webserver\n  container serves the data", "zh": "`emptyDir` 的一些用途：\n\n* 缓存空间，例如基于磁盘的归并排序。\n* 为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行。\n* 在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件。"}
{"en": "The `emptyDir.medium` field controls where `emptyDir` volumes are stored. By\ndefault `emptyDir` volumes are stored on whatever medium that backs the node\nsuch as disk, SSD, or network storage, depending on your environment. If you set\nthe `emptyDir.medium` field to `\"Memory\"`, Kubernetes mounts a tmpfs (RAM-backed\nfilesystem) for you instead.  While tmpfs is very fast be aware that, unlike\ndisks, files you write count against the memory limit of the container that wrote them.", "zh": "`emptyDir.medium` 字段用来控制 `emptyDir` 卷的存储位置。\n默认情况下，`emptyDir` 卷存储在该节点所使用的介质上；\n此处的介质可以是磁盘、SSD 或网络存储，这取决于你的环境。\n你可以将 `emptyDir.medium` 字段设置为 `\"Memory\"`，\n以告诉 Kubernetes 为你挂载 tmpfs（基于 RAM 的文件系统）。\n虽然 tmpfs 速度非常快，但是要注意它与磁盘不同，\n并且你所写入的所有文件都会计入容器的内存消耗，受容器内存限制约束。"}
{"en": "A size limit can be specified for the default medium, which limits the capacity\nof the `emptyDir` volume. The storage is allocated from [node ephemeral\nstorage](/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage).\nIf that is filled up from another source (for example, log files or image\noverlays), the `emptyDir` may run out of capacity before this limit.", "zh": "你可以通过为默认介质指定大小限制，来限制 `emptyDir` 卷的存储容量。\n此存储是从[节点临时存储](/zh-cn/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage)中分配的。\n如果来自其他来源（如日志文件或镜像分层数据）的数据占满了存储，`emptyDir`\n可能会在达到此限制之前发生存储容量不足的问题。\n\n{{< note >}}"}
{"en": "You can specify a size for memory backed volumes, provided that the `SizeMemoryBackedVolumes`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nis enabled in your cluster (this has been beta, and active by default, since the Kubernetes 1.22 release).\nIf you don't specify a volume size, memory backed volumes are sized to node allocatable memory.", "zh": "你可以指定内存作为介质的卷的大小，前提是集群中启用了 `SizeMemoryBackedVolumes`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)\n（自 Kubernetes 1.22 发布以来，此特性一直处于 Beta 阶段，并且默认启用）。\n如果你未指定大小，内存作为介质的卷的大小根据节点可分配内存进行调整。\n{{< /note>}}\n\n{{< caution >}}"}
{"en": "Please check [here](/docs/concepts/configuration/manage-resources-containers/#memory-backed-emptydir)\nfor points to note in terms of resource management when using memory-backed `emptyDir`.", "zh": "使用内存作为介质的 `emptyDir` 卷时，\n请查阅[此处](/zh-cn/docs/concepts/configuration/manage-resources-containers/#memory-backed-emptydir)，\n了解有关资源管理方面的注意事项。\n{{< /caution >}}"}
{"en": "#### emptyDir configuration example", "zh": "#### emptyDir 配置示例\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n  - image: registry.k8s.io/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /cache\n      name: cache-volume\n  volumes:\n  - name: cache-volume\n    emptyDir:\n      sizeLimit: 500Mi\n```"}
{"en": "### fc (fibre channel) {#fc}\n\nAn `fc` volume type allows an existing fibre channel block storage volume\nto mount in a Pod. You can specify single or multiple target world wide names (WWNs)\nusing the parameter `targetWWNs` in your Volume configuration. If multiple WWNs are specified,\ntargetWWNs expect that those WWNs are from multi-path connections.", "zh": "### fc (光纤通道) {#fc}\n\n`fc` 卷类型允许将现有的光纤通道块存储卷挂载到 Pod 中。\n可以使用卷配置中的参数 `targetWWNs` 来指定单个或多个目标 WWN（World Wide Names）。\n如果指定了多个 WWN，targetWWNs 期望这些 WWN 来自多路径连接。\n\n{{< note >}}"}
{"en": "You must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the target WWNs\nbeforehand so that Kubernetes hosts can access them.", "zh": "你必须配置 FC SAN Zoning，以便预先向目标 WWN 分配和屏蔽这些 LUN（卷），这样\nKubernetes 主机才可以访问它们。\n{{< /note >}}"}
{"en": "See the [fibre channel example](https://github.com/kubernetes/examples/tree/master/staging/volumes/fibre_channel)\nfor more details.", "zh": "更多详情请参考 [FC 示例](https://github.com/kubernetes/examples/tree/master/staging/volumes/fibre_channel)。"}
{"en": "### gcePersistentDisk (deprecated) {#gcepersistentdisk}\n\nIn Kubernetes {{< skew currentVersion >}}, all operations for the in-tree `gcePersistentDisk` type\nare redirected to the `pd.csi.storage.gke.io` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} driver.", "zh": "### gcePersistentDisk（已弃用） {#gcepersistentdisk}\n\n在 Kubernetes {{< skew currentVersion >}} 中，所有针对树内 `gcePersistentDisk`\n类型的操作都会被重定向到 `pd.csi.storage.gke.io` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} 驱动。"}
{"en": "The `gcePersistentDisk` in-tree storage driver was deprecated in the Kubernetes v1.17 release\nand then removed entirely in the v1.28 release.", "zh": "`gcePersistentDisk` 源代码树内卷存储驱动在 Kubernetes v1.17 版本中被弃用，在 v1.28 版本中被完全移除。"}
{"en": "The Kubernetes project suggests that you use the [Google Compute Engine Persistent Disk CSI](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver) \nthird party storage driver instead.", "zh": "Kubernetes 项目建议你转为使用\n[Google Compute Engine Persistent Disk CSI](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver)\n第三方存储驱动插件。"}
{"en": "#### GCE CSI migration", "zh": "#### GCE CSI 迁移  {#gce-csi-migration}\n\n{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}"}
{"en": "The `CSIMigration` feature for GCE PD, when enabled, redirects all plugin operations\nfrom the existing in-tree plugin to the `pd.csi.storage.gke.io` Container\nStorage Interface (CSI) Driver. In order to use this feature, the [GCE PD CSI\nDriver](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver)\nmust be installed on the cluster.", "zh": "启用 GCE PD 的 `CSIMigration` 特性后，所有插件操作将从现有的树内插件重定向到\n`pd.csi.storage.gke.io` 容器存储接口（CSI）驱动程序。\n为了使用此特性，必须在集群中上安装\n[GCE PD CSI 驱动程序](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver)。"}
{"en": "### gitRepo (deprecated) {#gitrepo}", "zh": "### gitRepo (已弃用)    {#gitrepo}\n\n{{< warning >}}"}
{"en": "The `gitRepo` volume type is deprecated.\n\nTo provision a Pod that has a Git repository mounted, you can\nmount an\n[`emptyDir`](#emptydir) volume into an [init container](/docs/concepts/workloads/pods/init-containers/) that\nclones the repo using Git, then mount the\n[EmptyDir](#emptydir) into the Pod's container.", "zh": "`gitRepo` 卷类型已经被弃用。\n\n如果需要制备已挂载 Git 仓库的 Pod，你可以将\n[EmptyDir](#emptydir) 卷挂载到 [Init 容器](/zh-cn/docs/concepts/workloads/pods/init-containers/) 中，\n使用 Git 命令完成仓库的克隆操作，然后将 [EmptyDir](#emptydir) 卷挂载到 Pod 的容器中。\n\n---"}
{"en": "You can restrict the use of `gitRepo` volumes in your cluster using\n[policies](/docs/concepts/policy/) such as\n[ValidatingAdmissionPolicy](/docs/reference/access-authn-authz/validating-admission-policy/).\nYou can use the following Common Expression Language (CEL) expression as\npart of a policy to reject use of `gitRepo` volumes:\n`has(object.spec.volumes) || !object.spec.volumes.exists(v, has(v.gitRepo))`.", "zh": "你可以使用 [ValidatingAdmissionPolicy](/zh-cn/docs/reference/access-authn-authz/validating-admission-policy/)\n这类[策略](/zh-cn/docs/concepts/policy/)来限制在你的集群中使用 `gitRepo` 卷。\n你可以使用以下通用表达语言（CEL）表达式作为策略的一部分，以拒绝使用 `gitRepo` 卷：\n`has(object.spec.volumes) || !object.spec.volumes.exists(v, has(v.gitRepo))`。\n{{< /warning >}}"}
{"en": "A `gitRepo` volume is an example of a volume plugin. This plugin\nmounts an empty directory and clones a git repository into this directory\nfor your Pod to use.\n\nHere is an example of a `gitRepo` volume:", "zh": "`gitRepo` 卷是一个卷插件的例子。\n该查卷挂载一个空目录，并将一个 Git 代码仓库克隆到这个目录中供 Pod 使用。\n\n下面给出一个 `gitRepo` 卷的示例：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: server\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n    volumeMounts:\n    - mountPath: /mypath\n      name: git-volume\n  volumes:\n  - name: git-volume\n    gitRepo:\n      repository: \"git@somewhere:me/my-git-repository.git\"\n      revision: \"22f1d8406d464b0c0874075539c1f2e96c253775\"\n```"}
{"en": "### glusterfs (removed) {#glusterfs}", "zh": "### glusterfs（已移除）   {#glusterfs}"}
{"en": "Kubernetes {{< skew currentVersion >}} does not include a `glusterfs` volume type.\n\nThe GlusterFS in-tree storage driver was deprecated in the Kubernetes v1.25 release\nand then removed entirely in the v1.26 release.", "zh": "Kubernetes {{< skew currentVersion >}} 不包含 `glusterfs` 卷类型。\n\nGlusterFS 树内存储驱动程序在 Kubernetes v1.25 版本中被弃用，然后在 v1.26 版本中被完全移除。\n\n### hostPath {#hostpath}"}
{"en": "A `hostPath` volume mounts a file or directory from the host node's filesystem\ninto your Pod. This is not something that most Pods will need, but it offers a\npowerful escape hatch for some applications.", "zh": "`hostPath` 卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中。\n虽然这不是大多数 Pod 需要的，但是它为一些应用提供了强大的逃生舱。\n\n{{< warning >}}"}
{"en": "Using the `hostPath` volume type presents many security risks.\nIf you can avoid using a `hostPath` volume, you should. For example,\ndefine a [`local` PersistentVolume](#local), and use that instead.\n\nIf you are restricting access to specific directories on the node using\nadmission-time validation, that restriction is only effective when you\nadditionally require that any mounts of that `hostPath` volume are\n**read only**. If you allow a read-write mount of any host path by an\nuntrusted Pod, the containers in that Pod may be able to subvert the\nread-write host mount.", "zh": "使用 `hostPath` 类型的卷存在许多安全风险。如果可以，你应该尽量避免使用 `hostPath` 卷。\n例如，你可以改为定义并使用 [`local` PersistentVolume](#local)。\n\n如果你通过准入时的验证来限制对节点上特定目录的访问，这种限制只有在你额外要求所有\n`hostPath` 卷的挂载都是**只读**的情况下才有效。如果你允许不受信任的 Pod 以读写方式挂载任意主机路径，\n则该 Pod 中的容器可能会破坏可读写主机挂载卷的安全性。\n\n---"}
{"en": "Take care when using `hostPath` volumes, whether these are mounted as read-only\nor as read-write, because:", "zh": "无论 `hostPath` 卷是以只读还是读写方式挂载，使用时都需要小心，这是因为："}
{"en": "* Access to the host filesystem can expose privileged system credentials (such as for the kubelet) or privileged APIs\n  (such as the container runtime socket), that can be used for container escape or to attack other\n  parts of the cluster.\n* Pods with identical configuration (such as created from a PodTemplate) may\n  behave differently on different nodes due to different files on the nodes.\n* `hostPath` volume usage is not treated as ephemeral storage usage.\n  You need to monitor the disk usage by yourself because excessive `hostPath` disk\n  usage will lead to disk pressure on the node.", "zh": "* 访问主机文件系统可能会暴露特权系统凭证（例如 kubelet 的凭证）或特权 API（例如容器运行时套接字），\n  这些可以被用于容器逃逸或攻击集群的其他部分。\n* 具有相同配置的 Pod（例如基于 PodTemplate 创建的 Pod）可能会由于节点上的文件不同而在不同节点上表现出不同的行为。\n* `hostPath` 卷的用量不会被视为临时存储用量。\n  你需要自己监控磁盘使用情况，因为过多的 `hostPath` 磁盘使用量会导致节点上的磁盘压力。\n{{< /warning >}}"}
{"en": "Some uses for a `hostPath` are:\n\n* running a container that needs access to node-level system components\n  (such as a container that transfers system logs to a central location,\n  accessing those logs using a read-only mount of `/var/log`)\n* making a configuration file stored on the host system available read-only\n  to a {{< glossary_tooltip text=\"static pod\" term_id=\"static-pod\" >}};\n  unlike normal Pods, static Pods cannot access ConfigMaps", "zh": "`hostPath` 的一些用法有：\n\n* 运行一个需要访问节点级系统组件的容器\n  （例如一个将系统日志传输到集中位置的容器，使用只读挂载 `/var/log` 来访问这些日志）\n* 让存储在主机系统上的配置文件可以被{{< glossary_tooltip text=\"静态 Pod\" term_id=\"static-pod\" >}}\n  以只读方式访问；与普通 Pod 不同，静态 Pod 无法访问 ConfigMap。"}
{"en": "#### `hostPath` volume types\n\nIn addition to the required `path` property, you can optionally specify a\n`type` for a `hostPath` volume.\n\nThe available values for `type` are:", "zh": "#### `hostPath` 卷类型\n\n除了必需的 `path` 属性外，你还可以选择为 `hostPath` 卷指定 `type`。\n\n`type` 的可用值有："}
{"en": "| Value | Behavior |\n|:------|:---------|\n| `‌\"\"` | Empty string (default) is for backward compatibility, which means that no checks will be performed before mounting the `hostPath` volume. |\n| `DirectoryOrCreate` | If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet. |\n| `Directory` | A directory must exist at the given path |\n| `FileOrCreate` | If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet. |\n| `File` | A file must exist at the given path |\n| `Socket` | A UNIX socket must exist at the given path |\n| `CharDevice` | _(Linux nodes only)_ A character device must exist at the given path |\n| `BlockDevice` | _(Linux nodes only)_ A block device must exist at the given path |", "zh": "| 取值  | 行为     |\n|:------|:---------|\n| `‌\"\"` | 空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。 |\n| `DirectoryOrCreate` | 如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。 |\n| `Directory` | 在给定路径上必须存在的目录。|\n| `FileOrCreate` | 如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。|\n| `File` | 在给定路径上必须存在的文件。|\n| `Socket` | 在给定路径上必须存在的 UNIX 套接字。|\n| `CharDevice` | **（仅 Linux 节点）** 在给定路径上必须存在的字符设备。|\n| `BlockDevice` | **（仅 Linux 节点）** 在给定路径上必须存在的块设备。|\n\n{{< caution >}}"}
{"en": "The `FileOrCreate` mode does **not** create the parent directory of the file. If the parent directory\nof the mounted file does not exist, the pod fails to start. To ensure that this mode works,\nyou can try to mount directories and files separately, as shown in the\n[`FileOrCreate` example](#hostpath-fileorcreate-example) for `hostPath`.", "zh": "`FileOrCreate` 模式**不会**创建文件的父目录。如果挂载文件的父目录不存在，Pod 将启动失败。\n为了确保这种模式正常工作，你可以尝试分别挂载目录和文件，如\n`hostPath` 的 [`FileOrCreate` 示例](#hostpath-fileorcreate-example)所示。\n{{< /caution >}}"}
{"en": "Some files or directories created on the underlying hosts might only be\naccessible by root. You then either need to run your process as root in a\n[privileged container](/docs/tasks/configure-pod-container/security-context/)\nor modify the file permissions on the host to be able to read from\n(or write to) a `hostPath` volume.", "zh": "下层主机上创建的某些文件或目录只能由 root 用户访问。\n此时，你需要在[特权容器](/zh-cn/docs/tasks/configure-pod-container/security-context/)中以\nroot 身份运行进程，或者修改主机上的文件权限，以便能够从 `hostPath` 卷读取数据（或将数据写入到 `hostPath` 卷）。"}
{"en": "#### hostPath configuration example", "zh": "#### hostPath 配置示例"}
{"en": "Linux node\n# This manifest mounts /data/foo on the host as /foo inside the\n# single container that runs within the hostpath-example-linux Pod.\n#\n# The mount into the container is read-only.\n\n# mount /data/foo, but only if that directory already exists\n\n# directory location on host\n# this field is optional", "zh": "{{< tabs name=\"hostpath_examples\" >}}\n{{< tab name=\"Linux 节点\" codelang=\"yaml\" >}}\n---\n# 此清单将主机上的 /data/foo 挂载为 hostpath-example-linux Pod 中运行的单个容器内的 /foo\n#\n# 容器中的挂载是只读的\napiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath-example-linux\nspec:\n  os: { name: linux }\n  nodeSelector:\n    kubernetes.io/os: linux\n  containers:\n  - name: example-container\n    image: registry.k8s.io/test-webserver\n    volumeMounts:\n    - mountPath: /foo\n      name: example-volume\n      readOnly: true\n  volumes:\n  - name: example-volume\n    # 挂载 /data/foo，但仅当该目录已经存在时\n    hostPath:\n      path: /data/foo # 主机上的目录位置\n      type: Directory # 此字段可选\n{{< /tab >}}"}
{"en": "Windows node\n# This manifest mounts C:\\Data\\foo on the host as C:\\foo, inside the\n# single container that runs within the hostpath-example-windows Pod.\n#\n# The mount into the container is read-only.\n\n# mount C:\\Data\\foo from the host, but only if that directory already exists\n\n# directory location on host\n# this field is optional", "zh": "{{< tab name=\"Windows 节点\" codelang=\"yaml\" >}}\n---\n# 此清单将主机上的 C:\\Data\\foo 挂载为 hostpath-example-windows Pod 中运行的单个容器内的 C:\\foo\n#\n# 容器中的挂载是只读的\napiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath-example-windows\nspec:\n  os: { name: windows }\n  nodeSelector:\n    kubernetes.io/os: windows\n  containers:\n  - name: example-container\n    image: microsoft/windowsservercore:1709\n    volumeMounts:\n    - name: example-volume\n      mountPath: \"C:\\\\foo\"\n      readOnly: true\n  volumes:\n    # 从主机挂载 C:\\Data\\foo，但仅当该目录已经存在时\n  - name: example-volume\n    hostPath:\n      path: \"C:\\\\Data\\\\foo\" # 主机上的目录位置\n      type: Directory       # 此字段可选\n{{< /tab >}}\n{{< /tabs >}}"}
{"en": "#### hostPath FileOrCreate configuration example {#hostpath-fileorcreate-example}", "zh": "#### hostPath FileOrCreate 配置示例  {#hostpath-fileorcreate-example}"}
{"en": "The following manifest defines a Pod that mounts `/var/local/aaa`\ninside the single container in the Pod. If the node does not\nalready have a path `/var/local/aaa`, the kubelet creates\nit as a directory and then mounts it into the Pod.\n\nIf `/var/local/aaa` already exists but is not a directory,\nthe Pod fails. Additionally, the kubelet attempts to make\na file named `/var/local/aaa/1.txt` inside that directory\n(as seen from the host); if something already exists at\nthat path and isn't a regular file, the Pod fails.\n\nHere's the example manifest:", "zh": "以下清单定义了一个 Pod，将 `/var/local/aaa` 挂载到 Pod 中的单个容器内。\n如果节点上还没有路径 `/var/local/aaa`，kubelet 会创建这一目录，然后将其挂载到 Pod 中。\n\n如果 `/var/local/aaa` 已经存在但不是一个目录，Pod 会失败。\n此外，kubelet 还会尝试在该目录内创建一个名为 `/var/local/aaa/1.txt` 的文件（从主机的视角来看）；\n如果在该路径上已经存在某个东西且不是常规文件，则 Pod 会失败。\n\n以下是清单示例："}
{"en": "# Ensure the file directory is created.", "zh": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-webserver\nspec:\n  os: { name: linux }\n  nodeSelector:\n    kubernetes.io/os: linux\n  containers:\n  - name: test-webserver\n    image: registry.k8s.io/test-webserver:latest\n    volumeMounts:\n    - mountPath: /var/local/aaa\n      name: mydir\n    - mountPath: /var/local/aaa/1.txt\n      name: myfile\n  volumes:\n  - name: mydir\n    hostPath:\n      # 确保文件所在目录成功创建。\n      path: /var/local/aaa\n      type: DirectoryOrCreate\n  - name: myfile\n    hostPath:\n      path: /var/local/aaa/1.txt\n      type: FileOrCreate\n```\n\n### image\n\n{{< feature-state feature_gate_name=\"ImageVolume\" >}}"}
{"en": "An `image` volume source represents an OCI object (a container image or\nartifact) which is available on the kubelet's host machine.\n\nOne example to use the `image` volume source is:", "zh": "`image` 卷源代表一个在 kubelet 主机上可用的 OCI 对象（容器镜像或工件）。\n\n使用 `image` 卷源的一个例子是：\n\n{{% code_sample file=\"pods/image-volumes.yaml\" %}}"}
{"en": "The volume is resolved at pod startup depending on which `pullPolicy` value is\nprovided:\n\n`Always`\n: the kubelet always attempts to pull the reference. If the pull fails, the kubelet sets the Pod to `Failed`.", "zh": "此卷在 Pod 启动时基于提供的 `pullPolicy` 值进行解析：\n\n`Always`\n: kubelet 始终尝试拉取此引用。如果拉取失败，kubelet 会将 Pod 设置为 `Failed`。"}
{"en": "`Never`\n: the kubelet never pulls the reference and only uses a local image or artifact. The Pod becomes `Failed` if any layers of the image aren't already present locally, or if the manifest for that image isn't already cached.\n\n`IfNotPresent`\n: the kubelet pulls if the reference isn't already present on disk. The Pod becomes `Failed` if the reference isn't present and the pull fails.", "zh": "`Never`\n: kubelet 从不拉取此引用，仅使用本地镜像或工件。\n  如果本地没有任何镜像层存在，或者该镜像的清单未被缓存，则 Pod 会变为 `Failed`。\n\n`IfNotPresent`\n: 如果引用在磁盘上不存在，kubelet 会进行拉取。\n  如果引用不存在且拉取失败，则 Pod 会变为 `Failed`。"}
{"en": "The volume gets re-resolved if the pod gets deleted and recreated, which means\nthat new remote content will become available on pod recreation. A failure to\nresolve or pull the image during pod startup will block containers from starting\nand may add significant latency. Failures will be retried using normal volume\nbackoff and will be reported on the pod reason and message.", "zh": "如果 Pod 被删除并重新创建，此卷会被重新解析，这意味着在 Pod 重新创建时将可以访问新的远程内容。\n在 Pod 启动期间解析或拉取镜像失败将导致容器无法启动，并可能显著增加延迟。\n如果失败，将使用正常的卷回退进行重试，并输出 Pod 失败的原因和相关消息。"}
{"en": "The types of objects that may be mounted by this volume are defined by the\ncontainer runtime implementation on a host machine and at minimum must include\nall valid types supported by the container image field. The OCI object gets\nmounted in a single directory (`spec.containers[*].volumeMounts.mountPath`) by\nwill be mounted read-only. On Linux, the container runtime typically also mounts the \nvolume with file execution blocked (`noexec`).", "zh": "此卷可以挂载的对象类型由主机上的容器运行时实现负责定义，至少必须包含容器镜像字段所支持的所有有效类型。\nOCI 对象将以只读方式被挂载到单个目录（`spec.containers[*].volumeMounts.mountPath`）中。\n在 Linux 上，容器运行时通常还会挂载阻止文件执行（`noexec`）的卷。"}
{"en": "Beside that:\n- Sub path mounts for containers are not supported\n  (`spec.containers[*].volumeMounts.subpath`).\n- The field `spec.securityContext.fsGroupChangePolicy` has no effect on this\n  volume type.\n- The [`AlwaysPullImages` Admission Controller](/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages)\n  does also work for this volume source like for container images.", "zh": "此外：\n\n- 不支持容器使用子路径挂载（`spec.containers[*].volumeMounts.subpath`）。\n- `spec.securityContext.fsGroupChangePolicy` 字段对这种卷没有效果。\n- [`AlwaysPullImages` 准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages)也适用于此卷源，\n  就像适用于容器镜像一样。"}
{"en": "The following fields are available for the `image` type:", "zh": "`image` 类型可用的字段如下："}
{"en": "`reference`\n: Artifact reference to be used. For example, you could specify\n`registry.k8s.io/conformance:v{{< skew currentPatchVersion >}}` to load the\nfiles from the Kubernetes conformance test image. Behaves in the same way as\n`pod.spec.containers[*].image`. Pull secrets will be assembled in the same way\nas for the container image by looking up node credentials, service account image\npull secrets, and pod spec image pull secrets. This field is optional to allow\nhigher level config management to default or override container images in\nworkload controllers like Deployments and StatefulSets.\n[More info about container images](/docs/concepts/containers/images)", "zh": "`reference`\n: 要使用的工件引用。例如，你可以指定 `registry.k8s.io/conformance:v{{< skew currentPatchVersion >}}`\n  来加载 Kubernetes 合规性测试镜像中的文件。其行为与 `pod.spec.containers[*].image` 相同。\n  拉取 Secret 的组装方式与容器镜像所用的方式相同，即通过查找节点凭据、服务账户镜像拉取 Secret\n  和 Pod 规约镜像拉取 Secret。此字段是可选的，允许更高层次的配置管理在 Deployment 和\n  StatefulSet 这类工作负载控制器中默认使用或重载容器镜像。\n  参阅[容器镜像更多细节](/zh-cn/docs/concepts/containers/images)。"}
{"en": "`pullPolicy`\n: Policy for pulling OCI objects. Possible values are: `Always`, `Never` or\n`IfNotPresent`. Defaults to `Always` if `:latest` tag is specified, or\n`IfNotPresent` otherwise.\n\nSee the [_Use an Image Volume With a Pod_](/docs/tasks/configure-pod-container/image-volumes)\nexample for more details on how to use the volume source.", "zh": "`pullPolicy`\n: 拉取 OCI 对象的策略。可能的值为：`Always`、`Never` 或 `IfNotPresent`。\n  如果指定了 `:latest` 标记，则默认为 `Always`，否则默认为 `IfNotPresent`。\n\n有关如何使用卷源的更多细节，\n请参见 [Pod 使用镜像卷](/zh-cn/docs/tasks/configure-pod-container/image-volumes)示例。\n\n### iscsi"}
{"en": "An `iscsi` volume allows an existing iSCSI (SCSI over IP) volume to be mounted\ninto your Pod. Unlike `emptyDir`, which is erased when a Pod is removed, the\ncontents of an `iscsi` volume are preserved and the volume is merely\nunmounted. This means that an iscsi volume can be pre-populated with data, and\nthat data can be shared between pods.", "zh": "`iscsi` 卷能将 iSCSI (基于 IP 的 SCSI) 卷挂载到你的 Pod 中。\n不像 `emptyDir` 那样会在删除 Pod 的同时也会被删除，`iscsi`\n卷的内容在删除 Pod 时会被保留，卷只是被卸载。\n这意味着 `iscsi` 卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。\n\n{{< note >}}"}
{"en": "You must have your own iSCSI server running with the volume created before you can use it.", "zh": "在使用 iSCSI 卷之前，你必须拥有自己的 iSCSI 服务器，并在上面创建卷。\n{{< /note >}}"}
{"en": "A feature of iSCSI is that it can be mounted as read-only by multiple consumers\nsimultaneously. This means that you can pre-populate a volume with your dataset\nand then serve it in parallel from as many Pods as you need. Unfortunately,\niSCSI volumes can only be mounted by a single consumer in read-write mode.\nSimultaneous writers are not allowed.", "zh": "iSCSI 的一个特点是它可以同时被多个用户以只读方式挂载。\n这意味着你可以用数据集预先填充卷，然后根据需要在尽可能多的 Pod 上使用它。\n不幸的是，iSCSI 卷只能由单个使用者以读写模式挂载。不允许同时写入。"}
{"en": "See the [iSCSI example](https://github.com/kubernetes/examples/tree/master/volumes/iscsi) for more details.", "zh": "更多详情请参考 [iSCSI 示例](https://github.com/kubernetes/examples/tree/master/volumes/iscsi)。"}
{"en": "### local\n\nA `local` volume represents a mounted local storage device such as a disk,\npartition or directory.\n\nLocal volumes can only be used as a statically created PersistentVolume. Dynamic\nprovisioning is not supported.", "zh": "### local\n\n`local` 卷所代表的是某个被挂载的本地存储设备，例如磁盘、分区或者目录。\n\n`local` 卷只能用作静态创建的持久卷。不支持动态配置。"}
{"en": "Compared to `hostPath` volumes, `local` volumes are used in a durable and\nportable manner without manually scheduling pods to nodes. The system is aware\nof the volume's node constraints by looking at the node affinity on the PersistentVolume.", "zh": "与 `hostPath` 卷相比，`local` 卷能够以持久和可移植的方式使用，而无需手动将 Pod\n调度到节点。系统通过查看 PersistentVolume 的节点亲和性配置，就能了解卷的节点约束。"}
{"en": "However, `local` volumes are subject to the availability of the underlying\nnode and are not suitable for all applications. If a node becomes unhealthy,\nthen the `local` volume becomes inaccessible by the pod. The pod using this volume\nis unable to run. Applications using `local` volumes must be able to tolerate this\nreduced availability, as well as potential data loss, depending on the\ndurability characteristics of the underlying disk.\n\nThe following example shows a PersistentVolume using a `local` volume and\n`nodeAffinity`:", "zh": "然而，`local` 卷仍然取决于底层节点的可用性，并不适合所有应用程序。\n如果节点变得不健康，那么 `local` 卷也将变得不可被 Pod 访问。使用它的 Pod 将不能运行。\n使用 `local` 卷的应用程序必须能够容忍这种可用性的降低，以及因底层磁盘的耐用性特征而带来的潜在的数据丢失风险。\n\n下面是一个使用 `local` 卷和 `nodeAffinity` 的持久卷示例：\n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: example-pv\nspec:\n  capacity:\n    storage: 100Gi\n  volumeMode: Filesystem\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: local-storage\n  local:\n    path: /mnt/disks/ssd1\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - example-node\n```"}
{"en": "You must set a PersistentVolume `nodeAffinity` when using `local` volumes.\nThe Kubernetes scheduler uses the PersistentVolume `nodeAffinity` to schedule\nthese Pods to the correct node.", "zh": "使用 `local` 卷时，你需要设置 PersistentVolume 对象的 `nodeAffinity` 字段。\nKubernetes 调度器使用 PersistentVolume 的 `nodeAffinity` 信息来将使用 `local`\n卷的 Pod 调度到正确的节点。"}
{"en": "PersistentVolume `volumeMode` can be set to \"Block\" (instead of the default\nvalue \"Filesystem\") to expose the local volume as a raw block device.", "zh": "PersistentVolume 对象的 `volumeMode` 字段可被设置为 \"Block\"\n（而不是默认值 \"Filesystem\"），以将 `local` 卷作为原始块设备暴露出来。"}
{"en": "When using local volumes, it is recommended to create a StorageClass with\n`volumeBindingMode` set to `WaitForFirstConsumer`. For more details, see the\nlocal [StorageClass](/docs/concepts/storage/storage-classes/#local) example.\nDelaying volume binding ensures that the PersistentVolumeClaim binding decision\nwill also be evaluated with any other node constraints the Pod may have,\nsuch as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity.", "zh": "使用 `local` 卷时，建议创建一个 StorageClass 并将其 `volumeBindingMode` 设置为\n`WaitForFirstConsumer`。要了解更多详细信息，请参考\n[local StorageClass 示例](/zh-cn/docs/concepts/storage/storage-classes/#local)。\n延迟卷绑定的操作可以确保 Kubernetes 在为 PersistentVolumeClaim 作出绑定决策时，会评估\nPod 可能具有的其他节点约束，例如：如节点资源需求、节点选择器、Pod 亲和性和 Pod 反亲和性。"}
{"en": "An external static provisioner can be run separately for improved management of\nthe local volume lifecycle. Note that this provisioner does not support dynamic\nprovisioning yet. For an example on how to run an external local provisioner,\nsee the [local volume provisioner user\nguide](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner).", "zh": "你可以在 Kubernetes 之外单独运行静态驱动以改进对 local 卷的生命周期管理。\n请注意，此驱动尚不支持动态配置。\n有关如何运行外部 `local` 卷驱动，请参考\n[local 卷驱动用户指南](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner)。\n\n{{< note >}}"}
{"en": "The local PersistentVolume requires manual cleanup and deletion by the\nuser if the external static provisioner is not used to manage the volume\nlifecycle.", "zh": "如果不使用外部静态驱动来管理卷的生命周期，用户需要手动清理和删除 local 类型的持久卷。\n{{< /note >}}\n\n### nfs"}
{"en": "An `nfs` volume allows an existing NFS (Network File System) share to be\nmounted into a Pod. Unlike `emptyDir`, which is erased when a Pod is\nremoved, the contents of an `nfs` volume are preserved and the volume is merely\nunmounted. This means that an NFS volume can be pre-populated with data, and\nthat data can be shared between pods. NFS can be mounted by multiple\nwriters simultaneously.", "zh": "`nfs` 卷能将 NFS (网络文件系统) 挂载到你的 Pod 中。\n不像 `emptyDir` 那样会在删除 Pod 的同时也会被删除，`nfs` 卷的内容在删除 Pod\n时会被保存，卷只是被卸载。\n这意味着 `nfs` 卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n  - image: registry.k8s.io/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /my-nfs-data\n      name: test-volume\n  volumes:\n  - name: test-volume\n    nfs:\n      server: my-nfs-server.example.com\n      path: /my-nfs-volume\n      readOnly: true\n```\n\n{{< note >}}"}
{"en": "You must have your own NFS server running with the share exported before you can use it.\n\nAlso note that you can't specify NFS mount options in a Pod spec. You can either set mount options server-side or\nuse [/etc/nfsmount.conf](https://man7.org/linux/man-pages/man5/nfsmount.conf.5.html).\nYou can also mount NFS volumes via PersistentVolumes which do allow you to set mount options.", "zh": "在使用 NFS 卷之前，你必须运行自己的 NFS 服务器并将目标 share 导出备用。\n\n还需要注意，不能在 Pod spec 中指定 NFS 挂载可选项。\n可以选择设置服务端的挂载可选项，或者使用\n[/etc/nfsmount.conf](https://man7.org/linux/man-pages/man5/nfsmount.conf.5.html)。\n此外，还可以通过允许设置挂载可选项的持久卷挂载 NFS 卷。\n{{< /note >}}"}
{"en": "See the [NFS example](https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs)\nfor an example of mounting NFS volumes with PersistentVolumes.", "zh": "如需了解用持久卷挂载 NFS 卷的示例，请参考 [NFS 示例](https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs)。\n\n### persistentVolumeClaim {#persistentvolumeclaim}"}
{"en": "A `persistentVolumeClaim` volume is used to mount a\n[PersistentVolume](/docs/concepts/storage/persistent-volumes/) into a Pod. PersistentVolumeClaims\nare a way for users to \"claim\" durable storage (such as an iSCSI volume)\nwithout knowing the details of the particular cloud environment.", "zh": "`persistentVolumeClaim` 卷用来将[持久卷](/zh-cn/docs/concepts/storage/persistent-volumes/)（PersistentVolume）挂载到 Pod 中。\n持久卷申领（PersistentVolumeClaim）是用户在不知道特定云环境细节的情况下“申领”持久存储（例如 iSCSI 卷）的一种方法。"}
{"en": "See the information about [PersistentVolumes](/docs/concepts/storage/persistent-volumes/) for more\ndetails.", "zh": "更多详情请参考[持久卷](/zh-cn/docs/concepts/storage/persistent-volumes/)。"}
{"en": "### portworxVolume (deprecated) {#portworxvolume}", "zh": "### portworxVolume（已弃用） {#portworxvolume}\n\n{{< feature-state for_k8s_version=\"v1.25\" state=\"deprecated\" >}}"}
{"en": "A `portworxVolume` is an elastic block storage layer that runs hyperconverged with\nKubernetes. [Portworx](https://portworx.com/use-case/kubernetes-storage/) fingerprints storage\nin a server, tiers based on capabilities, and aggregates capacity across multiple servers.\nPortworx runs in-guest in virtual machines or on bare metal Linux nodes.", "zh": "`portworxVolume` 是一个可伸缩的块存储层，能够以超融合（hyperconverged）的方式与 Kubernetes 一起运行。\n[Portworx](https://portworx.com/use-case/kubernetes-storage/)\n支持对服务器上存储的指纹处理、基于存储能力进行分层以及跨多个服务器整合存储容量。\nPortworx 可以以 in-guest 方式在虚拟机中运行，也可以在裸金属 Linux 节点上运行。"}
{"en": "A `portworxVolume` can be dynamically created through Kubernetes or it can also\nbe pre-provisioned and referenced inside a Pod.\nHere is an example Pod referencing a pre-provisioned Portworx volume:", "zh": "`portworxVolume` 类型的卷可以通过 Kubernetes 动态创建，也可以预先配备并在 Pod 内引用。\n下面是一个引用预先配备的 Portworx 卷的示例 Pod："}
{"en": "# This Portworx volume must already exist.", "zh": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-portworx-volume-pod\nspec:\n  containers:\n  - image: registry.k8s.io/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /mnt\n      name: pxvol\n  volumes:\n  - name: pxvol\n    # 此 Portworx 卷必须已经存在\n    portworxVolume:\n      volumeID: \"pxvol\"\n      fsType: \"<fs-type>\"\n```\n\n{{< note >}}"}
{"en": "Make sure you have an existing PortworxVolume with name `pxvol`\nbefore using it in the Pod.", "zh": "在 Pod 中使用 portworxVolume 之前，你要确保有一个名为 `pxvol` 的 PortworxVolume 存在。\n{{< /note >}}"}
{"en": "For more details, see the [Portworx volume](https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx/README.md) examples.", "zh": "更多详情可以参考 [Portworx 卷](https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx/README.md)。"}
{"en": "#### Portworx CSI migration", "zh": "#### Portworx CSI 迁移\n\n{{< feature-state for_k8s_version=\"v1.25\" state=\"beta\" >}}"}
{"en": "By default, Kubernetes {{% skew currentVersion %}} attempts to migrate legacy\nPortworx volumes to use CSI. (CSI migration for Portworx has been available since\nKubernetes v1.23, but was only turned on by default since the v1.31 release).\nIf you want to disable automatic migration, you can set the `CSIMigrationPortworx`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nto `false`; you need to make that change for the kube-controller-manager **and** on\nevery relevant kubelet.\n\nIt redirects all plugin operations from the existing in-tree plugin to the\n`pxd.portworx.com` Container Storage Interface (CSI) Driver.\n[Portworx CSI Driver](https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi)\nmust be installed on the cluster.", "zh": "默认情况下，Kubernetes {{% skew currentVersion %}} 尝试将传统的 Portworx 卷迁移为使用 CSI。\n（Portworx 的 CSI 迁移自 Kubernetes v1.23 版本以来一直可用，但从 v1.31 版本开始才默认启用）。\n如果你想禁用自动迁移，可以将 `CSIMigrationPortworx`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/) 设置为 `false`；\n你需要在 kube-controller-manager **和** 每个相关的 kubelet 上进行此更改。\n\n它将所有插件操作不再指向树内插件（In-Tree Plugin），转而指向\n`pxd.portworx.com` 容器存储接口（Container Storage Interface，CSI）驱动。\n[Portworx CSI 驱动程序](https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/storage-operations/csi)必须安装在集群上。"}
{"en": "### projected\n\nA projected volume maps several existing volume sources into the same\ndirectory. For more details, see [projected volumes](/docs/concepts/storage/projected-volumes/).", "zh": "### 投射（projected）   {#projected}\n\n投射卷能将若干现有的卷来源映射到同一目录上。更多详情请参考[投射卷](/zh-cn/docs/concepts/storage/projected-volumes/)。"}
{"en": "### rbd (removed) {#rbd}", "zh": "### rbd（已移除）  {#rbd}"}
{"en": "Kubernetes {{< skew currentVersion >}} does not include a `rbd` volume type.\n\nThe [Rados Block Device](https://docs.ceph.com/en/latest/rbd/) (RBD) in-tree storage driver and its csi migration support were deprecated in the Kubernetes v1.28 release\nand then removed entirely in the v1.31 release.", "zh": "Kubernetes {{< skew currentVersion >}} 不包括 `rbd` 卷类型。\n\n[Rados 块设备](https://docs.ceph.com/en/latest/rbd/)（RBD）\n树内存储驱动及其 CSI 迁移支持在 Kubernetes v1.28 版本中被弃用，并在 v1.31 版本中被完全移除。\n\n### secret"}
{"en": "A `secret` volume is used to pass sensitive information, such as passwords, to\nPods. You can store secrets in the Kubernetes API and mount them as files for\nuse by pods without coupling to Kubernetes directly. `secret` volumes are\nbacked by tmpfs (a RAM-backed filesystem) so they are never written to\nnon-volatile storage.", "zh": "`secret` 卷用来给 Pod 传递敏感信息，例如密码。你可以将 Secret 存储在 Kubernetes\nAPI 服务器上，然后以文件的形式挂载到 Pod 中，无需直接与 Kubernetes 耦合。\n`secret` 卷由 tmpfs（基于 RAM 的文件系统）提供存储，因此它们永远不会被写入非易失性（持久化的）存储器。\n\n{{< note >}}"}
{"en": "* You must create a Secret in the Kubernetes API before you can use it.\n\n* A Secret is always mounted as `readOnly`.\n\n* A container using a Secret as a [`subPath`](#using-subpath) volume mount will not\nreceive Secret updates.", "zh": "* 使用前你必须在 Kubernetes API 中创建 Secret。\n* Secret 总是以 `readOnly` 的模式挂载。\n* 容器以 [`subPath`](#using-subpath) 卷挂载方式使用 Secret 时，将无法接收 Secret 的更新。\n{{< /note >}}"}
{"en": "For more details, see [Configuring Secrets](/docs/concepts/configuration/secret/).", "zh": "更多详情请参考[配置 Secrets](/zh-cn/docs/concepts/configuration/secret/)。"}
{"en": "### vsphereVolume (deprecated) {#vspherevolume}", "zh": "### vsphereVolume（已弃用） {#vspherevolume}\n\n{{< note >}}"}
{"en": "The Kubernetes project recommends using the [vSphere CSI](https://github.com/kubernetes-sigs/vsphere-csi-driver)\nout-of-tree storage driver instead.", "zh": "Kubernetes 项目建议转为使用 [vSphere CSI](https://github.com/kubernetes-sigs/vsphere-csi-driver)\n树外存储驱动。\n{{< /note >}}"}
{"en": "A `vsphereVolume` is used to mount a vSphere VMDK Volume into your Pod.  The contents\nof a volume are preserved when it is unmounted. It supports both VMFS and VSAN datastore.", "zh": "`vsphereVolume` 用来将 vSphere VMDK 卷挂载到你的 Pod 中。\n在卸载卷时，卷的内容会被保留。\nvSphereVolume 卷类型支持 VMFS 和 VSAN 数据仓库。"}
{"en": "For more information, see the [vSphere volume](https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere) examples.", "zh": "进一步信息可参考\n[vSphere 卷](https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere)。"}
{"en": "#### vSphere CSI migration {#vsphere-csi-migration}", "zh": "#### vSphere CSI 迁移  {#vsphere-csi-migration}\n\n{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}"}
{"en": "In Kubernetes {{< skew currentVersion >}}, all operations for the in-tree `vsphereVolume` type\nare redirected to the `csi.vsphere.vmware.com` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} driver.", "zh": "在 Kubernetes {{< skew currentVersion >}} 中，对树内 `vsphereVolume`\n类的所有操作都会被重定向至 `csi.vsphere.vmware.com` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} 驱动程序。"}
{"en": "[vSphere CSI driver](https://github.com/kubernetes-sigs/vsphere-csi-driver)\nmust be installed on the cluster. You can find additional advice on how to migrate in-tree `vsphereVolume` in VMware's documentation page\n[Migrating In-Tree vSphere Volumes to vSphere Container Storage Plug-in](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/2.0/vmware-vsphere-csp-getting-started/GUID-968D421F-D464-4E22-8127-6CB9FF54423F.html).\nIf vSphere CSI Driver is not installed volume operations can not be performed on the PV created with the in-tree `vsphereVolume` type.", "zh": "[vSphere CSI 驱动](https://github.com/kubernetes-sigs/vsphere-csi-driver)必须安装到集群上。\n你可以在 VMware 的文档页面[迁移树内 vSphere 卷插件到 vSphere 容器存储插件](https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/2.0/vmware-vsphere-csp-getting-started/GUID-968D421F-D464-4E22-8127-6CB9FF54423F.html)\n中找到有关如何迁移树内 `vsphereVolume` 的其他建议。\n如果未安装 vSphere CSI 驱动程序，则无法对由树内 `vsphereVolume` 类型创建的 PV 执行卷操作。"}
{"en": "You must run vSphere 7.0u2 or later in order to migrate to the vSphere CSI driver.\n\nIf you are running a version of Kubernetes other than v{{< skew currentVersion >}}, consult\nthe documentation for that version of Kubernetes.", "zh": "你必须运行 vSphere 7.0u2 或更高版本才能迁移到 vSphere CSI 驱动程序。\n\n如果你正在运行 Kubernetes v{{< skew currentVersion >}}，请查阅该 Kubernetes 版本的文档。\n\n{{< note >}}"}
{"en": "The following StorageClass parameters from the built-in `vsphereVolume` plugin are not supported by the vSphere CSI driver:", "zh": "vSphere CSI 驱动不支持内置 `vsphereVolume` 的以下 StorageClass 参数：\n\n* `diskformat`\n* `hostfailurestotolerate`\n* `forceprovisioning`\n* `cachereservation`\n* `diskstripes`\n* `objectspacereservation`\n* `iopslimit`"}
{"en": "Existing volumes created using these parameters will be migrated to the vSphere CSI driver,\nbut new volumes created by the vSphere CSI driver will not be honoring these parameters.", "zh": "使用这些参数创建的现有卷将被迁移到 vSphere CSI 驱动，不过使用 vSphere\nCSI 驱动所创建的新卷都不会理会这些参数。\n\n{{< /note >}}"}
{"en": "#### vSphere CSI migration complete {#vsphere-csi-migration-complete}", "zh": "#### vSphere CSI 迁移完成   {#vsphere-csi-migration-complete}\n\n{{< feature-state for_k8s_version=\"v1.19\" state=\"beta\" >}}"}
{"en": "To turn off the `vsphereVolume` plugin from being loaded by the controller manager and the kubelet, you need to set `InTreePluginvSphereUnregister` feature flag to `true`. You must install a `csi.vsphere.vmware.com` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} driver on all worker nodes.", "zh": "为了避免控制器管理器和 kubelet 加载 `vsphereVolume` 插件，你需要将\n`InTreePluginvSphereUnregister` 特性设置为 `true`。你还必须在所有工作节点上安装\n`csi.vsphere.vmware.com` {{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} 驱动。"}
{"en": "## Using subPath {#using-subpath}\n\nSometimes, it is useful to share one volume for multiple uses in a single pod.\nThe `volumeMounts[*].subPath` property specifies a sub-path inside the referenced volume\ninstead of its root.", "zh": "## 使用 subPath  {#using-subpath}\n\n有时，在单个 Pod 中共享卷以供多方使用是很有用的。\n`volumeMounts[*].subPath` 属性可用于指定所引用的卷内的子路径，而不是其根路径。"}
{"en": "The following example shows how to configure a Pod with a LAMP stack (Linux Apache MySQL PHP)\nusing a single, shared volume. This sample `subPath` configuration is not recommended\nfor production use.\n\nThe PHP application's code and assets map to the volume's `html` folder and\nthe MySQL database is stored in the volume's `mysql` folder. For example:", "zh": "下面例子展示了如何配置某包含 LAMP 堆栈（Linux Apache MySQL PHP）的 Pod 使用同一共享卷。\n此示例中的 `subPath` 配置不建议在生产环境中使用。\nPHP 应用的代码和相关数据映射到卷的 `html` 文件夹，MySQL 数据库存储在卷的 `mysql` 文件夹中：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-lamp-site\nspec:\n    containers:\n    - name: mysql\n      image: mysql\n      env:\n      - name: MYSQL_ROOT_PASSWORD\n        value: \"rootpasswd\"\n      volumeMounts:\n      - mountPath: /var/lib/mysql\n        name: site-data\n        subPath: mysql\n    - name: php\n      image: php:7.0-apache\n      volumeMounts:\n      - mountPath: /var/www/html\n        name: site-data\n        subPath: html\n    volumes:\n    - name: site-data\n      persistentVolumeClaim:\n        claimName: my-lamp-site-data\n```"}
{"en": "### Using subPath with expanded environment variables {#using-subpath-expanded-environment}", "zh": "### 使用带有扩展环境变量的 subPath  {#using-subpath-expanded-environment}\n\n{{< feature-state for_k8s_version=\"v1.17\" state=\"stable\" >}}"}
{"en": "Use the `subPathExpr` field to construct `subPath` directory names from\ndownward API environment variables.\nThe `subPath` and `subPathExpr` properties are mutually exclusive.", "zh": "使用 `subPathExpr` 字段可以基于 downward API 环境变量来构造 `subPath` 目录名。\n`subPath` 和 `subPathExpr` 属性是互斥的。"}
{"en": "In this example, a `Pod` uses `subPathExpr` to create a directory `pod1` within\nthe `hostPath` volume `/var/log/pods`.\nThe `hostPath` volume takes the `Pod` name from the `downwardAPI`.\nThe host directory `/var/log/pods/pod1` is mounted at `/logs` in the container.", "zh": "在这个示例中，`Pod` 使用 `subPathExpr` 来 `hostPath` 卷 `/var/log/pods` 中创建目录 `pod1`。\n`hostPath` 卷采用来自 `downwardAPI` 的 Pod 名称生成目录名。\n宿主机目录 `/var/log/pods/pod1` 被挂载到容器的 `/logs` 中。"}
{"en": "# The variable expansion uses round brackets (not curly brackets).", "zh": "```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\nspec:\n  containers:\n  - name: container1\n    env:\n    - name: POD_NAME\n      valueFrom:\n        fieldRef:\n          apiVersion: v1\n          fieldPath: metadata.name\n    image: busybox:1.28\n    command: [ \"sh\", \"-c\", \"while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt\" ]\n    volumeMounts:\n    - name: workdir1\n      mountPath: /logs\n      # 包裹变量名的是小括号，而不是大括号\n      subPathExpr: $(POD_NAME)\n  restartPolicy: Never\n  volumes:\n  - name: workdir1\n    hostPath:\n      path: /var/log/pods\n```"}
{"en": "## Resources\n\nThe storage media (such as Disk or SSD) of an `emptyDir` volume is determined by the\nmedium of the filesystem holding the kubelet root dir (typically\n`/var/lib/kubelet`). There is no limit on how much space an `emptyDir` or\n`hostPath` volume can consume, and no isolation between containers or between\npods.", "zh": "## 资源   {#resources}\n\n`emptyDir` 卷的存储介质（例如磁盘、SSD 等）是由保存 kubelet\n数据的根目录（通常是 `/var/lib/kubelet`）的文件系统的介质确定。\nKubernetes 对 `emptyDir` 卷或者 `hostPath` 卷可以消耗的空间没有限制，容器之间或 Pod 之间也没有隔离。"}
{"en": "To learn about requesting space using a resource specification, see\n[how to manage resources](/docs/concepts/configuration/manage-resources-containers/).", "zh": "要了解如何使用资源规约来请求空间，\n可参考[如何管理资源](/zh-cn/docs/concepts/configuration/manage-resources-containers/)。"}
{"en": "## Out-of-tree volume plugins\n\nThe out-of-tree volume plugins include\n{{< glossary_tooltip text=\"Container Storage Interface\" term_id=\"csi\" >}} (CSI), and also FlexVolume (which is deprecated). These plugins enable storage vendors to create custom storage plugins\nwithout adding their plugin source code to the Kubernetes repository.", "zh": "## 树外（Out-of-Tree）卷插件    {#out-of-tree-volume-plugins}\n\nOut-of-Tree 卷插件包括{{< glossary_tooltip text=\"容器存储接口（CSI）\" term_id=\"csi\" >}}和\nFlexVolume（已弃用）。它们使存储供应商能够创建自定义存储插件，而无需将插件源码添加到\nKubernetes 代码仓库。"}
{"en": "Previously, all volume plugins were \"in-tree\". The \"in-tree\" plugins were built, linked, compiled,\nand shipped with the core Kubernetes binaries. This meant that adding a new storage system to\nKubernetes (a volume plugin) required checking code into the core Kubernetes code repository.", "zh": "以前，所有卷插件（如上面列出的卷类型）都是“树内（In-Tree）”的。\n“树内”插件是与 Kubernetes 的核心组件一同构建、链接、编译和交付的。\n这意味着向 Kubernetes 添加新的存储系统（卷插件）需要将代码合并到 Kubernetes 核心代码库中。"}
{"en": "Both CSI and FlexVolume allow volume plugins to be developed independent of\nthe Kubernetes code base, and deployed (installed) on Kubernetes clusters as\nextensions.\n\nFor storage vendors looking to create an out-of-tree volume plugin, please refer\nto the [volume plugin FAQ](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md).", "zh": "CSI 和 FlexVolume 都允许独立于 Kubernetes 代码库开发卷插件，并作为扩展部署（安装）在 Kubernetes 集群上。\n\n对于希望创建树外（Out-Of-Tree）卷插件的存储供应商，\n请参考[卷插件常见问题](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md)。\n\n### CSI"}
{"en": "[Container Storage Interface](https://github.com/container-storage-interface/spec/blob/master/spec.md)\n(CSI) defines a standard interface for container orchestration systems (like\nKubernetes) to expose arbitrary storage systems to their container workloads.", "zh": "[容器存储接口](https://github.com/container-storage-interface/spec/blob/master/spec.md) (CSI)\n为容器编排系统（如 Kubernetes）定义标准接口，以将任意存储系统暴露给它们的容器工作负载。"}
{"en": "Please read the [CSI design proposal](https://git.k8s.io/design-proposals-archive/storage/container-storage-interface.md) for more information.\n\nCSI support was introduced as alpha in Kubernetes v1.9, moved to beta in\nKubernetes v1.10, and is GA in Kubernetes v1.13.", "zh": "更多详情请阅读 [CSI 设计方案](https://git.k8s.io/design-proposals-archive/storage/container-storage-interface.md)。\n\n{{< note >}}"}
{"en": "Support for CSI spec versions 0.2 and 0.3 are deprecated in Kubernetes\nv1.13 and will be removed in a future release.", "zh": "Kubernetes v1.13 废弃了对 CSI 规范版本 0.2 和 0.3 的支持，并将在以后的版本中删除。\n{{< /note >}}\n\n{{< note >}}"}
{"en": "CSI drivers may not be compatible across all Kubernetes releases.\nPlease check the specific CSI driver's documentation for supported\ndeployments steps for each Kubernetes release and a compatibility matrix.", "zh": "CSI 驱动可能并非兼容所有的 Kubernetes 版本。\n请查看特定 CSI 驱动的文档，以了解各个 Kubernetes 版本所支持的部署步骤以及兼容性列表。\n{{< /note >}}"}
{"en": "Once a CSI compatible volume driver is deployed on a Kubernetes cluster, users\nmay use the `csi` volume type to attach or mount the volumes exposed by the\nCSI driver.\n\nA `csi` volume can be used in a Pod in three different ways:\n\n* through a reference to a [PersistentVolumeClaim](#persistentvolumeclaim)\n* with a [generic ephemeral volume](/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes)\n* with a [CSI ephemeral volume](/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes) if the driver supports that", "zh": "一旦在 Kubernetes 集群上部署了 CSI 兼容卷驱动程序，用户就可以使用\n`csi` 卷类型来挂接、挂载 CSI 驱动所提供的卷。\n\n`csi` 卷可以在 Pod 中以三种方式使用：\n\n* 通过 [PersistentVolumeClaim](#persistentvolumeclaim) 对象引用\n* 使用[一般性的临时卷](/zh-cn/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes)\n* 使用 [CSI 临时卷](/zh-cn/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes)，\n  前提是驱动支持这种用法"}
{"en": "The following fields are available to storage administrators to configure a CSI\npersistent volume:", "zh": "存储管理员可以使用以下字段来配置 CSI 持久卷："}
{"en": "* `driver`: A string value that specifies the name of the volume driver to use.\n  This value must correspond to the value returned in the `GetPluginInfoResponse`\n  by the CSI driver as defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo).\n  It is used by Kubernetes to identify which CSI driver to call out to, and by\n  CSI driver components to identify which PV objects belong to the CSI driver.", "zh": "* `driver`：指定要使用的卷驱动名称的字符串值。\n  这个值必须与 CSI 驱动程序在 `GetPluginInfoResponse` 中返回的值相对应；该接口定义在\n  [CSI 规范](https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo)中。\n  Kubernetes 使用所给的值来标识要调用的 CSI 驱动程序；CSI\n  驱动程序也使用该值来辨识哪些 PV 对象属于该 CSI 驱动程序。"}
{"en": "* `volumeHandle`: A string value that uniquely identifies the volume. This value\n  must correspond to the value returned in the `volume.id` field of the\n  `CreateVolumeResponse` by the CSI driver as defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume).\n  The value is passed as `volume_id` on all calls to the CSI volume driver when\n  referencing the volume.", "zh": "* `volumeHandle`：唯一标识卷的字符串值。\n  该值必须与 CSI 驱动在 `CreateVolumeResponse` 的 `volume_id` 字段中返回的值相对应；接口定义在\n  [CSI 规范](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume) 中。\n  在所有对 CSI 卷驱动程序的调用中，引用该 CSI 卷时都使用此值作为 `volume_id` 参数。"}
{"en": "* `readOnly`: An optional boolean value indicating whether the volume is to be\n  \"ControllerPublished\" (attached) as read only. Default is false. This value is passed\n  to the CSI driver via the `readonly` field in the `ControllerPublishVolumeRequest`.", "zh": "* `readOnly`：一个可选的布尔值，指示通过 `ControllerPublished` 关联该卷时是否设置该卷为只读。默认值是 false。\n  该值通过 `ControllerPublishVolumeRequest` 中的 `readonly` 字段传递给 CSI 驱动。"}
{"en": "* `fsType`: If the PV's `VolumeMode` is `Filesystem` then this field may be used\n  to specify the filesystem that should be used to mount the volume. If the\n  volume has not been formatted and formatting is supported, this value will be\n  used to format the volume.\n  This value is passed to the CSI driver via the `VolumeCapability` field of\n  `ControllerPublishVolumeRequest`, `NodeStageVolumeRequest`, and\n  `NodePublishVolumeRequest`.", "zh": "* `fsType`：如果 PV 的 `VolumeMode` 为 `Filesystem`，那么此字段指定挂载卷时应该使用的文件系统。\n  如果卷尚未格式化，并且支持格式化，此值将用于格式化卷。\n  此值可以通过 `ControllerPublishVolumeRequest`、`NodeStageVolumeRequest` 和\n  `NodePublishVolumeRequest` 的 `VolumeCapability` 字段传递给 CSI 驱动。"}
{"en": "* `volumeAttributes`: A map of string to string that specifies static properties\n  of a volume. This map must correspond to the map returned in the\n  `volume.attributes` field of the `CreateVolumeResponse` by the CSI driver as\n  defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume).\n  The map is passed to the CSI driver via the `volume_context` field in the\n  `ControllerPublishVolumeRequest`, `NodeStageVolumeRequest`, and\n  `NodePublishVolumeRequest`.", "zh": "* `volumeAttributes`：一个字符串到字符串的映射表，用来设置卷的静态属性。\n  该映射必须与 CSI 驱动程序返回的 `CreateVolumeResponse` 中的 `volume.attributes`\n  字段的映射相对应；\n  [CSI 规范](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume)中有相应的定义。\n  该映射通过`ControllerPublishVolumeRequest`、`NodeStageVolumeRequest` 和\n  `NodePublishVolumeRequest` 中的 `volume_context` 字段传递给 CSI 驱动。"}
{"en": "* `controllerPublishSecretRef`: A reference to the secret object containing\n  sensitive information to pass to the CSI driver to complete the CSI\n  `ControllerPublishVolume` and `ControllerUnpublishVolume` calls. This field is\n  optional, and may be empty if no secret is required. If the Secret\n  contains more than one secret, all secrets are passed.", "zh": "* `controllerPublishSecretRef`：对包含敏感信息的 Secret 对象的引用；\n  该敏感信息会被传递给 CSI 驱动来完成 CSI `ControllerPublishVolume` 和\n  `ControllerUnpublishVolume` 调用。\n  此字段是可选的；在不需要 Secret 时可以是空的。\n  如果 Secret 包含多个 Secret 条目，则所有的 Secret 条目都会被传递。"}
{"en": "* `nodeExpandSecretRef`: A reference to the secret containing sensitive\n  information to pass to the CSI driver to complete the CSI\n  `NodeExpandVolume` call. This field is optional, and may be empty if no\n  secret is required. If the object contains more than one secret, all\n  secrets are passed.  When you have configured secret data for node-initiated\n  volume expansion, the kubelet passes that data via the `NodeExpandVolume()`\n  call to the CSI driver. All supported versions of Kubernetes offer the\n  `nodeExpandSecretRef` field, and have it available by default. Kubernetes releases\n  prior to v1.25 did not include this support.\n* Enable the [feature gate](/docs/reference/command-line-tools-reference/feature-gates-removed/)\n  named `CSINodeExpandSecret` for each kube-apiserver and for the kubelet on every\n  node. Since Kubernetes version 1.27 this feature has been enabled by default\n  and no explicit enablement of the feature gate is required.\n  You must also be using a CSI driver that supports or requires secret data during\n  node-initiated storage resize operations.", "zh": "* `nodeExpandSecretRef`：对包含敏感信息的 Secret 对象的引用，\n  该信息会传递给 CSI 驱动以完成 CSI `NodeExpandVolume` 调用。\n  此字段是可选的，如果不需要 Secret，则可能是空的。\n  如果 Secret 包含多个 Secret 条目，则传递所有 Secret 条目。\n  当你为节点初始化的卷扩展配置 Secret 数据时，kubelet 会通过 `NodeExpandVolume()`\n  调用将该数据传递给 CSI 驱动。所有受支持的 Kubernetes 版本都提供 `nodeExpandSecretRef` 字段，\n  并且默认可用。Kubernetes v1.25 之前的版本不包括此支持。\n  为每个 kube-apiserver 和每个节点上的 kubelet 启用名为 `CSINodeExpandSecret` 的\n  [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates-removed/)。\n  自 Kubernetes 1.27 版本起，此特性已默认启用，无需显式启用特性门控。\n  在节点初始化的存储大小调整操作期间，你还必须使用支持或需要 Secret 数据的 CSI 驱动。"}
{"en": "* `nodePublishSecretRef`: A reference to the secret object containing\n  sensitive information to pass to the CSI driver to complete the CSI\n  `NodePublishVolume` call. This field is optional, and may be empty if no\n  secret is required. If the secret object contains more than one secret, all\n  secrets are passed.", "zh": "* `nodePublishSecretRef`：对包含敏感信息的 Secret 对象的引用。\n  该信息传递给 CSI 驱动来完成 CSI `NodePublishVolume` 调用。\n  此字段是可选的，如果不需要 Secret，则可能是空的。\n  如果 Secret 对象包含多个 Secret 条目，则传递所有 Secret 条目。"}
{"en": "* `nodeStageSecretRef`: A reference to the secret object containing\n  sensitive information to pass to the CSI driver to complete the CSI\n  `NodeStageVolume` call. This field is optional, and may be empty if no secret\n  is required. If the Secret contains more than one secret, all secrets\n  are passed.", "zh": "* `nodeStageSecretRef`：对包含敏感信息的 Secret 对象的引用，\n  该信息会传递给 CSI 驱动以完成 CSI `NodeStageVolume` 调用。\n  此字段是可选的，如果不需要 Secret，则可能是空的。\n  如果 Secret 包含多个 Secret 条目，则传递所有 Secret 条目。"}
{"en": "#### CSI raw block volume support", "zh": "#### CSI 原始块卷支持    {#csi-raw-block-volume-support}\n\n{{< feature-state for_k8s_version=\"v1.18\" state=\"stable\" >}}"}
{"en": "Vendors with external CSI drivers can implement raw block volume support\nin Kubernetes workloads.", "zh": "具有外部 CSI 驱动程序的供应商能够在 Kubernetes 工作负载中实现原始块卷支持。"}
{"en": "You can set up your\n[PersistentVolume/PersistentVolumeClaim with raw block volume support](/docs/concepts/storage/persistent-volumes/#raw-block-volume-support) as usual, without any CSI specific changes.", "zh": "你可以和以前一样，\n安装自己的[带有原始块卷支持的 PV/PVC](/zh-cn/docs/concepts/storage/persistent-volumes/#raw-block-volume-support)，\n采用 CSI 对此过程没有影响。"}
{"en": "#### CSI ephemeral volumes", "zh": "#### CSI 临时卷   {#csi-ephemeral-volumes}\n\n{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}"}
{"en": "You can directly configure CSI volumes within the Pod\nspecification. Volumes specified in this way are ephemeral and do not\npersist across pod restarts. See [Ephemeral\nVolumes](/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes)\nfor more information.", "zh": "你可以直接在 Pod 规约中配置 CSI 卷。采用这种方式配置的卷都是临时卷，\n无法在 Pod 重新启动后继续存在。\n进一步的信息可参阅[临时卷](/zh-cn/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes)。"}
{"en": "For more information on how to develop a CSI driver, refer to the\n[kubernetes-csi documentation](https://kubernetes-csi.github.io/docs/)", "zh": "有关如何开发 CSI 驱动的更多信息，请参考 [kubernetes-csi 文档](https://kubernetes-csi.github.io/docs/)。"}
{"en": "#### Windows CSI proxy", "zh": "#### Windows CSI 代理  {#windows-csi-proxy}\n\n{{< feature-state for_k8s_version=\"v1.22\" state=\"stable\" >}}"}
{"en": "CSI node plugins need to perform various privileged\noperations like scanning of disk devices and mounting of file systems. These operations\ndiffer for each host operating system. For Linux worker nodes, containerized CSI node\nnode plugins are typically deployed as privileged containers. For Windows worker nodes,\nprivileged operations for containerized CSI node plugins is supported using\n[csi-proxy](https://github.com/kubernetes-csi/csi-proxy), a community-managed,\nstand-alone binary that needs to be pre-installed on each Windows node.\n\nFor more details, refer to the deployment guide of the CSI plugin you wish to deploy.", "zh": "CSI 节点插件需要执行多种特权操作，例如扫描磁盘设备和挂载文件系统等。\n这些操作在每个宿主机操作系统上都是不同的。对于 Linux 工作节点而言，容器化的 CSI\n节点插件通常部署为特权容器。对于 Windows 工作节点而言，容器化 CSI\n节点插件的特权操作是通过 [csi-proxy](https://github.com/kubernetes-csi/csi-proxy)\n来支持的。csi-proxy 是一个由社区管理的、独立的可执行二进制文件，\n需要被预安装到每个 Windows 节点上。\n\n要了解更多的细节，可以参考你要部署的 CSI 插件的部署指南。"}
{"en": "#### Migrating to CSI drivers from in-tree plugins", "zh": "#### 从树内插件迁移到 CSI 驱动程序  {#migrating-to-csi-drivers-from-in-tree-plugins}\n\n{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}"}
{"en": "The `CSIMigration` feature directs operations against existing in-tree\nplugins to corresponding CSI plugins (which are expected to be installed and configured).\nAs a result, operators do not have to make any\nconfiguration changes to existing Storage Classes, PersistentVolumes or PersistentVolumeClaims\n(referring to in-tree plugins) when transitioning to a CSI driver that supersedes an in-tree plugin.", "zh": "`CSIMigration` 特性针对现有树内插件的操作会被定向到相应的 CSI 插件（应已安装和配置）。\n因此，操作员在过渡到取代树内插件的 CSI 驱动时，无需对现有存储类、PV 或 PVC（指树内插件）进行任何配置更改。\n\n{{< note >}}"}
{"en": "Existing PVs created by a in-tree volume plugin can still be used in the future without any configuration\nchanges, even after the migration to CSI is completed for that volume type, and even after you upgrade to a\nversion of Kubernetes that doesn't have compiled-in support for that kind of storage.\n\nAs part of that migration, you - or another cluster administrator - **must** have installed and configured\nthe appropriate CSI driver for that storage. The core of Kubernetes does not install that software for you.", "zh": "即使你针对这种卷完成了 CSI 迁移且你升级到不再内置对这种存储类别的支持的 Kubernetes 版本，\n现有的由树内卷插件所创建的 PV 在未来无需进行任何配置更改就可以使用，\n\n作为迁移的一部分，你或其他集群管理员**必须**安装和配置适用于该存储的 CSI 驱动。\nKubernetes 不会为你安装该软件。\n\n---"}
{"en": "After that migration, you can also define new PVCs and PVs that refer to the legacy, built-in\nstorage integrations.\nProvided you have the appropriate CSI driver installed and configured, the PV creation continues\nto work, even for brand new volumes. The actual storage management now happens through\nthe CSI driver.", "zh": "在完成迁移之后，你也可以定义新的 PVC 和 PV，引用原来的、内置的集成存储。\n只要你安装并配置了适当的 CSI 驱动，即使是全新的卷，PV 的创建仍然可以继续工作。\n实际的存储管理现在通过 CSI 驱动来进行。\n{{< /note >}}"}
{"en": "The operations and features that are supported include:\nprovisioning/delete, attach/detach, mount/unmount and resizing of volumes.", "zh": "所支持的操作和特性包括：配备（Provisioning）/删除、挂接（Attach）/解挂（Detach）、\n挂载（Mount）/卸载（Unmount）和调整卷大小。"}
{"en": "In-tree plugins that support `CSIMigration` and have a corresponding CSI driver implemented\nare listed in [Types of Volumes](#volume-types).\n\nThe following in-tree plugins support persistent storage on Windows nodes:", "zh": "上面的[卷类型](#volume-types)节列出了支持 `CSIMigration` 并已实现相应 CSI\n驱动程序的树内插件。\n\n下面是支持 Windows 节点上持久性存储的树内插件：\n\n* [`azureFile`](#azurefile)\n* [`gcePersistentDisk`](#gcepersistentdisk)\n* [`vsphereVolume`](#vspherevolume)"}
{"en": "### flexVolume (deprecated)   {#flexvolume}", "zh": "### flexVolume（已弃用）   {#flexvolume}\n\n{{< feature-state for_k8s_version=\"v1.23\" state=\"deprecated\" >}}"}
{"en": "FlexVolume is an out-of-tree plugin interface that uses an exec-based model to interface\nwith storage drivers. The FlexVolume driver binaries must be installed in a pre-defined\nvolume plugin path on each node and in some cases the control plane nodes as well.\n\nPods interact with FlexVolume drivers through the `flexVolume` in-tree volume plugin.\nFor more details, see the FlexVolume [README](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md#readme) document.", "zh": "FlexVolume 是一个使用基于 exec 的模型来与驱动程序对接的树外插件接口。\n用户必须在每个节点上的预定义卷插件路径中安装 FlexVolume\n驱动程序可执行文件，在某些情况下，控制平面节点中也要安装。\n\nPod 通过 `flexvolume` 树内插件与 FlexVolume 驱动程序交互。\n更多详情请参考 FlexVolume\n[README](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md#readme) 文档。"}
{"en": "The following FlexVolume [plugins](https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows),\ndeployed as PowerShell scripts on the host, support Windows nodes:", "zh": "下面的 FlexVolume [插件](https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows)\n以 PowerShell 脚本的形式部署在宿主机系统上，支持 Windows 节点：\n\n* [SMB](https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~smb.cmd)\n* [iSCSI](https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~iscsi.cmd)\n\n{{< note >}}"}
{"en": "FlexVolume is deprecated. Using an out-of-tree CSI driver is the recommended way to integrate external storage with Kubernetes.\n\nMaintainers of FlexVolume driver should implement a CSI Driver and help to migrate users of FlexVolume drivers to CSI.\nUsers of FlexVolume should move their workloads to use the equivalent CSI Driver.", "zh": "FlexVolume 已被弃用。推荐使用树外 CSI 驱动来将外部存储整合进 Kubernetes。\n\nFlexVolume 驱动的维护者应开发一个 CSI 驱动并帮助用户从 FlexVolume 驱动迁移到 CSI。\nFlexVolume 用户应迁移工作负载以使用对等的 CSI 驱动。\n{{< /note >}}"}
{"en": "## Mount propagation", "zh": "## 挂载卷的传播   {#mount-propagation}\n\n  {{< caution >}}"}
{"en": "Mount propagation is a low-level feature that does not work consistently on all\n  volume types. It is recommended to use only with `hostPath` or in-memory `emptyDir`\n  volumes. See [this discussion](https://github.com/kubernetes/kubernetes/issues/95049)\n  for more context.", "zh": "挂载卷的传播是一项底层功能，不能在所有类型的卷中以一致的方式工作。\n  建议只在 `hostPath` 或基于内存的 `emptyDir` 卷中使用。\n  详情请参考[讨论](https://github.com/kubernetes/kubernetes/issues/95049)。\n  {{< /caution >}}"}
{"en": "Mount propagation allows for sharing volumes mounted by a container to\nother containers in the same pod, or even to other pods on the same node.\n\nMount propagation of a volume is controlled by the `mountPropagation` field\nin `containers[*].volumeMounts`. Its values are:", "zh": "挂载卷的传播能力允许将容器安装的卷共享到同一 Pod 中的其他容器，甚至共享到同一节点上的其他 Pod。\n\n卷的挂载传播特性由 `containers[*].volumeMounts` 中的 `mountPropagation` 字段控制。\n它的值包括："}
{"en": "* `None` - This volume mount will not receive any subsequent mounts\n  that are mounted to this volume or any of its subdirectories by the host.\n  In similar fashion, no mounts created by the container will be visible on\n  the host. This is the default mode.\n\n  This mode is equal to `rprivate` mount propagation as described in\n  [`mount(8)`](https://man7.org/linux/man-pages/man8/mount.8.html)\n\n  However, the CRI runtime may choose `rslave` mount propagation (i.e.,\n  `HostToContainer`) instead, when `rprivate` propagation is not applicable.\n  cri-dockerd (Docker) is known to choose `rslave` mount propagation when the\n  mount source contains the Docker daemon's root directory (`/var/lib/docker`).", "zh": "* `None` - 此卷挂载将不会感知到主机后续在此卷或其任何子目录上执行的挂载变化。\n  类似的，容器所创建的卷挂载在主机上是不可见的。这是默认模式。\n\n  该模式等同于 [`mount(8)`](https://man7.org/linux/man-pages/man8/mount.8.html) 中描述的\n  `rprivate` 挂载传播选项。\n\n  然而，当 `rprivate` 传播选项不适用时，CRI 运行时可以转为选择 `rslave` 挂载传播选项\n  （即 `HostToContainer`）。当挂载源包含 Docker 守护进程的根目录（`/var/lib/docker`）时，\n  cri-dockerd (Docker) 已知可以选择 `rslave` 挂载传播选项。"}
{"en": "* `HostToContainer` - This volume mount will receive all subsequent mounts\n  that are mounted to this volume or any of its subdirectories.\n\n  In other words, if the host mounts anything inside the volume mount, the\n  container will see it mounted there.\n\n  Similarly, if any Pod with `Bidirectional` mount propagation to the same\n  volume mounts anything there, the container with `HostToContainer` mount\n  propagation will see it.\n\n  This mode is equal to `rslave` mount propagation as described in the\n  [`mount(8)`](https://man7.org/linux/man-pages/man8/mount.8.html)", "zh": "* `HostToContainer` - 此卷挂载将会感知到主机后续针对此卷或其任何子目录的挂载操作。\n\n  换句话说，如果主机在此挂载卷中挂载任何内容，容器将能看到它被挂载在那里。\n\n  类似的，配置了 `Bidirectional` 挂载传播选项的 Pod 如果在同一卷上挂载了内容，挂载传播设置为\n  `HostToContainer` 的容器都将能看到这一变化。\n\n  该模式等同于 [`mount(8)`](https://man7.org/linux/man-pages/man8/mount.8.html)中描述的\n  `rslave` 挂载传播选项。"}
{"en": "* `Bidirectional` - This volume mount behaves the same the `HostToContainer` mount.\n  In addition, all volume mounts created by the container will be propagated\n  back to the host and to all containers of all pods that use the same volume.\n\n  A typical use case for this mode is a Pod with a FlexVolume or CSI driver or\n  a Pod that needs to mount something on the host using a `hostPath` volume.\n\n  This mode is equal to `rshared` mount propagation as described in the\n  [`mount(8)`](https://man7.org/linux/man-pages/man8/mount.8.html)", "zh": "* `Bidirectional` - 这种卷挂载和 `HostToContainer` 挂载表现相同。\n  另外，容器创建的卷挂载将被传播回至主机和使用同一卷的所有 Pod 的所有容器。\n\n  该模式的典型用例是带有 FlexVolume 或 CSI 驱动的 Pod，或者需要通过\n  `hostPath` 卷在主机上挂载某些东西的 Pod。\n\n  该模式等同于 [`mount(8)`](https://man7.org/linux/man-pages/man8/mount.8.html) 中描述的\n  `rshared` 挂载传播选项。\n\n  {{< warning >}}"}
{"en": "`Bidirectional` mount propagation can be dangerous. It can damage\n  the host operating system and therefore it is allowed only in privileged\n  containers. Familiarity with Linux kernel behavior is strongly recommended.\n  In addition, any volume mounts created by containers in pods must be destroyed\n  (unmounted) by the containers on termination.", "zh": "`Bidirectional` 形式的挂载传播可能比较危险。\n  它可以破坏主机操作系统，因此它只被允许在特权容器中使用。\n  强烈建议你熟悉 Linux 内核行为。\n  此外，由 Pod 中的容器创建的任何卷挂载必须在终止时由容器销毁（卸载）。\n  {{< /warning >}}"}
{"en": "## Read-only mounts\n\nA mount can be made read-only by setting the `.spec.containers[].volumeMounts[].readOnly`\nfield to `true`.\nThis does not make the volume itself read-only, but that specific container will\nnot be able to write to it.\nOther containers in the Pod may mount the same volume as read-write.", "zh": "## 只读挂载   {#read-only-mounts}\n\n通过将 `.spec.containers[].volumeMounts[].readOnly` 字段设置为 `true` 可以使挂载只读。\n这不会使卷本身只读，但该容器将无法写入此卷。\nPod 中的其他容器可以以读写方式挂载同一个卷。"}
{"en": "On Linux, read-only mounts are not recursively read-only by default.\nFor example, consider a Pod which mounts the hosts `/mnt` as a `hostPath` volume.  If\nthere is another filesystem mounted read-write on `/mnt/<SUBMOUNT>` (such as tmpfs,\nNFS, or USB storage), the volume mounted into the container(s) will also have a writeable\n`/mnt/<SUBMOUNT>`, even if the mount itself was specified as read-only.", "zh": "在 Linux 上，只读挂载默认不会以递归方式只读。\n假如有一个 Pod 将主机的 `/mnt` 挂载为 `hostPath` 卷。\n如果在 `/mnt/<SUBMOUNT>` 上有另一个以读写方式挂载的文件系统（如 tmpfs、NFS 或 USB 存储），\n即使挂载本身被指定为只读，挂载到容器中的卷 `/mnt/<SUBMOUNT>` 也是可写的。"}
{"en": "### Recursive read-only mounts", "zh": "### 递归只读挂载    {#recursive-read-only-mounts}\n\n{{< feature-state feature_gate_name=\"RecursiveReadOnlyMounts\" >}}"}
{"en": "Recursive read-only mounts can be enabled by setting the\n`RecursiveReadOnlyMounts` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nfor kubelet and kube-apiserver, and setting the `.spec.containers[].volumeMounts[].recursiveReadOnly`\nfield for a pod.", "zh": "通过为 kubelet 和 kube-apiserver 设置 `RecursiveReadOnlyMounts`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)，\n并为 Pod 设置 `.spec.containers[].volumeMounts[].recursiveReadOnly` 字段，\n递归只读挂载可以被启用。"}
{"en": "The allowed values are:\n\n* `Disabled` (default): no effect.", "zh": "允许的值为：\n\n* `Disabled`（默认）：无效果。"}
{"en": "* `Enabled`: makes the mount recursively read-only.\n  Needs all the following requirements to be satisfied:\n  * `readOnly` is set to `true`\n  * `mountPropagation` is unset, or, set to `None`\n  * The host is running with Linux kernel v5.12 or later\n  * The [CRI-level](/docs/concepts/architecture/cri) container runtime supports recursive read-only mounts\n  * The OCI-level container runtime supports recursive read-only mounts.\n  It will fail if any of these is not true.", "zh": "* `Enabled`：使挂载递归只读。需要满足以下所有要求：\n\n  * `readOnly` 设置为 `true`\n  * `mountPropagation` 不设置，或设置为 `None`\n  * 主机运行 Linux 内核 v5.12 或更高版本\n  * [CRI 级别](/zh-cn/docs/concepts/architecture/cri)的容器运行时支持递归只读挂载\n  * OCI 级别的容器运行时支持递归只读挂载\n\n  如果其中任何一个不满足，递归只读挂载将会失败。"}
{"en": "* `IfPossible`: attempts to apply `Enabled`, and falls back to `Disabled`\n  if the feature is not supported by the kernel or the runtime class.\n\nExample:", "zh": "* `IfPossible`：尝试应用 `Enabled`，如果内核或运行时类不支持该特性，则回退为 `Disabled`。\n\n示例：\n\n{{% code_sample file=\"storage/rro.yaml\" %}}"}
{"en": "When this property is recognized by kubelet and kube-apiserver,\nthe `.status.containerStatuses[].volumeMounts[].recursiveReadOnly` field is set to either\n`Enabled` or `Disabled`.\n\n#### Implementations {#implementations-rro}", "zh": "当此属性被 kubelet 和 kube-apiserver 识别到时，\n`.status.containerStatuses[].volumeMounts[].recursiveReadOnly` 字段将被设置为 `Enabled` 或 `Disabled`。\n\n#### 实现   {#implementations-rro}\n\n{{% thirdparty-content %}}"}
{"en": "The following container runtimes are known to support recursive read-only mounts.\n\nCRI-level:\n- [containerd](https://containerd.io/), since v2.0\n\nOCI-level:\n- [runc](https://runc.io/), since v1.1\n- [crun](https://github.com/containers/crun), since v1.8.6", "zh": "以下容器运行时已知支持递归只读挂载。\n\nCRI 级别：\n\n- [containerd](https://containerd.io/)，自 v2.0 起\n\nOCI 级别：\n\n- [runc](https://runc.io/)，自 v1.1 起\n- [crun](https://github.com/containers/crun)，自 v1.8.6 起\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "Follow an example of [deploying WordPress and MySQL with Persistent Volumes](/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/).", "zh": "参考[使用持久卷部署 WordPress 和 MySQL](/zh-cn/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/) 示例。"}
{"en": "Dynamic volume provisioning allows storage volumes to be created on-demand.\nWithout dynamic provisioning, cluster administrators have to manually make\ncalls to their cloud or storage provider to create new storage volumes, and\nthen create [`PersistentVolume` objects](/docs/concepts/storage/persistent-volumes/)\nto represent them in Kubernetes. The dynamic provisioning feature eliminates\nthe need for cluster administrators to pre-provision storage. Instead, it\nautomatically provisions storage when users create\n[`PersistentVolumeClaim` objects](/docs/concepts/storage/persistent-volumes/).", "zh": "动态卷制备允许按需创建存储卷。\n如果没有动态制备，集群管理员必须手动地联系他们的云或存储提供商来创建新的存储卷，\n然后在 Kubernetes 集群创建\n[`PersistentVolume` 对象](/zh-cn/docs/concepts/storage/persistent-volumes/)来表示这些卷。\n动态制备功能消除了集群管理员预先配置存储的需要。相反，它在用户创建\n[`PersistentVolumeClaim` 对象](/zh-cn/docs/concepts/storage/persistent-volumes/)时自动制备存储。"}
{"en": "## Background", "zh": "## 背景    {#background}"}
{"en": "The implementation of dynamic volume provisioning is based on the API object `StorageClass`\nfrom the API group `storage.k8s.io`. A cluster administrator can define as many\n`StorageClass` objects as needed, each specifying a *volume plugin* (aka\n*provisioner*) that provisions a volume and the set of parameters to pass to\nthat provisioner when provisioning.", "zh": "动态卷制备的实现基于 `storage.k8s.io` API 组中的 `StorageClass` API 对象。\n集群管理员可以根据需要定义多个 `StorageClass` 对象，每个对象指定一个**卷插件**（又名 **provisioner**），\n卷插件向卷制备商提供在创建卷时需要的数据卷信息及相关参数。"}
{"en": "A cluster administrator can define and expose multiple flavors of storage (from\nthe same or different storage systems) within a cluster, each with a custom set\nof parameters. This design also ensures that end users don't have to worry\nabout the complexity and nuances of how storage is provisioned, but still\nhave the ability to select from multiple storage options.", "zh": "集群管理员可以在集群中定义和公开多种存储（来自相同或不同的存储系统），每种都具有自定义参数集。\n该设计也确保终端用户不必担心存储制备的复杂性和细微差别，但仍然能够从多个存储选项中进行选择。"}
{"en": "More information on storage classes can be found\n[here](/docs/concepts/storage/storage-classes/).", "zh": "点击[这里](/zh-cn/docs/concepts/storage/storage-classes/)查阅有关存储类的更多信息。"}
{"en": "## Enabling Dynamic Provisioning", "zh": "## 启用动态卷制备  {#enabling-dynamic-provisioning}"}
{"en": "To enable dynamic provisioning, a cluster administrator needs to pre-create\none or more StorageClass objects for users.\nStorageClass objects define which provisioner should be used and what parameters\nshould be passed to that provisioner when dynamic provisioning is invoked.\nThe name of a StorageClass object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).\n\nThe following manifest creates a storage class \"slow\" which provisions standard\ndisk-like persistent disks.", "zh": "要启用动态制备功能，集群管理员需要为用户预先创建一个或多个 `StorageClass` 对象。\n`StorageClass` 对象定义当动态制备被调用时，哪一个驱动将被使用和哪些参数将被传递给驱动。\nStorageClass 对象的名字必须是一个合法的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。\n以下清单创建了一个 `StorageClass` 存储类 \"slow\"，它提供类似标准磁盘的永久磁盘。\n\n```yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: slow\nprovisioner: kubernetes.io/gce-pd\nparameters:\n  type: pd-standard\n```"}
{"en": "The following manifest creates a storage class \"fast\" which provisions\nSSD-like persistent disks.", "zh": "以下清单创建了一个 \"fast\" 存储类，它提供类似 SSD 的永久磁盘。\n\n```yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast\nprovisioner: kubernetes.io/gce-pd\nparameters:\n  type: pd-ssd\n```"}
{"en": "## Using Dynamic Provisioning", "zh": "## 使用动态卷制备 {#using-dynamic-provisioning}"}
{"en": "Users request dynamically provisioned storage by including a storage class in\ntheir `PersistentVolumeClaim`. Before Kubernetes v1.6, this was done via the\n`volume.beta.kubernetes.io/storage-class` annotation. However, this annotation\nis deprecated since v1.9. Users now can and should instead use the\n`storageClassName` field of the `PersistentVolumeClaim` object. The value of\nthis field must match the name of a `StorageClass` configured by the\nadministrator (see [below](#enabling-dynamic-provisioning)).", "zh": "用户通过在 `PersistentVolumeClaim` 中包含存储类来请求动态制备的存储。\n在 Kubernetes v1.9 之前，这通过 `volume.beta.kubernetes.io/storage-class` 注解实现。\n然而，这个注解自 v1.6 起就不被推荐使用了。\n用户现在能够而且应该使用 `PersistentVolumeClaim` 对象的 `storageClassName` 字段。\n这个字段的值必须能够匹配到集群管理员配置的 `StorageClass` 名称（见[下面](#enabling-dynamic-provisioning)）。"}
{"en": "To select the \"fast\" storage class, for example, a user would create the\nfollowing PersistentVolumeClaim:", "zh": "例如，要选择 “fast” 存储类，用户将创建如下的 PersistentVolumeClaim：\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: claim1\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: fast\n  resources:\n    requests:\n      storage: 30Gi\n```"}
{"en": "This claim results in an SSD-like Persistent Disk being automatically\nprovisioned. When the claim is deleted, the volume is destroyed.", "zh": "该声明会自动制备一块类似 SSD 的永久磁盘。\n在删除该声明后，这个卷也会被销毁。"}
{"en": "## Defaulting Behavior", "zh": "## 设置默认值的行为    {#defaulting-behavior}"}
{"en": "Dynamic provisioning can be enabled on a cluster such that all claims are\ndynamically provisioned if no storage class is specified. A cluster administrator\ncan enable this behavior by:", "zh": "可以在集群上启用动态卷制备，以便在未指定存储类的情况下动态设置所有声明。\n集群管理员可以通过以下方式启用此行为："}
{"en": "- Marking one `StorageClass` object as *default*;\n- Making sure that the [`DefaultStorageClass` admission controller](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)\n  is enabled on the API server.", "zh": "- 标记一个 `StorageClass` 为 **默认**；\n- 确保 [`DefaultStorageClass` 准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)在\n  API 服务器端被启用。"}
{"en": "An administrator can mark a specific `StorageClass` as default by adding the\n[`storageclass.kubernetes.io/is-default-class` annotation](/docs/reference/labels-annotations-taints/#storageclass-kubernetes-io-is-default-class) to it.\nWhen a default `StorageClass` exists in a cluster and a user creates a\n`PersistentVolumeClaim` with `storageClassName` unspecified, the\n`DefaultStorageClass` admission controller automatically adds the\n`storageClassName` field pointing to the default storage class.", "zh": "管理员可以通过向其添加\n[`storageclass.kubernetes.io/is-default-class` 注解](/zh-cn/docs/reference/labels-annotations-taints/#storageclass-kubernetes-io-is-default-class)\n来将特定的 `StorageClass` 标记为默认。\n当集群中存在默认的 `StorageClass` 并且用户创建了一个未指定 `storageClassName` 的 `PersistentVolumeClaim` 时，\n`DefaultStorageClass` 准入控制器会自动向其中添加指向默认存储类的 `storageClassName` 字段。"}
{"en": "Note that if you set the `storageclass.kubernetes.io/is-default-class`\nannotation to true on more than one StorageClass in your cluster, and you then\ncreate a `PersistentVolumeClaim` with no `storageClassName` set, Kubernetes\nuses the most recently created default StorageClass.", "zh": "请注意，如果你在集群的多个 StorageClass 设置 `storageclass.kubernetes.io/is-default-class` 注解为 true，\n并之后创建了未指定 `storageClassName` 的 `PersistentVolumeClaim`，\nKubernetes 会使用最新创建的默认 StorageClass。"}
{"en": "## Topology Awareness", "zh": "## 拓扑感知 {#topology-awareness}"}
{"en": "In [Multi-Zone](/docs/setup/best-practices/multiple-zones/) clusters, Pods can be spread across\nZones in a Region. Single-Zone storage backends should be provisioned in the Zones where\nPods are scheduled. This can be accomplished by setting the\n[Volume Binding Mode](/docs/concepts/storage/storage-classes/#volume-binding-mode).", "zh": "在[多可用区](/zh-cn/docs/setup/best-practices/multiple-zones/)集群中，Pod 可以被分散到某个区域的多个可用区。\n单可用区存储后端应该被制备到 Pod 被调度到的可用区。\n这可以通过设置[卷绑定模式](/zh-cn/docs/concepts/storage/storage-classes/#volume-binding-mode)来实现。"}
{"en": "overview", "zh": "{{< feature-state feature_gate_name=\"VolumeAttributesClass\" >}}"}
{"en": "This page assumes that you are familiar with [StorageClasses](/docs/concepts/storage/storage-classes/),\n[volumes](/docs/concepts/storage/volumes/) and [PersistentVolumes](/docs/concepts/storage/persistent-volumes/)\nin Kubernetes.", "zh": "本页假设你已经熟悉 Kubernetes 中的 [StorageClass](/zh-cn/docs/concepts/storage/storage-classes/)、\n[Volume](/zh-cn/docs/concepts/storage/volumes/) 和\n[PersistentVolume](/zh-cn/docs/concepts/storage/persistent-volumes/)。"}
{"en": "A VolumeAttributesClass provides a way for administrators to describe the mutable\n\"classes\" of storage they offer. Different classes might map to different quality-of-service levels.\nKubernetes itself is un-opinionated about what these classes represent.\n\nThis is a beta feature and disabled by default.\n\nIf you want to test the feature whilst it's beta, you need to enable the `VolumeAttributesClass`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for the kube-controller-manager, kube-scheduler,\nand the kube-apiserver. You use the `--feature-gates` command line argument:", "zh": "卷属性类（VolumeAttributesClass）为管理员提供了一种描述可变更的存储“类”的方法。\n不同的类可以映射到不同的服务质量级别。Kubernetes 本身不关注这些类代表什么。\n\n这是一个 Beta 特性，默认被禁用。\n\n如果你想测试这一处于 Beta 阶段的特性，你需要为 kube-controller-manager、kube-scheduler 和 kube-apiserver 启用\n`VolumeAttributesClass` [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)。\n你可以使用 `--feature-gates` 命令行参数：\n\n```shell\n--feature-gates=\"...,VolumeAttributesClass=true\"\n```"}
{"en": "You will also have to enable the `storage.k8s.io/v1beta1` API group through the\n`kube-apiserver` [runtime-config](https://kubernetes.io/docs/tasks/administer-cluster/enable-disable-api/).\nYou use the following command line argument:", "zh": "你还必须通过 `kube-apiserver`\n[运行时配置](/zh-cn/docs/tasks/administer-cluster/enable-disable-api/)启用\n`storage.k8s.io/v1beta1` API 组：\n\n```shell\n--runtime-config=storage.k8s.io/v1beta1=true\n```"}
{"en": "You can also only use VolumeAttributesClasses with storage backed by\n{{< glossary_tooltip text=\"Container Storage Interface\" term_id=\"csi\" >}}, and only where the\nrelevant CSI driver implements the `ModifyVolume` API.", "zh": "另外你只有在使用{{< glossary_tooltip text=\"容器存储接口（CSI）\" term_id=\"csi\" >}}支持的存储时才能使用\nVolumeAttributesClass，并且要求相关的 CSI 驱动实现了 `ModifyVolume` API。"}
{"en": "## The VolumeAttributesClass API\n\nEach VolumeAttributesClass contains the `driverName` and `parameters`, which are\nused when a PersistentVolume (PV) belonging to the class needs to be dynamically provisioned\nor modified.\n\nThe name of a VolumeAttributesClass object is significant and is how users can request a particular class.\nAdministrators set the name and other parameters of a class when first creating VolumeAttributesClass objects.\nWhile the name of a VolumeAttributesClass object in a `PersistentVolumeClaim` is mutable, the parameters in an existing class are immutable.", "zh": "## VolumeAttributesClass API   {#the-volumeattributesclass-api}\n\n每个 VolumeAttributesClass 都包含 `driverName` 和 `parameters` 字段，\n当属于此类的持久卷（PV）需要被动态制备或修改时系统会使用这两个字段。\n\nVolumeAttributesClass 对象的名称比较重要，用户用对象名称来请求特定的类。\n管理员在首次创建 VolumeAttributesClass 对象时会设置某个类的名称和其他参数。\n虽然在 `PersistentVolumeClaim` 中 VolumeAttributesClass 对象的名称是可变的，\n但现有类中的参数是不可变的。\n\n```yaml\napiVersion: storage.k8s.io/v1beta1\nkind: VolumeAttributesClass\nmetadata:\n  name: silver\ndriverName: pd.csi.storage.gke.io\nparameters:\n  provisioned-iops: \"3000\"\n  provisioned-throughput: \"50\" \n```"}
{"en": "### Provisioner\n\nEach VolumeAttributesClass has a provisioner that determines what volume plugin is used for\nprovisioning PVs. The field `driverName` must be specified. \n\nThe feature support for VolumeAttributesClass is implemented in\n[kubernetes-csi/external-provisioner](https://github.com/kubernetes-csi/external-provisioner).", "zh": "### 存储制备器   {#provisioner}\n\n每个 VolumeAttributesClass 都有一个制备器（Provisioner），用来决定使用哪个卷插件制备 PV。\n`driverName` 字段是必填项。\n\n针对 VolumeAttributesClass 的特性支持在\n[kubernetes-csi/external-provisioner](https://github.com/kubernetes-csi/external-provisioner) 中实现。"}
{"en": "You are not restricted to specifying the [kubernetes-csi/external-provisioner](https://github.com/kubernetes-csi/external-provisioner).\nYou can also run and specify external provisioners,\nwhich are independent programs that follow a specification defined by Kubernetes.\nAuthors of external provisioners have full discretion over where their code lives, how\nthe provisioner is shipped, how it needs to be run, what volume plugin it uses, etc.", "zh": "你并非必须指定 [kubernetes-csi/external-provisioner](https://github.com/kubernetes-csi/external-provisioner)。\n你也可以运行并指定外部制备器，它们是遵循 Kubernetes 所定义的规范的独立程序。\n外部制备器的作者可以完全自行决定他们的代码放在哪儿、如何交付制备器、以何种方式运行、使用什么卷插件等。"}
{"en": "### Resizer\n\nEach VolumeAttributesClass has a resizer that determines what volume plugin is used\nfor modifying PVs. The field `driverName` must be specified. \n\nThe modifying volume feature support for VolumeAttributesClass is implemented in\n[kubernetes-csi/external-resizer](https://github.com/kubernetes-csi/external-resizer).\n\nFor example, an existing PersistentVolumeClaim is using a VolumeAttributesClass named silver:", "zh": "### 调整器   {#resizer}\n\n每个 VolumeAttributesClass 都有一个调整器（Resizer），用于确定修改 PV 所用的卷插件。\n`driverName` 字段是必填项。\n\n针对 VolumeAttributesClass 的修改卷特性支持在\n[kubernetes-csi/external-resizer](https://github.com/kubernetes-csi/external-resizer) 中实现。\n\n如以下 YAML 所示，有一个 PersistentVolumeClaim 使用名为 silver 的 VolumeAttributesClass：\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: test-pv-claim\nspec:\n  …\n  volumeAttributesClassName: silver\n  …\n```"}
{"en": "A new VolumeAttributesClass gold is available in the cluster:", "zh": "集群中有一个新的名为 gold 的 VolumeAttributesClass：\n\n```yaml\napiVersion: storage.k8s.io/v1beta1\nkind: VolumeAttributesClass\nmetadata:\n  name: gold\ndriverName: pd.csi.storage.gke.io\nparameters:\n  iops: \"4000\"\n  throughput: \"60\"\n```"}
{"en": "The end user can update the PVC with the new VolumeAttributesClass gold and apply:", "zh": "最终用户可以更新 PVC，使之使用新的名为 gold 的 VolumeAttributesClass，并应用此更新：\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: test-pv-claim\nspec:\n  …\n  volumeAttributesClassName: gold\n  …\n```"}
{"en": "## Parameters\n\nVolumeAttributeClasses have parameters that describe volumes belonging to them. Different parameters may be accepted\ndepending on the provisioner or the resizer. For example, the value `4000`, for the parameter `iops`,\nand the parameter `throughput` are specific to GCE PD.\nWhen a parameter is omitted, the default is used at volume provisioning.\nIf a user applies the PVC with a different VolumeAttributesClass with omitted parameters, the default value of\nthe parameters may be used depending on the CSI driver implementation.\nPlease refer to the related CSI driver documentation for more details.", "zh": "## 参数   {#parameters}\n\nVolumeAttributeClass 具有参数，用来描述隶属于该类的存储卷。可接受的参数可能因制备器或调整器而异。\n例如，参数 `iops` 的取值 `4000` 和参数 `throughput` 是特定于 GCE PD 的。\n如果某个参数被省略，则在卷制备时使用默认值。\n如果用户使用带有省略参数的不同 VolumeAttributesClass 来应用 PVC，参数的默认取值可能会因 CSI 驱动实现而异。\n有关细节参阅相关的 CSI 驱动文档。"}
{"en": "There can be at most 512 parameters defined for a VolumeAttributesClass.\nThe total length of the parameters object including its keys and values cannot exceed 256 KiB.", "zh": "VolumeAttributesClass 最多可以定义 512 个参数。\n这些参数对象的总长度（包括其键和值）不能超过 256 KiB。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.21\" state=\"alpha\" >}}"}
{"en": "{{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} volume health monitoring allows\nCSI Drivers to detect abnormal volume conditions from the underlying storage systems\nand report them as events on {{< glossary_tooltip text=\"PVCs\" term_id=\"persistent-volume-claim\" >}}\nor {{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}}.", "zh": "{{< glossary_tooltip text=\"CSI\" term_id=\"csi\" >}} 卷健康监测支持 CSI 驱动从底层的存储系统着手，\n探测异常的卷状态，并以事件的形式上报到 {{< glossary_tooltip text=\"PVC\" term_id=\"persistent-volume-claim\" >}}\n或 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}."}
{"en": "## Volume health monitoring", "zh": "## 卷健康监测 {#volume-health-monitoring}"}
{"en": "Kubernetes _volume health monitoring_ is part of how Kubernetes implements the\nContainer Storage Interface (CSI). Volume health monitoring feature is implemented\nin two components: an External Health Monitor controller, and the\n{{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}}.\n\nIf a CSI Driver supports Volume Health Monitoring feature from the controller side,\nan event will be reported on the related\n{{< glossary_tooltip text=\"PersistentVolumeClaim\" term_id=\"persistent-volume-claim\" >}} (PVC)\nwhen an abnormal volume condition is detected on a CSI volume.", "zh": "Kubernetes **卷健康监测**是 Kubernetes 容器存储接口（CSI）实现的一部分。\n卷健康监测特性由两个组件实现：外部健康监测控制器和 {{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}}。\n\n如果 CSI 驱动器通过控制器的方式支持卷健康监测特性，那么只要在 CSI 卷上监测到异常卷状态，就会在\n{{< glossary_tooltip text=\"PersistentVolumeClaim\" term_id=\"persistent-volume-claim\" >}} (PVC)\n中上报一个事件。"}
{"en": "The External Health Monitor {{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}\nalso watches for node failure events. You can enable node failure monitoring by setting\nthe `enable-node-watcher` flag to true. When the external health monitor detects a node\nfailure event, the controller reports an Event will be reported on the PVC to indicate\nthat pods using this PVC are on a failed node.\n\nIf a CSI Driver supports Volume Health Monitoring feature from the node side,\nan Event will be reported on every Pod using the PVC when an abnormal volume\ncondition is detected on a CSI volume. In addition, Volume Health information\nis exposed as Kubelet VolumeStats metrics. A new metric kubelet_volume_stats_health_status_abnormal\nis added. This metric includes two labels: `namespace` and `persistentvolumeclaim`.\nThe count is either 1 or 0. 1 indicates the volume is unhealthy, 0 indicates volume\nis healthy. For more information, please check\n[KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor#kubelet-metrics-changes).", "zh": "外部健康监测{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}也会监测节点失效事件。\n如果要启动节点失效监测功能，你可以设置标志 `enable-node-watcher` 为 `true`。\n当外部健康监测器检测到节点失效事件，控制器会报送一个事件，该事件会在 PVC 上继续上报，\n以表明使用此 PVC 的 Pod 正位于一个失效的节点上。\n\n如果 CSI 驱动程序支持节点侧的卷健康检测，那当在 CSI 卷上检测到异常卷时，\n会在使用该 PVC 的每个 Pod 上触发一个事件。\n此外，卷运行状况信息作为 Kubelet VolumeStats 指标公开。\n添加了一个新的指标 kubelet_volume_stats_health_status_abnormal。\n该指标包括两个标签：`namespace` 和 `persistentvolumeclaim`。\n计数为 1 或 0。1 表示卷不正常，0 表示卷正常。更多信息请访问\n[KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor#kubelet-metrics-changes)。"}
{"en": "You need to enable the `CSIVolumeHealth` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nto use this feature from the node side.", "zh": "{{< note >}}\n你需要启用 `CSIVolumeHealth`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)，\n才能在节点上使用此特性。\n{{< /note >}}\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "See the [CSI driver documentation](https://kubernetes-csi.github.io/docs/drivers.html)\nto find out which CSI drivers have implemented this feature.", "zh": "参阅 [CSI 驱动程序文档](https://kubernetes-csi.github.io/docs/drivers.html)，\n可以找出有哪些 CSI 驱动程序实现了此特性。"}
{"en": "If you want to control traffic flow at the IP address or port level for TCP, UDP, and SCTP protocols,\nthen you might consider using Kubernetes NetworkPolicies for particular applications in your cluster.\nNetworkPolicies are an application-centric construct which allow you to specify how a\n{{< glossary_tooltip text=\"pod\" term_id=\"pod\">}} is allowed to communicate with various network\n\"entities\" (we use the word \"entity\" here to avoid overloading the more common terms such as\n\"endpoints\" and \"services\", which have specific Kubernetes connotations) over the network.\nNetworkPolicies apply to a connection with a pod on one or both ends, and are not relevant to\nother connections.", "zh": "如果你希望针对 TCP、UDP 和 SCTP 协议在 IP 地址或端口层面控制网络流量，\n则你可以考虑为集群中特定应用使用 Kubernetes 网络策略（NetworkPolicy）。\nNetworkPolicy 是一种以应用为中心的结构，允许你设置如何允许\n{{< glossary_tooltip text=\"Pod\" term_id=\"pod\">}} 与网络上的各类网络“实体”\n（我们这里使用实体以避免过度使用诸如“端点”和“服务”这类常用术语，\n这些术语在 Kubernetes 中有特定含义）通信。\nNetworkPolicy 适用于一端或两端与 Pod 的连接，与其他连接无关。"}
{"en": "The entities that a Pod can communicate with are identified through a combination of the following\nthree identifiers:\n\n1. Other pods that are allowed (exception: a pod cannot block access to itself)\n1. Namespaces that are allowed\n1. IP blocks (exception: traffic to and from the node where a Pod is running is always allowed,\n   regardless of the IP address of the Pod or the node)", "zh": "Pod 可以与之通信的实体是通过如下三个标识符的组合来辩识的：\n\n1. 其他被允许的 Pod（例外：Pod 无法阻塞对自身的访问）\n2. 被允许的名字空间\n3. IP 组块（例外：与 Pod 运行所在的节点的通信总是被允许的，\n   无论 Pod 或节点的 IP 地址）"}
{"en": "When defining a pod- or namespace-based NetworkPolicy, you use a\n{{< glossary_tooltip text=\"selector\" term_id=\"selector\">}} to specify what traffic is allowed to\nand from the Pod(s) that match the selector.\n\nMeanwhile, when IP-based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).", "zh": "在定义基于 Pod 或名字空间的 NetworkPolicy 时，\n你会使用{{< glossary_tooltip text=\"选择算符\" term_id=\"selector\">}}来设定哪些流量可以进入或离开与该算符匹配的 Pod。\n\n另外，当创建基于 IP 的 NetworkPolicy 时，我们基于 IP 组块（CIDR 范围）来定义策略。"}
{"en": "## Prerequisites\n\nNetwork policies are implemented by the [network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/).\nTo use network policies, you must be using a networking solution which supports NetworkPolicy.\nCreating a NetworkPolicy resource without a controller that implements it will have no effect.", "zh": "## 前置条件   {#prerequisites}\n\n网络策略通过[网络插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)来实现。\n要使用网络策略，你必须使用支持 NetworkPolicy 的网络解决方案。\n创建一个 NetworkPolicy 资源对象而没有控制器来使它生效的话，是没有任何作用的。"}
{"en": "## The two Sorts of Pod isolation\n\nThere are two sorts of isolation for a pod: isolation for egress, and isolation for ingress.\nThey concern what connections may be established. \"Isolation\" here is not absolute, rather it\nmeans \"some restrictions apply\". The alternative, \"non-isolated for $direction\", means that no\nrestrictions apply in the stated direction. The two sorts of isolation (or not) are declared\nindependently, and are both relevant for a connection from one pod to another.", "zh": "## Pod 隔离的两种类型   {#the-two-sorts-of-pod-isolation}\n\nPod 有两种隔离：出口的隔离和入口的隔离。它们涉及到可以建立哪些连接。\n这里的“隔离”不是绝对的，而是意味着“有一些限制”。\n另外的，“非隔离方向”意味着在所述方向上没有限制。这两种隔离（或不隔离）是独立声明的，\n并且都与从一个 Pod 到另一个 Pod 的连接有关。"}
{"en": "By default, a pod is non-isolated for egress; all outbound connections are allowed.\nA pod is isolated for egress if there is any NetworkPolicy that both selects the pod and has\n\"Egress\" in its `policyTypes`; we say that such a policy applies to the pod for egress.\nWhen a pod is isolated for egress, the only allowed connections from the pod are those allowed by\nthe `egress` list of some NetworkPolicy that applies to the pod for egress. Reply traffic for those\nallowed connections will also be implicitly allowed.\nThe effects of those `egress` lists combine additively.", "zh": "默认情况下，一个 Pod 的出口是非隔离的，即所有外向连接都是被允许的。如果有任何的 NetworkPolicy\n选择该 Pod 并在其 `policyTypes` 中包含 \"Egress\"，则该 Pod 是出口隔离的，\n我们称这样的策略适用于该 Pod 的出口。当一个 Pod 的出口被隔离时，\n唯一允许的来自 Pod 的连接是适用于出口的 Pod 的某个 NetworkPolicy 的 `egress` 列表所允许的连接。\n针对那些允许连接的应答流量也将被隐式允许。\n这些 `egress` 列表的效果是相加的。"}
{"en": "By default, a pod is non-isolated for ingress; all inbound connections are allowed.\nA pod is isolated for ingress if there is any NetworkPolicy that both selects the pod and\nhas \"Ingress\" in its `policyTypes`; we say that such a policy applies to the pod for ingress.\nWhen a pod is isolated for ingress, the only allowed connections into the pod are those from\nthe pod's node and those allowed by the `ingress` list of some NetworkPolicy that applies to\nthe pod for ingress. Reply traffic for those allowed connections will also be implicitly allowed.\nThe effects of those `ingress` lists combine additively.", "zh": "默认情况下，一个 Pod 对入口是非隔离的，即所有入站连接都是被允许的。如果有任何的 NetworkPolicy\n选择该 Pod 并在其 `policyTypes` 中包含 “Ingress”，则该 Pod 被隔离入口，\n我们称这种策略适用于该 Pod 的入口。当一个 Pod 的入口被隔离时，唯一允许进入该 Pod\n的连接是来自该 Pod 节点的连接和适用于入口的 Pod 的某个 NetworkPolicy 的 `ingress`\n列表所允许的连接。针对那些允许连接的应答流量也将被隐式允许。这些 `ingress` 列表的效果是相加的。"}
{"en": "Network policies do not conflict; they are additive. If any policy or policies apply to a given\npod for a given direction, the connections allowed in that direction from that pod is the union of\nwhat the applicable policies allow. Thus, order of evaluation does not affect the policy result.\n\nFor a connection from a source pod to a destination pod to be allowed, both the egress policy on\nthe source pod and the ingress policy on the destination pod need to allow the connection. If\neither side does not allow the connection, it will not happen.", "zh": "网络策略是相加的，所以不会产生冲突。如果策略适用于 Pod 某一特定方向的流量，\nPod 在对应方向所允许的连接是适用的网络策略所允许的集合。\n因此，评估的顺序不影响策略的结果。\n\n要允许从源 Pod 到目的 Pod 的某个连接，源 Pod 的出口策略和目的 Pod 的入口策略都需要允许此连接。\n如果任何一方不允许此连接，则连接将会失败。"}
{"en": "## The NetworkPolicy resource {#networkpolicy-resource}\n\nSee the [NetworkPolicy](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#networkpolicy-v1-networking-k8s-io)\nreference for a full definition of the resource.\n\nAn example NetworkPolicy might look like this:", "zh": "## NetworkPolicy 资源 {#networkpolicy-resource}\n\n参阅 [NetworkPolicy](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#networkpolicy-v1-networking-k8s-io)\n来了解资源的完整定义。\n\n下面是一个 NetworkPolicy 的示例：\n\n{{< code_sample file=\"service/networking/networkpolicy.yaml\" >}}\n\n{{< note >}}"}
{"en": "POSTing this to the API server for your cluster will have no effect unless your chosen networking\nsolution supports network policy.", "zh": "除非选择支持网络策略的网络解决方案，否则将上述示例发送到 API 服务器没有任何效果。\n{{< /note >}}"}
{"en": "__Mandatory Fields__: As with all other Kubernetes config, a NetworkPolicy needs `apiVersion`,\n`kind`, and `metadata` fields. For general information about working with config files, see\n[Configure a Pod to Use a ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/),\nand [Object Management](/docs/concepts/overview/working-with-objects/object-management).\n\n**spec**: NetworkPolicy [spec](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)\nhas all the information needed to define a particular network policy in the given namespace.\n\n**podSelector**: Each NetworkPolicy includes a `podSelector` which selects the grouping of pods to\nwhich the policy applies. The example policy selects pods with the label \"role=db\". An empty\n`podSelector` selects all pods in the namespace.", "zh": "**必需字段**：与所有其他的 Kubernetes 配置一样，NetworkPolicy 需要 `apiVersion`、\n`kind` 和 `metadata` 字段。关于配置文件操作的一般信息，\n请参考[配置 Pod 以使用 ConfigMap](/zh-cn/docs/tasks/configure-pod-container/configure-pod-configmap/)\n和[对象管理](/zh-cn/docs/concepts/overview/working-with-objects/object-management)。\n\n**spec**：NetworkPolicy\n[规约](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)\n中包含了在一个名字空间中定义特定网络策略所需的所有信息。\n\n**podSelector**：每个 NetworkPolicy 都包括一个 `podSelector`，\n它对该策略所适用的一组 Pod 进行选择。示例中的策略选择带有 \"role=db\" 标签的 Pod。\n空的 `podSelector` 选择名字空间下的所有 Pod。"}
{"en": "**policyTypes**: Each NetworkPolicy includes a `policyTypes` list which may include either\n`Ingress`, `Egress`, or both. The `policyTypes` field indicates whether or not the given policy\napplies to ingress traffic to selected pod, egress traffic from selected pods, or both. If no\n`policyTypes` are specified on a NetworkPolicy then by default `Ingress` will always be set and\n`Egress` will be set if the NetworkPolicy has any egress rules.\n\n**ingress**: Each NetworkPolicy may include a list of allowed `ingress` rules. Each rule allows\ntraffic which matches both the `from` and `ports` sections. The example policy contains a single\nrule, which matches traffic on a single port, from one of three sources, the first specified via\nan `ipBlock`, the second via a `namespaceSelector` and the third via a `podSelector`.\n\n**egress**: Each NetworkPolicy may include a list of allowed `egress` rules. Each rule allows\ntraffic which matches both the `to` and `ports` sections. The example policy contains a single\nrule, which matches traffic on a single port to any destination in `10.0.0.0/24`.", "zh": "**policyTypes**：每个 NetworkPolicy 都包含一个 `policyTypes` 列表，其中包含\n`Ingress` 或 `Egress` 或两者兼具。`policyTypes` 字段表示给定的策略是应用于进入所选\nPod 的入站流量还是来自所选 Pod 的出站流量，或两者兼有。\n如果 NetworkPolicy 未指定 `policyTypes` 则默认情况下始终设置 `Ingress`；\n如果 NetworkPolicy 有任何出口规则的话则设置 `Egress`。\n\n**ingress**：每个 NetworkPolicy 可包含一个 `ingress` 规则的白名单列表。\n每个规则都允许同时匹配 `from` 和 `ports` 部分的流量。示例策略中包含一条简单的规则：\n它匹配某个特定端口，来自三个来源中的一个，第一个通过 `ipBlock`\n指定，第二个通过 `namespaceSelector` 指定，第三个通过 `podSelector` 指定。\n\n**egress**：每个 NetworkPolicy 可包含一个 `egress` 规则的白名单列表。\n每个规则都允许匹配 `to` 和 `port` 部分的流量。该示例策略包含一条规则，\n该规则将指定端口上的流量匹配到 `10.0.0.0/24` 中的任何目的地。"}
{"en": "So, the example NetworkPolicy:\n\n1. isolates `role=db` pods in the `default` namespace for both ingress and egress traffic\n   (if they weren't already isolated)\n1. (Ingress rules) allows connections to all pods in the `default` namespace with the label\n   `role=db` on TCP port 6379 from:\n\n   * any pod in the `default` namespace with the label `role=frontend`\n   * any pod in a namespace with the label `project=myproject`\n   * IP addresses in the ranges `172.17.0.0`–`172.17.0.255` and `172.17.2.0`–`172.17.255.255`\n     (ie, all of `172.17.0.0/16` except `172.17.1.0/24`)\n\n1. (Egress rules) allows connections from any pod in the `default` namespace with the label\n   `role=db` to CIDR `10.0.0.0/24` on TCP port 5978\n\nSee the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/)\nwalkthrough for further examples.", "zh": "所以，该网络策略示例：\n\n1. 隔离 `default` 名字空间下 `role=db` 的 Pod （如果它们不是已经被隔离的话）。\n2. （Ingress 规则）允许以下 Pod 连接到 `default` 名字空间下的带有 `role=db`\n   标签的所有 Pod 的 6379 TCP 端口：\n\n   * `default` 名字空间下带有 `role=frontend` 标签的所有 Pod\n   * 带有 `project=myproject` 标签的所有名字空间中的 Pod\n   * IP 地址范围为 `172.17.0.0–172.17.0.255` 和 `172.17.2.0–172.17.255.255`\n     （即，除了 `172.17.1.0/24` 之外的所有 `172.17.0.0/16`）\n\n3. （Egress 规则）允许 `default` 名字空间中任何带有标签 `role=db` 的 Pod 到 CIDR\n   `10.0.0.0/24` 下 5978 TCP 端口的连接。\n\n有关更多示例，请参阅[声明网络策略](/zh-cn/docs/tasks/administer-cluster/declare-network-policy/)演练。"}
{"en": "## Behavior of `to` and `from` selectors\n\nThere are four kinds of selectors that can be specified in an `ingress` `from` section or `egress`\n`to` section:\n\n**podSelector**: This selects particular Pods in the same namespace as the NetworkPolicy which\nshould be allowed as ingress sources or egress destinations.\n\n**namespaceSelector**: This selects particular namespaces for which all Pods should be allowed as\ningress sources or egress destinations.\n\n**namespaceSelector** *and* **podSelector**: A single `to`/`from` entry that specifies both\n`namespaceSelector` and `podSelector` selects particular Pods within particular namespaces. Be\ncareful to use correct YAML syntax. For example:", "zh": "## 选择器 `to` 和 `from` 的行为   {#behavior-of-to-and-from-selectors}\n\n可以在 `ingress` 的 `from` 部分或 `egress` 的 `to` 部分中指定四种选择器：\n\n**podSelector**：此选择器将在与 NetworkPolicy 相同的名字空间中选择特定的\nPod，应将其允许作为入站流量来源或出站流量目的地。\n\n**namespaceSelector**：此选择器将选择特定的名字空间，应将所有 Pod\n用作其入站流量来源或出站流量目的地。\n\n**namespaceSelector 和 podSelector**：一个指定 `namespaceSelector`\n和 `podSelector` 的 `to`/`from` 条目选择特定名字空间中的特定 Pod。\n注意使用正确的 YAML 语法；下面的策略：\n\n```yaml\n  ...\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          user: alice\n      podSelector:\n        matchLabels:\n          role: client\n  ...\n```"}
{"en": "This policy contains a single `from` element allowing connections from Pods with the label\n`role=client` in namespaces with the label `user=alice`. But the following policy is different:", "zh": "此策略在 `from` 数组中仅包含一个元素，只允许来自标有 `role=client` 的 Pod\n且该 Pod 所在的名字空间中标有 `user=alice` 的连接。但是**这项**策略：\n\n```yaml\n  ...\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          user: alice\n    - podSelector:\n        matchLabels:\n          role: client\n  ...\n```"}
{"en": "It contains two elements in the `from` array, and allows connections from Pods in the local\nNamespace with the label `role=client`, *or* from any Pod in any namespace with the label\n`user=alice`.", "zh": "它在 `from` 数组中包含两个元素，允许来自本地名字空间中标有 `role=client` 的\nPod 的连接，**或**来自任何名字空间中标有 `user=alice` 的任何 Pod 的连接。"}
{"en": "When in doubt, use `kubectl describe` to see how Kubernetes has interpreted the policy.\n\n<a name=\"behavior-of-ipblock-selectors\"></a>\n**ipBlock**: This selects particular IP CIDR ranges to allow as ingress sources or egress\ndestinations. These should be cluster-external IPs, since Pod IPs are ephemeral and unpredictable.\n\nCluster ingress and egress mechanisms often require rewriting the source or destination IP\nof packets. In cases where this happens, it is not defined whether this happens before or\nafter NetworkPolicy processing, and the behavior may be different for different\ncombinations of network plugin, cloud provider, `Service` implementation, etc.\n\nIn the case of ingress, this means that in some cases you may be able to filter incoming\npackets based on the actual original source IP, while in other cases, the \"source IP\" that\nthe NetworkPolicy acts on may be the IP of a `LoadBalancer` or of the Pod's node, etc.\n\nFor egress, this means that connections from pods to `Service` IPs that get rewritten to\ncluster-external IPs may or may not be subject to `ipBlock`-based policies.", "zh": "如有疑问，请使用 `kubectl describe` 查看 Kubernetes 如何解释该策略。\n\n**ipBlock**：此选择器将选择特定的 IP CIDR 范围以用作入站流量来源或出站流量目的地。\n这些应该是集群外部 IP，因为 Pod IP 存在时间短暂的且随机产生。\n\n集群的入站和出站机制通常需要重写数据包的源 IP 或目标 IP。\n在发生这种情况时，不确定在 NetworkPolicy 处理之前还是之后发生，\n并且对于网络插件、云提供商、`Service` 实现等的不同组合，其行为可能会有所不同。\n\n对入站流量而言，这意味着在某些情况下，你可以根据实际的原始源 IP 过滤传入的数据包，\n而在其他情况下，NetworkPolicy 所作用的 `源IP` 则可能是 `LoadBalancer` 或\nPod 的节点等。\n\n对于出站流量而言，这意味着从 Pod 到被重写为集群外部 IP 的 `Service` IP\n的连接可能会或可能不会受到基于 `ipBlock` 的策略的约束。"}
{"en": "## Default policies\n\nBy default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to\nand from pods in that namespace. The following examples let you change the default behavior\nin that namespace.", "zh": "## 默认策略   {#default-policies}\n\n默认情况下，如果名字空间中不存在任何策略，则所有进出该名字空间中 Pod 的流量都被允许。\n以下示例使你可以更改该名字空间中的默认行为。"}
{"en": "### Default deny all ingress traffic", "zh": "### 默认拒绝所有入站流量   {#default-deny-all-ingress-traffic}"}
{"en": "You can create a \"default\" ingress isolation policy for a namespace by creating a NetworkPolicy\nthat selects all pods but does not allow any ingress traffic to those pods.", "zh": "你可以通过创建选择所有 Pod 但不允许任何进入这些 Pod 的入站流量的 NetworkPolicy\n来为名字空间创建 \"default\" 隔离策略。\n\n{{< code_sample file=\"service/networking/network-policy-default-deny-ingress.yaml\" >}}"}
{"en": "This ensures that even pods that aren't selected by any other NetworkPolicy will still be isolated\nfor ingress. This policy does not affect isolation for egress from any pod.", "zh": "这确保即使没有被任何其他 NetworkPolicy 选择的 Pod 仍将被隔离以进行入口。\n此策略不影响任何 Pod 的出口隔离。"}
{"en": "### Allow all ingress traffic", "zh": "### 允许所有入站流量   {#allow-all-ingress-traffic}"}
{"en": "If you want to allow all incoming connections to all pods in a namespace, you can create a policy\nthat explicitly allows that.", "zh": "如果你想允许一个名字空间中所有 Pod 的所有入站连接，你可以创建一个明确允许的策略。\n\n{{< code_sample file=\"service/networking/network-policy-allow-all-ingress.yaml\" >}}"}
{"en": "With this policy in place, no additional policy or policies can cause any incoming connection to\nthose pods to be denied. This policy has no effect on isolation for egress from any pod.", "zh": "有了这个策略，任何额外的策略都不会导致到这些 Pod 的任何入站连接被拒绝。\n此策略对任何 Pod 的出口隔离没有影响。"}
{"en": "### Default deny all egress traffic\n\nYou can create a \"default\" egress isolation policy for a namespace by creating a NetworkPolicy\nthat selects all pods but does not allow any egress traffic from those pods.", "zh": "### 默认拒绝所有出站流量   {#default-deny-all-egress-traffic}\n\n你可以通过创建选择所有容器但不允许来自这些容器的任何出站流量的 NetworkPolicy\n来为名字空间创建 \"default\" 隔离策略。\n\n{{< code_sample file=\"service/networking/network-policy-default-deny-egress.yaml\" >}}"}
{"en": "This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed\negress traffic. This policy does not change the ingress isolation behavior of any pod.", "zh": "此策略可以确保即使没有被其他任何 NetworkPolicy 选择的 Pod 也不会被允许流出流量。\n此策略不会更改任何 Pod 的入站流量隔离行为。"}
{"en": "### Allow all egress traffic", "zh": "### 允许所有出站流量   {#allow-all-egress-traffic}"}
{"en": "If you want to allow all connections from all pods in a namespace, you can create a policy that\nexplicitly allows all outgoing connections from pods in that namespace.", "zh": "如果要允许来自名字空间中所有 Pod 的所有连接，\n则可以创建一个明确允许来自该名字空间中 Pod 的所有出站连接的策略。\n\n{{< code_sample file=\"service/networking/network-policy-allow-all-egress.yaml\" >}}"}
{"en": "With this policy in place, no additional policy or policies can cause any outgoing connection from\nthose pods to be denied. This policy has no effect on isolation for ingress to any pod.", "zh": "有了这个策略，任何额外的策略都不会导致来自这些 Pod 的任何出站连接被拒绝。\n此策略对进入任何 Pod 的隔离没有影响。"}
{"en": "### Default deny all ingress and all egress traffic\n\nYou can create a \"default\" policy for a namespace which prevents all ingress AND egress traffic by\ncreating the following NetworkPolicy in that namespace.", "zh": "### 默认拒绝所有入站和所有出站流量   {#default-deny-all-ingress-and-all-egress-traffic}\n\n你可以为名字空间创建 \"default\" 策略，以通过在该名字空间中创建以下 NetworkPolicy\n来阻止所有入站和出站流量。\n\n{{< code_sample file=\"service/networking/network-policy-default-deny-all.yaml\" >}}"}
{"en": "This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed\ningress or egress traffic.", "zh": "此策略可以确保即使没有被其他任何 NetworkPolicy 选择的 Pod 也不会被允许入站或出站流量。"}
{"en": "## Network traffic filtering\n\nNetworkPolicy is defined for [layer 4](https://en.wikipedia.org/wiki/OSI_model#Layer_4:_Transport_layer)\nconnections (TCP, UDP, and optionally SCTP). For all the other protocols, the behaviour may vary\nacross network plugins.", "zh": "## 网络流量过滤   {#network-traffic-filtering}\n\nNetworkPolicy 是为[第 4 层](https://zh.wikipedia.org/wiki/OSI%E6%A8%A1%E5%9E%8B#%E7%AC%AC4%E5%B1%A4_%E5%82%B3%E8%BC%B8%E5%B1%A4)连接\n（TCP、UDP 和可选的 SCTP）所定义的。对于所有其他协议，这种网络流量过滤的行为可能因网络插件而异。\n\n{{< note >}}"}
{"en": "You must be using a {{< glossary_tooltip text=\"CNI\" term_id=\"cni\" >}} plugin that supports SCTP\nprotocol NetworkPolicies.", "zh": "你必须使用支持 SCTP 协议 NetworkPolicy 的 {{< glossary_tooltip text=\"CNI\" term_id=\"cni\" >}} 插件。\n{{< /note >}}"}
{"en": "When a `deny all` network policy is defined, it is only guaranteed to deny TCP, UDP and SCTP\nconnections. For other protocols, such as ARP or ICMP, the behaviour is undefined.\nThe same applies to allow rules: when a specific pod is allowed as ingress source or egress destination,\nit is undefined what happens with (for example) ICMP packets. Protocols such as ICMP may be allowed by some\nnetwork plugins and denied by others.", "zh": "当 `deny all` 网络策略被定义时，此策略只能保证拒绝 TCP、UDP 和 SCTP 连接。\n对于 ARP 或 ICMP 这类其他协议，这种网络流量过滤行为是未定义的。\n相同的情况也适用于 allow 规则：当特定 Pod 被允许作为入口源或出口目的地时，\n对于（例如）ICMP 数据包会发生什么是未定义的。\nICMP 这类协议可能被某些网络插件所允许，而被另一些网络插件所拒绝。"}
{"en": "## Targeting a range of ports", "zh": "## 针对某个端口范围   {#targeting-a-range-of-ports}\n\n{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}"}
{"en": "When writing a NetworkPolicy, you can target a range of ports instead of a single port.\n\nThis is achievable with the usage of the `endPort` field, as the following example:", "zh": "在编写 NetworkPolicy 时，你可以针对一个端口范围而不是某个固定端口。\n\n这一目的可以通过使用 `endPort` 字段来实现，如下例所示：\n\n{{< code_sample file=\"service/networking/networkpolicy-multiport-egress.yaml\" >}}"}
{"en": "The above rule allows any Pod with label `role=db` on the namespace `default` to communicate\nwith any IP within the range `10.0.0.0/24` over TCP, provided that the target\nport is between the range 32000 and 32768.", "zh": "上面的规则允许名字空间 `default` 中所有带有标签 `role=db` 的 Pod 使用 TCP 协议与\n`10.0.0.0/24` 范围内的 IP 通信，只要目标端口介于 32000 和 32768 之间就可以。"}
{"en": "The following restrictions apply when using this field:\n\n* The `endPort` field must be equal to or greater than the `port` field.\n* `endPort` can only be defined if `port` is also defined.\n* Both ports must be numeric.", "zh": "使用此字段时存在以下限制：\n\n* `endPort` 字段必须等于或者大于 `port` 字段的值。\n* 只有在定义了 `port` 时才能定义 `endPort`。\n* 两个字段的设置值都只能是数字。\n\n{{< note >}}"}
{"en": "Your cluster must be using a {{< glossary_tooltip text=\"CNI\" term_id=\"cni\" >}} plugin that\nsupports the `endPort` field in NetworkPolicy specifications.\nIf your [network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)\ndoes not support the `endPort` field and you specify a NetworkPolicy with that,\nthe policy will be applied only for the single `port` field.", "zh": "你的集群所使用的 {{< glossary_tooltip text=\"CNI\" term_id=\"cni\" >}} 插件必须支持在\nNetworkPolicy 规约中使用 `endPort` 字段。\n如果你的[网络插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)不支持\n`endPort` 字段，而你指定了一个包含 `endPort` 字段的 NetworkPolicy，\n策略只对单个 `port` 字段生效。\n{{< /note >}}"}
{"en": "## Targeting multiple namespaces by label\n\nIn this scenario, your `Egress` NetworkPolicy targets more than one namespace using their\nlabel names. For this to work, you need to label the target namespaces. For example:", "zh": "## 按标签选择多个名字空间   {#targeting-multiple-namespaces-by-label}\n\n在这种情况下，你的 `Egress` NetworkPolicy 使用名字空间的标签名称来将多个名字空间作为其目标。\n为此，你需要为目标名字空间设置标签。例如：\n\n```shell\nkubectl label namespace frontend namespace=frontend\nkubectl label namespace backend namespace=backend\n```"}
{"en": "Add the labels under `namespaceSelector` in your NetworkPolicy document. For example:", "zh": "在 NetworkPolicy 文档中的 `namespaceSelector` 下添加标签。例如：\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: egress-namespaces\nspec:\n  podSelector:\n    matchLabels:\n      app: myapp\n  policyTypes:\n    - Egress\n  egress:\n    - to:\n      - namespaceSelector:\n          matchExpressions:\n            - key: namespace\n              operator: In\n              values: [\"frontend\", \"backend\"]\n```\n\n{{< note >}}"}
{"en": "It is not possible to directly specify the name of the namespaces in a NetworkPolicy.\nYou must use a `namespaceSelector` with `matchLabels` or `matchExpressions` to select the\nnamespaces based on their labels.", "zh": "你不可以在 NetworkPolicy 中直接指定名字空间的名称。\n你必须使用带有 `matchLabels` 或 `matchExpressions` 的 `namespaceSelector`\n来根据标签选择名字空间。\n{{< /note >}}"}
{"en": "## Targeting a Namespace by its name", "zh": "## 基于名字指向某名字空间   {#targeting-a-namespace-by-its-name}"}
{"en": "The Kubernetes control plane sets an immutable label `kubernetes.io/metadata.name` on all\nnamespaces, the value of the label is the namespace name.\n\nWhile NetworkPolicy cannot target a namespace by its name with some object field, you can use the\nstandardized label to target a specific namespace.", "zh": "Kubernetes 控制面会在所有名字空间上设置一个不可变更的标签\n`kubernetes.io/metadata.name`。该标签的值是名字空间的名称。\n\n如果 NetworkPolicy 无法在某些对象字段中指向某名字空间，\n你可以使用标准的标签方式来指向特定名字空间。"}
{"en": "## Pod lifecycle", "zh": "## Pod 生命周期   {#pod-lifecycle}\n\n{{< note >}}"}
{"en": "The following applies to clusters with a conformant networking plugin and a conformant implementation of\nNetworkPolicy.", "zh": "以下内容适用于使用了合规网络插件和 NetworkPolicy 合规实现的集群。\n{{< /note >}}"}
{"en": "When a new NetworkPolicy object is created, it may take some time for a network plugin\nto handle the new object. If a pod that is affected by a NetworkPolicy\nis created before the network plugin has completed NetworkPolicy handling,\nthat pod may be started unprotected, and isolation rules will be applied when\nthe NetworkPolicy handling is completed.", "zh": "当新的 NetworkPolicy 对象被创建时，网络插件可能需要一些时间来处理这个新对象。\n如果受到 NetworkPolicy 影响的 Pod 在网络插件完成 NetworkPolicy 处理之前就被创建了，\n那么该 Pod 可能会最初处于无保护状态，而在 NetworkPolicy 处理完成后被应用隔离规则。"}
{"en": "Once the NetworkPolicy is handled by a network plugin,\n\n1. All newly created pods affected by a given NetworkPolicy will be isolated before \n   they are started.\n   Implementations of NetworkPolicy must ensure that filtering is effective throughout\n   the Pod lifecycle, even from the very first instant that any container in that Pod is started.\n   Because they are applied at Pod level, NetworkPolicies apply equally to init containers,\n   sidecar containers, and regular containers.", "zh": "一旦 NetworkPolicy 被网络插件处理，\n\n1. 所有受给定 NetworkPolicy 影响的新建 Pod 都将在启动前被隔离。\n   NetworkPolicy 的实现必须确保过滤规则在整个 Pod 生命周期内是有效的，\n   这个生命周期要从该 Pod 的任何容器启动的第一刻算起。\n   因为 NetworkPolicy 在 Pod 层面被应用，所以 NetworkPolicy 同样适用于 Init 容器、边车容器和常规容器。"}
{"en": "2. Allow rules will be applied eventually after the isolation rules (or may be applied at the same time).\n   In the worst case, a newly created pod may have no network connectivity at all when it is first started, if\n   isolation rules were already applied, but no allow rules were applied yet.\n\nEvery created NetworkPolicy will be handled by a network plugin eventually, but there is no\nway to tell from the Kubernetes API when exactly that happens.", "zh": "2. Allow 规则最终将在隔离规则之后被应用（或者可能同时被应用）。\n   在最糟的情况下，如果隔离规则已被应用，但 allow 规则尚未被应用，\n   那么新建的 Pod 在初始启动时可能根本没有网络连接。\n\n用户所创建的每个 NetworkPolicy 最终都会被网络插件处理，但无法使用 Kubernetes API\n来获知确切的处理时间。"}
{"en": "Therefore, pods must be resilient against being started up with different network\nconnectivity than expected. If you need to make sure the pod can reach certain destinations\nbefore being started, you can use an [init container](/docs/concepts/workloads/pods/init-containers/)\nto wait for those destinations to be reachable before kubelet starts the app containers.", "zh": "因此，若 Pod 启动时使用非预期的网络连接，它必须保持稳定。\n如果你需要确保 Pod 在启动之前能够访问特定的目标，可以使用\n[Init 容器](/zh-cn/docs/concepts/workloads/pods/init-containers/)在\nkubelet 启动应用容器之前等待这些目的地变得可达。"}
{"en": "Every NetworkPolicy will be applied to all selected pods eventually.\nBecause the network plugin may implement NetworkPolicy in a distributed manner,\nit is possible that pods may see a slightly inconsistent view of network policies\nwhen the pod is first created, or when pods or policies change.\nFor example, a newly-created pod that is supposed to be able to reach both Pod A\non Node 1 and Pod B on Node 2 may find that it can reach Pod A immediately,\nbut cannot reach Pod B until a few seconds later.", "zh": "每个 NetworkPolicy 最终都会被应用到所选定的所有 Pod 之上。\n由于网络插件可能以分布式的方式实现 NetworkPolicy，所以当 Pod 被首次创建时或当\nPod 或策略发生变化时，Pod 可能会看到稍微不一致的网络策略视图。\n例如，新建的 Pod 本来应能立即访问 Node 1 上的 Pod A 和 Node 2 上的 Pod B，\n但可能你会发现新建的 Pod 可以立即访问 Pod A，但要在几秒后才能访问 Pod B。"}
{"en": "## NetworkPolicy and `hostNetwork` pods\n\nNetworkPolicy behaviour for `hostNetwork` pods is undefined, but it should be limited to 2 possibilities:", "zh": "## NetworkPolicy 和 `hostNetwork` Pod    {#networkpolicy-and-hostnetwork-pods}\n\n针对 `hostNetwork` Pod 的 NetworkPolicy 行为是未定义的，但应限制为以下两种可能："}
{"en": "- The network plugin can distinguish `hostNetwork` pod traffic from all other traffic\n  (including being able to distinguish traffic from different `hostNetwork` pods on\n  the same node), and will apply NetworkPolicy to `hostNetwork` pods just like it does\n  to pod-network pods.", "zh": "- 网络插件可以从所有其他流量中辨别出 `hostNetwork` Pod 流量\n  （包括能够从同一节点上的不同 `hostNetwork` Pod 中辨别流量），\n  网络插件还可以像处理 Pod 网络流量一样，对 `hostNetwork` Pod 应用 NetworkPolicy。"}
{"en": "- The network plugin cannot properly distinguish `hostNetwork` pod traffic,\n  and so it ignores `hostNetwork` pods when matching `podSelector` and `namespaceSelector`.\n  Traffic to/from `hostNetwork` pods is treated the same as all other traffic to/from the node IP.\n  (This is the most common implementation.)", "zh": "- 网络插件无法正确辨别 `hostNetwork` Pod 流量，因此在匹配 `podSelector` 和 `namespaceSelector`\n  时会忽略 `hostNetwork` Pod。流向/来自 `hostNetwork` Pod 的流量的处理方式与流向/来自节点 IP\n  的所有其他流量一样。（这是最常见的实现方式。）"}
{"en": "This applies when", "zh": "这适用于以下情形："}
{"en": "1. a `hostNetwork` pod is selected by `spec.podSelector`.", "zh": "1. `hostNetwork` Pod 被 `spec.podSelector` 选中。\n   \n   ```yaml\n     ...\n     spec:\n       podSelector:\n         matchLabels:\n           role: client\n     ...\n   ```"}
{"en": "1. a `hostNetwork` pod is selected by a `podSelector` or `namespaceSelector` in an `ingress` or `egress` rule.", "zh": "2. `hostNetwork` Pod 在 `ingress` 或 `egress` 规则中被 `podSelector` 或\n   `namespaceSelector` 选中。\n\n   ```yaml\n     ...\n     ingress:\n       - from:\n         - podSelector:\n             matchLabels:\n               role: client\n     ...\n   ```"}
{"en": "At the same time, since `hostNetwork` pods have the same IP addresses as the nodes they reside on,\ntheir connections will be treated as node connections. For example, you can allow traffic\nfrom a `hostNetwork` Pod using an `ipBlock` rule.", "zh": "同时，由于 `hostNetwork` Pod 具有与其所在节点相同的 IP 地址，所以它们的连接将被视为节点连接。\n例如，你可以使用 `ipBlock` 规则允许来自 `hostNetwork` Pod 的流量。"}
{"en": "## What you can't do with network policies (at least, not yet)\n\nAs of Kubernetes {{< skew currentVersion >}}, the following functionality does not exist in the\nNetworkPolicy API, but you might be able to implement workarounds using Operating System\ncomponents (such as SELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingress\ncontrollers, Service Mesh implementations) or admission controllers. In case you are new to\nnetwork security in Kubernetes, its worth noting that the following User Stories cannot (yet) be\nimplemented using the NetworkPolicy API.", "zh": "## 通过网络策略（至少目前还）无法完成的工作   {#what-you-can-t-do-with-network-policies-at-least-not-yet}\n\n到 Kubernetes {{< skew currentVersion >}} 为止，NetworkPolicy API 还不支持以下功能，\n不过你可能可以使用操作系统组件（如 SELinux、OpenVSwitch、IPTables 等等）\n或者第七层技术（Ingress 控制器、服务网格实现）或准入控制器来实现一些替代方案。\n如果你对 Kubernetes 中的网络安全性还不太了解，了解使用 NetworkPolicy API\n还无法实现下面的用户场景是很值得的。"}
{"en": "- Forcing internal cluster traffic to go through a common gateway (this might be best served with\n  a service mesh or other proxy).\n- Anything TLS related (use a service mesh or ingress controller for this).\n- Node specific policies (you can use CIDR notation for these, but you cannot target nodes by\n  their Kubernetes identities specifically).\n- Targeting of services by name (you can, however, target pods or namespaces by their\n  {{< glossary_tooltip text=\"labels\" term_id=\"label\" >}}, which is often a viable workaround).\n- Creation or management of \"Policy requests\" that are fulfilled by a third party.", "zh": "- 强制集群内部流量经过某公用网关（这种场景最好通过服务网格或其他代理来实现）；\n- 与 TLS 相关的场景（考虑使用服务网格或者 Ingress 控制器）；\n- 特定于节点的策略（你可以使用 CIDR 来表达这一需求不过你无法使用节点在\n  Kubernetes 中的其他标识信息来辩识目标节点）；\n- 基于名字来选择服务（不过，你可以使用 {{< glossary_tooltip text=\"标签\" term_id=\"label\" >}}\n  来选择目标 Pod 或名字空间，这也通常是一种可靠的替代方案）；\n- 创建或管理由第三方来实际完成的“策略请求”；"}
{"en": "- Default policies which are applied to all namespaces or pods (there are some third party\n  Kubernetes distributions and projects which can do this).\n- Advanced policy querying and reachability tooling.\n- The ability to log network security events (for example connections that are blocked or accepted).\n- The ability to explicitly deny policies (currently the model for NetworkPolicies are deny by\n  default, with only the ability to add allow rules).\n- The ability to prevent loopback or incoming host traffic (Pods cannot currently block localhost\n  access, nor do they have the ability to block access from their resident node).", "zh": "- 实现适用于所有名字空间或 Pod 的默认策略（某些第三方 Kubernetes 发行版本或项目可以做到这点）；\n- 高级的策略查询或者可达性相关工具；\n- 生成网络安全事件日志的能力（例如，被阻塞或接收的连接请求）；\n- 显式地拒绝策略的能力（目前，NetworkPolicy 的模型默认采用拒绝操作，\n  其唯一的能力是添加允许策略）；\n- 禁止本地回路或指向宿主的网络流量（Pod 目前无法阻塞 localhost 访问，\n  它们也无法禁止来自所在节点的访问请求）。"}
{"en": "## NetworkPolicy's impact on existing connections\n\nWhen the set of NetworkPolicies that applies to an existing connection changes - this could happen\neither due to a change in NetworkPolicies or if the relevant labels of the namespaces/pods selected by the\npolicy (both subject and peers) are changed in the middle of an existing connection - it is\nimplementation defined as to whether the change will take effect for that existing connection or not.\nExample: A policy is created that leads to denying a previously allowed connection, the underlying network plugin\nimplementation is responsible for defining if that new policy will close the existing connections or not.\nIt is recommended not to modify policies/pods/namespaces in ways that might affect existing connections.", "zh": "## NetworkPolicy 对现有连接的影响   {#networkpolicys-impact-on-existing-connections}\n\n当应用到现有连接的 NetworkPolicy 集合发生变化时，这可能是由于 NetworkPolicy\n发生变化或者在现有连接过程中（主体和对等方）策略所选择的名字空间或 Pod 的相关标签发生变化，\n此时是否对该现有连接生效是由实现定义的。例如：某个策略的创建导致之前允许的连接被拒绝，\n底层网络插件的实现负责定义这个新策略是否会关闭现有的连接。\n建议不要以可能影响现有连接的方式修改策略、Pod 或名字空间。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- See the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/)\n  walkthrough for further examples.\n- See more [recipes](https://github.com/ahmetb/kubernetes-network-policy-recipes) for common\n  scenarios enabled by the NetworkPolicy resource.", "zh": "- 有关更多示例，请参阅[声明网络策略](/zh-cn/docs/tasks/administer-cluster/declare-network-policy/)演练。\n- 有关 NetworkPolicy 资源所支持的常见场景的更多信息，\n  请参见[此指南](https://github.com/ahmetb/kubernetes-network-policy-recipes)。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}"}
{"en": "Kubernetes' _EndpointSlice_ API provides a way to track network endpoints\nwithin a Kubernetes cluster. EndpointSlices offer a more scalable and extensible\nalternative to [Endpoints](/docs/concepts/services-networking/service/#endpoints).", "zh": "Kubernetes 的 _EndpointSlice_ API 提供了一种简单的方法来跟踪\nKubernetes 集群中的网络端点（network endpoints）。EndpointSlices 为\n[Endpoints](/zh-cn/docs/concepts/services-networking/service/#endpoints)\n提供了一种可扩缩和可拓展的替代方案。"}
{"en": "## EndpointSlice API {#endpointslice-resource}\n\nIn Kubernetes, an EndpointSlice contains references to a set of network\nendpoints. The control plane automatically creates EndpointSlices\nfor any Kubernetes Service that has a {{< glossary_tooltip text=\"selector\"\nterm_id=\"selector\" >}} specified. These EndpointSlices include\nreferences to all the Pods that match the Service selector. EndpointSlices group\nnetwork endpoints together by unique combinations of protocol, port number, and\nService name.\nThe name of a EndpointSlice object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).\n\nAs an example, here's a sample EndpointSlice object, that's owned by the `example`\nKubernetes Service.", "zh": "## EndpointSlice API {#endpointslice-resource}\n\n在 Kubernetes 中，`EndpointSlice` 包含对一组网络端点的引用。\n控制面会自动为设置了{{< glossary_tooltip text=\"选择算符\" term_id=\"selector\" >}}的\nKubernetes Service 创建 EndpointSlice。\n这些 EndpointSlice 将包含对与 Service 选择算符匹配的所有 Pod 的引用。\nEndpointSlice 通过唯一的协议、端口号和 Service 名称将网络端点组织在一起。\nEndpointSlice 的名称必须是合法的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。\n\n例如，下面是 Kubernetes Service `example` 所拥有的 EndpointSlice 对象示例。\n\n```yaml\napiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\n  name: example-abc\n  labels:\n    kubernetes.io/service-name: example\naddressType: IPv4\nports:\n  - name: http\n    protocol: TCP\n    port: 80\nendpoints:\n  - addresses:\n      - \"10.1.2.3\"\n    conditions:\n      ready: true\n    hostname: pod-1\n    nodeName: node-1\n    zone: us-west2-a\n```"}
{"en": "By default, the control plane creates and manages EndpointSlices to have no\nmore than 100 endpoints each. You can configure this with the\n`--max-endpoints-per-slice`\n{{< glossary_tooltip text=\"kube-controller-manager\" term_id=\"kube-controller-manager\" >}}\nflag, up to a maximum of 1000.\n\nEndpointSlices can act as the source of truth for\n{{< glossary_tooltip term_id=\"kube-proxy\" text=\"kube-proxy\" >}} when it comes to\nhow to route internal traffic.", "zh": "默认情况下，控制面创建和管理的 EndpointSlice 将包含不超过 100 个端点。\n你可以使用 {{< glossary_tooltip text=\"kube-controller-manager\" term_id=\"kube-controller-manager\" >}}\n的 `--max-endpoints-per-slice` 标志设置此值，最大值为 1000。\n\n当涉及如何路由内部流量时，EndpointSlice 可以充当\n{{< glossary_tooltip term_id=\"kube-proxy\" text=\"kube-proxy\" >}}\n的决策依据。"}
{"en": "### Address types\n\nEndpointSlices support three address types:\n\n* IPv4\n* IPv6\n* FQDN (Fully Qualified Domain Name)\n\nEach `EndpointSlice` object represents a specific IP address type. If you have\na Service that is available via IPv4 and IPv6, there will be at least two\n`EndpointSlice` objects (one for IPv4, and one for IPv6).", "zh": "### 地址类型\n\nEndpointSlice 支持三种地址类型：\n\n* IPv4\n* IPv6\n* FQDN (完全合格的域名)\n\n每个 `EndpointSlice` 对象代表一个特定的 IP 地址类型。如果你有一个支持 IPv4 和 IPv6 的 Service，\n那么将至少有两个 `EndpointSlice` 对象（一个用于 IPv4，一个用于 IPv6）。"}
{"en": "### Conditions\n\nThe EndpointSlice API stores conditions about endpoints that may be useful for consumers.\nThe three conditions are `ready`, `serving`, and `terminating`.", "zh": "### 状况\n\nEndpointSlice API 存储了可能对使用者有用的、有关端点的状况。\n这三个状况分别是 `ready`、`serving` 和 `terminating`。"}
{"en": "#### Ready\n\n`ready` is a condition that maps to a Pod's `Ready` condition. A running Pod with the `Ready`\ncondition set to `True` should have this EndpointSlice condition also set to `true`. For\ncompatibility reasons, `ready` is NEVER `true` when a Pod is terminating. Consumers should refer\nto the `serving` condition to inspect the readiness of terminating Pods. The only exception to\nthis rule is for Services with `spec.publishNotReadyAddresses` set to `true`. Endpoints for these\nServices will always have the `ready` condition set to `true`.", "zh": "#### Ready（就绪）\n\n`ready` 状况是映射 Pod 的 `Ready` 状况的。\n对于处于运行中的 Pod，它的 `Ready` 状况被设置为 `True`，应该将此 EndpointSlice 状况也设置为 `true`。\n出于兼容性原因，当 Pod 处于终止过程中，`ready` 永远不会为 `true`。\n消费者应参考 `serving` 状况来检查处于终止中的 Pod 的就绪情况。\n该规则的唯一例外是将 `spec.publishNotReadyAddresses` 设置为 `true` 的 Service。\n这些 Service 的端点将始终将 `ready` 状况设置为 `true`。"}
{"en": "#### Serving\n\n{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}\n\nThe `serving` condition is almost identical to the `ready` condition. The difference is that\nconsumers of the EndpointSlice API should check the `serving` condition if they care about pod readiness while\nthe pod is also terminating.", "zh": "#### Serving（服务中）\n\n{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}\n\n`serving` 状况几乎与 `ready` 状况相同，不同之处在于它不考虑终止状态。\n如果 EndpointSlice API 的使用者关心 Pod 终止时的就绪情况，就应检查 `serving` 状况。\n\n{{< note >}}"}
{"en": "Although `serving` is almost identical to `ready`, it was added to prevent breaking the existing meaning\nof `ready`. It may be unexpected for existing clients if `ready` could be `true` for terminating\nendpoints, since historically terminating endpoints were never included in the Endpoints or\nEndpointSlice API to begin with. For this reason, `ready` is _always_ `false` for terminating\nendpoints, and a new condition `serving` was added in v1.20 so that clients can track readiness\nfor terminating pods independent of the existing semantics for `ready`.", "zh": "尽管 `serving` 与 `ready` 几乎相同，但是它是为防止破坏 `ready` 的现有含义而增加的。\n如果对于处于终止中的端点，`ready` 可能是 `true`，那么对于现有的客户端来说可能是有些意外的，\n因为从始至终，Endpoints 或 EndpointSlice API 从未包含处于终止中的端点。\n出于这个原因，`ready` 对于处于终止中的端点 **总是** `false`，\n并且在 v1.20 中添加了新的状况 `serving`，以便客户端可以独立于 `ready`\n的现有语义来跟踪处于终止中的 Pod 的就绪情况。\n{{< /note >}}"}
{"en": "#### Terminating\n\n{{< feature-state for_k8s_version=\"v1.22\" state=\"beta\" >}}\n\n`Terminating` is a condition that indicates whether an endpoint is terminating.\nFor pods, this is any pod that has a deletion timestamp set.", "zh": "#### Terminating（终止中）\n\n{{< feature-state for_k8s_version=\"v1.22\" state=\"beta\" >}}\n\n`Terminating` 是表示端点是否处于终止中的状况。\n对于 Pod 来说，这是设置了删除时间戳的 Pod。"}
{"en": "### Topology information {#topology}\n\nEach endpoint within an EndpointSlice can contain relevant topology information.\nThe topology information includes the location of the endpoint and information\nabout the corresponding Node and zone. These are available in the following\nper endpoint fields on EndpointSlices:", "zh": "### 拓扑信息   {#topology}\n\nEndpointSlice 中的每个端点都可以包含一定的拓扑信息。\n拓扑信息包括端点的位置，对应节点、可用区的信息。\n这些信息体现为 EndpointSlices 的如下端点字段："}
{"en": "* `nodeName` - The name of the Node this endpoint is on.\n* `zone` - The zone this endpoint is in.", "zh": "* `nodeName` - 端点所在的 Node 名称；\n* `zone` - 端点所处的可用区。\n\n{{< note >}}"}
{"en": "In the v1 API, the per endpoint `topology` was effectively removed in favor of\nthe dedicated fields `nodeName` and `zone`.", "zh": "在 v1 API 中，逐个端点设置的 `topology` 实际上被去除，\n以鼓励使用专用的字段 `nodeName` 和 `zone`。"}
{"en": "Setting arbitrary topology fields on the `endpoint` field of an `EndpointSlice`\nresource has been deprecated and is not supported in the v1 API.\nInstead, the v1 API supports setting individual `nodeName` and `zone` fields.\nThese fields are automatically translated between API versions. For example, the\nvalue of the `\"topology.kubernetes.io/zone\"` key in the `topology` field in\nthe v1beta1 API is accessible as the `zone` field in the v1 API.", "zh": "对 `EndpointSlice` 对象的 `endpoint` 字段设置任意的拓扑结构信息这一操作已被废弃，\n不再被 v1 API 所支持。取而代之的是 v1 API 所支持的 `nodeName` 和 `zone`\n这些独立的字段。这些字段可以在不同的 API 版本之间自动完成转译。\n例如，v1beta1 API 中 `topology` 字段的 `topology.kubernetes.io/zone`\n取值可以在 v1 API 中通过 `zone` 字段访问。\n{{< /note >}}"}
{"en": "### Management\n\nMost often, the control plane (specifically, the endpoint slice\n{{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}) creates and\nmanages EndpointSlice objects. There are a variety of other use cases for\nEndpointSlices, such as service mesh implementations, that could result in other\nentities or controllers managing additional sets of EndpointSlices.", "zh": "### 管理   {#management}\n\n通常，控制面（尤其是端点切片的{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}）\n会创建和管理 EndpointSlice 对象。EndpointSlice 对象还有一些其他使用场景，\n例如作为服务网格（Service Mesh）的实现。\n这些场景都会导致有其他实体或者控制器负责管理额外的 EndpointSlice 集合。"}
{"en": "To ensure that multiple entities can manage EndpointSlices without interfering\nwith each other, Kubernetes defines the\n{{< glossary_tooltip term_id=\"label\" text=\"label\" >}}\n`endpointslice.kubernetes.io/managed-by`, which indicates the entity managing\nan EndpointSlice.\nThe endpoint slice controller sets `endpointslice-controller.k8s.io` as the value\nfor this label on all EndpointSlices it manages. Other entities managing\nEndpointSlices should also set a unique value for this label.", "zh": "为了确保多个实体可以管理 EndpointSlice 而且不会相互产生干扰，\nKubernetes 定义了{{< glossary_tooltip term_id=\"label\" text=\"标签\" >}}\n`endpointslice.kubernetes.io/managed-by`，用来标明哪个实体在管理某个 EndpointSlice。\n端点切片控制器会在自己所管理的所有 EndpointSlice 上将该标签值设置为\n`endpointslice-controller.k8s.io`。\n管理 EndpointSlice 的其他实体也应该为此标签设置一个唯一值。"}
{"en": "### Ownership\n\nIn most use cases, EndpointSlices are owned by the Service that the endpoint\nslice object tracks endpoints for. This ownership is indicated by an owner\nreference on each EndpointSlice as well as a `kubernetes.io/service-name`\nlabel that enables simple lookups of all EndpointSlices belonging to a Service.", "zh": "### 属主关系   {#ownership}\n\n在大多数场合下，EndpointSlice 都由某个 Service 所有，\n（因为）该端点切片正是为该服务跟踪记录其端点。这一属主关系是通过为每个 EndpointSlice\n设置一个属主（owner）引用，同时设置 `kubernetes.io/service-name` 标签来标明的，\n目的是方便查找隶属于某 Service 的所有 EndpointSlice。"}
{"en": "### EndpointSlice mirroring\n\nIn some cases, applications create custom Endpoints resources. To ensure that\nthese applications do not need to concurrently write to both Endpoints and\nEndpointSlice resources, the cluster's control plane mirrors most Endpoints\nresources to corresponding EndpointSlices.", "zh": "### EndpointSlice 镜像    {#endpointslice-mirroring}\n\n在某些场合，应用会创建定制的 Endpoints 资源。为了保证这些应用不需要并发的更改\nEndpoints 和 EndpointSlice 资源，集群的控制面将大多数 Endpoints\n映射到对应的 EndpointSlice 之上。"}
{"en": "The control plane mirrors Endpoints resources unless:\n\n* the Endpoints resource has a `endpointslice.kubernetes.io/skip-mirror` label\n  set to `true`.\n* the Endpoints resource has a `control-plane.alpha.kubernetes.io/leader`\n  annotation.\n* the corresponding Service resource does not exist.\n* the corresponding Service resource has a non-nil selector.", "zh": "控制面对 Endpoints 资源进行映射的例外情况有：\n\n* Endpoints 资源上标签 `endpointslice.kubernetes.io/skip-mirror` 值为 `true`。\n* Endpoints 资源包含标签 `control-plane.alpha.kubernetes.io/leader`。\n* 对应的 Service 资源不存在。\n* 对应的 Service 的选择算符不为空。"}
{"en": "Individual Endpoints resources may translate into multiple EndpointSlices. This\nwill occur if an Endpoints resource has multiple subsets or includes endpoints\nwith multiple IP families (IPv4 and IPv6). A maximum of 1000 addresses per\nsubset will be mirrored to EndpointSlices.", "zh": "每个 Endpoints 资源可能会被转译到多个 EndpointSlices 中去。\n当 Endpoints 资源中包含多个子网或者包含多个 IP 协议族（IPv4 和 IPv6）的端点时，\n就有可能发生这种状况。\n每个子网最多有 1000 个地址会被镜像到 EndpointSlice 中。"}
{"en": "### Distribution of EndpointSlices\n\nEach EndpointSlice has a set of ports that applies to all endpoints within the\nresource. When named ports are used for a Service, Pods may end up with\ndifferent target port numbers for the same named port, requiring different\nEndpointSlices. This is similar to the logic behind how subsets are grouped\nwith Endpoints.", "zh": "### EndpointSlices 的分布问题  {#distribution-of-endpointslices}\n\n每个 EndpointSlice 都有一组端口值，适用于资源内的所有端点。\n当为 Service 使用命名端口时，Pod 可能会就同一命名端口获得不同的端口号，\n因而需要不同的 EndpointSlice。这有点像 Endpoints 用来对子网进行分组的逻辑。"}
{"en": "The control plane tries to fill EndpointSlices as full as possible, but does not\nactively rebalance them. The logic is fairly straightforward:\n\n1. Iterate through existing EndpointSlices, remove endpoints that are no longer\n   desired and update matching endpoints that have changed.\n2. Iterate through EndpointSlices that have been modified in the first step and\n   fill them up with any new endpoints needed.\n3. If there's still new endpoints left to add, try to fit them into a previously\n   unchanged slice and/or create new ones.", "zh": "控制面尝试尽量将 EndpointSlice 填满，不过不会主动地在若干 EndpointSlice\n之间执行再平衡操作。这里的逻辑也是相对直接的：\n\n1. 列举所有现有的 EndpointSlices，移除那些不再需要的端点并更新那些已经变化的端点。\n2. 列举所有在第一步中被更改过的 EndpointSlices，用新增加的端点将其填满。\n3. 如果还有新的端点未被添加进去，尝试将这些端点添加到之前未更改的切片中，\n   或者创建新切片。"}
{"en": "Importantly, the third step prioritizes limiting EndpointSlice updates over a\nperfectly full distribution of EndpointSlices. As an example, if there are 10\nnew endpoints to add and 2 EndpointSlices with room for 5 more endpoints each,\nthis approach will create a new EndpointSlice instead of filling up the 2\nexisting EndpointSlices. In other words, a single EndpointSlice creation is\npreferable to multiple EndpointSlice updates.", "zh": "这里比较重要的是，与在 EndpointSlice 之间完成最佳的分布相比，第三步中更看重限制\nEndpointSlice 更新的操作次数。例如，如果有 10 个端点待添加，有两个 EndpointSlice\n中各有 5 个空位，上述方法会创建一个新的 EndpointSlice 而不是将现有的两个\nEndpointSlice 都填满。换言之，与执行多个 EndpointSlice 更新操作相比较，\n方法会优先考虑执行一个 EndpointSlice 创建操作。"}
{"en": "With kube-proxy running on each Node and watching EndpointSlices, every change\nto an EndpointSlice becomes relatively expensive since it will be transmitted to\nevery Node in the cluster. This approach is intended to limit the number of\nchanges that need to be sent to every Node, even if it may result with multiple\nEndpointSlices that are not full.", "zh": "由于 kube-proxy 在每个节点上运行并监视 EndpointSlice 状态，EndpointSlice\n的每次变更都变得相对代价较高，因为这些状态变化要传递到集群中每个节点上。\n这一方法尝试限制要发送到所有节点上的变更消息个数，即使这样做可能会导致有多个\nEndpointSlice 没有被填满。"}
{"en": "In practice, this less than ideal distribution should be rare. Most changes\nprocessed by the EndpointSlice controller will be small enough to fit in an\nexisting EndpointSlice, and if not, a new EndpointSlice is likely going to be\nnecessary soon anyway. Rolling updates of Deployments also provide a natural\nrepacking of EndpointSlices with all Pods and their corresponding endpoints\ngetting replaced.", "zh": "在实践中，上面这种并非最理想的分布是很少出现的。大多数被 EndpointSlice\n控制器处理的变更都是足够小的，可以添加到某已有 EndpointSlice 中去的。\n并且，假使无法添加到已有的切片中，不管怎样都很快就会创建一个新的\nEndpointSlice 对象。Deployment 的滚动更新为重新为 EndpointSlice\n打包提供了一个自然的机会，所有 Pod 及其对应的端点在这一期间都会被替换掉。"}
{"en": "### Duplicate endpoints\n\nDue to the nature of EndpointSlice changes, endpoints may be represented in more\nthan one EndpointSlice at the same time. This naturally occurs as changes to\ndifferent EndpointSlice objects can arrive at the Kubernetes client watch / cache\nat different times.", "zh": "### 重复的端点   {#duplicate-endpoints}\n\n由于 EndpointSlice 变化的自身特点，端点可能会同时出现在不止一个 EndpointSlice\n中。鉴于不同的 EndpointSlice 对象在不同时刻到达 Kubernetes 的监视/缓存中，\n这种情况的出现是很自然的。\n\n{{< note >}}"}
{"en": "Clients of the EndpointSlice API must iterate through all the existing EndpointSlices\nassociated to a Service and build a complete list of unique network endpoints. It is\nimportant to mention that endpoints may be duplicated in different EndpointSlices.\n\nYou can find a reference implementation for how to perform this endpoint aggregation\nand deduplication as part of the `EndpointSliceCache` code within `kube-proxy`.", "zh": "EndpointSlice API 的客户端必须遍历与 Service 关联的所有现有 EndpointSlices，\n并构建唯一网络端点的完整列表。值得一提的是端点可能在不同的 EndpointSlices 中重复。\n\n你可以在 `kube-proxy` 中的 `EndpointSliceCache` 代码中找到有关如何执行此端点聚合和重复数据删除的参考实现。\n{{< /note >}}"}
{"en": "## Comparison with Endpoints {#motivation}\n\nThe original Endpoints API provided a simple and straightforward way of\ntracking network endpoints in Kubernetes. As Kubernetes clusters\nand {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}} grew to handle\nmore traffic and to send more traffic to more backend Pods, the\nlimitations of that original API became more visible.\nMost notably, those included challenges with scaling to larger numbers of\nnetwork endpoints.", "zh": "## 与 Endpoints 的比较 {#motivation}\n原来的 Endpoints API 提供了在 Kubernetes 中跟踪网络端点的一种简单而直接的方法。随着 Kubernetes\n集群和{{< glossary_tooltip text=\"服务\" term_id=\"service\" >}}逐渐开始为更多的后端 Pod 处理和发送请求，\n原来的 API 的局限性变得越来越明显。最明显的是那些因为要处理大量网络端点而带来的挑战。"}
{"en": "Since all network endpoints for a Service were stored in a single Endpoints\nobject, those Endpoints objects could get quite large. For Services that stayed\nstable (the same set of endpoints over a long period of time) the impact was\nless noticeable; even then, some use cases of Kubernetes weren't well served.", "zh": "由于任一 Service 的所有网络端点都保存在同一个 Endpoints 对象中，这些 Endpoints\n对象可能变得非常巨大。对于保持稳定的服务（长时间使用同一组端点），影响不太明显；\n即便如此，Kubernetes 的一些使用场景也没有得到很好的服务。"}
{"en": "When a Service had a lot of backend endpoints and the workload was either\n scaling frequently, or rolling out new changes frequently, each update to\nthe single Endpoints object for that Service meant a lot of traffic between\nKubernetes cluster components (within the control plane, and also between\nnodes and the API server). This extra traffic also had a cost in terms of\nCPU use.", "zh": "当某 Service 存在很多后端端点并且该工作负载频繁扩缩或上线新更改时，对该 Service 的单个 Endpoints\n对象的每次更新都意味着（在控制平面内以及在节点和 API 服务器之间）Kubernetes 集群组件之间会出现大量流量。\n这种额外的流量在 CPU 使用方面也有开销。"}
{"en": "With EndpointSlices, adding or removing a single Pod triggers the same _number_\nof updates to clients that are watching for changes, but the size of those\nupdate message is much smaller at large scale.", "zh": "使用 EndpointSlices 时，添加或移除单个 Pod 对于正监视变更的客户端会触发相同数量的更新，\n但这些更新消息的大小在大规模场景下要小得多。"}
{"en": "EndpointSlices also enabled innovation around new features such dual-stack\nnetworking and topology-aware routing.", "zh": "EndpointSlices 还支持围绕双栈网络和拓扑感知路由等新功能的创新。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Follow the [Connecting Applications with Services](/docs/tutorials/services/connect-applications-service/) tutorial\n* Read the [API reference](/docs/reference/kubernetes-api/service-resources/endpoint-slice-v1/) for the EndpointSlice API\n* Read the [API reference](/docs/reference/kubernetes-api/service-resources/endpoints-v1/) for the Endpoints API", "zh": "* 遵循[使用 Service 连接到应用](/zh-cn/docs/tutorials/services/connect-applications-service/)教程\n* 阅读 EndpointSlice API 的 [API 参考](/zh-cn/docs/reference/kubernetes-api/service-resources/endpoint-slice-v1/)\n* 阅读 Endpoints API 的 [API 参考](/zh-cn/docs/reference/kubernetes-api/service-resources/endpoints-v1/)"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}"}
{"en": "_Service Internal Traffic Policy_ enables internal traffic restrictions to only route\ninternal traffic to endpoints within the node the traffic originated from. The\n\"internal\" traffic here refers to traffic originated from Pods in the current\ncluster. This can help to reduce costs and improve performance.", "zh": "**服务内部流量策略**开启了内部流量限制，将内部流量只路由到发起方所处节点内的服务端点。\n这里的”内部“流量指当前集群中的 Pod 所发起的流量。\n这种机制有助于节省开销，提升效率。"}
{"en": "## Using Service Internal Traffic Policy", "zh": "## 使用服务内部流量策略 {#using-service-internal-traffic-policy}"}
{"en": "You can enable the internal-only traffic policy for a\n{{< glossary_tooltip text=\"Service\" term_id=\"service\" >}}, by setting its\n`.spec.internalTrafficPolicy` to `Local`. This tells kube-proxy to only use node local\nendpoints for cluster internal traffic.", "zh": "你可以通过将 {{< glossary_tooltip text=\"Service\" term_id=\"service\" >}} 的\n`.spec.internalTrafficPolicy` 项设置为 `Local`，\n来为它指定一个内部专用的流量策略。\n此设置就相当于告诉 kube-proxy 对于集群内部流量只能使用节点本地的服务端口。"}
{"en": "For pods on nodes with no endpoints for a given Service, the Service\nbehaves as if it has zero endpoints (for Pods on this node) even if the service\ndoes have endpoints on other nodes.", "zh": "{{< note >}}\n如果某节点上的 Pod 均不提供指定 Service 的服务端点，\n即使该 Service 在其他节点上有可用的服务端点，\nService 的行为看起来也像是它只有 0 个服务端点（只针对此节点上的 Pod）。\n{{< /note >}}"}
{"en": "The following example shows what a Service looks like when you set\n`.spec.internalTrafficPolicy` to `Local`:", "zh": "以下示例展示了把 Service 的 `.spec.internalTrafficPolicy` 项设为 `Local` 时，\nService 的样子：\n\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n  internalTrafficPolicy: Local\n```"}
{"en": "## How it works", "zh": "## 工作原理 {#how-it-works}"}
{"en": "The kube-proxy filters the endpoints it routes to based on the\n`spec.internalTrafficPolicy` setting. When it's set to `Local`, only node local\nendpoints are considered. When it's `Cluster` (the default), or is not set,\nKubernetes considers all endpoints.", "zh": "kube-proxy 基于 `spec.internalTrafficPolicy` 的设置来过滤路由的目标服务端点。\n当它的值设为 `Local` 时，只会选择节点本地的服务端点。\n当它的值设为 `Cluster` 或缺省时，Kubernetes 会选择所有的服务端点。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about [Topology Aware Routing](/docs/concepts/services-networking/topology-aware-routing)\n* Read about [Service External Traffic Policy](/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip)\n* Follow the [Connecting Applications with Services](/docs/tutorials/services/connect-applications-service/) tutorial", "zh": "* 请阅读[拓扑感知路由](/zh-cn/docs/concepts/services-networking/topology-aware-routing/)\n* 请阅读 [Service 的外部流量策略](/zh-cn/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip)\n* 遵循[使用 Service 连接到应用](/zh-cn/docs/tutorials/services/connect-applications-service/)教程"}
{"en": "In order for the Ingress resource to work, the cluster must have an ingress controller running. \n\nUnlike other types of controllers which run as part of the `kube-controller-manager` binary, Ingress controllers \nare not started automatically with a cluster. Use this page to choose the ingress controller implementation \nthat best fits your cluster.\n\nKubernetes as a project supports and maintains [AWS](https://github.com/kubernetes-sigs/aws-load-balancer-controller#readme), [GCE](https://git.k8s.io/ingress-gce/README.md#readme), and\n  [nginx](https://git.k8s.io/ingress-nginx/README.md#readme) ingress controllers.", "zh": "为了让 Ingress 资源工作，集群必须有一个正在运行的 Ingress 控制器。\n\n与作为 `kube-controller-manager` 可执行文件的一部分运行的其他类型的控制器不同，\nIngress 控制器不是随集群自动启动的。\n基于此页面，你可选择最适合你的集群的 ingress 控制器实现。\n\nKubernetes 作为一个项目，目前支持和维护\n[AWS](https://github.com/kubernetes-sigs/aws-load-balancer-controller#readme)、\n[GCE](https://git.k8s.io/ingress-gce/README.md#readme)\n和 [Nginx](https://git.k8s.io/ingress-nginx/README.md#readme) Ingress 控制器。"}
{"en": "## Additional controllers", "zh": "## 其他控制器  {#additional-controllers}\n\n{{% thirdparty-content %}}"}
{"en": "* [AKS Application Gateway Ingress Controller](https://docs.microsoft.com/azure/application-gateway/tutorial-ingress-controller-add-on-existing?toc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Faks%2Ftoc.json&bc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fbread%2Ftoc.json) is an ingress controller that configures the [Azure Application Gateway](https://docs.microsoft.com/azure/application-gateway/overview).\n* [Alibaba Cloud MSE Ingress](https://www.alibabacloud.com/help/en/mse/user-guide/overview-of-mse-ingress-gateways) is an ingress controller that configures the [Alibaba Cloud Native Gateway](https://www.alibabacloud.com/help/en/mse/product-overview/cloud-native-gateway-overview?spm=a2c63.p38356.0.0.20563003HJK9is), which is also the commercial version of [Higress](https://github.com/alibaba/higress).\n* [Apache APISIX ingress controller](https://github.com/apache/apisix-ingress-controller) is an [Apache APISIX](https://github.com/apache/apisix)-based ingress controller.\n* [Avi Kubernetes Operator](https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes) provides L4-L7 load-balancing using [VMware NSX Advanced Load Balancer](https://avinetworks.com/).", "zh": "* [AKS 应用程序网关 Ingress 控制器](https://docs.microsoft.com/zh-cn/azure/application-gateway/tutorial-ingress-controller-add-on-existing?toc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Faks%2Ftoc.json&bc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fbread%2Ftoc.json)\n  是一个配置 [Azure 应用程序网关](https://docs.microsoft.com/zh-cn/azure/application-gateway/overview)\n  的 Ingress 控制器。\n* [阿里云 MSE Ingress](https://www.alibabacloud.com/help/zh/mse/user-guide/overview-of-mse-ingress-gateways)\n  是一个 Ingress 控制器，它负责配置[阿里云原生网关](https://www.alibabacloud.com/help/en/mse/product-overview/cloud-native-gateway-overview?spm=a2c63.p38356.0.0.20563003HJK9is)，\n  也是 [Higress](https://github.com/alibaba/higress) 的商业版本。\n* [Apache APISIX Ingress 控制器](https://github.com/apache/apisix-ingress-controller)\n  是一个基于 [Apache APISIX 网关](https://github.com/apache/apisix) 的 Ingress 控制器。\n* [Avi Kubernetes Operator](https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes)\n  使用 [VMware NSX Advanced Load Balancer](https://avinetworks.com/)\n  提供第 4 到第 7 层的负载均衡。"}
{"en": "* [BFE Ingress Controller](https://github.com/bfenetworks/ingress-bfe) is a [BFE](https://www.bfe-networks.net)-based ingress controller.\n* [Cilium Ingress Controller](https://docs.cilium.io/en/stable/network/servicemesh/ingress/) is an ingress controller powered by [Cilium](https://cilium.io/).\n* The [Citrix ingress controller](https://github.com/citrix/citrix-k8s-ingress-controller#readme) works with\n  Citrix Application Delivery Controller.\n* [Contour](https://projectcontour.io/) is an [Envoy](https://www.envoyproxy.io/) based ingress controller.\n* [Emissary-Ingress](https://www.getambassador.io/products/api-gateway) API Gateway is an [Envoy](https://www.envoyproxy.io)-based ingress\n  controller.\n* [EnRoute](https://getenroute.io/) is an [Envoy](https://www.envoyproxy.io) based API gateway that can run as an ingress controller.\n* [Easegress IngressController](https://megaease.com/docs/easegress/04.cloud-native/4.1.kubernetes-ingress-controller/) is an [Easegress](https://megaease.com/easegress/) based API gateway that can run as an ingress controller.", "zh": "* [BFE Ingress 控制器](https://github.com/bfenetworks/ingress-bfe)是一个基于\n  [BFE](https://www.bfe-networks.net) 的 Ingress 控制器。\n* [Cilium Ingress 控制器](https://docs.cilium.io/en/stable/network/servicemesh/ingress/)是一个由\n  [Cilium](https://cilium.io/) 出品支持的 Ingress 控制器。\n* [Citrix Ingress 控制器](https://github.com/citrix/citrix-k8s-ingress-controller#readme)\n  可以用来与 Citrix Application Delivery Controller 一起使用。\n* [Contour](https://projectcontour.io/) 是一个基于 [Envoy](https://www.envoyproxy.io/)\n  的 Ingress 控制器。\n* [Emissary-Ingress](https://www.getambassador.io/products/api-gateway) API 网关是一个基于 \n  [Envoy](https://www.envoyproxy.io/) 的入口控制器。\n* [EnRoute](https://getenroute.io/) 是一个基于 [Envoy](https://www.envoyproxy.io)\n  的 API 网关，可以用作 Ingress 控制器。\n* [Easegress IngressController](https://megaease.com/docs/easegress/04.cloud-native/4.1.kubernetes-ingress-controller/)\n  是一个基于 [Easegress](https://megaease.com/easegress/) 的 API 网关，可以用作 Ingress 控制器。"}
{"en": "* F5 BIG-IP [Container Ingress Services for Kubernetes](https://clouddocs.f5.com/containers/latest/userguide/kubernetes/)\n  lets you use an Ingress to configure F5 BIG-IP virtual servers.\n* [FortiADC Ingress Controller](https://docs.fortinet.com/document/fortiadc/7.0.0/fortiadc-ingress-controller/742835/fortiadc-ingress-controller-overview) support the Kubernetes Ingress resources and allows you to manage FortiADC objects from Kubernetes\n* [Gloo](https://gloo.solo.io) is an open-source ingress controller based on [Envoy](https://www.envoyproxy.io),\n  which offers API gateway functionality.\n* [HAProxy Ingress](https://haproxy-ingress.github.io/) is an ingress controller for\n  [HAProxy](https://www.haproxy.org/#desc).\n* [Higress](https://github.com/alibaba/higress) is an [Envoy](https://www.envoyproxy.io) based API gateway that can run as an ingress controller.\n* The [HAProxy Ingress Controller for Kubernetes](https://github.com/haproxytech/kubernetes-ingress#readme)\n  is also an ingress controller for [HAProxy](https://www.haproxy.org/#desc).\n* [Istio Ingress](https://istio.io/latest/docs/tasks/traffic-management/ingress/kubernetes-ingress/)\n  is an [Istio](https://istio.io/) based ingress controller.", "zh": "* F5 BIG-IP 的\n  [用于 Kubernetes 的容器 Ingress 服务](https://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/latest)\n  让你能够使用 Ingress 来配置 F5 BIG-IP 虚拟服务器。\n* [FortiADC Ingress 控制器](https://docs.fortinet.com/document/fortiadc/7.0.0/fortiadc-ingress-controller/742835/fortiadc-ingress-controller-overview)\n  支持 Kubernetes Ingress 资源，并允许你从 Kubernetes 管理 FortiADC 对象。\n* [Gloo](https://gloo.solo.io) 是一个开源的、基于 [Envoy](https://www.envoyproxy.io) 的\n  Ingress 控制器，能够提供 API 网关功能。\n* [HAProxy Ingress](https://haproxy-ingress.github.io/) 是一个针对\n  [HAProxy](https://www.haproxy.org/#desc) 的 Ingress 控制器。\n* [Higress](https://github.com/alibaba/higress) 是一个基于 [Envoy](https://www.envoyproxy.io) 的 API 网关，\n  可以作为一个 Ingress 控制器运行。\n* [用于 Kubernetes 的 HAProxy Ingress 控制器](https://github.com/haproxytech/kubernetes-ingress#readme)\n  也是一个针对 [HAProxy](https://www.haproxy.org/#desc) 的 Ingress 控制器。\n* [Istio Ingress](https://istio.io/latest/zh/docs/tasks/traffic-management/ingress/kubernetes-ingress/)\n  是一个基于 [Istio](https://istio.io/zh/) 的 Ingress 控制器。"}
{"en": "* The [Kong Ingress Controller for Kubernetes](https://github.com/Kong/kubernetes-ingress-controller#readme)\n  is an ingress controller driving [Kong Gateway](https://konghq.com/kong/).\n* [Kusk Gateway](https://kusk.kubeshop.io/) is an OpenAPI-driven ingress controller based on [Envoy](https://www.envoyproxy.io).\n* The [NGINX Ingress Controller for Kubernetes](https://www.nginx.com/products/nginx-ingress-controller/)\n  works with the [NGINX](https://www.nginx.com/resources/glossary/nginx/) webserver (as a proxy).\n* The [ngrok Kubernetes Ingress Controller](https://github.com/ngrok/kubernetes-ingress-controller) is an open source controller for adding secure public access to your K8s services using the [ngrok platform](https://ngrok.com).\n* The [OCI Native Ingress Controller](https://github.com/oracle/oci-native-ingress-controller#readme) is an Ingress controller for Oracle Cloud Infrastructure which allows you to manage the [OCI Load Balancer](https://docs.oracle.com/en-us/iaas/Content/Balance/home.htm).\n* [OpenNJet Ingress Controller](https://gitee.com/njet-rd/open-njet-kic) is a [OpenNJet](https://njet.org.cn/)-based ingress controller.\n* The [Pomerium Ingress Controller](https://www.pomerium.com/docs/k8s/ingress.html) is based on [Pomerium](https://pomerium.com/), which offers context-aware access policy.\n* [Skipper](https://opensource.zalando.com/skipper/kubernetes/ingress-controller/) HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress, designed as a library to build your custom proxy.", "zh": "* [用于 Kubernetes 的 Kong Ingress 控制器](https://github.com/Kong/kubernetes-ingress-controller#readme)\n  是一个用来驱动 [Kong Gateway](https://konghq.com/kong/) 的 Ingress 控制器。\n* [Kusk Gateway](https://kusk.kubeshop.io/) 是一个基于 [Envoy](https://www.envoyproxy.io) 的、\n  OpenAPI 驱动的 Ingress 控制器。\n* [用于 Kubernetes 的 NGINX Ingress 控制器](https://www.nginx.com/products/nginx-ingress-controller/)\n  能够与 [NGINX](https://www.nginx.com/resources/glossary/nginx/)\n  网页服务器（作为代理）一起使用。\n* [ngrok Kubernetes Ingress 控制器](https://github.com/ngrok/kubernetes-ingress-controller)\n  是一个开源控制器，通过使用 [ngrok 平台](https://ngrok.com)为你的 K8s 服务添加安全的公开访问权限。\n* [OCI Native Ingress Controller](https://github.com/oracle/oci-native-ingress-controller#readme)\n  是一个适用于 Oracle Cloud Infrastructure 的 Ingress 控制器，可帮助你管理\n  [OCI 负载均衡](https://docs.oracle.com/en-us/iaas/Content/Balance/home.htm)。\n* [OpenNJet Ingress Controller](https://gitee.com/njet-rd/open-njet-kic) 是一个基于 \n  [OpenNJet](https://njet.org.cn/) 的 Ingress 控制器。\n* [Pomerium Ingress 控制器](https://www.pomerium.com/docs/k8s/ingress.html)\n  基于 [Pomerium](https://pomerium.com/)，能提供上下文感知的准入策略。\n* [Skipper](https://opensource.zalando.com/skipper/kubernetes/ingress-controller/) HTTP\n  路由器和反向代理可用于服务组装，支持包括 Kubernetes Ingress\n  这类使用场景，是一个用以构造你自己的定制代理的库。"}
{"en": "* The [Traefik Kubernetes Ingress provider](https://doc.traefik.io/traefik/providers/kubernetes-ingress/) is an\n  ingress controller for the [Traefik](https://traefik.io/traefik/) proxy.\n* [Tyk Operator](https://github.com/TykTechnologies/tyk-operator) extends Ingress with Custom Resources to bring API Management capabilities to Ingress. Tyk Operator works with the Open Source Tyk Gateway & Tyk Cloud control plane.\n* [Voyager](https://voyagermesh.com) is an ingress controller for\n  [HAProxy](https://www.haproxy.org/#desc).\n* [Wallarm Ingress Controller](https://www.wallarm.com/solutions/waf-for-kubernetes) is an Ingress Controller that provides WAAP (WAF) and API Security capabilities.", "zh": "* [Traefik Kubernetes Ingress 提供程序](https://doc.traefik.io/traefik/providers/kubernetes-ingress/)\n  是一个用于 [Traefik](https://traefik.io/traefik/) 代理的 Ingress 控制器。\n* [Tyk Operator](https://github.com/TykTechnologies/tyk-operator)\n  使用自定义资源扩展 Ingress，为之带来 API 管理能力。Tyk Operator\n  使用开源的 Tyk Gateway & Tyk Cloud 控制面。\n* [Voyager](https://voyagermesh.com) 是一个针对\n  [HAProxy](https://www.haproxy.org/#desc) 的 Ingress 控制器。\n* [Wallarm Ingress Controller](https://www.wallarm.com/solutions/waf-for-kubernetes) 是提供 WAAP（WAF）\n  和 API 安全功能的 Ingress Controller。"}
{"en": "## Using multiple Ingress controllers", "zh": "## 使用多个 Ingress 控制器  {#using-multiple-ingress-controllers}"}
{"en": "You may deploy any number of ingress controllers using [ingress class](/docs/concepts/services-networking/ingress/#ingress-class)\nwithin a cluster. Note the `.metadata.name` of your ingress class resource. When you create an ingress you would need that name to specify the `ingressClassName` field on your Ingress object (refer to [IngressSpec v1 reference](/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec)). `ingressClassName` is a replacement of the older [annotation method](/docs/concepts/services-networking/ingress/#deprecated-annotation).", "zh": "你可以使用\n[Ingress 类](/zh-cn/docs/concepts/services-networking/ingress/#ingress-class)在集群中部署任意数量的\nIngress 控制器。\n请注意你的 Ingress 类资源的 `.metadata.name` 字段。\n当你创建 Ingress 时，你需要用此字段的值来设置 Ingress 对象的 `ingressClassName` 字段（请参考\n[IngressSpec v1 reference](/zh-cn/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec)）。\n`ingressClassName`\n是之前的[注解](/zh-cn/docs/concepts/services-networking/ingress/#deprecated-annotation)做法的替代。"}
{"en": "If you do not specify an IngressClass for an Ingress, and your cluster has exactly one IngressClass marked as default, then Kubernetes [applies](/docs/concepts/services-networking/ingress/#default-ingress-class) the cluster's default IngressClass to the Ingress.\nYou mark an IngressClass as default by setting the [`ingressclass.kubernetes.io/is-default-class` annotation](/docs/reference/labels-annotations-taints/#ingressclass-kubernetes-io-is-default-class) on that IngressClass, with the string value `\"true\"`.\n\nIdeally, all ingress controllers should fulfill this specification, but the various ingress\ncontrollers operate slightly differently.", "zh": "如果你不为 Ingress 指定 IngressClass，并且你的集群中只有一个 IngressClass 被标记为默认，那么\nKubernetes 会将此集群的默认 IngressClass\n[应用](/zh-cn/docs/concepts/services-networking/ingress/#default-ingress-class)到 Ingress 上。\nIngressClass。\n你可以通过将\n[`ingressclass.kubernetes.io/is-default-class` 注解](/zh-cn/docs/reference/labels-annotations-taints/#ingressclass-kubernetes-io-is-default-class)\n的值设置为 `\"true\"` 来将一个 IngressClass 标记为集群默认。\n\n理想情况下，所有 Ingress 控制器都应满足此规范，但各种 Ingress 控制器的操作略有不同。"}
{"en": "Make sure you review your ingress controller's documentation to understand the caveats of choosing it.", "zh": "{{< note >}}\n确保你查看了 ingress 控制器的文档，以了解选择它的注意事项。\n{{< /note >}}\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn more about [Ingress](/docs/concepts/services-networking/ingress/).\n* [Set up Ingress on Minikube with the NGINX Controller](/docs/tasks/access-application-cluster/ingress-minikube).", "zh": "* 进一步了解 [Ingress](/zh-cn/docs/concepts/services-networking/ingress/)。\n* [在 Minikube 上使用 NGINX 控制器安装 Ingress](/zh-cn/docs/tasks/access-application-cluster/ingress-minikube)。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}"}
{"en": "IPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses to\n{{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} and {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}}.", "zh": "IPv4/IPv6 双协议栈网络能够将 IPv4 和 IPv6 地址分配给\n{{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 和\n{{< glossary_tooltip text=\"Service\" term_id=\"service\" >}}。"}
{"en": "IPv4/IPv6 dual-stack networking is enabled by default for your Kubernetes cluster starting in\n1.21, allowing the simultaneous assignment of both IPv4 and IPv6 addresses.", "zh": "从 1.21 版本开始，Kubernetes 集群默认启用 IPv4/IPv6 双协议栈网络，\n以支持同时分配 IPv4 和 IPv6 地址。"}
{"en": "## Supported Features", "zh": "## 支持的功能  {#supported-features}"}
{"en": "IPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features:", "zh": "Kubernetes 集群的 IPv4/IPv6 双协议栈可提供下面的功能："}
{"en": "* Dual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)\n* IPv4 and IPv6 enabled Services\n* Pod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfaces", "zh": "* 双协议栈 Pod 网络（每个 Pod 分配一个 IPv4 和 IPv6 地址）\n* IPv4 和 IPv6 启用的 Service\n* Pod 的集群外出口通过 IPv4 和 IPv6 路由"}
{"en": "## Prerequisites", "zh": "## 先决条件  {#prerequisites}"}
{"en": "The following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes clusters:", "zh": "为了使用 IPv4/IPv6 双栈的 Kubernetes 集群，需要满足以下先决条件："}
{"en": "* Kubernetes 1.20 or later  \n  For information about using dual-stack services with earlier\n  Kubernetes versions, refer to the documentation for that version\n  of Kubernetes.\n* Provider support for dual-stack networking (Cloud provider or otherwise must be able to provide\n  Kubernetes nodes with routable IPv4/IPv6 network interfaces)\n* A [network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) that\n  supports dual-stack networking.", "zh": "* Kubernetes 1.20 版本或更高版本，有关更早 Kubernetes 版本的使用双栈 Service 的信息，\n  请参考对应版本的 Kubernetes 文档。\n* 提供商支持双协议栈网络（云提供商或其他提供商必须能够为 Kubernetes\n  节点提供可路由的 IPv4/IPv6 网络接口）。\n* 支持双协议栈的[网络插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)。"}
{"en": "## Configure IPv4/IPv6 dual-stack", "zh": "## 配置 IPv4/IPv6 双协议栈"}
{"en": "To configure IPv4/IPv6 dual-stack, set dual-stack cluster network assignments:", "zh": "如果配置 IPv4/IPv6 双栈，请分配双栈集群网络："}
{"en": "* kube-apiserver:\n  * `--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>`\n* kube-controller-manager:\n  * `--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>`\n  * `--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>`\n  * `--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6` defaults to /24 for IPv4 and /64 for IPv6\n* kube-proxy:\n  * `--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>`\n* kubelet:\n  * `--node-ip=<IPv4 IP>,<IPv6 IP>`\n    * This option is required for bare metal dual-stack nodes (nodes that do not define a\n      cloud provider with the `--cloud-provider` flag). If you are using a cloud provider\n      and choose to override the node IPs chosen by the cloud provider, set the\n      `--node-ip` option.\n    * (The legacy built-in cloud providers do not support dual-stack `--node-ip`.)", "zh": "* kube-apiserver：\n  * `--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>`\n* kube-controller-manager：\n  * `--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>` \n  * `--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>`\n  * `--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6` 对于 IPv4 默认为 /24，\n    对于 IPv6 默认为 /64\n* kube-proxy：\n  * `--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>`\n* kubelet：\n  * `--node-ip=<IPv4 IP>,<IPv6 IP>`\n    * 裸机双栈节点（未使用 `--cloud-provider` 标志定义云平台的节点）需要此选项。\n      如果你使用了某个云平台并选择覆盖云平台所选择的节点 IP，请设置 `--node-ip` 选项。\n    * （传统的内置云平台实现不支持双栈 `--node-ip`。）\n\n{{< note >}}"}
{"en": "An example of an IPv4 CIDR: `10.244.0.0/16` (though you would supply your own address range)\n\nAn example of an IPv6 CIDR: `fdXY:IJKL:MNOP:15::/64` (this shows the format but is not a valid\naddress - see [RFC 4193](https://tools.ietf.org/html/rfc4193))", "zh": "IPv4 CIDR 的一个例子：`10.244.0.0/16`（尽管你会提供你自己的地址范围）。\n\nIPv6 CIDR 的一个例子：`fdXY:IJKL:MNOP:15::/64`\n（这里演示的是格式而非有效地址 - 请看 [RFC 4193](https://tools.ietf.org/html/rfc4193)）。\n{{< /note >}}"}
{"en": "## Services", "zh": "## Service  {#services}"}
{"en": "You can create {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}} which can use IPv4, IPv6, or both.\n\nThe address family of a Service defaults to the address family of the first service cluster IP\nrange (configured via the `--service-cluster-ip-range` flag to the kube-apiserver).\n\nWhen you define a Service you can optionally configure it as dual stack. To specify the behavior you want, you\nset the `.spec.ipFamilyPolicy` field to one of the following values:", "zh": "你可以使用 IPv4 或 IPv6 地址来创建\n{{< glossary_tooltip text=\"Service\" term_id=\"service\" >}}。\n\nService 的地址族默认为第一个服务集群 IP 范围的地址族（通过 kube-apiserver 的\n`--service-cluster-ip-range` 参数配置）。\n\n当你定义 Service 时，可以选择将其配置为双栈。若要指定所需的行为，你可以设置\n`.spec.ipFamilyPolicy` 字段为以下值之一："}
{"en": "* `SingleStack`: Single-stack service. The control plane allocates a cluster IP for the Service,\n  using the first configured service cluster IP range.\n* `PreferDualStack`:Allocates both IPv4 and IPv6 cluster IPs for the Service when dual-stack is enabled. If dual-stack is not enabled or supported, it falls back to single-stack behavior.\n* `RequireDualStack`: Allocates Service `.spec.clusterIPs` from both IPv4 and IPv6 address ranges when dual-stack is enabled. If dual-stack is not enabled or supported, the Service API object creation fails.\n  * Selects the `.spec.clusterIP` from the list of `.spec.clusterIPs` based on the address family\n    of the first element in the `.spec.ipFamilies` array.", "zh": "* `SingleStack`：单栈 Service。控制面使用第一个配置的服务集群 IP 范围为 Service 分配集群 IP。\n* `PreferDualStack`：启用双栈时，为 Service 同时分配 IPv4 和 IPv6 集群 IP 地址。\n  如果双栈未被启用或不被支持，则会返回到单栈行为。\n* `RequireDualStack`：启用双栈时，同时从 IPv4 和 IPv6 的地址范围中分配 Service 的 `.spec.clusterIPs`。\n  如果双栈未被启用或不被支持，则 Service API 对象创建失败。\n  * 从基于在 `.spec.ipFamilies` 数组中第一个元素的地址族的 `.spec.clusterIPs`\n    列表中选择 `.spec.clusterIP`"}
{"en": "If you would like to define which IP family to use for single stack or define the order of IP\nfamilies for dual-stack, you can choose the address families by setting an optional field,\n`.spec.ipFamilies`, on the Service.", "zh": "如果你想要定义哪个 IP 族用于单栈或定义双栈 IP 族的顺序，可以通过设置\nService 上的可选字段 `.spec.ipFamilies` 来选择地址族。\n\n{{< note >}}"}
{"en": "The `.spec.ipFamilies` field is conditionally mutable: you can add or remove a secondary\nIP address family, but you cannot change the primary IP address family of an existing Service.", "zh": "`.spec.ipFamilies` 字段修改是有条件的：你可以添加或删除第二个 IP 地址族，\n但你不能更改现有 Service 的主要 IP 地址族。\n{{< /note >}}"}
{"en": "You can set `.spec.ipFamilies` to any of the following array values:", "zh": "你可以设置 `.spec.ipFamily` 为以下任何数组值："}
{"en": "- `[\"IPv4\"]`\n- `[\"IPv6\"]`\n- `[\"IPv4\",\"IPv6\"]` (dual stack)\n- `[\"IPv6\",\"IPv4\"]` (dual stack)", "zh": "- `[\"IPv4\"]`\n- `[\"IPv6\"]`\n- `[\"IPv4\",\"IPv6\"]` （双栈）\n- `[\"IPv6\",\"IPv4\"]` （双栈）"}
{"en": "The first family you list is used for the legacy `.spec.clusterIP` field.", "zh": "你所列出的第一个地址族用于原来的 `.spec.clusterIP` 字段。"}
{"en": "### Dual-stack Service configuration scenarios\n\nThese examples demonstrate the behavior of various dual-stack Service configuration scenarios.", "zh": "### 双栈 Service 配置场景   {#dual-stack-service-configuration-scenarios}\n\n以下示例演示多种双栈 Service 配置场景下的行为。"}
{"en": "#### Dual-stack options on new Services", "zh": "#### 新 Service 的双栈选项    {#dual-stack-options-on-new-services}"}
{"en": "1. This Service specification does not explicitly define `.spec.ipFamilyPolicy`. When you create\n   this Service, Kubernetes assigns a cluster IP for the Service from the first configured\n   `service-cluster-ip-range` and sets the `.spec.ipFamilyPolicy` to `SingleStack`. ([Services\n   without selectors](/docs/concepts/services-networking/service/#services-without-selectors) and\n   [headless Services](/docs/concepts/services-networking/service/#headless-services) with selectors\n   will behave in this same way.)", "zh": "1. 此 Service 规约中没有显式设定 `.spec.ipFamilyPolicy`。当你创建此 Service 时，Kubernetes\n   从所配置的第一个 `service-cluster-ip-range` 中为 Service 分配一个集群 IP，并设置\n   `.spec.ipFamilyPolicy` 为 `SingleStack`。\n   （[无选择算符的 Service](/zh-cn/docs/concepts/services-networking/service/#services-without-selectors)\n   和[无头服务（Headless Service）](/zh-cn/docs/concepts/services-networking/service/#headless-services)的行为方式与此相同。）\n\n   {{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}"}
{"en": "1. This Service specification explicitly defines `PreferDualStack` in `.spec.ipFamilyPolicy`. When\n   you create this Service on a dual-stack cluster, Kubernetes assigns both IPv4 and IPv6\n   addresses for the service. The control plane updates the `.spec` for the Service to record the IP\n   address assignments. The field `.spec.clusterIPs` is the primary field, and contains both assigned\n   IP addresses; `.spec.clusterIP` is a secondary field with its value calculated from\n   `.spec.clusterIPs`.\n\n   * For the `.spec.clusterIP` field, the control plane records the IP address that is from the\n     same address family as the first service cluster IP range.\n   * On a single-stack cluster, the `.spec.clusterIPs` and `.spec.clusterIP` fields both only list\n     one address.\n   * On a cluster with dual-stack enabled, specifying `RequireDualStack` in `.spec.ipFamilyPolicy`\n     behaves the same as `PreferDualStack`.", "zh": "2. 此 Service 规约显式地将 `.spec.ipFamilyPolicy` 设置为 `PreferDualStack`。\n   当你在双栈集群上创建此 Service 时，Kubernetes 会为此 Service 分配 IPv4 和 IPv6 地址。\n   控制平面更新 Service 的 `.spec` 以记录 IP 地址分配。\n   字段 `.spec.clusterIPs` 是主要字段，包含两个分配的 IP 地址；`.spec.clusterIP` 是次要字段，\n   其取值从 `.spec.clusterIPs` 计算而来。\n\n   * 对于 `.spec.clusterIP` 字段，控制面记录来自第一个服务集群 IP\n     范围对应的地址族的 IP 地址。\n   * 对于单协议栈的集群，`.spec.clusterIPs` 和 `.spec.clusterIP` 字段都\n     仅仅列出一个地址。\n   * 对于启用了双协议栈的集群，将 `.spec.ipFamilyPolicy` 设置为\n     `RequireDualStack` 时，其行为与 `PreferDualStack` 相同。\n\n   {{% code_sample file=\"service/networking/dual-stack-preferred-svc.yaml\" %}}"}
{"en": "1. This Service specification explicitly defines `IPv6` and `IPv4` in `.spec.ipFamilies` as well\n   as defining `PreferDualStack` in `.spec.ipFamilyPolicy`. When Kubernetes assigns an IPv6 and\n   IPv4 address in `.spec.clusterIPs`, `.spec.clusterIP` is set to the IPv6 address because that is\n   the first element in the `.spec.clusterIPs` array, overriding the default.", "zh": "3. 下面的 Service 规约显式地在 `.spec.ipFamilies` 中指定 `IPv6` 和 `IPv4`，并将\n   `.spec.ipFamilyPolicy` 设定为 `PreferDualStack`。\n   当 Kubernetes 为 `.spec.clusterIPs` 分配一个 IPv6 和一个 IPv4 地址时，\n   `.spec.clusterIP` 被设置成 IPv6 地址，因为它是 `.spec.clusterIPs` 数组中的第一个元素，\n   覆盖其默认值。\n\n   {{% code_sample file=\"service/networking/dual-stack-preferred-ipfamilies-svc.yaml\" %}}"}
{"en": "#### Dual-stack defaults on existing Services", "zh": "#### 现有 Service 的双栈默认值   {#dual-stack-defaults-on-existing-services}"}
{"en": "These examples demonstrate the default behavior when dual-stack is newly enabled on a cluster\nwhere Services already exist. (Upgrading an existing cluster to 1.21 or beyond will enable\ndual-stack.)", "zh": "下面示例演示了在 Service 已经存在的集群上新启用双栈时的默认行为。\n（将现有集群升级到 1.21 或者更高版本会启用双协议栈支持。）"}
{"en": "1. When dual-stack is enabled on a cluster, existing Services (whether `IPv4` or `IPv6`) are\n   configured by the control plane to set `.spec.ipFamilyPolicy` to `SingleStack` and set\n   `.spec.ipFamilies` to the address family of the existing Service. The existing Service cluster IP\n   will be stored in `.spec.clusterIPs`.", "zh": "1. 在集群上启用双栈时，控制面会将现有 Service（无论是 `IPv4` 还是 `IPv6`）配置\n   `.spec.ipFamilyPolicy` 为 `SingleStack` 并设置 `.spec.ipFamilies`\n   为 Service 的当前地址族。\n\n   {{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}"}
{"en": "You can validate this behavior by using kubectl to inspect an existing service.", "zh": "你可以通过使用 kubectl 检查现有 Service 来验证此行为。\n\n   ```shell\n   kubectl get svc my-service -o yaml\n   ```\n\n   ```yaml\n   apiVersion: v1\n   kind: Service\n   metadata:\n     labels:\n       app.kubernetes.io/name: MyApp\n     name: my-service\n   spec:\n     clusterIP: 10.0.197.123\n     clusterIPs:\n     - 10.0.197.123\n     ipFamilies:\n     - IPv4\n     ipFamilyPolicy: SingleStack\n     ports:\n     - port: 80\n       protocol: TCP\n       targetPort: 80\n     selector:\n       app.kubernetes.io/name: MyApp\n     type: ClusterIP\n   status:\n     loadBalancer: {}\n   ```"}
{"en": "1. When dual-stack is enabled on a cluster, existing\n   [headless Services](/docs/concepts/services-networking/service/#headless-services) with selectors are\n   configured by the control plane to set `.spec.ipFamilyPolicy` to `SingleStack` and set\n   `.spec.ipFamilies` to the address family of the first service cluster IP range (configured via the\n   `--service-cluster-ip-range` flag to the kube-apiserver) even though `.spec.clusterIP` is set to\n   `None`.", "zh": "2. 在集群上启用双栈时，带有选择算符的现有\n   [无头服务](/zh-cn/docs/concepts/services-networking/service/#headless-services)\n   由控制面设置 `.spec.ipFamilyPolicy` 为 `SingleStack`\n   并设置 `.spec.ipFamilies` 为第一个服务集群 IP 范围的地址族（通过配置 kube-apiserver 的\n   `--service-cluster-ip-range` 参数），即使 `.spec.clusterIP` 的设置值为 `None` 也如此。\n\n   {{% code_sample file=\"service/networking/dual-stack-default-svc.yaml\" %}}"}
{"en": "You can validate this behavior by using kubectl to inspect an existing headless service with selectors.", "zh": "你可以通过使用 kubectl 检查带有选择算符的现有无头服务来验证此行为。\n\n   ```shell\n   kubectl get svc my-service -o yaml\n   ```\n\n   ```yaml\n   apiVersion: v1\n   kind: Service\n   metadata:\n     labels:\n       app.kubernetes.io/name: MyApp\n     name: my-service\n   spec:\n     clusterIP: None\n     clusterIPs:\n     - None\n     ipFamilies:\n     - IPv4\n     ipFamilyPolicy: SingleStack\n     ports:\n     - port: 80\n       protocol: TCP\n       targetPort: 80\n     selector:\n       app.kubernetes.io/name: MyApp\n   ```"}
{"en": "#### Switching Services between single-stack and dual-stack", "zh": "#### 在单栈和双栈之间切换 Service   {#switching-services-between-single-stack-and-dual-stack}"}
{"en": "Services can be changed from single-stack to dual-stack and from dual-stack to single-stack.", "zh": "Service 可以从单栈更改为双栈，也可以从双栈更改为单栈。"}
{"en": "1. To change a Service from single-stack to dual-stack, change `.spec.ipFamilyPolicy` from\n   `SingleStack` to `PreferDualStack` or `RequireDualStack` as desired. When you change this\n   Service from single-stack to dual-stack, Kubernetes assigns the missing address family so that the\n   Service now has IPv4 and IPv6 addresses.\n\n   Edit the Service specification updating the `.spec.ipFamilyPolicy` from `SingleStack` to `PreferDualStack`.", "zh": "1. 要将 Service 从单栈更改为双栈，根据需要将 `.spec.ipFamilyPolicy` 从 `SingleStack` 改为\n   `PreferDualStack` 或 `RequireDualStack`。\n   当你将此 Service 从单栈更改为双栈时，Kubernetes 将分配缺失的地址族，\n   以便现在此 Service具有 IPv4 和 IPv6 地址。\n   编辑 Service 规约将 `.spec.ipFamilyPolicy` 从 `SingleStack` 改为 `PreferDualStack`。"}
{"en": "Before:", "zh": "之前：\n\n   ```yaml\n   spec:\n     ipFamilyPolicy: SingleStack\n   ```"}
{"en": "After:", "zh": "之后：\n\n   ```yaml\n   spec:\n     ipFamilyPolicy: PreferDualStack\n   ```"}
{"en": "1. To change a Service from dual-stack to single-stack, change `.spec.ipFamilyPolicy` from\n   `PreferDualStack` or `RequireDualStack` to `SingleStack`. When you change this Service from\n   dual-stack to single-stack, Kubernetes retains only the first element in the `.spec.clusterIPs`\n   array, and sets `.spec.clusterIP` to that IP address and sets `.spec.ipFamilies` to the address\n   family of `.spec.clusterIPs`.", "zh": "2. 要将 Service 从双栈更改为单栈，请将 `.spec.ipFamilyPolicy` 从 `PreferDualStack` 或\n   `RequireDualStack` 改为 `SingleStack`。\n   当你将此 Service 从双栈更改为单栈时，Kubernetes 只保留 `.spec.clusterIPs`\n   数组中的第一个元素，并设置 `.spec.clusterIP` 为那个 IP 地址，\n   并设置 `.spec.ipFamilies` 为 `.spec.clusterIPs` 地址族。"}
{"en": "### Headless Services without selector", "zh": "### 无选择算符的无头服务   {#headless-services-without-selector}"}
{"en": "For [Headless Services without selectors](/docs/concepts/services-networking/service/#without-selectors)\nand without `.spec.ipFamilyPolicy` explicitly set, the `.spec.ipFamilyPolicy` field defaults to\n`RequireDualStack`.", "zh": "对于[不带选择算符的无头服务](/zh-cn/docs/concepts/services-networking/service/#without-selectors)，\n若没有显式设置 `.spec.ipFamilyPolicy`，则 `.spec.ipFamilyPolicy`\n字段默认设置为 `RequireDualStack`。"}
{"en": "### Service type LoadBalancer", "zh": "### LoadBalancer 类型 Service   {#service-type-loadbalancer}"}
{"en": "To provision a dual-stack load balancer for your Service:\n\n* Set the `.spec.type` field to `LoadBalancer`\n* Set `.spec.ipFamilyPolicy` field to `PreferDualStack` or `RequireDualStack`", "zh": "要为你的 Service 提供双栈负载均衡器：\n\n* 将 `.spec.type` 字段设置为 `LoadBalancer` \n* 将 `.spec.ipFamilyPolicy` 字段设置为 `PreferDualStack` 或者 `RequireDualStack`\n\n{{< note >}}"}
{"en": "To use a dual-stack `LoadBalancer` type Service, your cloud provider must support IPv4 and IPv6\nload balancers.", "zh": "为了使用双栈的负载均衡器类型 Service，你的云驱动必须支持 IPv4 和 IPv6 的负载均衡器。\n{{< /note >}}"}
{"en": "## Egress traffic", "zh": "## 出站流量    {#egress-traffic}"}
{"en": "If you want to enable egress traffic in order to reach off-cluster destinations (eg. the public\nInternet) from a Pod that uses non-publicly routable IPv6 addresses, you need to enable the Pod to\nuse a publicly routed IPv6 address via a mechanism such as transparent proxying or IP\nmasquerading. The [ip-masq-agent](https://github.com/kubernetes-sigs/ip-masq-agent) project\nsupports IP masquerading on dual-stack clusters.", "zh": "如果你要启用出站流量，以便使用非公开路由 IPv6 地址的 Pod 到达集群外地址\n（例如公网），则需要通过透明代理或 IP 伪装等机制使 Pod 使用公共路由的\nIPv6 地址。\n[ip-masq-agent](https://github.com/kubernetes-sigs/ip-masq-agent)项目\n支持在双栈集群上进行 IP 伪装。\n\n{{< note >}}"}
{"en": "Ensure your {{< glossary_tooltip text=\"CNI\" term_id=\"cni\" >}} provider supports IPv6.", "zh": "确认你的 {{< glossary_tooltip text=\"CNI\" term_id=\"cni\" >}} 驱动支持 IPv6。\n{{< /note >}}"}
{"en": "## Windows support\n\nKubernetes on Windows does not support single-stack \"IPv6-only\" networking. However,\ndual-stack IPv4/IPv6 networking for pods and nodes with single-family services\nis supported.\n\nYou can use IPv4/IPv6 dual-stack networking with `l2bridge` networks.", "zh": "## Windows 支持   {#windows-support}\n\nWindows 上的 Kubernetes 不支持单栈“仅 IPv6” 网络。 然而，\n对于 Pod 和节点而言，仅支持单栈形式 Service 的双栈 IPv4/IPv6 网络是被支持的。\n\n你可以使用 `l2bridge` 网络来实现 IPv4/IPv6 双栈联网。\n\n{{< note >}}"}
{"en": "Overlay (VXLAN) networks on Windows **do not** support dual-stack networking.", "zh": "Windows 上的 Overlay（VXLAN）网络**不**支持双栈网络。\n{{< /note >}}"}
{"en": "You can read more about the different network modes for Windows within the\n[Networking on Windows](/docs/concepts/services-networking/windows-networking#network-modes) topic.", "zh": "关于 Windows 的不同网络模式，你可以进一步阅读\n[Windows 上的网络](/zh-cn/docs/concepts/services-networking/windows-networking#network-modes)。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* [Validate IPv4/IPv6 dual-stack](/docs/tasks/network/validate-dual-stack) networking\n* [Enable dual-stack networking using kubeadm](/docs/setup/production-environment/tools/kubeadm/dual-stack-support/)", "zh": "* [验证 IPv4/IPv6 双协议栈](/zh-cn/docs/tasks/network/validate-dual-stack)网络\n* [使用 kubeadm 启用双协议栈网络](/zh-cn/docs/setup/production-environment/tools/kubeadm/dual-stack-support/)"}
{"en": "Make network services available by using an extensible, role-oriented, protocol-aware configuration\nmechanism. [Gateway API](https://gateway-api.sigs.k8s.io/) is an {{<glossary_tooltip text=\"add-on\" term_id=\"addons\">}}\ncontaining API [kinds](https://gateway-api.sigs.k8s.io/references/spec/) that provide dynamic infrastructure\nprovisioning and advanced traffic routing.", "zh": "[Gateway API](https://gateway-api.sigs.k8s.io/) 通过使用可扩展的、角色导向的、\n协议感知的配置机制来提供网络服务。它是一个{{<glossary_tooltip text=\"附加组件\" term_id=\"addons\">}}，\n包含可提供动态基础设施配置和高级流量路由的\nAPI [类别](https://gateway-api.sigs.k8s.io/references/spec/)。"}
{"en": "## Design principles\n\nThe following principles shaped the design and architecture of Gateway API:", "zh": "## 设计原则 {#design-principles}\n\nGateway API 的设计和架构遵从以下原则："}
{"en": "* __Role-oriented:__ Gateway API kinds are modeled after organizational roles that are\n  responsible for managing Kubernetes service networking:\n  * __Infrastructure Provider:__ Manages infrastructure that allows multiple isolated clusters\n    to serve multiple tenants, e.g. a cloud provider.\n  * __Cluster Operator:__ Manages clusters and is typically concerned with policies, network\n    access, application permissions, etc.\n  * __Application Developer:__ Manages an application running in a cluster and is typically\n    concerned with application-level configuration and [Service](/docs/concepts/services-networking/service/)\n    composition.", "zh": "* **角色导向：** Gateway API 类别是基于负责管理 Kubernetes 服务网络的组织角色建模的：\n  * **基础设施提供者：** 管理使用多个独立集群为多个租户提供服务的基础设施，例如，云提供商。\n  * **集群操作员：** 管理集群，通常关注策略、网络访问、应用程序权限等。\n  * **应用程序开发人员：** 管理在集群中运行的应用程序，通常关注应用程序级配置和 [Service](/zh-cn/docs/concepts/services-networking/service/) 组合。"}
{"en": "* __Portable:__ Gateway API specifications are defined as [custom resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources)\n  and are supported by many [implementations](https://gateway-api.sigs.k8s.io/implementations/).\n* __Expressive:__ Gateway API kinds support functionality for common traffic routing use cases\n  such as header-based matching, traffic weighting, and others that were only possible in\n  [Ingress](/docs/concepts/services-networking/ingress/) by using custom annotations.\n* __Extensible:__ Gateway allows for custom resources to be linked at various layers of the API.\n  This makes granular customization possible at the appropriate places within the API structure.", "zh": "* **可移植：** Gateway API 规范用[自定义资源](/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources)来定义，\n  并受到许多[实现](https://gateway-api.sigs.k8s.io/implementations/)的支持。\n* **表达能力强：** Gateway API 类别支持常见流量路由场景的功能，例如基于标头的匹配、流量加权以及其他只能在\n  [Ingress](/zh-cn/docs/concepts/services-networking/ingress/) 中使用自定义注解才能实现的功能。\n* **可扩展的：** Gateway 允许在 API 的各个层链接自定义资源。这使得在 API 结构内的适当位置进行精细定制成为可能。"}
{"en": "## Resource model\n\nGateway API has three stable API kinds:", "zh": "## 资源模型 {#resource-model}\n\nGateway API 具有三种稳定的 API 类别："}
{"en": "* __GatewayClass:__ Defines a set of gateways with common configuration and managed by a controller\n  that implements the class.\n\n* __Gateway:__ Defines an instance of traffic handling infrastructure, such as cloud load balancer.\n\n* __HTTPRoute:__ Defines HTTP-specific rules for mapping traffic from a Gateway listener to a\n  representation of backend network endpoints. These endpoints are often represented as a\n  {{<glossary_tooltip text=\"Service\" term_id=\"service\">}}.", "zh": "* **GatewayClass：** 定义一组具有配置相同的网关，由实现该类的控制器管理。\n\n* **Gateway：** 定义流量处理基础设施（例如云负载均衡器）的一个实例。\n\n* **HTTPRoute：** 定义特定于 HTTP 的规则，用于将流量从网关监听器映射到后端网络端点的表示。\n  这些端点通常表示为 {{<glossary_tooltip text=\"Service\" term_id=\"service\">}}。"}
{"en": "Gateway API is organized into different API kinds that have interdependent relationships to support\nthe role-oriented nature of organizations. A Gateway object is associated with exactly one GatewayClass;\nthe GatewayClass describes the gateway controller responsible for managing Gateways of this class.\nOne or more route kinds such as HTTPRoute, are then associated to Gateways. A Gateway can filter the routes\nthat may be attached to its `listeners`, forming a bidirectional trust model with routes.", "zh": "Gateway API 被组织成不同的 API 类别，这些 API 类别具有相互依赖的关系，以支持组织中角色导向的特点。\n一个 Gateway 对象只能与一个 GatewayClass 相关联；GatewayClass 描述负责管理此类 Gateway 的网关控制器。\n各个（可以是多个）路由类别（例如 HTTPRoute）可以关联到此 Gateway 对象。\nGateway 可以对能够挂接到其 `listeners` 的路由进行过滤，从而与路由形成双向信任模型。"}
{"en": "The following figure illustrates the relationships of the three stable Gateway API kinds:\n\n{{< figure src=\"/docs/images/gateway-kind-relationships.svg\" alt=\"A figure illustrating the relationships of the three stable Gateway API kinds\" class=\"diagram-medium\" >}}", "zh": "下图说明这三个稳定的 Gateway API 类别之间的关系：\n\n{{< figure src=\"/docs/images/gateway-kind-relationships.svg\" alt=\"此图呈现的是三个稳定的 Gateway API 类别之间的关系\" class=\"diagram-medium\" >}}"}
{"en": "### GatewayClass {#api-kind-gateway-class}\n\nGateways can be implemented by different controllers, often with different configurations. A Gateway\nmust reference a GatewayClass that contains the name of the controller that implements the\nclass.\n\nA minimal GatewayClass example:", "zh": "### GatewayClass {#api-kind-gateway-class}\n\nGateway 可以由不同的控制器实现，通常具有不同的配置。\nGateway 必须引用某 GatewayClass，而后者中包含实现该类的控制器的名称。\n\n下面是一个最精简的 GatewayClass 示例：\n\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: example-class\nspec:\n  controllerName: example.com/gateway-controller\n```"}
{"en": "In this example, a controller that has implemented Gateway API is configured to manage GatewayClasses\nwith the controller name `example.com/gateway-controller`. Gateways of this class will be managed by\nthe implementation's controller.\n\nSee the [GatewayClass](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.GatewayClass)\nreference for a full definition of this API kind.", "zh": "在此示例中，一个实现了 Gateway API 的控制器被配置为管理某些 GatewayClass 对象，\n这些对象的控制器名为 `example.com/gateway-controller`。\n归属于此类的 Gateway 对象将由此实现的控制器来管理。\n\n有关此 API 类别的完整定义，请参阅\n[GatewayClass](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.GatewayClass)。"}
{"en": "### Gateway {#api-kind-gateway}\n\nA Gateway describes an instance of traffic handling infrastructure. It defines a network endpoint\nthat can be used for processing traffic, i.e. filtering, balancing, splitting, etc. for backends\nsuch as a Service. For example, a Gateway may represent a cloud load balancer or an in-cluster proxy\nserver that is configured to accept HTTP traffic.\n\nA minimal Gateway resource example:", "zh": "### Gateway {#api-kind-gateway}\n\nGateway 用来描述流量处理基础设施的一个实例。Gateway 定义了一个网络端点，该端点可用于处理流量，\n即对 Service 等后端进行过滤、平衡、拆分等。\n例如，Gateway 可以代表某个云负载均衡器，或配置为接受 HTTP 流量的集群内代理服务器。\n\n下面是一个精简的 Gateway 资源示例：\n\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: example-gateway\nspec:\n  gatewayClassName: example-class\n  listeners:\n  - name: http\n    protocol: HTTP\n    port: 80\n```"}
{"en": "In this example, an instance of traffic handling infrastructure is programmed to listen for HTTP\ntraffic on port 80. Since the `addresses` field is unspecified, an address or hostname is assigned\nto the Gateway by the implementation's controller. This address is used as a network endpoint for\nprocessing traffic of backend network endpoints defined in routes.\n\nSee the [Gateway](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.Gateway)\nreference for a full definition of this API kind.", "zh": "在此示例中，流量处理基础设施的实例被编程为监听 80 端口上的 HTTP 流量。\n由于未指定 `addresses` 字段，因此对应实现的控制器负责将地址或主机名设置到 Gateway 之上。\n该地址用作网络端点，用于处理路由中定义的后端网络端点的流量。\n\n有关此类 API 的完整定义，请参阅 [Gateway](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.Gateway)。"}
{"en": "### HTTPRoute {#api-kind-httproute}\n\nThe HTTPRoute kind specifies routing behavior of HTTP requests from a Gateway listener to backend network\nendpoints. For a Service backend, an implementation may represent the backend network endpoint as a Service\nIP or the backing Endpoints of the Service. An HTTPRoute represents configuration that is applied to the\nunderlying Gateway implementation. For example, defining a new HTTPRoute may result in configuring additional\ntraffic routes in a cloud load balancer or in-cluster proxy server.\n\nA minimal HTTPRoute example:", "zh": "### HTTPRoute {#api-kind-httproute}\n\nHTTPRoute 类别指定从 Gateway 监听器到后端网络端点的 HTTP 请求的路由行为。\n对于服务后端，实现可以将后端网络端点表示为服务 IP 或服务的支持端点。\nHTTPRoute 表示将被应用到下层 Gateway 实现的配置。\n例如，定义新的 HTTPRoute 可能会导致在云负载均衡器或集群内代理服务器中配置额外的流量路由。\n\n下面是一个最精简的 HTTPRoute 示例：\n\n```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: example-httproute\nspec:\n  parentRefs:\n  - name: example-gateway\n  hostnames:\n  - \"www.example.com\"\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: /login\n    backendRefs:\n    - name: example-svc\n      port: 8080\n```"}
{"en": "In this example, HTTP traffic from Gateway `example-gateway` with the Host: header set to `www.example.com`\nand the request path specified as `/login` will be routed to Service `example-svc` on port `8080`.\n\nSee the [HTTPRoute](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.HTTPRoute)\nreference for a full definition of this API kind.", "zh": "在此示例中，来自 Gateway `example-gateway` 的 HTTP 流量，\n如果 Host 的标头设置为 `www.example.com` 且请求路径指定为 `/login`，\n将被路由到 Service `example-svc` 的 `8080` 端口。\n\n有关此类 API 的完整定义，请参阅 [HTTPRoute](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io/v1.HTTPRoute)。"}
{"en": "## Request flow\n\nHere is a simple example of HTTP traffic being routed to a Service by using a Gateway and an HTTPRoute:\n\n{{< figure src=\"/docs/images/gateway-request-flow.svg\" alt=\"A diagram that provides an example of HTTP traffic being routed to a Service by using a Gateway and an HTTPRoute\" class=\"diagram-medium\" >}}\n\nIn this example, the request flow for a Gateway implemented as a reverse proxy is:", "zh": "## 请求数据流 {#request-flow}\n\n以下是使用 Gateway 和 HTTPRoute 将 HTTP 流量路由到服务的简单示例：\n\n{{< figure src=\"/docs/images/gateway-request-flow.svg\" alt=\"此图为使用 Gateway 和 HTTPRoute 将 HTTP 流量路由到服务的示例\" class=\"diagram-medium\" >}}\n\n在此示例中，实现为反向代理的 Gateway 的请求数据流如下："}
{"en": "1. The client starts to prepare an HTTP request for the URL `http://www.example.com`\n2. The client's DNS resolver queries for the destination name and learns a mapping to\n   one or more IP addresses associated with the Gateway.\n3. The client sends a request to the Gateway IP address; the reverse proxy receives the HTTP\n   request and uses the Host: header to match a configuration that was derived from the Gateway\n   and attached HTTPRoute.\n4. Optionally, the reverse proxy can perform request header and/or path matching based\n   on match rules of the HTTPRoute.\n5. Optionally, the reverse proxy can modify the request; for example, to add or remove headers,\n   based on filter rules of the HTTPRoute.\n6. Lastly, the reverse proxy forwards the request to one or more backends.", "zh": "1. 客户端开始准备 URL 为 `http://www.example.com` 的 HTTP 请求\n2. 客户端的 DNS 解析器查询目标名称并了解与 Gateway 关联的一个或多个 IP 地址的映射。\n3. 客户端向 Gateway IP 地址发送请求；反向代理接收 HTTP 请求并使用 Host: \n   标头来匹配基于 Gateway 和附加的 HTTPRoute 所获得的配置。\n4. 可选的，反向代理可以根据 HTTPRoute 的匹配规则进行请求头和（或）路径匹配。\n5. 可选地，反向代理可以修改请求；例如，根据 HTTPRoute 的过滤规则添加或删除标头。\n6. 最后，反向代理将请求转发到一个或多个后端。"}
{"en": "## Conformance\n\nGateway API covers a broad set of features and is widely implemented. This combination requires\nclear conformance definitions and tests to ensure that the API provides a consistent experience\nwherever it is used.\n\nSee the [conformance](https://gateway-api.sigs.k8s.io/concepts/conformance/) documentation to\nunderstand details such as release channels, support levels, and running conformance tests.", "zh": "## 标准合规性 {#conformance}\n\nGateway API 涵盖广泛的功能并得到广泛实现。\n这种组合需要明确的标准合规性定义和测试，以确保 API 在任何地方使用时都能提供一致的体验。\n\n请参阅[合规性](https://gateway-api.sigs.k8s.io/concepts/conformance/)相关的文档，\n以了解发布渠道、支持级别和运行合规性测试等详细信息。"}
{"en": "## Migrating from Ingress\n\nGateway API is the successor to the [Ingress](/docs/concepts/services-networking/ingress/) API.\nHowever, it does not include the Ingress kind. As a result, a one-time conversion from your existing\nIngress resources to Gateway API resources is necessary.\n\nRefer to the [ingress migration](https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress/#migrating-from-ingress)\nguide for details on migrating Ingress resources to Gateway API resources.", "zh": "## 从 Ingress 迁移 {#migrating-from-ingress}\n\nGateway API 是 [Ingress](/zh-cn/docs/concepts/services-networking/ingress/) API 的后继者。\n但是其中不包括 Ingress 类型。因此，需要将现有 Ingress 资源一次性转换为 Gateway API 资源。\n\n有关将 Ingress 资源迁移到 Gateway API 资源的详细信息，请参阅\n[Ingress 迁移](https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress/#migrating-from-ingress)指南。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "Instead of Gateway API resources being natively implemented by Kubernetes, the specifications\nare defined as [Custom Resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\nsupported by a wide range of [implementations](https://gateway-api.sigs.k8s.io/implementations/).\n[Install](https://gateway-api.sigs.k8s.io/guides/#installing-gateway-api) the Gateway API CRDs or\nfollow the installation instructions of your selected implementation. After installing an\nimplementation, use the [Getting Started](https://gateway-api.sigs.k8s.io/guides/) guide to help\nyou quickly start working with Gateway API.", "zh": "Gateway API 资源不是由 Kubernetes 原生实现的，\n而是被定义为受广泛[实现](https://gateway-api.sigs.k8s.io/implementations/)支持的[自定义资源](/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources/)。\n用户需要[安装](https://gateway-api.sigs.k8s.io/guides/#installing-gateway-api) Gateway API CRD\n或按照所选实现的安装说明进行操作。\n安装完成后，使用[入门](https://gateway-api.sigs.k8s.io/guides/)指南来帮助你快速开始使用 Gateway API。\n\n{{< note >}}"}
{"en": "Make sure to review the documentation of your selected implementation to understand any caveats.", "zh": "请务必查看所选实现的文档以了解可能存在的注意事项。\n{{< /note >}}"}
{"en": "Refer to the [API specification](https://gateway-api.sigs.k8s.io/reference/spec/) for additional\ndetails of all Gateway API kinds.", "zh": "有关所有 Gateway API 类型的其他详细信息，请参阅 [API 规范](https://gateway-api.sigs.k8s.io/reference/spec/)。"}
{"en": "In Kubernetes, [Services](/docs/concepts/services-networking/service/) are an abstract way to expose\nan application running on a set of Pods. Services\ncan have a cluster-scoped virtual IP address (using a Service of `type: ClusterIP`).\nClients can connect using that virtual IP address, and Kubernetes then load-balances traffic to that\nService across the different backing Pods.", "zh": "在 Kubernetes 中，[Service](/zh-cn/docs/concepts/services-networking/service/) 是一种抽象的方式，\n用于公开在一组 Pod 上运行的应用。\nService 可以具有集群作用域的虚拟 IP 地址（使用 `type: ClusterIP` 的 Service）。\n客户端可以使用该虚拟 IP 地址进行连接，Kubernetes 通过不同的后台 Pod 对该 Service 的流量进行负载均衡。"}
{"en": "## How Service ClusterIPs are allocated?\nWhen Kubernetes needs to assign a virtual IP address for a Service,\nthat assignment happens one of two ways:\n\n_dynamically_\n: the cluster's control plane automatically picks a free IP address from within the configured IP range for `type: ClusterIP` Services.\n\n_statically_\n: you specify an IP address of your choice, from within the configured IP range for Services.\n\nAcross your whole cluster, every Service `ClusterIP` must be unique.\nTrying to create a Service with a specific `ClusterIP` that has already\nbeen allocated will return an error.", "zh": "## Service ClusterIP 是如何分配的？\n当 Kubernetes 需要为 Service 分配虚拟 IP 地址时，该分配会通过以下两种方式之一进行：\n\n**动态分配**\n: 集群的控制面自动从所配置的 IP 范围内为 `type: ClusterIP` 选择一个空闲 IP 地址。\n\n**静态分配**\n: 根据为 Service 所配置的 IP 范围，选定并设置你的 IP 地址。\n\n在整个集群中，每个 Service 的 `ClusterIP` 都必须是唯一的。\n尝试使用已分配的 `ClusterIP` 创建 Service 将返回错误。"}
{"en": "## Why do you need to reserve Service Cluster IPs?\nSometimes you may want to have Services running in well-known IP addresses, so other components and\nusers in the cluster can use them.\nThe best example is the DNS Service for the cluster. As a soft convention, some Kubernetes installers assign the 10th IP address from\nthe Service IP range to the DNS service. Assuming you configured your cluster with Service IP range\n10.96.0.0/16 and you want your DNS Service IP to be 10.96.0.10, you'd have to create a Service like\nthis:", "zh": "## 为什么需要预留 Service 的 ClusterIP ？\n\n有时你可能希望 Services 在众所周知的 IP 上面运行，以便集群中的其他组件和用户可以使用它们。\n\n最好的例子是集群的 DNS Service。作为一种非强制性的约定，一些 Kubernetes 安装程序\n将 Service IP 范围中的第 10 个 IP 地址分配给 DNS 服务。假设将集群的 Service IP 范围配置为 \n10.96.0.0/16，并且希望 DNS Service IP 为 10.96.0.10，则必须创建如下 Service：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    kubernetes.io/name: CoreDNS\n  name: kube-dns\n  namespace: kube-system\nspec:\n  clusterIP: 10.96.0.10\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n    targetPort: 53\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n    targetPort: 53\n  selector:\n    k8s-app: kube-dns\n  type: ClusterIP\n```"}
{"en": "but as it was explained before, the IP address 10.96.0.10 has not been reserved; if other Services are created\nbefore or in parallel with dynamic allocation, there is a chance they can allocate this IP, hence,\nyou will not be able to create the DNS Service because it will fail with a conflict error.", "zh": "但如前所述，IP 地址 10.96.0.10 尚未被保留。如果在 DNS 启动之前或同时采用动态分配机制创建其他 Service，\n则它们有可能被分配此 IP，因此，你将无法创建 DNS Service，因为它会因冲突错误而失败。"}
{"en": "## How can you avoid Service ClusterIP conflicts? {#avoid-ClusterIP-conflict}\nThe allocation strategy implemented in Kubernetes to allocate ClusterIPs to Services reduces the\nrisk of collision.\nThe `ClusterIP` range is divided, based on the formula `min(max(16, cidrSize / 16), 256)`,\ndescribed as _never less than 16 or more than 256 with a graduated step between them_.\nDynamic IP assignment uses the upper band by default, once this has been exhausted it will\nuse the lower range. This will allow users to use static allocations on the lower band with a low\nrisk of collision.", "zh": "## 如何避免 Service ClusterIP 冲突？{#avoid-ClusterIP-conflict}\n\nKubernetes 中用來将 ClusterIP 分配给 Service 的分配策略降低了冲突的风险。\n\n`ClusterIP` 范围根据公式 `min(max(16, cidrSize / 16), 256)` 进行划分，\n描述为不小于 16 且不大于 256，并在二者之间有一个渐进的步长。\n\n默认情况下，动态 IP 分配使用地址较高的一段，一旦用完，它将使用较低范围。\n这将允许用户在冲突风险较低的较低地址段上使用静态分配。"}
{"en": "## Examples {#allocation-examples}", "zh": "## 示例 {#allocation-examples}"}
{"en": "### Example 1 {#allocation-example-1}\nThis example uses the IP address range: 10.96.0.0/24 (CIDR notation) for the IP addresses\nof Services.", "zh": "### 示例 1 {#allocation-example-1}\n\n此示例使用 IP 地址范围：10.96.0.0/24（CIDR 表示法）作为 Service 的 IP 地址。"}
{"en": "Range Size: 2<sup>8</sup> - 2 = 254  \nBand Offset: `min(max(16, 256/16), 256)` = `min(16, 256)` = 16  \nStatic band start: 10.96.0.1  \nStatic band end: 10.96.0.16  \nRange end: 10.96.0.254   \n\n{{< mermaid >}}\npie showData\n    title 10.96.0.0/24\n    \"Static\" : 16\n    \"Dynamic\" : 238\n{{< /mermaid >}}", "zh": "范围大小：2<sup>8</sup> - 2 = 254  \n带宽偏移量：`min(max(16, 256/16), 256)` = `min(16, 256)` = 16  \n静态带宽起始地址：10.96.0.1  \n静态带宽结束地址：10.96.0.16  \n范围结束地址：10.96.0.254  \n\n{{< mermaid >}}\npie showData\n    title 10.96.0.0/24\n    \"静态分配\" : 16\n    \"动态分配\" : 238\n{{< /mermaid >}}"}
{"en": "### Example 2 {#allocation-example-2}\nThis example uses the IP address range: 10.96.0.0/20 (CIDR notation) for the IP addresses\nof Services.", "zh": "### 示例 2 {#allocation-example-2}\n\n此示例使用 IP 地址范围 10.96.0.0/20（CIDR 表示法）作为 Service 的 IP 地址。"}
{"en": "Range Size: 2<sup>12</sup> - 2 = 4094  \nBand Offset: `min(max(16, 4096/16), 256)` = `min(256, 256)` = 256  \nStatic band start: 10.96.0.1  \nStatic band end: 10.96.1.0  \nRange end: 10.96.15.254  \n\n{{< mermaid >}}\npie showData\n    title 10.96.0.0/20\n    \"Static\" : 256\n    \"Dynamic\" : 3838\n{{< /mermaid >}}", "zh": "范围大小：2<sup>12</sup> - 2 = 4094  \n带宽偏移量：`min(max(16, 4096/16), 256)` = `min(256, 256)` = 256  \n静态带宽起始地址：10.96.0.1  \n静态带宽结束地址：10.96.1.0  \n范围结束地址：10.96.15.254  \n\n{{< mermaid >}}\npie showData\n    title 10.96.0.0/20\n    \"静态分配\" : 256\n    \"动态分配\" : 3838\n{{< /mermaid >}}"}
{"en": "### Example 3 {#allocation-example-3}\nThis example uses the IP address range: 10.96.0.0/16 (CIDR notation) for the IP addresses\nof Services.", "zh": "### 示例 3 {#allocation-example-3}\n\n此示例使用 IP 地址范围 10.96.0.0/16（CIDR 表示法）作为 Service 的 IP 地址。"}
{"en": "Range Size: 2<sup>16</sup> - 2 = 65534  \nBand Offset: `min(max(16, 65536/16), 256)` = `min(4096, 256)` = 256  \nStatic band start: 10.96.0.1  \nStatic band ends: 10.96.1.0  \nRange end: 10.96.255.254  \n\n{{< mermaid >}}\npie showData\n    title 10.96.0.0/16\n    \"Static\" : 256\n    \"Dynamic\" : 65278\n{{< /mermaid >}}", "zh": "范围大小：2<sup>16</sup> - 2 = 65534  \n带宽偏移量：`min(max(16, 65536/16), 256)` = `min(4096, 256)` = 256  \n静态带宽起始地址：10.96.0.1  \n静态带宽结束地址：10.96.1.0  \n范围结束地址：10.96.255.254  \n\n{{< mermaid >}}\npie showData\n    title 10.96.0.0/16\n    \"静态分配\" : 256\n    \"动态分配\" : 65278\n{{< /mermaid >}}"}
{"en": "## {{% heading \"whatsnext\" %}}\n* Read about [Service External Traffic Policy](/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip)\n* Read about [Connecting Applications with Services](/docs/tutorials/services/connect-applications-service/)\n* Read about [Services](/docs/concepts/services-networking/service/)", "zh": "## {{% heading \"whatsnext\" %}}\n\n* 阅读[服务外部流量策略](/zh-cn/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip)\n* 阅读[应用程序与服务连接](/zh-cn/docs/tutorials/services/connect-applications-service/)\n* 阅读[服务](/zh-cn/docs/concepts/services-networking/service/)"}
{"en": "overview", "zh": "{{< glossary_definition term_id=\"service\" length=\"short\" prepend=\"Kubernetes 中 Service 是\" >}}"}
{"en": "A key aim of Services in Kubernetes is that you don't need to modify your existing\napplication to use an unfamiliar service discovery mechanism.\nYou can run code in Pods, whether this is a code designed for a cloud-native world, or\nan older app you've containerized. You use a Service to make that set of Pods available\non the network so that clients can interact with it.", "zh": "Kubernetes 中 Service 的一个关键目标是让你无需修改现有应用以使用某种不熟悉的服务发现机制。\n你可以在 Pod 集合中运行代码，无论该代码是为云原生环境设计的，还是被容器化的老应用。\n你可以使用 Service 让一组 Pod 可在网络上访问，这样客户端就能与之交互。"}
{"en": "If you use a {{< glossary_tooltip term_id=\"deployment\" >}} to run your app,\nthat Deployment can create and destroy Pods dynamically. From one moment to the next,\nyou don't know how many of those Pods are working and healthy; you might not even know\nwhat those healthy Pods are named.\nKubernetes {{< glossary_tooltip term_id=\"pod\" text=\"Pods\" >}} are created and destroyed\nto match the desired state of your cluster. Pods are ephemeral resources (you should not\nexpect that an individual Pod is reliable and durable).", "zh": "如果你使用 {{< glossary_tooltip term_id=\"deployment\" >}} 来运行你的应用，\nDeployment 可以动态地创建和销毁 Pod。\n在任何时刻，你都不知道有多少个这样的 Pod 正在工作以及它们健康与否；\n你可能甚至不知道如何辨别健康的 Pod。\nKubernetes {{< glossary_tooltip term_id=\"pod\" text=\"Pod\" >}} 的创建和销毁是为了匹配集群的预期状态。\nPod 是临时资源（你不应该期待单个 Pod 既可靠又耐用）。"}
{"en": "Each Pod gets its own IP address (Kubernetes expects network plugins to ensure this).\nFor a given Deployment in your cluster, the set of Pods running in one moment in\ntime could be different from the set of Pods running that application a moment later.\n\nThis leads to a problem: if some set of Pods (call them \"backends\") provides\nfunctionality to other Pods (call them \"frontends\") inside your cluster,\nhow do the frontends find out and keep track of which IP address to connect\nto, so that the frontend can use the backend part of the workload?", "zh": "每个 Pod 会获得属于自己的 IP 地址（Kubernetes 期待网络插件来保证这一点）。\n对于集群中给定的某个 Deployment，这一刻运行的 Pod 集合可能不同于下一刻运行该应用的\nPod 集合。\n\n这就带来了一个问题：如果某组 Pod（称为“后端”）为集群内的其他 Pod（称为“前端”）\n集合提供功能，前端要如何发现并跟踪要连接的 IP 地址，以便其使用负载的后端组件呢？"}
{"en": "## Services in Kubernetes\n\nThe Service API, part of Kubernetes, is an abstraction to help you expose groups of\nPods over a network. Each Service object defines a logical set of endpoints (usually\nthese endpoints are Pods) along with a policy about how to make those pods accessible.", "zh": "## Kubernetes 中的 Service   {#services-in-kubernetes}\n\nService API 是 Kubernetes 的组成部分，它是一种抽象，帮助你将 Pod 集合在网络上公开出去。\n每个 Service 对象定义端点的一个逻辑集合（通常这些端点就是 Pod）以及如何访问到这些 Pod 的策略。"}
{"en": "For example, consider a stateless image-processing backend which is running with\n3 replicas.  Those replicas are fungible&mdash;frontends do not care which backend\nthey use.  While the actual Pods that compose the backend set may change, the\nfrontend clients should not need to be aware of that, nor should they need to keep\ntrack of the set of backends themselves.\n\nThe Service abstraction enables this decoupling.", "zh": "例如，考虑一个无状态的图像处理后端，其中运行 3 个副本（Replicas）。\n这些副本是可互换的 —— 前端不需要关心它们调用的是哪个后端。\n即便构成后端集合的实际 Pod 可能会发生变化，前端客户端不应该也没必要知道这些，\n而且它们也不必亲自跟踪后端的状态变化。\n\nService 抽象使这种解耦成为可能。"}
{"en": "The set of Pods targeted by a Service is usually determined\nby a {{< glossary_tooltip text=\"selector\" term_id=\"selector\" >}} that you\ndefine.\nTo learn about other ways to define Service endpoints,\nsee [Services _without_ selectors](#services-without-selectors).", "zh": "Service 所对应的 Pod 集合通常由你定义的{{< glossary_tooltip text=\"选择算符\" term_id=\"selector\" >}}来确定。\n若想了解定义 Service 端点的其他方式，可以查阅[**不带**选择算符的 Service](#services-without-selectors)。"}
{"en": "If your workload speaks HTTP, you might choose to use an\n[Ingress](/docs/concepts/services-networking/ingress/) to control how web traffic\nreaches that workload.\nIngress is not a Service type, but it acts as the entry point for your\ncluster. An Ingress lets you consolidate your routing rules into a single resource, so\nthat you can expose multiple components of your workload, running separately in your\ncluster, behind a single listener.", "zh": "如果你的工作负载使用 HTTP 通信，你可能会选择使用\n[Ingress](/zh-cn/docs/concepts/services-networking/ingress/) 来控制\nWeb 流量如何到达该工作负载。Ingress 不是一种 Service，但它可用作集群的入口点。\nIngress 能让你将路由规则整合到同一个资源内，这样你就能将工作负载的多个组件公开出去，\n这些组件使用同一个侦听器，但各自独立地运行在集群中。"}
{"en": "The [Gateway](https://gateway-api.sigs.k8s.io/#what-is-the-gateway-api) API for Kubernetes\nprovides extra capabilities beyond Ingress and Service. You can add Gateway to your cluster -\nit is a family of extension APIs, implemented using\n{{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CustomResourceDefinitions\" >}} -\nand then use these to configure access to network services that are running in your cluster.", "zh": "用于 Kubernetes 的 [Gateway](https://gateway-api.sigs.k8s.io/#what-is-the-gateway-api) API\n能够提供 Ingress 和 Service 所不具备的一些额外能力。\nGateway 是使用 {{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CustomResourceDefinitions\" >}}\n实现的一系列扩展 API。\n你可以添加 Gateway 到你的集群中，之后就可以使用它们配置如何访问集群中运行的网络服务。"}
{"en": "### Cloud-native service discovery\n\nIf you're able to use Kubernetes APIs for service discovery in your application,\nyou can query the {{< glossary_tooltip text=\"API server\" term_id=\"kube-apiserver\" >}}\nfor matching EndpointSlices. Kubernetes updates the EndpointSlices for a Service\nwhenever the set of Pods in a Service changes.\n\nFor non-native applications, Kubernetes offers ways to place a network port or load\nbalancer in between your application and the backend Pods.", "zh": "### 云原生服务发现   {#cloud-native-service-discovery}\n\n如果你想要在自己的应用中使用 Kubernetes API 进行服务发现，可以查询\n{{< glossary_tooltip text=\"API 服务器\" term_id=\"kube-apiserver\" >}}，\n寻找匹配的 EndpointSlice 对象。\n只要 Service 中的 Pod 集合发生变化，Kubernetes 就会为其更新 EndpointSlice。\n\n对于非本地应用，Kubernetes 提供了在应用和后端 Pod 之间放置网络端口或负载均衡器的方法。"}
{"en": "Either way, your workload can use these [service discovery](#discovering-services)\nmechanisms to find the target it wants to connect to.", "zh": "无论采用那种方式，你的负载都可以使用这里的[服务发现](#discovering-services)机制找到希望连接的目标。"}
{"en": "## Defining a Service\n\nA Service in Kubernetes is an\n{{< glossary_tooltip text=\"object\" term_id=\"object\" >}}\n(the same way that a Pod or a ConfigMap is an object). You can create,\nview or modify Service definitions using the Kubernetes API. Usually\nyou use a tool such as `kubectl` to make those API calls for you.", "zh": "## 定义 Service   {#defining-a-service}\n\nKubernetes 中的 Service 是一个{{< glossary_tooltip text=\"对象\" term_id=\"object\" >}}\n（与 Pod 或 ConfigMap 类似）。你可以使用 Kubernetes API 创建、查看或修改 Service 定义。\n通常你会使用 `kubectl` 这类工具来替你发起这些 API 调用。"}
{"en": "For example, suppose you have a set of Pods that each listen on TCP port 9376\nand are labelled as `app.kubernetes.io/name=MyApp`. You can define a Service to\npublish that TCP listener:", "zh": "例如，假定有一组 Pod，每个 Pod 都在侦听 TCP 端口 9376，并且它们还被打上\n`app.kubernetes.io/name=MyApp` 标签。你可以定义一个 Service 来发布该 TCP 侦听器。\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 9376\n```"}
{"en": "Applying this manifest creates a new Service named \"my-service\" with the default\nClusterIP [service type](#publishing-services-service-types). The Service\ntargets TCP port 9376 on any Pod with the `app.kubernetes.io/name: MyApp` label.\n\nKubernetes assigns this Service an IP address (the _cluster IP_),\nthat is used by the virtual IP address mechanism. For more details on that mechanism,\nread [Virtual IPs and Service Proxies](/docs/reference/networking/virtual-ips/).", "zh": "应用上述清单时，系统将创建一个名为 \"my-service\" 的、\n[服务类型](#publishing-services-service-types)默认为 ClusterIP 的 Service。\n该 Service 指向带有标签 `app.kubernetes.io/name: MyApp` 的所有 Pod 的 TCP 端口 9376。\n\nKubernetes 为该 Service 分配一个 IP 地址（称为 “集群 IP”），供虚拟 IP 地址机制使用。\n有关该机制的更多详情，请阅读[虚拟 IP 和服务代理](/zh-cn/docs/reference/networking/virtual-ips/)。"}
{"en": "The controller for that Service continuously scans for Pods that\nmatch its selector, and then makes any necessary updates to the set of\nEndpointSlices for the Service.", "zh": "此 Service 的控制器不断扫描与其选择算符匹配的 Pod 集合，然后对 Service 的\nEndpointSlice 集合执行必要的更新。"}
{"en": "The name of a Service object must be a valid\n[RFC 1035 label name](/docs/concepts/overview/working-with-objects/names#rfc-1035-label-names).", "zh": "Service 对象的名称必须是有效的\n[RFC 1035 标签名称](/zh-cn/docs/concepts/overview/working-with-objects/names#rfc-1035-label-names)。\n\n{{< note >}}"}
{"en": "A Service can map _any_ incoming `port` to a `targetPort`. By default and\nfor convenience, the `targetPort` is set to the same value as the `port`\nfield.", "zh": "Service 能够将**任意**入站 `port` 映射到某个 `targetPort`。\n默认情况下，出于方便考虑，`targetPort` 会被设置为与 `port` 字段相同的值。\n{{< /note >}}"}
{"en": "### Port definitions {#field-spec-ports}\n\nPort definitions in Pods have names, and you can reference these names in the\n`targetPort` attribute of a Service. For example, we can bind the `targetPort`\nof the Service to the Pod port in the following way:", "zh": "### 端口定义 {#field-spec-ports}\n\nPod 中的端口定义是有名字的，你可以在 Service 的 `targetPort` 属性中引用这些名字。\n例如，我们可以通过以下方式将 Service 的 `targetPort` 绑定到 Pod 端口：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app.kubernetes.io/name: proxy\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n    ports:\n      - containerPort: 80\n        name: http-web-svc\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app.kubernetes.io/name: proxy\n  ports:\n  - name: name-of-service-port\n    protocol: TCP\n    port: 80\n    targetPort: http-web-svc\n```"}
{"en": "This works even if there is a mixture of Pods in the Service using a single\nconfigured name, with the same network protocol available via different\nport numbers. This offers a lot of flexibility for deploying and evolving\nyour Services. For example, you can change the port numbers that Pods expose\nin the next version of your backend software, without breaking clients.", "zh": "即使在 Service 中混合使用配置名称相同的多个 Pod，各 Pod 通过不同的端口号支持相同的网络协议，\n此机制也可以工作。这一机制为 Service 的部署和演化提供了较高的灵活性。\n例如，你可以在后端软件的新版本中更改 Pod 公开的端口号，但不会影响到客户端。"}
{"en": "The default protocol for Services is\n[TCP](/docs/reference/networking/service-protocols/#protocol-tcp); you can also\nuse any other [supported protocol](/docs/reference/networking/service-protocols/).\n\nBecause many Services need to expose more than one port, Kubernetes supports\n+[multiple port definitions](#multi-port-services) for a single Service.\nEach port definition can have the same `protocol`, or a different one.", "zh": "Service 的默认协议是 [TCP](/zh-cn/docs/reference/networking/service-protocols/#protocol-tcp)；\n你还可以使用其他[受支持的任何协议](/zh-cn/docs/reference/networking/service-protocols/)。\n\n由于许多 Service 需要公开多个端口，所以 Kubernetes 为同一 Service 定义[多个端口](#multi-port-services)。\n每个端口定义可以具有相同的 `protocol`，也可以具有不同协议。"}
{"en": "### Services without selectors\n\nServices most commonly abstract access to Kubernetes Pods thanks to the selector,\nbut when used with a corresponding set of\n{{<glossary_tooltip term_id=\"endpoint-slice\" text=\"EndpointSlices\">}}\nobjects and without a selector, the Service can abstract other kinds of backends,\nincluding ones that run outside the cluster.", "zh": "### 没有选择算符的 Service   {#services-without-selectors}\n\n由于选择算符的存在，Service 的最常见用法是为 Kubernetes Pod 集合提供访问抽象，\n但是当与相应的 {{<glossary_tooltip term_id=\"endpoint-slice\" text=\"EndpointSlice\">}}\n对象一起使用且没有设置选择算符时，Service 也可以为其他类型的后端提供抽象，\n包括在集群外运行的后端。"}
{"en": "For example:\n\n* You want to have an external database cluster in production, but in your\n  test environment you use your own databases.\n* You want to point your Service to a Service in a different\n  {{< glossary_tooltip term_id=\"namespace\" >}} or on another cluster.\n* You are migrating a workload to Kubernetes. While evaluating the approach,\n  you run only a portion of your backends in Kubernetes.", "zh": "例如：\n\n* 你希望在生产环境中使用外部数据库集群，但在测试环境中使用自己的数据库。\n* 你希望让你的 Service 指向另一个{{< glossary_tooltip term_id=\"namespace\" >}}中或其它集群中的服务。\n* 你正在将工作负载迁移到 Kubernetes 上来。在评估所采用的方法时，你仅在 Kubernetes\n  中运行一部分后端。"}
{"en": "In any of these scenarios you can define a Service _without_ specifying a\nselector to match Pods. For example:", "zh": "在所有这些场景中，你都可以定义**不**指定用来匹配 Pod 的选择算符的 Service。例如：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n```"}
{"en": "Because this Service has no selector, the corresponding EndpointSlice (and\nlegacy Endpoints) objects are not created automatically. You can map the Service\nto the network address and port where it's running, by adding an EndpointSlice\nobject manually. For example:", "zh": "由于此 Service 没有选择算符，因此不会自动创建对应的 EndpointSlice（和旧版的 Endpoints）对象。\n你可以通过手动添加 EndpointSlice 对象，将 Service 映射到该服务运行位置的网络地址和端口："}
{"en": "```yaml\napiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\n  name: my-service-1 # by convention, use the name of the Service\n                     # as a prefix for the name of the EndpointSlice\n  labels:\n    # You should set the \"kubernetes.io/service-name\" label.\n    # Set its value to match the name of the Service\n    kubernetes.io/service-name: my-service\naddressType: IPv4\nports:\n  - name: http # should match with the name of the service port defined above\n    appProtocol: http\n    protocol: TCP\n    port: 9376\nendpoints:\n  - addresses:\n      - \"10.4.5.6\"\n  - addresses:\n      - \"10.1.2.3\"\n```", "zh": "```yaml\napiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\n  name: my-service-1 # 按惯例将 Service 的名称用作 EndpointSlice 名称的前缀\n  labels:\n    # 你应设置 \"kubernetes.io/service-name\" 标签。\n    # 设置其值以匹配 Service 的名称\n    kubernetes.io/service-name: my-service\naddressType: IPv4\nports:\n  - name: '' # 应与上面定义的 Service 端口的名称匹配\n    appProtocol: http\n    protocol: TCP\n    port: 9376\nendpoints:  # 此列表中的 IP 地址可以按任何顺序显示\n  - addresses:\n      - \"10.4.5.6\"\n  - addresses:\n      - \"10.1.2.3\"\n```"}
{"en": "#### Custom EndpointSlices\n\nWhen you create an [EndpointSlice](#endpointslices) object for a Service, you can\nuse any name for the EndpointSlice. Each EndpointSlice in a namespace must have a\nunique name. You link an EndpointSlice to a Service by setting the\n`kubernetes.io/service-name` {{< glossary_tooltip text=\"label\" term_id=\"label\" >}}\non that EndpointSlice.", "zh": "#### 自定义 EndpointSlices  {#custom-endpointslices}\n\n当为 Service 创建 [EndpointSlice](#endpointslices) 对象时，可以为 EndpointSlice 使用任何名称。\n一个名字空间中的各个 EndpointSlice 都必须具有一个唯一的名称。通过在 EndpointSlice 上设置\n`kubernetes.io/service-name` {{< glossary_tooltip text=\"标签\" term_id=\"label\" >}}可以将\nEndpointSlice 链接到 Service。\n\n{{< note >}}"}
{"en": "The endpoint IPs _must not_ be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or\nlink-local (169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6).\n\nThe endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services,\nbecause {{< glossary_tooltip term_id=\"kube-proxy\" >}} doesn't support virtual IPs\nas a destination.", "zh": "端点 IP 地址**必须不是**：本地回路地址（IPv4 的 127.0.0.0/8、IPv6 的 ::1/128）\n或链路本地地址（IPv4 的 169.254.0.0/16 和 224.0.0.0/24、IPv6 的 fe80::/64）。\n\n端点 IP 地址不能是其他 Kubernetes 服务的集群 IP，因为\n{{< glossary_tooltip term_id =\"kube-proxy\">}} 不支持将虚拟 IP 作为目标地址。\n{{< /note >}}"}
{"en": "For an EndpointSlice that you create yourself, or in your own code,\nyou should also pick a value to use for the label\n[`endpointslice.kubernetes.io/managed-by`](/docs/reference/labels-annotations-taints/#endpointslicekubernetesiomanaged-by).\nIf you create your own controller code to manage EndpointSlices, consider using a\nvalue similar to `\"my-domain.example/name-of-controller\"`. If you are using a third\nparty tool, use the name of the tool in all-lowercase and change spaces and other\npunctuation to dashes (`-`).\nIf people are directly using a tool such as `kubectl` to manage EndpointSlices,\nuse a name that describes this manual management, such as `\"staff\"` or\n`\"cluster-admins\"`. You should\navoid using the reserved value `\"controller\"`, which identifies EndpointSlices\nmanaged by Kubernetes' own control plane.", "zh": "对于你自己或在你自己代码中创建的 EndpointSlice，你还应该为\n[`endpointslice.kubernetes.io/managed-by`](/zh-cn/docs/reference/labels-annotations-taints/#endpointslicekubernetesiomanaged-by)\n标签设置一个值。如果你创建自己的控制器代码来管理 EndpointSlice，\n请考虑使用类似于 `\"my-domain.example/name-of-controller\"` 的值。\n如果你使用的是第三方工具，请使用全小写的工具名称，并将空格和其他标点符号更改为短划线 (`-`)。\n如果直接使用 `kubectl` 之类的工具来管理 EndpointSlice 对象，请使用用来描述这种手动管理的名称，\n例如 `\"staff\"` 或 `\"cluster-admins\"`。你要避免使用保留值 `\"controller\"`；\n该值标识由 Kubernetes 自己的控制平面管理的 EndpointSlice。"}
{"en": "#### Accessing a Service without a selector {#service-no-selector-access}\n\nAccessing a Service without a selector works the same as if it had a selector.\nIn the [example](#services-without-selectors) for a Service without a selector,\ntraffic is routed to one of the two endpoints defined in\nthe EndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376.", "zh": "#### 访问没有选择算符的 Service   {#service-no-selector-access}\n\n访问没有选择算符的 Service 与有选择算符的 Service 的原理相同。\n在没有选择算符的 Service [示例](#services-without-selectors)中，\n流量被路由到 EndpointSlice 清单中定义的两个端点之一：\n通过 TCP 协议连接到 10.1.2.3 或 10.4.5.6 的端口 9376。\n\n{{< note >}}"}
{"en": "The Kubernetes API server does not allow proxying to endpoints that are not mapped to\npods. Actions such as `kubectl port-forward service/<service-name> forwardedPort:servicePort` where the service has no\nselector will fail due to this constraint. This prevents the Kubernetes API server\nfrom being used as a proxy to endpoints the caller may not be authorized to access.", "zh": "Kubernetes API 服务器不允许将流量代理到未被映射至 Pod 上的端点。由于此约束，当 Service\n没有选择算符时，诸如 `kubectl port-forward service/<service-name> forwardedPort:servicePort` 之类的操作将会失败。\n这可以防止 Kubernetes API 服务器被用作调用者可能无权访问的端点的代理。\n{{< /note >}}"}
{"en": "An `ExternalName` Service is a special case of Service that does not have\nselectors and uses DNS names instead. For more information, see the\n[ExternalName](#externalname) section.", "zh": "`ExternalName` Service 是 Service 的特例，它没有选择算符，而是使用 DNS 名称。\n更多的相关信息，请参阅 [ExternalName](#externalname) 一节。"}
{"en": "### EndpointSlices", "zh": "### EndpointSlices\n\n{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}"}
{"en": "[EndpointSlices](/docs/concepts/services-networking/endpoint-slices/) are objects that\nrepresent a subset (a _slice_) of the backing network endpoints for a Service.\n\nYour Kubernetes cluster tracks how many endpoints each EndpointSlice represents.\nIf there are so many endpoints for a Service that a threshold is reached, then\nKubernetes adds another empty EndpointSlice and stores new endpoint information\nthere.\nBy default, Kubernetes makes a new EndpointSlice once the existing EndpointSlices\nall contain at least 100 endpoints. Kubernetes does not make the new EndpointSlice\nuntil an extra endpoint needs to be added.\n\nSee [EndpointSlices](/docs/concepts/services-networking/endpoint-slices/) for more\ninformation about this API.", "zh": "[EndpointSlice](/zh-cn/docs/concepts/services-networking/endpoint-slices/)\n对象表示某个 Service 的后端网络端点的子集（**切片**）。\n\n你的 Kubernetes 集群会跟踪每个 EndpointSlice 所表示的端点数量。\n如果 Service 的端点太多以至于达到阈值，Kubernetes 会添加另一个空的\nEndpointSlice 并在其中存储新的端点信息。\n默认情况下，一旦现有 EndpointSlice 都包含至少 100 个端点，Kubernetes\n就会创建一个新的 EndpointSlice。\n在需要添加额外的端点之前，Kubernetes 不会创建新的 EndpointSlice。\n\n参阅 [EndpointSlice](/zh-cn/docs/concepts/services-networking/endpoint-slices/)\n了解有关该 API 的更多信息。"}
{"en": "### Endpoints\n\nIn the Kubernetes API, an\n[Endpoints](/docs/reference/kubernetes-api/service-resources/endpoints-v1/)\n(the resource kind is plural) defines a list of network endpoints, typically\nreferenced by a Service to define which Pods the traffic can be sent to.\n\nThe EndpointSlice API is the recommended replacement for Endpoints.", "zh": "### Endpoints\n\n在 Kubernetes API 中，[Endpoints](/zh-cn/docs/reference/kubernetes-api/service-resources/endpoints-v1/)\n（该资源类别为复数形式）定义的是网络端点的列表，通常由 Service 引用，\n以定义可以将流量发送到哪些 Pod。\n\n推荐使用 EndpointSlice API 替换 Endpoints。"}
{"en": "#### Over-capacity endpoints\n\nKubernetes limits the number of endpoints that can fit in a single Endpoints\nobject. When there are over 1000 backing endpoints for a Service, Kubernetes\ntruncates the data in the Endpoints object. Because a Service can be linked\nwith more than one EndpointSlice, the 1000 backing endpoint limit only\naffects the legacy Endpoints API.", "zh": "#### 超出容量的端点 {#over-capacity-endpoints}\n\nKubernetes 限制单个 Endpoints 对象中可以容纳的端点数量。\n当一个 Service 拥有 1000 个以上支撑端点时，Kubernetes 会截断 Endpoints 对象中的数据。\n由于一个 Service 可以链接到多个 EndpointSlice 之上，所以 1000 个支撑端点的限制仅影响旧版的\nEndpoints API。"}
{"en": "In that case, Kubernetes selects at most 1000 possible backend endpoints to store\ninto the Endpoints object, and sets an\n{{< glossary_tooltip text=\"annotation\" term_id=\"annotation\" >}} on the Endpoints:\n[`endpoints.kubernetes.io/over-capacity: truncated`](/docs/reference/labels-annotations-taints/#endpoints-kubernetes-io-over-capacity).\nThe control plane also removes that annotation if the number of backend Pods drops below 1000.", "zh": "如出现端点过多的情况，Kubernetes 选择最多 1000 个可能的后端端点存储到 Endpoints 对象中，\n并在 Endpoints 上设置{{< glossary_tooltip text=\"注解\" term_id=\"annotation\" >}}\n[`endpoints.kubernetes.io/over-capacity: truncated`](/zh-cn/docs/reference/labels-annotations-taints/#endpoints-kubernetes-io-over-capacity)。\n如果后端 Pod 的数量降至 1000 以下，控制平面也会移除该注解。"}
{"en": "Traffic is still sent to backends, but any load balancing mechanism that relies on the\nlegacy Endpoints API only sends traffic to at most 1000 of the available backing endpoints.\n\nThe same API limit means that you cannot manually update an Endpoints to have more than 1000 endpoints.", "zh": "请求流量仍会被发送到后端，但任何依赖旧版 Endpoints API 的负载均衡机制最多只能将流量发送到\n1000 个可用的支撑端点。\n\n这一 API 限制也意味着你不能手动将 Endpoints 更新为拥有超过 1000 个端点。"}
{"en": "### Application protocol", "zh": "### 应用协议    {#application-protocol}\n\n{{< feature-state for_k8s_version=\"v1.20\" state=\"stable\" >}}"}
{"en": "The `appProtocol` field provides a way to specify an application protocol for\neach Service port. This is used as a hint for implementations to offer\nricher behavior for protocols that they understand.\nThe value of this field is mirrored by the corresponding\nEndpoints and EndpointSlice objects.", "zh": "`appProtocol` 字段提供了一种为每个 Service 端口设置应用协议的方式。\n此字段被实现代码用作一种提示信息，以便针对实现能够理解的协议提供更为丰富的行为。\n此字段的取值会被映射到对应的 Endpoints 和 EndpointSlice 对象中。"}
{"en": "This field follows standard Kubernetes label syntax. Valid values are one of:\n\n* [IANA standard service names](https://www.iana.org/assignments/service-names).\n\n* Implementation-defined prefixed names such as `mycompany.com/my-custom-protocol`.\n\n* Kubernetes-defined prefixed names:\n\n| Protocol | Description |\n|----------|-------------|\n| `kubernetes.io/h2c` | HTTP/2 over cleartext as described in [RFC 7540](https://www.rfc-editor.org/rfc/rfc7540) |\n| `kubernetes.io/ws`  | WebSocket over cleartext as described in [RFC 6455](https://www.rfc-editor.org/rfc/rfc6455) |\n| `kubernetes.io/wss` | WebSocket over TLS as described in [RFC 6455](https://www.rfc-editor.org/rfc/rfc6455) |", "zh": "此字段遵循标准的 Kubernetes 标签语法。合法的取值值可以是以下之一：\n\n- [IANA 标准服务名称](https://www.iana.org/assignments/service-names)。\n- 由具体实现所定义的、带有 `mycompany.com/my-custom-protocol` 这类前缀的名称。\n- Kubernetes 定义的前缀名称：\n\n  | 协议     | 描述        |\n  |----------|-------------|\n  | `kubernetes.io/h2c` | 基于明文的 HTTP/2 协议，如 [RFC 7540](https://www.rfc-editor.org/rfc/rfc7540) 所述     |\n  | `kubernetes.io/ws`  | 基于明文的 WebSocket 协议，如 [RFC 6455](https://www.rfc-editor.org/rfc/rfc6455) 所述  |\n  | `kubernetes.io/wss` | 基于 TLS 的 WebSocket 协议，如 [RFC 6455](https://www.rfc-editor.org/rfc/rfc6455) 所述 |"}
{"en": "### Multi-Port Services\n\nFor some Services, you need to expose more than one port.\nKubernetes lets you configure multiple port definitions on a Service object.\nWhen using multiple ports for a Service, you must give all of your ports names\nso that these are unambiguous.\nFor example:", "zh": "### 多端口 Service   {#multi-port-services}\n\n对于某些 Service，你需要公开多个端口。Kubernetes 允许你为 Service 对象配置多个端口定义。\n为 Service 使用多个端口时，必须为所有端口提供名称，以使它们无歧义。\n例如：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 9376\n    - name: https\n      protocol: TCP\n      port: 443\n      targetPort: 9377\n```\n\n{{< note >}}"}
{"en": "As with Kubernetes {{< glossary_tooltip term_id=\"name\" text=\"names\">}} in general, names for ports\nmust only contain lowercase alphanumeric characters and `-`. Port names must\nalso start and end with an alphanumeric character.\n\nFor example, the names `123-abc` and `web` are valid, but `123_abc` and `-web` are not.", "zh": "与一般的 Kubernetes 名称一样，端口名称只能包含小写字母、数字和 `-`。\n端口名称还必须以字母或数字开头和结尾。\n\n例如，名称 `123-abc` 和 `web` 是合法的，但是 `123_abc` 和 `-web` 不合法。\n{{< /note >}}"}
{"en": "## Service type   {#publishing-services-service-types}\n\nFor some parts of your application (for example, frontends) you may want to expose a\nService onto an external IP address, one that's accessible from outside of your\ncluster.\n\nKubernetes Service types allow you to specify what kind of Service you want.\n\nThe available `type` values and their behaviors are:", "zh": "## 服务类型     {#publishing-services-service-types}\n\n对一些应用的某些部分（如前端），你可能希望将其公开于某外部 IP 地址，\n也就是可以从集群外部访问的某个地址。\n\nKubernetes Service 类型允许指定你所需要的 Service 类型。\n\n可用的 `type` 值及其行为有："}
{"en": "[`ClusterIP`](#type-clusterip)\n: Exposes the Service on a cluster-internal IP. Choosing this value\n  makes the Service only reachable from within the cluster. This is the\n  default that is used if you don't explicitly specify a `type` for a Service.\n  You can expose the Service to the public internet using an\n  [Ingress](/docs/concepts/services-networking/ingress/) or a\n  [Gateway](https://gateway-api.sigs.k8s.io/).\n\n[`NodePort`](#type-nodeport)\n: Exposes the Service on each Node's IP at a static port (the `NodePort`).\n  To make the node port available, Kubernetes sets up a cluster IP address,\n  the same as if you had requested a Service of `type: ClusterIP`.", "zh": "`ClusterIP`\n: 通过集群的内部 IP 公开 Service，选择该值时 Service 只能够在集群内部访问。\n  这也是你没有为 Service 显式指定 `type` 时使用的默认值。\n  你可以使用 [Ingress](/zh-cn/docs/concepts/services-networking/ingress/)\n  或者 [Gateway API](https://gateway-api.sigs.k8s.io/) 向公共互联网公开服务。\n\n[`NodePort`](#type-nodeport)\n: 通过每个节点上的 IP 和静态端口（`NodePort`）公开 Service。\n  为了让 Service 可通过节点端口访问，Kubernetes 会为 Service 配置集群 IP 地址，\n  相当于你请求了 `type: ClusterIP` 的 Service。"}
{"en": "[`LoadBalancer`](#loadbalancer)\n: Exposes the Service externally using an external load balancer. Kubernetes\n  does not directly offer a load balancing component; you must provide one, or\n  you can integrate your Kubernetes cluster with a cloud provider.\n\n[`ExternalName`](#externalname)\n: Maps the Service to the contents of the `externalName` field (for example,\n  to the hostname `api.foo.bar.example`). The mapping configures your cluster's\n  DNS server to return a `CNAME` record with that external hostname value.\n  No proxying of any kind is set up.", "zh": "[`LoadBalancer`](#loadbalancer)\n: 使用云平台的负载均衡器向外部公开 Service。Kubernetes 不直接提供负载均衡组件；\n  你必须提供一个，或者将你的 Kubernetes 集群与某个云平台集成。\n\n[`ExternalName`](#externalname)\n: 将服务映射到 `externalName` 字段的内容（例如，映射到主机名 `api.foo.bar.example`）。\n  该映射将集群的 DNS 服务器配置为返回具有该外部主机名值的 `CNAME` 记录。 \n  集群不会为之创建任何类型代理。"}
{"en": "The `type` field in the Service API is designed as nested functionality - each level\nadds to the previous. However there is an exception to this nested design. You can\ndefine a `LoadBalancer` Service by\n[disabling the load balancer `NodePort` allocation](/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation).", "zh": "服务 API 中的 `type` 字段被设计为层层递进的形式 - 每层都建立在前一层的基础上。\n但是，这种层层递进的形式有一个例外。\n你可以在定义 `LoadBalancer` Service 时[禁止负载均衡器分配 `NodePort`](/zh-cn/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation)。"}
{"en": "### `type: ClusterIP` {#type-clusterip}\n\nThis default Service type assigns an IP address from a pool of IP addresses that\nyour cluster has reserved for that purpose.\n\nSeveral of the other types for Service build on the `ClusterIP` type as a\nfoundation.\n\nIf you define a Service that has the `.spec.clusterIP` set to `\"None\"` then\nKubernetes does not assign an IP address. See [headless Services](#headless-services)\nfor more information.", "zh": "### `type: ClusterIP` {#type-clusterip}\n\n此默认 Service 类型从你的集群中为此预留的 IP 地址池中分配一个 IP 地址。\n\n其他几种 Service 类型在 `ClusterIP` 类型的基础上进行构建。\n\n如果你定义的 Service 将 `.spec.clusterIP` 设置为 `\"None\"`，则 Kubernetes\n不会为其分配 IP 地址。有关详细信息，请参阅[无头服务](#headless-services)。"}
{"en": "#### Choosing your own IP address\n\nYou can specify your own cluster IP address as part of a `Service` creation\nrequest.  To do this, set the `.spec.clusterIP` field. For example, if you\nalready have an existing DNS entry that you wish to reuse, or legacy systems\nthat are configured for a specific IP address and difficult to re-configure.\n\nThe IP address that you choose must be a valid IPv4 or IPv6 address from within the\n`service-cluster-ip-range` CIDR range that is configured for the API server.\nIf you try to create a Service with an invalid clusterIP address value, the API\nserver will return a 422 HTTP status code to indicate that there's a problem.", "zh": "#### 选择自己的 IP 地址   {#choosing-your-own-ip-address}\n\n在创建 `Service` 的请求中，你可以通过设置 `spec.clusterIP` 字段来指定自己的集群 IP 地址。\n比如，希望复用一个已存在的 DNS 条目，或者遗留系统已经配置了一个固定的 IP 且很难重新配置。\n\n你所选择的 IP 地址必须是合法的 IPv4 或者 IPv6 地址，并且这个 IP 地址在 API 服务器上所配置的\n`service-cluster-ip-range` CIDR 范围内。\n如果你尝试创建一个带有非法 `clusterIP` 地址值的 Service，API 服务器会返回 HTTP 状态码 422，\n表示值不合法。"}
{"en": "Read [avoiding collisions](/docs/reference/networking/virtual-ips/#avoiding-collisions)\nto learn how Kubernetes helps reduce the risk and impact of two different Services\nboth trying to use the same IP address.", "zh": "请阅读[避免冲突](/zh-cn/docs/reference/networking/virtual-ips/#avoiding-collisions)节，\n以了解 Kubernetes 如何协助降低两个不同的 Service 试图使用相同 IP 地址的风险和影响。"}
{"en": "### `type: NodePort` {#type-nodeport}\n\nIf you set the `type` field to `NodePort`, the Kubernetes control plane\nallocates a port from a range specified by `--service-node-port-range` flag (default: 30000-32767).\nEach node proxies that port (the same port number on every Node) into your Service.\nYour Service reports the allocated port in its `.spec.ports[*].nodePort` field.\n\nUsing a NodePort gives you the freedom to set up your own load balancing solution,\nto configure environments that are not fully supported by Kubernetes, or even\nto expose one or more nodes' IP addresses directly.", "zh": "### `type: NodePort`  {#type-nodeport}\n\n如果你将 `type` 字段设置为 `NodePort`，则 Kubernetes 控制平面将在\n`--service-node-port-range` 标志所指定的范围内分配端口（默认值：30000-32767）。\n每个节点将该端口（每个节点上的相同端口号）上的流量代理到你的 Service。\n你的 Service 在其 `.spec.ports[*].nodePort` 字段中报告已分配的端口。\n\n使用 NodePort 可以让你自由设置自己的负载均衡解决方案，\n配置 Kubernetes 不完全支持的环境，\n甚至直接公开一个或多个节点的 IP 地址。"}
{"en": "For a node port Service, Kubernetes additionally allocates a port (TCP, UDP or\nSCTP to match the protocol of the Service). Every node in the cluster configures\nitself to listen on that assigned port and to forward traffic to one of the ready\nendpoints associated with that Service. You'll be able to contact the `type: NodePort`\nService, from outside the cluster, by connecting to any node using the appropriate\nprotocol (for example: TCP), and the appropriate port (as assigned to that Service).", "zh": "对于 NodePort 类型 Service，Kubernetes 额外分配一个端口（TCP、UDP 或 SCTP 以匹配 Service 的协议）。\n集群中的每个节点都将自己配置为监听所分配的端口，并将流量转发到与该 Service 关联的某个就绪端点。\n通过使用合适的协议（例如 TCP）和适当的端口（分配给该 Service）连接到任何一个节点，\n你就能够从集群外部访问 `type: NodePort` 服务。"}
{"en": "#### Choosing your own port {#nodeport-custom-port}\n\nIf you want a specific port number, you can specify a value in the `nodePort`\nfield. The control plane will either allocate you that port or report that\nthe API transaction failed.\nThis means that you need to take care of possible port collisions yourself.\nYou also have to use a valid port number, one that's inside the range configured\nfor NodePort use.\n\nHere is an example manifest for a Service of `type: NodePort` that specifies\na NodePort value (30007, in this example):", "zh": "#### 选择你自己的端口   {#nodeport-custom-port}\n\n如果需要特定的端口号，你可以在 `nodePort` 字段中指定一个值。\n控制平面将或者为你分配该端口，或者报告 API 事务失败。\n这意味着你需要自行注意可能发生的端口冲突。\n你还必须使用有效的端口号，该端口号在配置用于 NodePort 的范围内。\n\n以下是 `type: NodePort` 服务的一个清单示例，其中指定了 NodePort 值（在本例中为 30007）："}
{"en": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: NodePort\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: 80\n      # By default and for convenience, the `targetPort` is set to\n      # the same value as the `port` field.\n      targetPort: 80\n      # Optional field\n      # By default and for convenience, the Kubernetes control plane\n      # will allocate a port from a range (default: 30000-32767)\n      nodePort: 30007\n```", "zh": "```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: NodePort\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    # 默认情况下，为了方便起见，`targetPort` 被设置为与 `port` 字段相同的值。\n    - port: 80\n      targetPort: 80\n      # 可选字段\n      # 默认情况下，为了方便起见，Kubernetes 控制平面会从某个范围内分配一个端口号\n      #（默认：30000-32767）\n      nodePort: 30007\n```"}
{"en": "#### Reserve Nodeport ranges to avoid collisions  {#avoid-nodeport-collisions}", "zh": "#### 预留 NodePort 端口范围以避免发生冲突  {#avoid-nodeport-collisions}"}
{"en": "The policy for assigning ports to NodePort services applies to both the auto-assignment and\nthe manual assignment scenarios. When a user wants to create a NodePort service that\nuses a specific port, the target port may conflict with another port that has already been assigned.\n\nTo avoid this problem, the port range for NodePort services is divided into two bands.\nDynamic port assignment uses the upper band by default, and it may use the lower band once the \nupper band has been exhausted. Users can then allocate from the lower band with a lower risk of port collision.", "zh": "为 NodePort 服务分配端口的策略既适用于自动分配的情况，也适用于手动分配的场景。\n当某个用于希望创建一个使用特定端口的 NodePort 服务时，该目标端口可能与另一个已经被分配的端口冲突。\n\n为了避免这个问题，用于 NodePort 服务的端口范围被分为两段。\n动态端口分配默认使用较高的端口段，并且在较高的端口段耗尽时也可以使用较低的端口段。\n用户可以从较低端口段中分配端口，降低端口冲突的风险。"}
{"en": "#### Custom IP address configuration for `type: NodePort` Services {#service-nodeport-custom-listen-address}\n\nYou can set up nodes in your cluster to use a particular IP address for serving node port\nservices. You might want to do this if each node is connected to multiple networks (for example:\none network for application traffic, and another network for traffic between nodes and the\ncontrol plane).\n\nIf you want to specify particular IP address(es) to proxy the port, you can set the\n`--nodeport-addresses` flag for kube-proxy or the equivalent `nodePortAddresses`\nfield of the [kube-proxy configuration file](/docs/reference/config-api/kube-proxy-config.v1alpha1/)\nto particular IP block(s).", "zh": "#### 为 `type: NodePort` 服务自定义 IP 地址配置  {#service-nodeport-custom-listen-address}\n\n你可以配置集群中的节点使用特定 IP 地址来支持 NodePort 服务。\n如果每个节点都连接到多个网络（例如：一个网络用于应用流量，另一网络用于节点和控制平面之间的流量），\n你可能想要这样做。\n\n如果你要指定特定的 IP 地址来为端口提供代理，可以将 kube-proxy 的 `--nodeport-addresses` 标志或\n[kube-proxy 配置文件](/zh-cn/docs/reference/config-api/kube-proxy-config.v1alpha1/)中的等效字段\n`nodePortAddresses` 设置为特定的 IP 段。"}
{"en": "This flag takes a comma-delimited list of IP blocks (e.g. `10.0.0.0/8`, `192.0.2.0/25`)\nto specify IP address ranges that kube-proxy should consider as local to this node.\n\nFor example, if you start kube-proxy with the `--nodeport-addresses=127.0.0.0/8` flag,\nkube-proxy only selects the loopback interface for NodePort Services.\nThe default for `--nodeport-addresses` is an empty list.\nThis means that kube-proxy should consider all available network interfaces for NodePort.\n(That's also compatible with earlier Kubernetes releases.)", "zh": "此标志接受逗号分隔的 IP 段列表（例如 `10.0.0.0/8`、`192.0.2.0/25`），用来设置 IP 地址范围。\nkube-proxy 应视将其视为所在节点的本机地址。\n\n例如，如果你使用 `--nodeport-addresses=127.0.0.0/8` 标志启动 kube-proxy，\n则 kube-proxy 仅选择 NodePort 服务的本地回路接口。\n`--nodeport-addresses` 的默认值是一个空的列表。\n这意味着 kube-proxy 将认为所有可用网络接口都可用于 NodePort 服务\n（这也与早期的 Kubernetes 版本兼容。）\n\n{{< note >}}"}
{"en": "This Service is visible as `<NodeIP>:spec.ports[*].nodePort` and `.spec.clusterIP:spec.ports[*].port`.\nIf the `--nodeport-addresses` flag for kube-proxy or the equivalent field\nin the kube-proxy configuration file is set, `<NodeIP>` would be a filtered\nnode IP address (or possibly IP addresses).", "zh": "此 Service 的可见形式为 `<NodeIP>:spec.ports[*].nodePort` 以及 `.spec.clusterIP:spec.ports[*].port`。\n如果设置了 kube-proxy 的 `--nodeport-addresses` 标志或 kube-proxy 配置文件中的等效字段，\n则 `<NodeIP>` 将是一个被过滤的节点 IP 地址（或可能是多个 IP 地址）。\n{{< /note >}}"}
{"en": "### `type: LoadBalancer` {#loadbalancer}\n\nOn cloud providers which support external load balancers, setting the `type`\nfield to `LoadBalancer` provisions a load balancer for your Service.\nThe actual creation of the load balancer happens asynchronously, and\ninformation about the provisioned balancer is published in the Service's\n`.status.loadBalancer` field.\nFor example:", "zh": "### `type: LoadBalancer`  {#loadbalancer}\n\n在使用支持外部负载均衡器的云平台时，如果将 `type` 设置为 `\"LoadBalancer\"`，\n则平台会为 Service 提供负载均衡器。\n负载均衡器的实际创建过程是异步进行的，关于所制备的负载均衡器的信息将会通过 Service 的\n`status.loadBalancer` 字段公开出来。\n例如：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n  clusterIP: 10.0.171.239\n  type: LoadBalancer\nstatus:\n  loadBalancer:\n    ingress:\n    - ip: 192.0.2.127\n```"}
{"en": "Traffic from the external load balancer is directed at the backend Pods. The cloud\nprovider decides how it is load balanced.", "zh": "来自外部负载均衡器的流量将被直接重定向到后端各个 Pod 上，云平台决定如何进行负载平衡。"}
{"en": "To implement a Service of `type: LoadBalancer`, Kubernetes typically starts off\nby making the changes that are equivalent to you requesting a Service of\n`type: NodePort`. The cloud-controller-manager component then configures the external\nload balancer to forward traffic to that assigned node port.\n\nYou can configure a load balanced Service to\n[omit](#load-balancer-nodeport-allocation) assigning a node port, provided that the\ncloud provider implementation supports this.", "zh": "要实现 `type: LoadBalancer` 的服务，Kubernetes 通常首先进行与请求 `type: NodePort`\n服务类似的更改。cloud-controller-manager 组件随后配置外部负载均衡器，\n以将流量转发到所分配的节点端口。\n\n你可以将负载均衡 Service 配置为[忽略](#load-balancer-nodeport-allocation)分配节点端口，\n前提是云平台实现支持这点。"}
{"en": "Some cloud providers allow you to specify the `loadBalancerIP`. In those cases, the load-balancer is created\nwith the user-specified `loadBalancerIP`. If the `loadBalancerIP` field is not specified,\nthe load Balancer is set up with an ephemeral IP address. If you specify a `loadBalancerIP`\nbut your cloud provider does not support the feature, the `loadbalancerIP` field that you\nset is ignored.", "zh": "某些云平台允许你设置 `loadBalancerIP`。这时，平台将使用用户指定的 `loadBalancerIP`\n来创建负载均衡器。如果没有设置 `loadBalancerIP` 字段，平台将会给负载均衡器分配一个临时 IP。\n如果设置了 `loadBalancerIP`，但云平台并不支持这一特性，所设置的 `loadBalancerIP` 值将会被忽略。\n\n{{< note >}}"}
{"en": "The`.spec.loadBalancerIP` field for a Service was deprecated in Kubernetes v1.24.\n\nThis field was under-specified and its meaning varies across implementations.\nIt also cannot support dual-stack networking. This field may be removed in a future API version.", "zh": "针对 Service 的 `.spec.loadBalancerIP` 字段已在 Kubernetes v1.24 中被弃用。\n\n此字段的定义模糊，其含义因实现而异。它也不支持双协议栈联网。\n此字段可能会在未来的 API 版本中被移除。"}
{"en": "If you're integrating with a provider that supports specifying the load balancer IP address(es)\nfor a Service via a (provider specific) annotation, you should switch to doing that.\n\nIf you are writing code for a load balancer integration with Kubernetes, avoid using this field.\nYou can integrate with [Gateway](https://gateway-api.sigs.k8s.io/) rather than Service, or you\ncan define your own (provider specific) annotations on the Service that specify the equivalent detail.", "zh": "如果你正在集成某云平台，该平台通过（特定于平台的）注解为 Service 指定负载均衡器 IP 地址，\n你应该切换到这种做法。\n\n如果你正在为集成到 Kubernetes 的负载均衡器编写代码，请避免使用此字段。\n你可以与 [Gateway](https://gateway-api.sigs.k8s.io/) 而不是 Service 集成，\n或者你可以在 Service 上定义自己的（特定于提供商的）注解，以指定等效的细节。\n{{< /note >}}"}
{"en": "#### Node liveness impact on load balancer traffic\n\nLoad balancer health checks are critical to modern applications. They are used to\ndetermine which server (virtual machine, or IP address) the load balancer should\ndispatch traffic to. The Kubernetes APIs do not define how health checks have to be\nimplemented for Kubernetes managed load balancers, instead it's the cloud providers\n(and the people implementing integration code) who decide on the behavior. Load\nbalancer health checks are extensively used within the context of supporting the\n`externalTrafficPolicy` field for Services.", "zh": "#### 节点存活态对负载均衡器流量的影响\n\n负载均衡器运行状态检查对于现代应用程序至关重要，\n它们用于确定负载均衡器应将流量分派到哪个服务器（虚拟机或 IP 地址）。\nKubernetes API 没有定义如何为 Kubernetes 托管负载均衡器实施运行状况检查，\n而是由云提供商（以及集成代码的实现人员）决定其行为。\n负载均衡器运行状态检查广泛用于支持 Service 的 `externalTrafficPolicy` 字段。"}
{"en": "#### Load balancers with mixed protocol types", "zh": "#### 混合协议类型的负载均衡器\n\n{{< feature-state feature_gate_name=\"MixedProtocolLBService\" >}}"}
{"en": "By default, for LoadBalancer type of Services, when there is more than one port defined, all\nports must have the same protocol, and the protocol must be one which is supported\nby the cloud provider.\n\nThe feature gate `MixedProtocolLBService` (enabled by default for the kube-apiserver as of v1.24) allows the use of\ndifferent protocols for LoadBalancer type of Services, when there is more than one port defined.", "zh": "默认情况下，对于 LoadBalancer 类型的 Service，当其中定义了多个端口时，\n所有端口必须使用相同的协议，并且该协议必须是被云平台支持的。\n\n当服务中定义了多个端口时，特性门控 `MixedProtocolLBService`（从 kube-apiserver 1.24\n版本起默认为启用）允许 LoadBalancer 类型的服务使用不同的协议。\n\n{{< note >}}"}
{"en": "The set of protocols that can be used for load balanced Services is defined by your\ncloud provider; they may impose restrictions beyond what the Kubernetes API enforces.", "zh": "可用于负载均衡服务的协议集合由你的云平台决定，他们可能在\nKubernetes API 强制执行的限制之外另加一些约束。\n{{< /note >}}"}
{"en": "#### Disabling load balancer NodePort allocation {#load-balancer-nodeport-allocation}", "zh": "### 禁用负载均衡服务的节点端口分配 {#load-balancer-nodeport-allocation}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "You can optionally disable node port allocation for a Service of `type: LoadBalancer`, by setting\nthe field `spec.allocateLoadBalancerNodePorts` to `false`. This should only be used for load balancer implementations\nthat route traffic directly to pods as opposed to using node ports. By default, `spec.allocateLoadBalancerNodePorts`\nis `true` and type LoadBalancer Services will continue to allocate node ports. If `spec.allocateLoadBalancerNodePorts`\nis set to `false` on an existing Service with allocated node ports, those node ports will **not** be de-allocated automatically.\nYou must explicitly remove the `nodePorts` entry in every Service port to de-allocate those node ports.", "zh": "通过设置 Service 的 `spec.allocateLoadBalancerNodePorts` 为 `false`，你可以对 LoadBalancer\n类型的 Service 禁用节点端口分配操作。\n这仅适用于负载均衡器的实现能够直接将流量路由到 Pod 而不是使用节点端口的情况。\n默认情况下，`spec.allocateLoadBalancerNodePorts` 为 `true`，LoadBalancer 类型的 Service\n也会继续分配节点端口。如果某已有 Service 已被分配节点端口，如果将其属性\n`spec.allocateLoadBalancerNodePorts` 设置为 `false`，这些节点端口**不会**被自动释放。\n你必须显式地在每个 Service 端口中删除 `nodePorts` 项以释放对应的端口。"}
{"en": "#### Specifying class of load balancer implementation {#load-balancer-class}", "zh": "#### 设置负载均衡器实现的类别 {#load-balancer-class}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "For a Service with `type` set to `LoadBalancer`, the `.spec.loadBalancerClass` field\nenables you to use a load balancer implementation other than the cloud provider default.\n\nBy default, `.spec.loadBalancerClass` is not set and a `LoadBalancer`\ntype of Service uses the cloud provider's default load balancer implementation if the\ncluster is configured with a cloud provider using the `--cloud-provider` component\nflag.", "zh": "对于 `type` 设置为 `LoadBalancer` 的 Service，`spec.loadBalancerClass`\n字段允许你使用有别于云平台的默认负载均衡器的实现。\n\n默认情况下，`.spec.loadBalancerClass` 是未设置的，如果集群使用 `--cloud-provider`\n件标志配置了云平台，`LoadBalancer` 类型 Service 会使用云平台的默认负载均衡器实现。"}
{"en": "If you specify `.spec.loadBalancerClass`, it is assumed that a load balancer\nimplementation that matches the specified class is watching for Services.\nAny default load balancer implementation (for example, the one provided by\nthe cloud provider) will ignore Services that have this field set.\n`spec.loadBalancerClass` can be set on a Service of type `LoadBalancer` only.\nOnce set, it cannot be changed.", "zh": "如果你设置了 `.spec.loadBalancerClass`，则假定存在某个与所指定的类相匹配的负载均衡器实现在监视\nService 变更。所有默认的负载均衡器实现（例如，由云平台所提供的）都会忽略设置了此字段的 Service。\n`.spec.loadBalancerClass` 只能设置到类型为 `LoadBalancer` 的 Service 之上，\n而且一旦设置之后不可变更。"}
{"en": "The value of `spec.loadBalancerClass` must be a label-style identifier,\nwith an optional prefix such as \"`internal-vip`\" or \"`example.com/internal-vip`\".\nUnprefixed names are reserved for end-users.", "zh": "`.spec.loadBalancerClass` 的值必须是一个标签风格的标识符，\n可以有选择地带有类似 \"`internal-vip`\" 或 \"`example.com/internal-vip`\" 这类前缀。\n没有前缀的名字是保留给最终用户的。"}
{"en": "#### Specifying IPMode of load balancer status {#load-balancer-ip-mode}", "zh": "#### 指定负载均衡器状态的 IPMode    {#load-balancer-ip-mode}\n\n{{< feature-state feature_gate_name=\"LoadBalancerIPMode\" >}}"}
{"en": "As a Beta feature in Kubernetes 1.30,\na [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) \nnamed `LoadBalancerIPMode` allows you to set the `.status.loadBalancer.ingress.ipMode` \nfor a Service with `type` set to `LoadBalancer`. \nThe `.status.loadBalancer.ingress.ipMode` specifies how the load-balancer IP behaves. \nIt may be specified only when the `.status.loadBalancer.ingress.ip` field is also specified.", "zh": "作为 Kubernetes 1.30 中的 Beta 级别特性，通过名为 `LoadBalancerIPMode`\n的[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)允许你为\n`type` 为 `LoadBalancer` 的服务设置 `.status.loadBalancer.ingress.ipMode`。\n`.status.loadBalancer.ingress.ipMode` 指定负载均衡器 IP 的行为方式。\n此字段只能在 `.status.loadBalancer.ingress.ip` 字段也被指定时才能指定。"}
{"en": "There are two possible values for `.status.loadBalancer.ingress.ipMode`: \"VIP\" and \"Proxy\". \nThe default value is \"VIP\" meaning that traffic is delivered to the node \nwith the destination set to the load-balancer's IP and port. \nThere are two cases when setting this to \"Proxy\", depending on how the load-balancer \nfrom the cloud provider delivers the traffics:", "zh": "`.status.loadBalancer.ingress.ipMode` 有两个可能的值：\"VIP\" 和 \"Proxy\"。\n默认值是 \"VIP\"，意味着流量被传递到目的地设置为负载均衡器 IP 和端口的节点上。\n将此字段设置为 \"Proxy\" 时会出现两种情况，具体取决于云驱动提供的负载均衡器如何传递流量："}
{"en": "- If the traffic is delivered to the node then DNATed to the pod, the destination would be set to the node's IP and node port;\n- If the traffic is delivered directly to the pod, the destination would be set to the pod's IP and port.", "zh": "- 如果流量被传递到节点，然后 DNAT 到 Pod，则目的地将被设置为节点的 IP 和节点端口；\n- 如果流量被直接传递到 Pod，则目的地将被设置为 Pod 的 IP 和端口。"}
{"en": "Service implementations may use this information to adjust traffic routing.", "zh": "服务实现可以使用此信息来调整流量路由。"}
{"en": "#### Internal load balancer\n\nIn a mixed environment it is sometimes necessary to route traffic from Services inside the same\n(virtual) network address block.\n\nIn a split-horizon DNS environment you would need two Services to be able to route both external\nand internal traffic to your endpoints.\n\nTo set an internal load balancer, add one of the following annotations to your Service\ndepending on the cloud service provider you're using:", "zh": "#### 内部负载均衡器 {#internal-load-balancer}\n\n在混合环境中，有时有必要在同一（虚拟）网络地址段内路由来自 Service 的流量。\n\n在水平分割（Split-Horizon）DNS 环境中，你需要两个 Service 才能将内部和外部流量都路由到你的端点。\n\n如要设置内部负载均衡器，请根据你所使用的云平台，为 Service 添加以下注解之一：\n\n{{< tabs name=\"service_tabs\" >}}\n{{% tab name=\"Default\" %}}"}
{"en": "Select one of the tabs.", "zh": "选择一个标签。\n\n{{% /tab %}}\n{{% tab name=\"GCP\" %}}\n\n```yaml\nmetadata:\n  name: my-service\n  annotations:\n    networking.gke.io/load-balancer-type: \"Internal\"\n```\n\n{{% /tab %}}\n{{% tab name=\"AWS\" %}}\n\n```yaml\nmetadata:\n  name: my-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"\n```\n\n{{% /tab %}}\n{{% tab name=\"Azure\" %}}\n\n```yaml\nmetadata:\n  name: my-service\n  annotations:\n    service.beta.kubernetes.io/azure-load-balancer-internal: \"true\"\n```\n\n{{% /tab %}}\n{{% tab name=\"IBM Cloud\" %}}\n\n```yaml\nmetadata:\n  name: my-service\n  annotations:\n    service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type: \"private\"\n```\n\n{{% /tab %}}\n{{% tab name=\"OpenStack\" %}}\n\n```yaml\nmetadata:\n  name: my-service\n  annotations:\n    service.beta.kubernetes.io/openstack-internal-load-balancer: \"true\"\n```\n\n{{% /tab %}}"}
{"en": "Baidu Cloud", "zh": "{{% tab name=\"百度云\" %}}\n\n```yaml\nmetadata:\n  name: my-service\n  annotations:\n    service.beta.kubernetes.io/cce-load-balancer-internal-vpc: \"true\"\n```\n\n{{% /tab %}}"}
{"en": "Tencent Cloud", "zh": "{{% tab name=\"腾讯云\" %}}\n\n```yaml\nmetadata:\n  annotations:\n    service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-xxxxx\n```\n\n{{% /tab %}}"}
{"en": "Alibaba Cloud", "zh": "{{% tab name=\"阿里云\" %}}\n\n```yaml\nmetadata:\n  annotations:\n    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: \"intranet\"\n```\n\n{{% /tab %}}\n{{% tab name=\"OCI\" %}}\n\n```yaml\nmetadata:\n  name: my-service\n  annotations:\n      service.beta.kubernetes.io/oci-load-balancer-internal: true\n```\n{{% /tab %}}\n{{< /tabs >}}"}
{"en": "### `type: ExternalName` {#externalname}\n\nServices of type ExternalName map a Service to a DNS name, not to a typical selector such as\n`my-service` or `cassandra`. You specify these Services with the `spec.externalName` parameter.\n\nThis Service definition, for example, maps\nthe `my-service` Service in the `prod` namespace to `my.database.example.com`:", "zh": "### ExternalName 类型         {#externalname}\n\n类型为 ExternalName 的 Service 将 Service 映射到 DNS 名称，而不是典型的选择算符，\n例如 `my-service` 或者 `cassandra`。你可以使用 `spec.externalName` 参数指定这些服务。\n\n例如，以下 Service 定义将 `prod` 名字空间中的 `my-service` 服务映射到 `my.database.example.com`：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\n  namespace: prod\nspec:\n  type: ExternalName\n  externalName: my.database.example.com\n```\n\n{{< note >}}"}
{"en": "A Service of `type: ExternalName` accepts an IPv4 address string,\nbut treats that string as a DNS name comprised of digits,\nnot as an IP address (the internet does not however allow such names in DNS).\nServices with external names that resemble IPv4\naddresses are not resolved by DNS servers.\n\nIf you want to map a Service directly to a specific IP address, consider using\n[headless Services](#headless-services).", "zh": "`type: ExternalName` 的服务接受 IPv4 地址字符串，但将该字符串视为由数字组成的 DNS 名称，\n而不是 IP 地址（然而，互联网不允许在 DNS 中使用此类名称）。\n类似于 IPv4 地址的外部名称无法被 DNS 服务器解析。\n\n如果你想要将服务直接映射到某特定 IP 地址，请考虑使用[无头服务](#headless-services)。\n{{< /note >}}"}
{"en": "When looking up the host `my-service.prod.svc.cluster.local`, the cluster DNS Service\nreturns a `CNAME` record with the value `my.database.example.com`. Accessing\n`my-service` works in the same way as other Services but with the crucial\ndifference that redirection happens at the DNS level rather than via proxying or\nforwarding. Should you later decide to move your database into your cluster, you\ncan start its Pods, add appropriate selectors or endpoints, and change the\nService's `type`.", "zh": "当查找主机 `my-service.prod.svc.cluster.local` 时，集群 DNS 服务返回 `CNAME` 记录，\n其值为 `my.database.example.com`。访问 `my-service` 的方式与访问其他 Service 的方式相同，\n主要区别在于重定向发生在 DNS 级别，而不是通过代理或转发来完成。\n如果后来你决定将数据库移到集群中，则可以启动其 Pod，添加适当的选择算符或端点并更改\nService 的 `type`。\n\n{{< caution >}}"}
{"en": "You may have trouble using ExternalName for some common protocols, including HTTP and HTTPS.\nIf you use ExternalName then the hostname used by clients inside your cluster is different from\nthe name that the ExternalName references.\n\nFor protocols that use hostnames this difference may lead to errors or unexpected responses.\nHTTP requests will have a `Host:` header that the origin server does not recognize;\nTLS servers will not be able to provide a certificate matching the hostname that the client connected to.", "zh": "针对 ExternalName 服务使用一些常见的协议，包括 HTTP 和 HTTPS，可能会有问题。\n如果你使用 ExternalName 服务，那么集群内客户端使用的主机名与 ExternalName 引用的名称不同。\n\n对于使用主机名的协议，这一差异可能会导致错误或意外响应。\nHTTP 请求将具有源服务器无法识别的 `Host:` 标头；\nTLS 服务器将无法提供与客户端连接的主机名匹配的证书。\n{{< /caution >}}"}
{"en": "## Headless Services  {#headless-services}\n\nSometimes you don't need load-balancing and a single Service IP.  In\nthis case, you can create what are termed _headless Services_, by explicitly\nspecifying `\"None\"` for the cluster IP address (`.spec.clusterIP`).\n\nYou can use a headless Service to interface with other service discovery mechanisms,\nwithout being tied to Kubernetes' implementation.\n\nFor headless Services, a cluster IP is not allocated, kube-proxy does not handle\nthese Services, and there is no load balancing or proxying done by the platform for them.", "zh": "## 无头服务（Headless Services）  {#headless-services}\n\n有时你并不需要负载均衡，也不需要单独的 Service IP。遇到这种情况，可以通过显式设置\n集群 IP（`spec.clusterIP`）的值为 `\"None\"` 来创建**无头服务（Headless Service）**。\n\n你可以使用无头 Service 与其他服务发现机制交互，而不必绑定到 Kubernetes 的实现。\n\n无头 Service 不会获得集群 IP，kube-proxy 不会处理这类 Service，\n而且平台也不会为它们提供负载均衡或路由支持。"}
{"en": "A headless Service allows a client to connect to whichever Pod it prefers, directly. Services that are headless don't\nconfigure routes and packet forwarding using\n[virtual IP addresses and proxies](/docs/reference/networking/virtual-ips/); instead, headless Services report the\nendpoint IP addresses of the individual pods via internal DNS records, served through the cluster's\n[DNS service](/docs/concepts/services-networking/dns-pod-service/).\nTo define a headless Service, you make a Service with `.spec.type` set to ClusterIP (which is also the default for `type`),\nand you additionally set `.spec.clusterIP` to None.", "zh": "无头 Service 允许客户端直接连接到它所偏好的任一 Pod。\n无头 Service 不使用[虚拟 IP 地址和代理](/zh-cn/docs/reference/networking/virtual-ips/)\n配置路由和数据包转发；相反，无头 Service 通过内部 DNS 记录报告各个\nPod 的端点 IP 地址，这些 DNS 记录是由集群的\n[DNS 服务](/zh-cn/docs/concepts/services-networking/dns-pod-service/)所提供的。\n这些 DNS 记录是由集群内部 DNS 服务所提供的\n要定义无头 Service，你需要将 `.spec.type` 设置为 ClusterIP（这也是 `type`\n的默认值），并进一步将 `.spec.clusterIP` 设置为 `None`。"}
{"en": "The string value None is a special case and is not the same as leaving the `.spec.clusterIP` field unset.\n\nHow DNS is automatically configured depends on whether the Service has selectors defined:", "zh": "字符串值 None 是一种特殊情况，与未设置 `.spec.clusterIP` 字段不同。\n\nDNS 如何自动配置取决于 Service 是否定义了选择器："}
{"en": "### With selectors\n\nFor headless Services that define selectors, the Kubernetes control plane creates\nEndpointSlices in the Kubernetes API, and modifies the DNS configuration to return\nA or AAAA records (IPv4 or IPv6 addresses) that point directly to the Pods backing\nthe Service.", "zh": "### 带选择算符的服务 {#with-selectors}\n\n对定义了选择算符的无头 Service，Kubernetes 控制平面在 Kubernetes API 中创建\nEndpointSlice 对象，并且修改 DNS 配置返回 A 或 AAAA 记录（IPv4 或 IPv6 地址），\n这些记录直接指向 Service 的后端 Pod 集合。"}
{"en": "### Without selectors\n\nFor headless Services that do not define selectors, the control plane does\nnot create EndpointSlice objects. However, the DNS system looks for and configures\neither:", "zh": "### 无选择算符的服务  {#without-selectors}\n\n对没有定义选择算符的无头 Service，控制平面不会创建 EndpointSlice 对象。\n然而 DNS 系统会执行以下操作之一："}
{"en": "* DNS CNAME records for [`type: ExternalName`](#externalname) Services.\n* DNS A / AAAA records for all IP addresses of the Service's ready endpoints,\n  for all Service types other than `ExternalName`.\n  * For IPv4 endpoints, the DNS system creates A records.\n  * For IPv6 endpoints, the DNS system creates AAAA records.", "zh": "* 对于 [`type: ExternalName`](#externalname) Service，查找和配置其 CNAME 记录；\n* 对所有其他类型的 Service，针对 Service 的就绪端点的所有 IP 地址，查找和配置\n  DNS A / AAAA 记录：\n  * 对于 IPv4 端点，DNS 系统创建 A 记录。\n  * 对于 IPv6 端点，DNS 系统创建 AAAA 记录。"}
{"en": "When you define a headless Service without a selector, the `port` must\nmatch the `targetPort`.", "zh": "当你定义无选择算符的无头 Service 时，`port` 必须与 `targetPort` 匹配。"}
{"en": "## Discovering services\n\nFor clients running inside your cluster, Kubernetes supports two primary modes of\nfinding a Service: environment variables and DNS.", "zh": "## 服务发现  {#discovering-services}\n\n对于在集群内运行的客户端，Kubernetes 支持两种主要的服务发现模式：环境变量和 DNS。"}
{"en": "### Environment variables\n\nWhen a Pod is run on a Node, the kubelet adds a set of environment variables\nfor each active Service. It adds `{SVCNAME}_SERVICE_HOST` and `{SVCNAME}_SERVICE_PORT` variables,\nwhere the Service name is upper-cased and dashes are converted to underscores.\n\nFor example, the Service `redis-primary` which exposes TCP port 6379 and has been\nallocated cluster IP address 10.0.0.11, produces the following environment\nvariables:", "zh": "### 环境变量   {#environment-variables}\n\n当 Pod 运行在某 Node 上时，kubelet 会在其中为每个活跃的 Service 添加一组环境变量。\nkubelet 会添加环境变量 `{SVCNAME}_SERVICE_HOST` 和 `{SVCNAME}_SERVICE_PORT`。\n这里 Service 的名称被转为大写字母，横线被转换成下划线。\n\n例如，一个 Service `redis-primary` 公开了 TCP 端口 6379，\n同时被分配了集群 IP 地址 10.0.0.11，这个 Service 生成的环境变量如下：\n\n```shell\nREDIS_PRIMARY_SERVICE_HOST=10.0.0.11\nREDIS_PRIMARY_SERVICE_PORT=6379\nREDIS_PRIMARY_PORT=tcp://10.0.0.11:6379\nREDIS_PRIMARY_PORT_6379_TCP=tcp://10.0.0.11:6379\nREDIS_PRIMARY_PORT_6379_TCP_PROTO=tcp\nREDIS_PRIMARY_PORT_6379_TCP_PORT=6379\nREDIS_PRIMARY_PORT_6379_TCP_ADDR=10.0.0.11\n```\n\n{{< note >}}"}
{"en": "When you have a Pod that needs to access a Service, and you are using\nthe environment variable method to publish the port and cluster IP to the client\nPods, you must create the Service *before* the client Pods come into existence.\nOtherwise, those client Pods won't have their environment variables populated.\n\nIf you only use DNS to discover the cluster IP for a Service, you don't need to\nworry about this ordering issue.", "zh": "当你的 Pod 需要访问某 Service，并且你在使用环境变量方法将端口和集群 IP 发布到客户端\nPod 时，必须在客户端 Pod 出现**之前**创建该 Service。\n否则，这些客户端 Pod 中将不会出现对应的环境变量。\n\n如果仅使用 DNS 来发现 Service 的集群 IP，则无需担心此顺序问题。\n{{< /note >}}"}
{"en": "Kubernetes also supports and provides variables that are compatible with Docker\nEngine's \"_[legacy container links](https://docs.docker.com/network/links/)_\" feature.\nYou can read [`makeLinkVariables`](https://github.com/kubernetes/kubernetes/blob/dd2d12f6dc0e654c15d5db57a5f9f6ba61192726/pkg/kubelet/envvars/envvars.go#L72)\nto see how this is implemented in Kubernetes.", "zh": "Kubernetes 还支持并提供与 Docker Engine 的\n\"**[legacy container links](https://docs.docker.com/network/links/)**\"\n兼容的变量。\n你可以阅读 [makeLinkVariables](https://github.com/kubernetes/kubernetes/blob/dd2d12f6dc0e654c15d5db57a5f9f6ba61192726/pkg/kubelet/envvars/envvars.go#L72)\n来了解这是如何在 Kubernetes 中实现的。\n\n### DNS"}
{"en": "You can (and almost always should) set up a DNS service for your Kubernetes\ncluster using an [add-on](/docs/concepts/cluster-administration/addons/).\n\nA cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new\nServices and creates a set of DNS records for each one.  If DNS has been enabled\nthroughout your cluster then all Pods should automatically be able to resolve\nServices by their DNS name.", "zh": "你可以（并且几乎总是应该）使用[插件（add-on）](/zh-cn/docs/concepts/cluster-administration/addons/)\n来为 Kubernetes 集群安装 DNS 服务。\n\n能够感知集群的 DNS 服务器（例如 CoreDNS）会监视 Kubernetes API 中的新 Service，\n并为每个 Service 创建一组 DNS 记录。如果在整个集群中都启用了 DNS，则所有 Pod\n都应该能够通过 DNS 名称自动解析 Service。"}
{"en": "For example, if you have a Service called `my-service` in a Kubernetes\nnamespace `my-ns`, the control plane and the DNS Service acting together\ncreate a DNS record for `my-service.my-ns`. Pods in the `my-ns` namespace\nshould be able to find the service by doing a name lookup for `my-service`\n(`my-service.my-ns` would also work).\n\nPods in other namespaces must qualify the name as `my-service.my-ns`. These names\nwill resolve to the cluster IP assigned for the Service.", "zh": "例如，如果你在 Kubernetes 命名空间 `my-ns` 中有一个名为 `my-service` 的 Service，\n则控制平面和 DNS 服务共同为 `my-service.my-ns` 生成 DNS 记录。\n名字空间 `my-ns` 中的 Pod 应该能够通过按名检索 `my-service` 来找到服务\n（`my-service.my-ns` 也可以）。\n\n其他名字空间中的 Pod 必须将名称限定为 `my-service.my-ns`。\n这些名称将解析为分配给 Service 的集群 IP。"}
{"en": "Kubernetes also supports DNS SRV (Service) records for named ports.  If the\n`my-service.my-ns` Service has a port named `http` with the protocol set to\n`TCP`, you can do a DNS SRV query for `_http._tcp.my-service.my-ns` to discover\nthe port number for `http`, as well as the IP address.\n\nThe Kubernetes DNS server is the only way to access `ExternalName` Services.\nYou can find more information about `ExternalName` resolution in\n[DNS for Services and Pods](/docs/concepts/services-networking/dns-pod-service/).", "zh": "Kubernetes 还支持命名端口的 DNS SRV（Service）记录。\n如果 Service `my-service.my-ns` 具有名为 `http`　的端口，且协议设置为 TCP，\n则可以用 `_http._tcp.my-service.my-ns` 执行 DNS SRV 查询以发现 `http` 的端口号以及 IP 地址。\n\nKubernetes DNS 服务器是唯一的一种能够访问 `ExternalName` 类型的 Service 的方式。\n关于 `ExternalName` 解析的更多信息可以查看\n[Service 与 Pod 的 DNS](/zh-cn/docs/concepts/services-networking/dns-pod-service/)。"}
{"en": "preserve existing hyperlinks", "zh": "<a id=\"shortcomings\" />\n<a id=\"the-gory-details-of-virtual-ips\" />\n<a id=\"proxy-modes\" />\n<a id=\"proxy-mode-userspace\" />\n<a id=\"proxy-mode-iptables\" />\n<a id=\"proxy-mode-ipvs\" />\n<a id=\"ips-and-vips\" />"}
{"en": "## Virtual IP addressing mechanism\n\nRead [Virtual IPs and Service Proxies](/docs/reference/networking/virtual-ips/) to learn about the\nmechanism Kubernetes provides to expose a Service with a virtual IP address.", "zh": "## 虚拟 IP 寻址机制   {#virtual-ip-addressing-mechanism}\n\n阅读[虚拟 IP 和 Service 代理](/zh-cn/docs/reference/networking/virtual-ips/)以了解\nKubernetes 提供的使用虚拟 IP 地址公开服务的机制。"}
{"en": "### Traffic distribution", "zh": "### 流量分发"}
{"en": "The `.spec.trafficDistribution` field provides another way to influence traffic\nrouting within a Kubernetes Service. While traffic policies focus on strict\nsemantic guarantees, traffic distribution allows you to express _preferences_\n(such as routing to topologically closer endpoints). This can help optimize for\nperformance, cost, or reliability. This optional field can be used if you have\nenabled the `ServiceTrafficDistribution` [feature\ngate](/docs/reference/command-line-tools-reference/feature-gates/) for your\ncluster and all of its nodes. In Kubernetes {{< skew currentVersion >}}, the\nfollowing field value is supported:", "zh": "`.spec.trafficDistribution` 字段提供了另一种影响 Kubernetes Service 内流量路由的方法。\n虽然流量策略侧重于严格的语义保证，但流量分发允许你表达一定的**偏好**（例如路由到拓扑上更接近的端点）。\n这一机制有助于优化性能、成本或可靠性。\n如果你为集群及其所有节点启用了 `ServiceTrafficDistribution`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)，\n则可以使用此可选字段。\nKubernetes {{< skew currentVersion >}} 支持以下字段值："}
{"en": "`PreferClose`\n: Indicates a preference for routing traffic to endpoints that are topologically\n  proximate to the client. The interpretation of \"topologically proximate\" may\n  vary across implementations and could encompass endpoints within the same\n  node, rack, zone, or even region. Setting this value gives implementations\n  permission to make different tradeoffs, e.g. optimizing for proximity rather\n  than equal distribution of load. Users should not set this value if such\n  tradeoffs are not acceptable.", "zh": "`PreferClose`\n: 表示优先将流量路由到拓扑上最接近客户端的端点。\n  “拓扑上邻近”的解释可能因实现而异，并且可能涵盖同一节点、机架、区域甚至区域内的端点。\n  设置此值允许实现进行不同的权衡，例如按距离优化而不是平均分配负载。\n  如果这种权衡不可接受，用户不应设置此值。"}
{"en": "If the field is not set, the implementation will apply its default routing strategy.\n\nSee [Traffic\nDistribution](/docs/reference/networking/virtual-ips/#traffic-distribution) for\nmore details", "zh": "如果未设置该字段，实现将应用其默认路由策略，\n详见[流量分发](/zh-cn/docs/reference/networking/virtual-ips/#traffic-distribution)。"}
{"en": "### Traffic policies\n\nYou can set the `.spec.internalTrafficPolicy` and `.spec.externalTrafficPolicy` fields\nto control how Kubernetes routes traffic to healthy (“ready”) backends.\n\nSee [Traffic Policies](/docs/reference/networking/virtual-ips/#traffic-policies) for more details.", "zh": "### 流量策略    {#traffic-policies}\n\n你可以设置 `.spec.internalTrafficPolicy` 和 `.spec.externalTrafficPolicy`\n字段来控制 Kubernetes 如何将流量路由到健康（“就绪”）的后端。\n\n有关详细信息，请参阅[流量策略](/zh-cn/docs/reference/networking/virtual-ips/#traffic-policies)。"}
{"en": "## Session stickiness\n\nIf you want to make sure that connections from a particular client are passed to\nthe same Pod each time, you can configure session affinity based on the client's\nIP address. Read [session affinity](/docs/reference/networking/virtual-ips/#session-affinity)\nto learn more.", "zh": "## 会话的黏性   {#session-stickiness}\n\n如果你想确保来自特定客户端的连接每次都传递到同一个 Pod，你可以配置基于客户端 IP\n地址的会话亲和性。可阅读[会话亲和性](/zh-cn/docs/reference/networking/virtual-ips/#session-affinity)\n来进一步学习。"}
{"en": "### External IPs\n\nIf there are external IPs that route to one or more cluster nodes, Kubernetes Services\ncan be exposed on those `externalIPs`. When network traffic arrives into the cluster, with\nthe external IP (as destination IP) and the port matching that Service, rules and routes\nthat Kubernetes has configured ensure that the traffic is routed to one of the endpoints\nfor that Service.\n\nWhen you define a Service, you can specify `externalIPs` for any\n[service type](#publishing-services-service-types).\nIn the example below, the Service named `\"my-service\"` can be accessed by clients using TCP,\non `\"198.51.100.32:80\"` (calculated from `.spec.externalIPs[]` and `.spec.ports[].port`).", "zh": "### 外部 IP  {#external-ips}\n\n如果有外部 IP 能够路由到一个或多个集群节点上，则 Kubernetes Service 可以在这些 `externalIPs`\n上公开出去。当网络流量进入集群时，如果外部 IP（作为目的 IP 地址）和端口都与该 Service 匹配，\nKubernetes 所配置的规则和路由会确保流量被路由到该 Service 的端点之一。\n\n定义 Service 时，你可以为任何[服务类型](#publishing-services-service-types)指定 `externalIPs`。\n\n在下面的例子中，名为 `my-service` 的 Service 可以在 \"`198.51.100.32:80`\"\n（根据 `.spec.externalIPs[]` 和 `.spec.ports[].port` 得出）上被客户端使用 TCP 协议访问。\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 49152\n  externalIPs:\n    - 198.51.100.32\n```\n\n{{< note >}}"}
{"en": "Kubernetes does not manage allocation of `externalIPs`; these are the responsibility\nof the cluster administrator.", "zh": "Kubernetes 不负责管理 `externalIPs` 的分配，这一工作是集群管理员的职责。\n{{< /note >}}"}
{"en": "## API Object\n\nService is a top-level resource in the Kubernetes REST API. You can find more details\nabout the [Service API object](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#service-v1-core).", "zh": "## API 对象   {#api-object}\n\nService 是 Kubernetes REST API 中的顶级资源。你可以找到有关\n[Service 对象 API](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#service-v1-core)\n的更多详细信息。"}
{"en": "preserve existing hyperlinks", "zh": "<a id=\"shortcomings\" /><a id=\"#the-gory-details-of-virtual-ips\" />\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "Learn more about Services and how they fit into Kubernetes:\n\n* Follow the [Connecting Applications with Services](/docs/tutorials/services/connect-applications-service/)\n  tutorial.\n* Read about [Ingress](/docs/concepts/services-networking/ingress/), which\n  exposes HTTP and HTTPS routes from outside the cluster to Services within\n  your cluster.\n* Read about [Gateway](/docs/concepts/services-networking/gateway/), an extension to\n  Kubernetes that provides more flexibility than Ingress.", "zh": "进一步学习 Service 及其在 Kubernetes 中所发挥的作用：\n\n* 完成[使用 Service 连接到应用](/zh-cn/docs/tutorials/services/connect-applications-service/)教程。\n* 阅读 [Ingress](/zh-cn/docs/concepts/services-networking/ingress/) 文档。Ingress\n  负责将来自集群外部的 HTTP 和 HTTPS 请求路由给集群内的服务。\n* 阅读 [Gateway](/zh-cn/docs/concepts/services-networking/gateway/) 文档。Gateway 作为 Kubernetes 的扩展提供比\n  Ingress 更高的灵活性。"}
{"en": "For more context, read the following:\n\n* [Virtual IPs and Service Proxies](/docs/reference/networking/virtual-ips/)\n* [EndpointSlices](/docs/concepts/services-networking/endpoint-slices/)\n* [Service API reference](/docs/reference/kubernetes-api/service-resources/service-v1/)\n* [EndpointSlice API reference](/docs/reference/kubernetes-api/service-resources/endpoint-slice-v1/)\n* [Endpoint API reference (legacy)](/docs/reference/kubernetes-api/service-resources/endpoints-v1/)", "zh": "更多上下文，可以阅读以下内容：\n\n* [虚拟 IP 和 Service 代理](/zh-cn/docs/reference/networking/virtual-ips/)\n* [EndpointSlice](/zh-cn/docs/concepts/services-networking/endpoint-slices/)\n* [Service API 参考](/zh-cn/docs/reference/kubernetes-api/service-resources/service-v1/)\n* [EndpointSlice API 参考](/zh-cn/docs/reference/kubernetes-api/service-resources/endpoint-slice-v1/)\n* [Endpoints API 参考](/zh-cn/docs/reference/kubernetes-api/service-resources/endpoints-v1/)"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.23\" state=\"beta\" >}}\n\n{{< note >}}"}
{"en": "Prior to Kubernetes 1.27, this feature was known as _Topology Aware Hints_.", "zh": "在 Kubernetes 1.27 之前，此特性称为**拓扑感知提示（Topology Aware Hint）**。\n{{</ note >}}"}
{"en": "_Topology Aware Routing_ adjusts routing behavior to prefer keeping traffic in\nthe zone it originated from. In some cases this can help reduce costs or improve\nnetwork performance.", "zh": "**拓扑感知路由（Toplogy Aware Routing）** 调整路由行为，以优先保持流量在其发起区域内。\n在某些情况下，这有助于降低成本或提高网络性能。"}
{"en": "## Motivation\n\nKubernetes clusters are increasingly deployed in multi-zone environments.\n_Topology Aware Routing_ provides a mechanism to help keep traffic within the\nzone it originated from. When calculating the endpoints for a {{<\nglossary_tooltip term_id=\"Service\" >}}, the EndpointSlice controller considers\nthe topology (region and zone) of each endpoint and populates the hints field to\nallocate it to a zone. Cluster components such as {{< glossary_tooltip\nterm_id=\"kube-proxy\" text=\"kube-proxy\" >}} can then consume those hints, and use\nthem to influence how the traffic is routed (favoring topologically closer\nendpoints).", "zh": "## 动机   {#motivation}\n\nKubernetes 集群越来越多地部署在多区域环境中。\n**拓扑感知路由** 提供了一种机制帮助流量保留在其发起所在的区域内。\n计算 {{<glossary_tooltip term_id=\"Service\">}} 的端点时，\nEndpointSlice 控制器考虑每个端点的物理拓扑（地区和区域），并填充提示字段以将其分配到区域。\n诸如 {{<glossary_tooltip term_id=\"kube-proxy\" text=\"kube-proxy\">}}\n等集群组件可以使用这些提示，影响流量的路由方式（优先考虑物理拓扑上更近的端点）。"}
{"en": "## Enabling Topology Aware Routing", "zh": "## 启用拓扑感知路由   {#enabling-topology-aware-routing}\n\n{{< note >}}"}
{"en": "Prior to Kubernetes 1.27, this behavior was controlled using the\n`service.kubernetes.io/topology-aware-hints` annotation.", "zh": "在 Kubernetes 1.27 之前，此行为是通过 `service.kubernetes.io/topology-aware-hints` 注解来控制的。\n{{</ note >}}"}
{"en": "You can enable Topology Aware Routing for a Service by setting the\n`service.kubernetes.io/topology-mode` annotation to `Auto`. When there are\nenough endpoints available in each zone, Topology Hints will be populated on\nEndpointSlices to allocate individual endpoints to specific zones, resulting in\ntraffic being routed closer to where it originated from.", "zh": "你可以通过将 `service.kubernetes.io/topology-mode` 注解设置为 `Auto` 来启用 Service 的拓扑感知路由。\n当每个区域中有足够的端点可用时，系统将为 EndpointSlices 填充拓扑提示，把每个端点分配给特定区域，\n从而使流量被路由到更接近其来源的位置。"}
{"en": "## When it works best\n\nThis feature works best when:", "zh": "## 何时效果最佳   {#when-it-works-best}\n\n此特性在以下场景中的工作效果最佳："}
{"en": "### 1. Incoming traffic is evenly distributed\n\nIf a large proportion of traffic is originating from a single zone, that traffic\ncould overload the subset of endpoints that have been allocated to that zone.\nThis feature is not recommended when incoming traffic is expected to originate\nfrom a single zone.", "zh": "### 1. 入站流量均匀分布   {#incoming-traffic-is-evently-distributed}\n\n如果大部分流量源自同一个区域，则该流量可能会使分配到该区域的端点子集过载。\n当预计入站流量源自同一区域时，不建议使用此特性。"}
{"en": "### 2. The Service has 3 or more endpoints per zone {#three-or-more-endpoints-per-zone}\n\nIn a three zone cluster, this means 9 or more endpoints. If there are fewer than\n3 endpoints per zone, there is a high (≈50%) probability that the EndpointSlice\ncontroller will not be able to allocate endpoints evenly and instead will fall\nback to the default cluster-wide routing approach.", "zh": "### 2. 服务在每个区域具有至少 3 个端点 {#three-or-more-endpoints-per-zone}\n\n在一个三区域的集群中，这意味着有至少 9 个端点。如果每个区域的端点少于 3 个，\n则 EndpointSlice 控制器很大概率（约 50％）无法平均分配端点，而是回退到默认的集群范围的路由方法。"}
{"en": "## How It Works\n\nThe \"Auto\" heuristic attempts to proportionally allocate a number of endpoints\nto each zone. Note that this heuristic works best for Services that have a\nsignificant number of endpoints.", "zh": "## 工作原理 {#how-it-works}\n\n“自动”启发式算法会尝试按比例分配一定数量的端点到每个区域。\n请注意，这种启发方式对具有大量端点的 Service 效果最佳。"}
{"en": "### EndpointSlice controller {#implementation-control-plane}\n\nThe EndpointSlice controller is responsible for setting hints on EndpointSlices\nwhen this heuristic is enabled. The controller allocates a proportional amount of\nendpoints to each zone. This proportion is based on the\n[allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\nCPU cores for nodes running in that zone. For example, if one zone had 2 CPU\ncores and another zone only had 1 CPU core, the controller would allocate twice\nas many endpoints to the zone with 2 CPU cores.", "zh": "### EndpointSlice 控制器 {#implementation-control-plane}\n\n当启用此启发方式时，EndpointSlice 控制器负责在各个 EndpointSlice 上设置提示信息。\n控制器按比例给每个区域分配一定比例数量的端点。\n这个比例基于在该区域中运行的节点的[可分配](/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\nCPU 核心数。例如，如果一个区域有 2 个 CPU 核心，而另一个区域只有 1 个 CPU 核心，\n那么控制器将给那个有 2 CPU 的区域分配两倍数量的端点。"}
{"en": "The following example shows what an EndpointSlice looks like when hints have\nbeen populated:", "zh": "以下示例展示了提供提示信息后 EndpointSlice 的样子：\n\n```yaml\napiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\n  name: example-hints\n  labels:\n    kubernetes.io/service-name: example-svc\naddressType: IPv4\nports:\n  - name: http\n    protocol: TCP\n    port: 80\nendpoints:\n  - addresses:\n      - \"10.1.2.3\"\n    conditions:\n      ready: true\n    hostname: pod-1\n    zone: zone-a\n    hints:\n      forZones:\n        - name: \"zone-a\"\n```\n\n### kube-proxy {#implementation-kube-proxy}"}
{"en": "The kube-proxy component filters the endpoints it routes to based on the hints set by\nthe EndpointSlice controller. In most cases, this means that the kube-proxy is able\nto route traffic to endpoints in the same zone. Sometimes the controller allocates endpoints\nfrom a different zone to ensure more even distribution of endpoints between zones.\nThis would result in some traffic being routed to other zones.", "zh": "kube-proxy 组件依据 EndpointSlice 控制器设置的提示，过滤由它负责路由的端点。\n在大多数场合，这意味着 kube-proxy 可以把流量路由到同一个区域的端点。\n有时，控制器在另一不同的区域中分配端点，以确保在多个区域之间更平均地分配端点。\n这会导致部分流量被路由到其他区域。"}
{"en": "## Safeguards", "zh": "## 保护措施 {#safeguards}"}
{"en": "The Kubernetes control plane and the kube-proxy on each node apply some\nsafeguard rules before using Topology Aware Hints. If these don't check out,\nthe kube-proxy selects endpoints from anywhere in your cluster, regardless of the\nzone.", "zh": "Kubernetes 控制平面和每个节点上的 kube-proxy 在使用拓扑感知提示信息前，会应用一些保护措施规则。\n如果规则无法顺利通过，kube-proxy 将无视区域限制，从集群中的任意位置选择端点。"}
{"en": "1. **Insufficient number of endpoints:** If there are less endpoints than zones\n   in a cluster, the controller will not assign any hints.", "zh": "1. **端点数量不足：** 如果一个集群中，端点数量少于区域数量，控制器不创建任何提示。"}
{"en": "2. **Impossible to achieve balanced allocation:** In some cases, it will be\n   impossible to achieve a balanced allocation of endpoints among zones. For\n   example, if zone-a is twice as large as zone-b, but there are only 2\n   endpoints, an endpoint allocated to zone-a may receive twice as much traffic\n   as zone-b. The controller does not assign hints if it can't get this \"expected\n   overload\" value below an acceptable threshold for each zone. Importantly this\n   is not based on real-time feedback. It is still possible for individual\n   endpoints to become overloaded.", "zh": "2. **不可能实现均衡分配：** 在一些场合中，不可能实现端点在区域中的平衡分配。\n   例如，假设 zone-a 比 zone-b 大两倍，但只有 2 个端点，\n   那分配到 zone-a 的端点可能收到比 zone-b 多两倍的流量。\n   如果控制器不能确保此“期望的过载”值低于每一个区域可接受的阈值，控制器将不添加提示信息。\n   重要的是，这不是基于实时反馈。所以对于特定的端点仍有可能超载。"}
{"en": "3. **One or more Nodes has insufficient information:** If any node does not have\n   a `topology.kubernetes.io/zone` label or is not reporting a value for\n   allocatable CPU, the control plane does not set any topology-aware endpoint\n   hints and so kube-proxy does not filter endpoints by zone.", "zh": "3. **一个或多个 Node 信息不足：** 如果任一节点没有设置标签 `topology.kubernetes.io/zone`，\n   或没有上报可分配的 CPU 数据，控制平面将不会设置任何拓扑感知提示，\n   进而 kube-proxy 也就不能根据区域来过滤端点。"}
{"en": "4. **One or more endpoints does not have a zone hint:** When this happens,\n   the kube-proxy assumes that a transition from or to Topology Aware Hints is\n   underway. Filtering endpoints for a Service in this state would be dangerous\n   so the kube-proxy falls back to using all endpoints.", "zh": "4. **至少一个端点没有设置区域提示：** 当这种情况发生时，\n   kube-proxy 会假设从拓扑感知提示到拓扑感知路由（或反方向）的迁移仍在进行中，\n   在这种场合下过滤 Service 的端点是有风险的，所以 kube-proxy 回退到使用所有端点。"}
{"en": "5. **A zone is not represented in hints:** If the kube-proxy is unable to find\n   at least one endpoint with a hint targeting the zone it is running in, it falls\n   back to using endpoints from all zones. This is most likely to happen as you add\n   a new zone into your existing cluster.", "zh": "5. **提示中不存在某区域：** 如果 kube-proxy 无法找到提示中指向它当前所在的区域的端点，\n   它将回退到使用来自所有区域的端点。当你向现有集群新增新的区域时，这种情况发生概率很高。"}
{"en": "## Constraints", "zh": "## 限制 {#constraints}"}
{"en": "* Topology Aware Hints are not used when `internalTrafficPolicy` is set to `Local`\n  on a Service. It is possible to use both features in the same cluster on different\n  Services, just not on the same Service.", "zh": "* 当 Service 的 `internalTrafficPolicy` 值设置为 `Local` 时，\n  系统将不使用拓扑感知提示信息。你可以在同一集群中的不同 Service 上使用这两个特性，\n  但不能在同一个 Service 上这么做。"}
{"en": "* This approach will not work well for Services that have a large proportion of\n  traffic originating from a subset of zones. Instead this assumes that incoming\n  traffic will be roughly proportional to the capacity of the Nodes in each\n  zone.", "zh": "* 这种方法不适用于大部分流量来自于一部分区域的 Service。\n  相反，这项技术的假设是入站流量与各区域中节点的服务能力成比例关系。"}
{"en": "* The EndpointSlice controller ignores unready nodes as it calculates the\n  proportions of each zone. This could have unintended consequences if a large\n  portion of nodes are unready.", "zh": "* EndpointSlice 控制器在计算各区域的比例时，会忽略未就绪的节点。\n  在大部分节点未就绪的场景下，这样做会带来非预期的结果。"}
{"en": "* The EndpointSlice controller ignores nodes with the\n  `node-role.kubernetes.io/control-plane` or `node-role.kubernetes.io/master`\n  label set. This could be problematic if workloads are also running on those\n  nodes.", "zh": "* EndpointSlice 控制器忽略设置了 `node-role.kubernetes.io/control-plane` 或\n  `node-role.kubernetes.io/master` 标签的节点。如果工作负载也在这些节点上运行，也可能会产生问题。"}
{"en": "* The EndpointSlice controller does not take into account {{< glossary_tooltip\n  text=\"tolerations\" term_id=\"toleration\" >}} when deploying or calculating the\n  proportions of each zone. If the Pods backing a Service are limited to a\n  subset of Nodes in the cluster, this will not be taken into account.", "zh": "* EndpointSlice 控制器在分派或计算各区域的比例时，并不会考虑\n  {{< glossary_tooltip text=\"容忍度\" term_id=\"toleration\" >}}。\n  如果 Service 背后的各 Pod 被限制只能运行在集群节点的一个子集上，计算比例时不会考虑这点。"}
{"en": "* This may not work well with autoscaling. For example, if a lot of traffic is\n  originating from a single zone, only the endpoints allocated to that zone will\n  be handling that traffic. That could result in {{< glossary_tooltip\n  text=\"Horizontal Pod Autoscaler\" term_id=\"horizontal-pod-autoscaler\" >}}\n  either not picking up on this event, or newly added pods starting in a\n  different zone.", "zh": "* 这项技术和自动扩缩容机制之间可能存在冲突。例如，如果大量流量来源于同一个区域，\n  那只有分配到该区域的端点才可用来处理流量。这会导致\n  {{< glossary_tooltip text=\"Pod 自动水平扩缩容\" term_id=\"horizontal-pod-autoscaler\" >}}\n  要么不能处理这种场景，要么会在别的区域添加 Pod。"}
{"en": "## Custom heuristics", "zh": "## 自定义启发方式   {#custom-heuristics}"}
{"en": "Kubernetes is deployed in many different ways, there is no single heuristic for\nallocating endpoints to zones will work for every use case. A key goal of this\nfeature is to enable custom heuristics to be developed if the built in heuristic\ndoes not work for your use case. The first steps to enable custom heuristics\nwere included in the 1.27 release. This is a limited implementation that may not\nyet cover some relevant and plausible situations.", "zh": "Kubernetes 的部署方式有很多种，没有一种按区域分配端点的启发式方法能够适用于所有场景。\n此特性的一个关键目标是：如果内置的启发方式不能满足你的使用场景，则可以开发自定义的启发方式。\n启用自定义启发方式的第一步包含在了 1.27 版本中。\n这是一个限制性较强的实现，可能尚未涵盖一些重要的、可进一步探索的场景。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Follow the [Connecting Applications with Services](/docs/tutorials/services/connect-applications-service/) tutorial\n* Learn about the\n  [trafficDistribution](/docs/concepts/services-networking/service/#traffic-distribution)\n  field, which is closely related to the `service.kubernetes.io/topology-mode`\n  annotation and provides flexible options for traffic routing within\n  Kubernetes.", "zh": "* 参阅[使用 Service 连接到应用](/zh-cn/docs/tutorials/services/connect-applications-service/)教程。\n* 进一步了解 [trafficDistribution](/zh-cn/docs/concepts/services-networking/service/#traffic-distribution)字段，\n  该字段与 `service.kubernetes.io/topology-mode` 注解密切相关，并为 Kubernetes 中的流量路由提供灵活的配置选项。"}
{"en": "Kubernetes supports running nodes on either Linux or Windows. You can mix both kinds of node within a single cluster.\nThis page provides an overview to networking specific to the Windows operating system.", "zh": "Kubernetes 支持运行 Linux 或 Windows 节点。\n你可以在统一集群内混布这两种节点。\n本页提供了特定于 Windows 操作系统的网络概述。"}
{"en": "## Container networking on Windows {#networking}\n\nNetworking for Windows containers is exposed through\n[CNI plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/).\nWindows containers function similarly to virtual machines in regards to\nnetworking. Each container has a virtual network adapter (vNIC) which is connected\nto a Hyper-V virtual switch (vSwitch). The Host Networking Service (HNS) and the\nHost Compute Service (HCS) work together to create containers and attach container\nvNICs to networks. HCS is responsible for the management of containers whereas HNS\nis responsible for the management of networking resources such as:\n\n* Virtual networks (including creation of vSwitches)\n* Endpoints / vNICs\n* Namespaces\n* Policies including packet encapsulations, load-balancing rules, ACLs, and NAT rules.", "zh": "## Windows 容器网络 {#networking}\n\nWindows 容器网络通过 [CNI 插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)暴露。\nWindows 容器网络的工作方式与虚拟机类似。\n每个容器都有一个连接到 Hyper-V 虚拟交换机（vSwitch）的虚拟网络适配器（vNIC）。\n主机网络服务（Host Networking Service，HNS）和主机计算服务（Host Compute Service，HCS）\n协同创建容器并将容器 vNIC 挂接到网络。\nHCS 负责管理容器，而 HNS 负责管理以下网络资源：\n\n* 虚拟网络（包括创建 vSwitch）\n* Endpoint / vNIC\n* 命名空间\n* 包括数据包封装、负载均衡规则、ACL 和 NAT 规则在内的策略。"}
{"en": "The Windows HNS and vSwitch implement namespacing and can\ncreate virtual NICs as needed for a pod or container. However, many configurations such\nas DNS, routes, and metrics are stored in the Windows registry database rather than as\nfiles inside `/etc`, which is how Linux stores those configurations. The Windows registry for the container\nis separate from that of the host, so concepts like mapping `/etc/resolv.conf` from\nthe host into a container don't have the same effect they would on Linux. These must\nbe configured using Windows APIs run in the context of that container. Therefore\nCNI implementations need to call the HNS instead of relying on file mappings to pass\nnetwork details into the pod or container.", "zh": "Windows HNS 和 vSwitch 实现命名空间划分，且可以按需为 Pod 或容器创建虚拟 NIC。\n然而，诸如 DNS、路由和指标等许多配置将存放在 Windows 注册表数据库中，\n而不是像 Linux 将这些配置作为文件存放在 `/etc` 内。\n针对容器的 Windows 注册表与主机的注册表是分开的，因此将 `/etc/resolv.conf`\n从主机映射到一个容器的类似概念与 Linux 上的效果不同。\n这些必须使用容器环境中运行的 Windows API 进行配置。\n因此，实现 CNI 时需要调用 HNS，而不是依赖文件映射将网络详情传递到 Pod 或容器中。"}
{"en": "## Network modes\n\nWindows supports five different networking drivers/modes: L2bridge, L2tunnel,\nOverlay (Beta), Transparent, and NAT. In a heterogeneous cluster with Windows and Linux\nworker nodes, you need to select a networking solution that is compatible on both\nWindows and Linux. The following table lists the out-of-tree plugins are supported on Windows,\nwith recommendations on when to use each CNI:", "zh": "## 网络模式 {#network-mode}\n\nWindows 支持五种不同的网络驱动/模式：L2bridge、L2tunnel、Overlay (Beta)、Transparent 和 NAT。\n在 Windows 和 Linux 工作节点组成的异构集群中，你需要选择一个同时兼容 Windows 和 Linux 的网络方案。\n下表列出了 Windows 支持的树外插件，并给出了何时使用每种 CNI 的建议："}
{"en": "| Network Driver | Description | Container Packet Modifications | Network Plugins | Network Plugin Characteristics |\n| -------------- | ----------- | ------------------------------ | --------------- | ------------------------------ |\n| L2bridge       | Containers are attached to an external vSwitch. Containers are attached to the underlay network, although the physical network doesn't need to learn the container MACs because they are rewritten on ingress/egress. | MAC is rewritten to host MAC, IP may be rewritten to host IP using HNS OutboundNAT policy. | [win-bridge](https://www.cni.dev/plugins/current/main/win-bridge/), [Azure-CNI](https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md), [Flannel host-gateway](https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#host-gw) uses win-bridge | win-bridge uses L2bridge network mode, connects containers to the underlay of hosts, offering best performance. Requires user-defined routes (UDR) for inter-node connectivity. |\n| L2Tunnel | This is a special case of l2bridge, but only used on Azure. All packets are sent to the virtualization host where SDN policy is applied. | MAC rewritten, IP visible on the underlay network | [Azure-CNI](https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md) | Azure-CNI allows integration of containers with Azure vNET, and allows them to leverage the set of capabilities that [Azure Virtual Network provides](https://azure.microsoft.com/en-us/services/virtual-network/). For example, securely connect to Azure services or use Azure NSGs. See [azure-cni for some examples](https://docs.microsoft.com/azure/aks/concepts-network#azure-cni-advanced-networking) |\n| Overlay | Containers are given a vNIC connected to an external vSwitch. Each overlay network gets its own IP subnet, defined by a custom IP prefix.The overlay network driver uses VXLAN encapsulation. | Encapsulated with an outer header. | [win-overlay](https://www.cni.dev/plugins/current/main/win-overlay/), [Flannel VXLAN](https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#vxlan) (uses win-overlay) | win-overlay should be used when virtual container networks are desired to be isolated from underlay of hosts (e.g. for security reasons). Allows for IPs to be re-used for different overlay networks (which have different VNID tags)  if you are restricted on IPs in your datacenter.  This option requires [KB4489899](https://support.microsoft.com/help/4489899) on Windows Server 2019. |\n| Transparent (special use case for [ovn-kubernetes](https://github.com/openvswitch/ovn-kubernetes)) | Requires an external vSwitch. Containers are attached to an external vSwitch which enables intra-pod communication via logical networks (logical switches and routers). | Packet is encapsulated either via [GENEVE](https://datatracker.ietf.org/doc/draft-gross-geneve/) or [STT](https://datatracker.ietf.org/doc/draft-davie-stt/) tunneling to reach pods which are not on the same host.  <br/> Packets are forwarded or dropped via the tunnel metadata information supplied by the ovn network controller. <br/> NAT is done for north-south communication. | [ovn-kubernetes](https://github.com/openvswitch/ovn-kubernetes) | [Deploy via ansible](https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib). Distributed ACLs can be applied via Kubernetes policies. IPAM support. Load-balancing can be achieved without kube-proxy. NATing is done without using iptables/netsh. |\n| NAT (*not used in Kubernetes*) | Containers are given a vNIC connected to an internal vSwitch. DNS/DHCP is provided using an internal component called [WinNAT](https://techcommunity.microsoft.com/t5/virtualization/windows-nat-winnat-capabilities-and-limitations/ba-p/382303) | MAC and IP is rewritten to host MAC/IP. | [nat](https://github.com/Microsoft/windows-container-networking/tree/master/plugins/nat) | Included here for completeness |", "zh": "| 网络驱动 | 描述 | 容器数据包修改 | 网络插件 | 网络插件特点 |\n| -------------- | ----------- | ------------------------------ | --------------- | ------------------------------ |\n| L2bridge       | 容器挂接到一个外部 vSwitch。容器挂接到下层网络，但物理网络不需要了解容器的 MAC，因为这些 MAC 在入站/出站时被重写。 | MAC 被重写为主机 MAC，可使用 HNS OutboundNAT 策略将 IP 重写为主机 IP。 | [win-bridge](https://www.cni.dev/plugins/current/main/win-bridge/), [Azure-CNI](https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md), [Flannel host-gateway](https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#host-gw) 使用 win-bridge| win-bridge 使用 L2bridge 网络模式，将容器连接到主机的下层，提供最佳性能。节点间连接需要用户定义的路由（UDR）。 |\n| L2Tunnel | 这是 L2bridge 的一种特例，但仅用在 Azure 上。所有数据包都会被发送到应用了 SDN 策略的虚拟化主机。 | MAC 被重写，IP 在下层网络上可见。| [Azure-CNI](https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md) | Azure-CNI 允许将容器集成到 Azure vNET，允许容器充分利用 [Azure 虚拟网络](https://azure.microsoft.com/zh-cn/services/virtual-network/)所提供的能力集合。例如，安全地连接到 Azure 服务或使用 Azure NSG。参考 [azure-cni 了解有关示例](https://docs.microsoft.com/zh-cn/azure/aks/concepts-network#azure-cni-advanced-networking)。 |\n| Overlay | 容器被赋予一个 vNIC，连接到外部 vSwitch。每个上层网络都有自己的 IP 子网，由自定义 IP 前缀进行定义。该上层网络驱动使用 VXLAN 封装。 | 用外部头进行封装。 | [win-overlay](https://www.cni.dev/plugins/current/main/win-overlay/), [Flannel VXLAN](https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#vxlan)（使用 win-overlay） | 当需要将虚拟容器网络与主机的下层隔离时（例如出于安全原因），应使用 win-overlay。如果你的数据中心的 IP 个数有限，可以将 IP 在不同的上层网络中重用（带有不同的 VNID 标记）。在 Windows Server 2019 上这个选项需要 [KB4489899](https://support.microsoft.com/zh-cn/help/4489899)。 |\n| Transparent（[ovn-kubernetes](https://github.com/openvswitch/ovn-kubernetes) 的特殊用例） | 需要一个外部 vSwitch。容器挂接到一个外部 vSwitch，由后者通过逻辑网络（逻辑交换机和路由器）实现 Pod 内通信。 | 数据包通过 [GENEVE](https://datatracker.ietf.org/doc/draft-gross-geneve/) 或 [STT](https://datatracker.ietf.org/doc/draft-davie-stt/) 隧道进行封装，以到达其它主机上的 Pod。  <br/> 数据包基于 OVN 网络控制器提供的隧道元数据信息被转发或丢弃。<br/>南北向通信使用 NAT。 | [ovn-kubernetes](https://github.com/openvswitch/ovn-kubernetes) | [通过 ansible 部署](https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib)。通过 Kubernetes 策略可以实施分布式 ACL。支持 IPAM。无需 kube-proxy 即可实现负载均衡。无需 iptables/netsh 即可进行 NAT。 |\n| NAT（**Kubernetes 中未使用**） | 容器被赋予一个 vNIC，连接到内部 vSwitch。DNS/DHCP 是使用一个名为 [WinNAT 的内部组件](https://techcommunity.microsoft.com/t5/virtualization/windows-nat-winnat-capabilities-and-limitations/ba-p/382303)实现的 | MAC 和 IP 重写为主机 MAC/IP。 | [nat](https://github.com/Microsoft/windows-container-networking/tree/master/plugins/nat) | 放在此处保持完整性。 |"}
{"en": "As outlined above, the [Flannel](https://github.com/coreos/flannel)\n[CNI plugin](https://github.com/flannel-io/cni-plugin)\nis also [supported](https://github.com/flannel-io/cni-plugin#windows-support-experimental) on Windows via the\n[VXLAN network backend](https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan) (**Beta support** ; delegates to win-overlay)\nand [host-gateway network backend](https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw) (stable support; delegates to win-bridge).", "zh": "如上所述，Windows 通过 [VXLAN 网络后端](https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan)（**Beta 支持**；委派给 win-overlay）\n和 [host-gateway 网络后端](https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw)（稳定支持；委派给 win-bridge）\n也[支持](https://github.com/flannel-io/cni-plugin#windows-support-experimental) [Flannel](https://github.com/coreos/flannel) 的 [CNI 插件](https://github.com/flannel-io/cni-plugin)。"}
{"en": "This plugin supports delegating to one of the reference CNI plugins (win-overlay,\nwin-bridge), to work in conjunction with Flannel daemon on Windows (Flanneld) for\nautomatic node subnet lease assignment and HNS network creation. This plugin reads\nin its own configuration file (cni.conf), and aggregates it with the environment\nvariables from the FlannelD generated subnet.env file. It then delegates to one of\nthe reference CNI plugins for network plumbing, and sends the correct configuration\ncontaining the node-assigned subnet to the IPAM plugin (for example: `host-local`).", "zh": "此插件支持委派给参考 CNI 插件（win-overlay、win-bridge）之一，配合使用 Windows\n上的 Flannel 守护程序（Flanneld），以便自动分配节点子网租赁并创建 HNS 网络。\n该插件读取自己的配置文件（cni.conf），并聚合 FlannelD 生成的 subnet.env 文件中的环境变量。\n然后，委派给网络管道的参考 CNI 插件之一，并将包含节点分配子网的正确配置发送给 IPAM 插件（例如：`host-local`）。"}
{"en": "For Node, Pod, and Service objects, the following network flows are supported for\nTCP/UDP traffic:\n\n* Pod → Pod (IP)\n* Pod → Pod (Name)\n* Pod → Service (Cluster IP)\n* Pod → Service (PQDN, but only if there are no \".\")\n* Pod → Service (FQDN)\n* Pod → external (IP)\n* Pod → external (DNS)\n* Node → Pod\n* Pod → Node", "zh": "对于 Node、Pod 和 Service 对象，TCP/UDP 流量支持以下网络流：\n\n* Pod → Pod（IP）\n* Pod → Pod（名称）\n* Pod → Service（集群 IP）\n* Pod → Service（PQDN，但前提是没有 \".\"）\n* Pod → Service（FQDN）\n* Pod → 外部（IP）\n* Pod → 外部（DNS）\n* Node → Pod\n* Pod → Node"}
{"en": "## IP address management (IPAM) {#ipam}\n\nThe following IPAM options are supported on Windows:\n\n* [host-local](https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local)\n* [azure-vnet-ipam](https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md) (for azure-cni only)\n* [Windows Server IPAM](https://docs.microsoft.com/windows-server/networking/technologies/ipam/ipam-top) (fallback option if no IPAM is set)", "zh": "## IP 地址管理（IPAM） {#ipam}\n\nWindows 支持以下 IPAM 选项：\n\n* [host-local](https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local)\n* [azure-vnet-ipam](https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md)（仅适用于 azure-cni）\n* [Windows Server IPAM](https://docs.microsoft.com/zh-cn/windows-server/networking/technologies/ipam/ipam-top)（未设置 IPAM 时的回滚选项）"}
{"en": "## Load balancing and Services\n\nA Kubernetes {{< glossary_tooltip text=\"Service\" term_id=\"service\" >}} is an abstraction\nthat defines a logical set of Pods and a means to access them over a network.\nIn a cluster that includes Windows nodes, you can use the following types of Service:", "zh": "## 负载均衡和 Service {#load-balancing-and-services}\n\nKubernetes {{< glossary_tooltip text=\"Service\" term_id=\"service\" >}} 是一种抽象：定义了逻辑上的一组 Pod 和一种通过网络访问这些 Pod 的方式。\n在包含 Windows 节点的集群中，你可以使用以下类别的 Service：\n\n* `NodePort`\n* `ClusterIP`\n* `LoadBalancer`\n* `ExternalName`"}
{"en": "Windows container networking differs in some important ways from Linux networking.\nThe [Microsoft documentation for Windows Container Networking](https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture)\nprovides additional details and background.\n\nOn Windows, you can use the following settings to configure Services and load\nbalancing behavior:", "zh": "Windows 容器网络与 Linux 网络有着很重要的差异。\n更多细节和背景信息，参考 [Microsoft Windows 容器网络文档](https://docs.microsoft.com/zh-cn/virtualization/windowscontainers/container-networking/architecture)。\n\n在 Windows 上，你可以使用以下设置来配置 Service 和负载均衡行为："}
{"en": "{{< table caption=\"Windows Service Settings\" >}}\n| Feature | Description | Minimum Supported Windows OS build | How to enable |\n| ------- | ----------- | -------------------------- | ------------- |\n| Session affinity | Ensures that connections from a particular client are passed to the same Pod each time. | Windows Server 2022 | Set `service.spec.sessionAffinity` to \"ClientIP\" |\n| Direct Server Return (DSR) | Load balancing mode where the IP address fixups and the LBNAT occurs at the container vSwitch port directly; service traffic arrives with the source IP set as the originating pod IP. | Windows Server 2019 | Set the following flags in kube-proxy: `--feature-gates=\"WinDSR=true\" --enable-dsr=true` |\n| Preserve-Destination | Skips DNAT of service traffic, thereby preserving the virtual IP of the target service in packets reaching the backend Pod. Also disables node-node forwarding. | Windows Server, version 1903 | Set `\"preserve-destination\": \"true\"` in service annotations and enable DSR in kube-proxy. |\n| IPv4/IPv6 dual-stack networking | Native IPv4-to-IPv4 in parallel with IPv6-to-IPv6 communications to, from, and within a cluster | Windows Server 2019 | See [IPv4/IPv6 dual-stack](/docs/concepts/services-networking/dual-stack/#windows-support) |\n| Client IP preservation | Ensures that source IP of incoming ingress traffic gets preserved. Also disables node-node forwarding. |  Windows Server 2019  | Set `service.spec.externalTrafficPolicy` to \"Local\" and enable DSR in kube-proxy |\n{{< /table >}}", "zh": "{{< table caption=\"Windows Service 设置\" >}}\n| 功能特性 | 描述 | 支持的 Windows 操作系统最低版本 | 启用方式 |\n| ------- | ----------- | -------------------------- | ------------- |\n| 会话亲和性 | 确保每次都将来自特定客户端的连接传递到同一个 Pod。 | Windows Server 2022 | 将 `service.spec.sessionAffinity` 设为 “ClientIP” |\n| Direct Server Return (DSR) | 在负载均衡模式中 IP 地址修正和 LBNAT 直接发生在容器 vSwitch 端口；服务流量到达时源 IP 设置为原始 Pod IP。 | Windows Server 2019 | 在 kube-proxy 中设置以下标志：`--feature-gates=\"WinDSR=true\" --enable-dsr=true` |\n| 保留目标（Preserve-Destination） | 跳过服务流量的 DNAT，从而在到达后端 Pod 的数据包中保留目标服务的虚拟 IP。也会禁用节点间的转发。 | Windows Server，version 1903 | 在服务注解中设置 `\"preserve-destination\": \"true\"` 并在 kube-proxy 中启用 DSR。 |\n| IPv4/IPv6 双栈网络 | 进出集群和集群内通信都支持原生的 IPv4 间与 IPv6 间流量 | Windows Server 2019 | 参考 [IPv4/IPv6 双栈](/zh-cn/docs/concepts/services-networking/dual-stack/#windows-support)。 |\n| 客户端 IP 保留 | 确保入站流量的源 IP 得到保留。也会禁用节点间转发。 |  Windows Server 2019  | 将 `service.spec.externalTrafficPolicy` 设置为 “Local” 并在 kube-proxy 中启用 DSR。 |\n{{< /table >}}"}
{"en": "There are known issue with NodePort Services on overlay networking, if the destination node is running Windows Server 2022.\nTo avoid the issue entirely, you can configure the service with `externalTrafficPolicy: Local`.\n\nThere are known issues with Pod to Pod connectivity on l2bridge network on Windows Server 2022 with KB5005619 or higher installed.\nTo workaround the issue and restore Pod to Pod connectivity, you can disable the WinDSR feature in kube-proxy.\n\nThese issues require OS fixes.\nPlease follow https://github.com/microsoft/Windows-Containers/issues/204 for updates.", "zh": "{{< warning >}} \n如果目的地节点在运行 Windows Server 2022，则上层网络的 NodePort Service 存在已知问题。\n要完全避免此问题，可以使用 `externalTrafficPolicy: Local` 配置服务。\n\n在安装了 KB5005619 的 Windows Server 2022 或更高版本上，采用 L2bridge 网络时\nPod 间连接存在已知问题。\n要解决此问题并恢复 Pod 间连接，你可以在 kube-proxy 中禁用 WinDSR 功能。\n\n这些问题需要操作系统修复。\n有关更新，请参考 https://github.com/microsoft/Windows-Containers/issues/204。\n{{< /warning >}}"}
{"en": "## Limitations\n\nThe following networking functionality is _not_ supported on Windows nodes:\n\n* Host networking mode\n* Local NodePort access from the node itself (works for other nodes or external clients)\n* More than 64 backend pods (or unique destination addresses) for a single Service\n* IPv6 communication between Windows pods connected to overlay networks\n* Local Traffic Policy in non-DSR mode", "zh": "## 限制 {#limitations}\n\nWindows 节点**不支持**以下网络功能：\n\n* 主机网络模式\n* 从节点本身访问本地 NodePort（可以从其他节点或外部客户端进行访问）\n* 为同一 Service 提供 64 个以上后端 Pod（或不同目的地址）\n* 在连接到上层网络的 Windows Pod 之间使用 IPv6 通信\n* 非 DSR 模式中的本地流量策略（Local Traffic Policy）"}
{"en": "* Outbound communication using the ICMP protocol via the `win-overlay`, `win-bridge`, or using the Azure-CNI plugin.\\\n  Specifically, the Windows data plane ([VFP](https://www.microsoft.com/research/project/azure-virtual-filtering-platform/))\n  doesn't support ICMP packet transpositions, and this means:\n  * ICMP packets directed to destinations within the same network (such as pod to pod communication via ping) \n    work as expected;\n  * TCP/UDP packets work as expected;\n  * ICMP packets directed to pass through a remote network (e.g. pod to external internet communication via ping) \n    cannot be transposed and thus will not be routed back to their source;\n  * Since TCP/UDP packets can still be transposed, you can substitute `ping <destination>` with\n    `curl <destination>` when debugging connectivity with the outside world.", "zh": "* 通过 `win-overlay`、`win-bridge` 使用 ICMP 协议，或使用 Azure-CNI 插件进行出站通信。  \n  具体而言，Windows 数据平面（[VFP](https://www.microsoft.com/research/project/azure-virtual-filtering-platform/)）不支持 ICMP 数据包转换，这意味着：\n  * 指向同一网络内目的地址的 ICMP 数据包（例如 Pod 间的 ping 通信）可正常工作；\n  * TCP/UDP 数据包可正常工作；\n  * 通过远程网络指向其它地址的 ICMP 数据包（例如通过 ping 从 Pod 到外部公网的通信）无法被转换，\n    因此无法被路由回到这些数据包的源点；\n  * 由于 TCP/UDP 数据包仍可被转换，所以在调试与外界的连接时，\n    你可以将 `ping <destination>` 替换为 `curl <destination>`。"}
{"en": "Other limitations:\n\n* Windows reference network plugins win-bridge and win-overlay do not implement\n  [CNI spec](https://github.com/containernetworking/cni/blob/master/SPEC.md) v0.4.0,\n  due to a missing `CHECK` implementation.\n* The Flannel VXLAN CNI plugin has the following limitations on Windows:\n  * Node-pod connectivity is only possible for local pods with Flannel v0.12.0 (or higher).\n  * Flannel is restricted to using VNI 4096 and UDP port 4789. See the official\n    [Flannel VXLAN](https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan)\n    backend docs for more details on these parameters.", "zh": "其他限制：\n\n* 由于缺少 `CHECK` 实现，Windows 参考网络插件 win-bridge 和 win-overlay 未实现\n[CNI 规约](https://github.com/containernetworking/cni/blob/master/SPEC.md) 的 v0.4.0 版本。\n* Flannel VXLAN CNI 插件在 Windows 上有以下限制：\n  * 使用 Flannel v0.12.0（或更高版本）时，节点到 Pod 的连接仅适用于本地 Pod。\n  * Flannel 仅限于使用 VNI 4096 和 UDP 端口 4789。\n  有关这些参数的更多详细信息，请参考官方的 [Flannel VXLAN](https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan) 后端文档。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.19\" state=\"stable\" >}}\n\n{{< glossary_definition term_id=\"ingress\" length=\"all\" >}}\n\n{{< note >}}"}
{"en": "Ingress is frozen. New features are being added to the [Gateway API](/docs/concepts/services-networking/gateway/).", "zh": "入口（Ingress）目前已停止更新。新的功能正在集成至[网关 API](/zh-cn/docs/concepts/services-networking/gateway/) 中。\n{{< /note >}}"}
{"en": "## Terminology\n\nFor clarity, this guide defines the following terms:", "zh": "## 术语  {#terminology}\n\n为了表达更加清晰，本指南定义以下术语："}
{"en": "* Node: A worker machine in Kubernetes, part of a cluster.\n* Cluster: A set of Nodes that run containerized applications managed by Kubernetes.\n  For this example, and in most common Kubernetes deployments, nodes in the cluster\n  are not part of the public internet.\n* Edge router: A router that enforces the firewall policy for your cluster. This\n  could be a gateway managed by a cloud provider or a physical piece of hardware.\n* Cluster network: A set of links, logical or physical, that facilitate communication\n  within a cluster according to the Kubernetes [networking model](/docs/concepts/cluster-administration/networking/).\n* Service: A Kubernetes {{< glossary_tooltip term_id=\"service\" >}} that identifies\n  a set of Pods using {{< glossary_tooltip text=\"label\" term_id=\"label\" >}} selectors.\n  Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.", "zh": "* 节点（Node）: Kubernetes 集群中的一台工作机器，是集群的一部分。\n* 集群（Cluster）: 一组运行容器化应用程序的 Node，这些应用由 Kubernetes 管理。\n  在此示例和在大多数常见的 Kubernetes 部署环境中，集群中的节点都不在公共网络中。\n* 边缘路由器（Edge Router）: 在集群中强制执行防火墙策略的路由器。\n  可以是由云提供商管理的网关，也可以是物理硬件。\n* 集群网络（Cluster Network）: 一组逻辑的或物理的连接，基于 Kubernetes\n  [网络模型](/zh-cn/docs/concepts/cluster-administration/networking/)实现集群内的通信。\n* 服务（Service）：Kubernetes {{< glossary_tooltip term_id=\"service\" >}}，\n  使用{{< glossary_tooltip text=\"标签\" term_id=\"label\" >}}选择算符（Selectors）\n  来选择一组 Pod。除非另作说明，否则假定 Service 具有只能在集群网络内路由的虚拟 IP。"}
{"en": "## What is Ingress?\n\n[Ingress](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#ingress-v1-networking-k8s-io)\nexposes HTTP and HTTPS routes from outside the cluster to\n{{< link text=\"services\" url=\"/docs/concepts/services-networking/service/\" >}} within the cluster.\nTraffic routing is controlled by rules defined on the Ingress resource.", "zh": "## Ingress 是什么？  {#what-is-ingress}\n\n[Ingress](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#ingress-v1-networking-k8s-io)\n提供从集群外部到集群内[服务](/zh-cn/docs/concepts/services-networking/service/)的\nHTTP 和 HTTPS 路由。\n流量路由由 Ingress 资源所定义的规则来控制。"}
{"en": "Here is a simple example where an Ingress sends all its traffic to one Service:", "zh": "下面是 Ingress 的一个简单示例，可将所有流量都发送到同一 Service：\n\n{{< figure src=\"/zh-cn/docs/images/ingress.svg\" alt=\"ingress-diagram\" class=\"diagram-large\" caption=\"图. Ingress\" link=\"https://mermaid.live/edit#pako:eNqNkktLAzEQgP9KSC8Ku6XWBxKlJz0IHsQeuz1kN7M2uC-SrA9sb6X26MFLFZGKoCC0CIIn_Td1139halZq8eJlE2a--TI7yRn2YgaYYCc6EDRpod39DSdCyAs4RGqhMRndffRfs6dxc9Euox0NgZR2NhpmF73sqos2XVFD-ctt_vY2uTnPh8PJ4BGV7Ro3ZKOoaH5Li6Bt19r56zi7fM4fupP-oC1BHHEPGnWzGlimruno87qXvd__qjdpw2pXErOlxl7Mmn_j1VkcImb-i0q5BT5KAsoj5PMgICXGmCWViA-BlHzfL_b2MWeqRVaSE8uLg1iQUqVS2ZiTHK7LQrFcXfNg9V8WnZu3eEEqFYjCNCslJdd15zXVmcacODP9TMcqJmBN5zL9VKdt_uLM1ZoBzIVNF8WqM06ELRyCCCln-oWcTVkHqxaE4GCitwx8mgbK0Y-no9E0YVTBNuMqFpj4NJBgYZqquH4aeZgokcIPtMWpvtywoDpfU3_yww\" >}}"}
{"en": "An Ingress may be configured to give Services externally-reachable URLs,\nload balance traffic, terminate SSL / TLS, and offer name-based virtual hosting.\nAn [Ingress controller](/docs/concepts/services-networking/ingress-controllers)\nis responsible for fulfilling the Ingress, usually with a load balancer, though\nit may also configure your edge router or additional frontends to help handle the traffic.", "zh": "通过配置，Ingress 可为 Service 提供外部可访问的 URL、对其流量作负载均衡、\n终止 SSL/TLS，以及基于名称的虚拟托管等能力。\n[Ingress 控制器](/zh-cn/docs/concepts/services-networking/ingress-controllers)\n负责完成 Ingress 的工作，具体实现上通常会使用某个负载均衡器，\n不过也可以配置边缘路由器或其他前端来帮助处理流量。"}
{"en": "An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically\nuses a service of type [Service.Type=NodePort](/docs/concepts/services-networking/service/#type-nodeport) or\n[Service.Type=LoadBalancer](/docs/concepts/services-networking/service/#loadbalancer).", "zh": "Ingress 不会随意公开端口或协议。\n将 HTTP 和 HTTPS 以外的服务开放到 Internet 时，通常使用\n[Service.Type=NodePort](/zh-cn/docs/concepts/services-networking/service/#type-nodeport)\n或 [Service.Type=LoadBalancer](/zh-cn/docs/concepts/services-networking/service/#loadbalancer)\n类型的 Service。"}
{"en": "## Prerequisites\n\nYou must have an [Ingress controller](/docs/concepts/services-networking/ingress-controllers)\nto satisfy an Ingress. Only creating an Ingress resource has no effect.", "zh": "## 环境准备  {#prerequisites}\n\n你必须拥有一个 [Ingress 控制器](/zh-cn/docs/concepts/services-networking/ingress-controllers) 才能满足\nIngress 的要求。仅创建 Ingress 资源本身没有任何效果。"}
{"en": "You may need to deploy an Ingress controller such as [ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/).\nYou can choose from a number of [Ingress controllers](/docs/concepts/services-networking/ingress-controllers).", "zh": "你可能需要部署一个 Ingress 控制器，例如 [ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/)。\n你可以从许多 [Ingress 控制器](/zh-cn/docs/concepts/services-networking/ingress-controllers)中进行选择。"}
{"en": "Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress\ncontrollers operate slightly differently.", "zh": "理想情况下，所有 Ingress 控制器都应遵从参考规范。\n但实际上，各个 Ingress 控制器操作略有不同。\n\n{{< note >}}"}
{"en": "Make sure you review your Ingress controller's documentation to understand the caveats of choosing it.", "zh": "确保你查看了 Ingress 控制器的文档，以了解选择它的注意事项。\n{{< /note >}}"}
{"en": "## The Ingress resource\n\nA minimal Ingress resource example:", "zh": "## Ingress 资源  {#the-ingress-resource}\n\n一个最小的 Ingress 资源示例：\n\n{{% code_sample file=\"service/networking/minimal-ingress.yaml\" %}}"}
{"en": "An Ingress needs `apiVersion`, `kind`, `metadata` and `spec` fields.\nThe name of an Ingress object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).\nFor general information about working with config files, see\n[deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/),\n[configuring containers](/docs/tasks/configure-pod-container/configure-pod-configmap/),\n[managing resources](/docs/concepts/cluster-administration/manage-deployment/).\nIngress frequently uses annotations to configure some options depending on the Ingress controller, an example of which\nis the [rewrite-target annotation](https://github.com/kubernetes/ingress-nginx/blob/main/docs/examples/rewrite/README.md).\nDifferent [Ingress controllers](/docs/concepts/services-networking/ingress-controllers) support different annotations.\nReview the documentation for your choice of Ingress controller to learn which annotations are supported.", "zh": "Ingress 需要指定 `apiVersion`、`kind`、 `metadata`和 `spec` 字段。\nIngress 对象的命名必须是合法的 [DNS 子域名名称](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。\n关于如何使用配置文件的一般性信息，请参见[部署应用](/zh-cn/docs/tasks/run-application/run-stateless-application-deployment/)、\n[配置容器](/zh-cn/docs/tasks/configure-pod-container/configure-pod-configmap/)、\n[管理资源](/zh-cn/docs/concepts/cluster-administration/manage-deployment/)。\nIngress 经常使用注解（Annotations）来配置一些选项，具体取决于 Ingress 控制器，\n例如 [rewrite-target 注解](https://github.com/kubernetes/ingress-nginx/blob/main/docs/examples/rewrite/README.md)。\n不同的 [Ingress 控制器](/zh-cn/docs/concepts/services-networking/ingress-controllers)支持不同的注解。\n查看你所选的 Ingress 控制器的文档，以了解其所支持的注解。"}
{"en": "The [Ingress spec](/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec)\nhas all the information needed to configure a load balancer or proxy server. Most importantly, it\ncontains a list of rules matched against all incoming requests. Ingress resource only supports rules\nfor directing HTTP(S) traffic.", "zh": "[Ingress 规约](/zh-cn/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec)\n提供了配置负载均衡器或者代理服务器所需要的所有信息。\n最重要的是，其中包含对所有入站请求进行匹配的规则列表。\nIngress 资源仅支持用于转发 HTTP(S) 流量的规则。"}
{"en": "If the `ingressClassName` is omitted, a [default Ingress class](#default-ingress-class)\nshould be defined.\n\nThere are some ingress controllers, that work without the definition of a\ndefault `IngressClass`. For example, the Ingress-NGINX controller can be\nconfigured with a [flag](https://kubernetes.github.io/ingress-nginx/user-guide/k8s-122-migration/#what-is-the-flag-watch-ingress-without-class)\n`--watch-ingress-without-class`. It is [recommended](https://kubernetes.github.io/ingress-nginx/user-guide/k8s-122-migration/#i-have-only-one-ingress-controller-in-my-cluster-what-should-i-do) though, to specify the\ndefault `IngressClass` as shown [below](#default-ingress-class).", "zh": "如果 `ingressClassName` 被省略，那么你应该定义一个[默认的 Ingress 类](#default-ingress-class)。\n\n有些 Ingress 控制器不需要定义默认的 `IngressClass`。比如：Ingress-NGINX\n控制器可以通过[参数](https://kubernetes.github.io/ingress-nginx/user-guide/k8s-122-migration/#what-is-the-flag-watch-ingress-without-class)\n`--watch-ingress-without-class` 来配置。\n不过仍然[推荐](https://kubernetes.github.io/ingress-nginx/user-guide/k8s-122-migration/#i-have-only-one-ingress-controller-in-my-cluster-what-should-i-do)\n按[下文](#default-ingress-class)所示来设置默认的 `IngressClass`。"}
{"en": "### Ingress rules\n\nEach HTTP rule contains the following information:", "zh": "### Ingress 规则  {#ingress-rules}\n\n每个 HTTP 规则都包含以下信息："}
{"en": "* An optional host. In this example, no host is specified, so the rule applies to all inbound\n  HTTP traffic through the IP address specified. If a host is provided (for example,\n  foo.bar.com), the rules apply to that host.\n* A list of paths (for example, `/testpath`), each of which has an associated\n  backend defined with a `service.name` and a `service.port.name` or\n  `service.port.number`. Both the host and path must match the content of an\n  incoming request before the load balancer directs traffic to the referenced\n  Service.\n* A backend is a combination of Service and port names as described in the\n  [Service doc](/docs/concepts/services-networking/service/) or a [custom resource backend](#resource-backend)\n  by way of a {{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CRD\" >}}. HTTP (and HTTPS) requests to the\n  Ingress that match the host and path of the rule are sent to the listed backend.", "zh": "* 可选的 `host`。在此示例中，未指定 `host`，因此该规则基于所指定 IP 地址来匹配所有入站 HTTP 流量。\n  如果提供了 `host`（例如 `foo.bar.com`），则 `rules` 适用于所指定的主机。\n* 路径列表（例如 `/testpath`）。每个路径都有一个由 `service.name` 和 `service.port.name`\n  或 `service.port.number` 确定的关联后端。\n  主机和路径都必须与入站请求的内容相匹配，负载均衡器才会将流量引导到所引用的 Service，\n* `backend`（后端）是 [Service 文档](/zh-cn/docs/concepts/services-networking/service/)中所述的 Service 和端口名称的组合，\n  或者是通过 {{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CRD\" >}}\n  方式来实现的[自定义资源后端](#resource-backend)。\n  对于发往 Ingress 的 HTTP（和 HTTPS）请求，如果与规则中的主机和路径匹配，\n  则会被发送到所列出的后端。"}
{"en": "A `defaultBackend` is often configured in an Ingress controller to service any requests that do not\nmatch a path in the spec.", "zh": "通常会在 Ingress 控制器中配置 `defaultBackend`（默认后端），\n以便为无法与规约中任何路径匹配的所有请求提供服务。"}
{"en": "### DefaultBackend {#default-backend}\n\nAn Ingress with no rules sends all traffic to a single default backend and `.spec.defaultBackend`\nis the backend that should handle requests in that case.\nThe `defaultBackend` is conventionally a configuration option of the\n[Ingress controller](/docs/concepts/services-networking/ingress-controllers) and\nis not specified in your Ingress resources.\nIf no `.spec.rules` are specified, `.spec.defaultBackend` must be specified.\nIf `defaultBackend` is not set, the handling of requests that do not match any of the rules will be up to the\ningress controller (consult the documentation for your ingress controller to find out how it handles this case). \n\nIf none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is\nrouted to your default backend.", "zh": "### 默认后端  {#default-backend}\n\n没有设置规则的 Ingress 将所有流量发送到同一个默认后端，而在这种情况下\n`.spec.defaultBackend` 则是负责处理请求的那个默认后端。\n`defaultBackend` 通常是\n[Ingress 控制器](/zh-cn/docs/concepts/services-networking/ingress-controllers)的配置选项，\n而非在 Ingress 资源中设置。\n如果未设置 `.spec.rules`，则必须设置 `.spec.defaultBackend`。\n如果未设置 `defaultBackend`，那么如何处理与所有规则都不匹配的流量将交由\nIngress 控制器决定（请参考你的 Ingress 控制器的文档以了解它是如何处理这种情况的）。\n\n如果 Ingress 对象中主机和路径都没有与 HTTP 请求匹配，则流量将被路由到默认后端。"}
{"en": "### Resource backends {#resource-backend}\n\nA `Resource` backend is an ObjectRef to another Kubernetes resource within the\nsame namespace as the Ingress object. A `Resource` is a mutually exclusive\nsetting with Service, and will fail validation if both are specified. A common\nusage for a `Resource` backend is to ingress data to an object storage backend\nwith static assets.", "zh": "### 资源后端  {#resource-backend}\n\n`Resource` 后端是一个 ObjectRef 对象，指向同一名字空间中的另一个 Kubernetes 资源，\n将其视为 Ingress 对象。\n`Resource` 后端与 Service 后端是互斥的，在二者均被设置时会无法通过合法性检查。\n`Resource` 后端的一种常见用法是将所有入站数据导向保存静态资产的对象存储后端。\n\n{{% code_sample file=\"service/networking/ingress-resource-backend.yaml\" %}}"}
{"en": "After creating the Ingress above, you can view it with the following command:", "zh": "创建了如上的 Ingress 之后，你可以使用下面的命令查看它：\n\n```bash\nkubectl describe ingress ingress-resource-backend\n```\n\n```\nName:             ingress-resource-backend\nNamespace:        default\nAddress:\nDefault backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets\nRules:\n  Host        Path  Backends\n  ----        ----  --------\n  *\n              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets\nAnnotations:  <none>\nEvents:       <none>\n```"}
{"en": "### Path types\n\nEach path in an Ingress is required to have a corresponding path type. Paths\nthat do not include an explicit `pathType` will fail validation. There are three\nsupported path types:", "zh": "### 路径类型  {#path-types}\n\nIngress 中的每个路径都需要有对应的路径类型（Path Type）。未明确设置 `pathType`\n的路径无法通过合法性检查。当前支持的路径类型有三种："}
{"en": "* `ImplementationSpecific`: With this path type, matching is up to\n  the IngressClass. Implementations can treat this as a separate `pathType` or\n  treat it identically to `Prefix` or `Exact` path types.\n\n* `Exact`: Matches the URL path exactly and with case sensitivity.\n\n* `Prefix`: Matches based on a URL path prefix split by `/`. Matching is case\n  sensitive and done on a path element by element basis. A path element refers\n  to the list of labels in the path split by the `/` separator. A request is a\n  match for path _p_ if every _p_ is an element-wise prefix of _p_ of the\n  request path.\n\n  If the last element of the path is a substring of the last\n  element in request path, it is not a match (for example: `/foo/bar`\n  matches `/foo/bar/baz`, but does not match `/foo/barbaz`).", "zh": "* `ImplementationSpecific`：对于这种路径类型，匹配方法取决于 IngressClass。\n  具体实现可以将其作为单独的 `pathType` 处理或者作与 `Prefix` 或 `Exact`\n  类型相同的处理。\n\n* `Exact`：精确匹配 URL 路径，且区分大小写。\n\n* `Prefix`：基于以 `/` 分隔的 URL 路径前缀匹配。匹配区分大小写，\n  并且对路径中各个元素逐个执行匹配操作。\n  路径元素指的是由 `/` 分隔符分隔的路径中的标签列表。\n  如果每个 _p_ 都是请求路径 _p_ 的元素前缀，则请求与路径 _p_ 匹配。\n\n  {{< note >}}\n  如果路径的最后一个元素是请求路径中最后一个元素的子字符串，则不会被视为匹配\n  （例如：`/foo/bar` 匹配 `/foo/bar/baz`, 但不匹配 `/foo/barbaz`）。\n  {{< /note >}}"}
{"en": "### Examples\n\n| Kind   | Path(s)                         | Request path(s)             | Matches?                           |\n|--------|---------------------------------|-----------------------------|------------------------------------|\n| Prefix | `/`                             | (all paths)                 | Yes                                |\n| Exact  | `/foo`                          | `/foo`                      | Yes                                |\n| Exact  | `/foo`                          | `/bar`                      | No                                 |\n| Exact  | `/foo`                          | `/foo/`                     | No                                 |\n| Exact  | `/foo/`                         | `/foo`                      | No                                 |\n| Prefix | `/foo`                          | `/foo`, `/foo/`             | Yes                                |\n| Prefix | `/foo/`                         | `/foo`, `/foo/`             | Yes                                |\n| Prefix | `/aaa/bb`                       | `/aaa/bbb`                  | No                                 |\n| Prefix | `/aaa/bbb`                      | `/aaa/bbb`                  | Yes                                |\n| Prefix | `/aaa/bbb/`                     | `/aaa/bbb`                  | Yes, ignores trailing slash        |\n| Prefix | `/aaa/bbb`                      | `/aaa/bbb/`                 | Yes,  matches trailing slash       |\n| Prefix | `/aaa/bbb`                      | `/aaa/bbb/ccc`              | Yes, matches subpath               |\n| Prefix | `/aaa/bbb`                      | `/aaa/bbbxyz`               | No, does not match string prefix   |\n| Prefix | `/`, `/aaa`                     | `/aaa/ccc`                  | Yes, matches `/aaa` prefix         |\n| Prefix | `/`, `/aaa`, `/aaa/bbb`         | `/aaa/bbb`                  | Yes, matches `/aaa/bbb` prefix     |\n| Prefix | `/`, `/aaa`, `/aaa/bbb`         | `/ccc`                      | Yes, matches `/` prefix            |\n| Prefix | `/aaa`                          | `/ccc`                      | No, uses default backend           |\n| Mixed  | `/foo` (Prefix), `/foo` (Exact) | `/foo`                      | Yes, prefers Exact                 |", "zh": "### 示例\n\n| 类型   | 路径                            | 请求路径        | 匹配与否？               |\n|--------|---------------------------------|-----------------|--------------------------|\n| Prefix | `/`                             | （所有路径）    | 是                       |\n| Exact  | `/foo`                          | `/foo`          | 是                       |\n| Exact  | `/foo`                          | `/bar`          | 否                       |\n| Exact  | `/foo`                          | `/foo/`         | 否                       |\n| Exact  | `/foo/`                         | `/foo`          | 否                       |\n| Prefix | `/foo`                          | `/foo`, `/foo/` | 是                       |\n| Prefix | `/foo/`                         | `/foo`, `/foo/` | 是                       |\n| Prefix | `/aaa/bb`                       | `/aaa/bbb`      | 否                       |\n| Prefix | `/aaa/bbb`                      | `/aaa/bbb`      | 是                       |\n| Prefix | `/aaa/bbb/`                     | `/aaa/bbb`      | 是，忽略尾部斜线         |\n| Prefix | `/aaa/bbb`                      | `/aaa/bbb/`     | 是，匹配尾部斜线         |\n| Prefix | `/aaa/bbb`                      | `/aaa/bbb/ccc`  | 是，匹配子路径           |\n| Prefix | `/aaa/bbb`                      | `/aaa/bbbxyz`   | 否，字符串前缀不匹配     |\n| Prefix | `/`, `/aaa`                     | `/aaa/ccc`      | 是，匹配 `/aaa` 前缀     |\n| Prefix | `/`, `/aaa`, `/aaa/bbb`         | `/aaa/bbb`      | 是，匹配 `/aaa/bbb` 前缀 |\n| Prefix | `/`, `/aaa`, `/aaa/bbb`         | `/ccc`          | 是，匹配 `/` 前缀        |\n| Prefix | `/aaa`                          | `/ccc`          | 否，使用默认后端         |\n| 混合   | `/foo` (Prefix), `/foo` (Exact) | `/foo`          | 是，优选 Exact 类型      |"}
{"en": "#### Multiple matches\n\nIn some cases, multiple paths within an Ingress will match a request. In those\ncases precedence will be given first to the longest matching path. If two paths\nare still equally matched, precedence will be given to paths with an exact path\ntype over prefix path type.", "zh": "#### 多重匹配  {#multiple-matches}\n\n在某些情况下，Ingress 中会有多条路径与同一个请求匹配。这时匹配路径最长者优先。\n如果仍然有两条同等的匹配路径，则精确路径类型优先于前缀路径类型。"}
{"en": "## Hostname wildcards\n\nHosts can be precise matches (for example “`foo.bar.com`”) or a wildcard (for\nexample “`*.foo.com`”). Precise matches require that the HTTP `host` header\nmatches the `host` field. Wildcard matches require the HTTP `host` header is\nequal to the suffix of the wildcard rule.", "zh": "## 主机名通配符   {#hostname-wildcards}\n\n主机名可以是精确匹配（例如 “`foo.bar.com`”）或者使用通配符来匹配\n（例如 “`*.foo.com`”）。\n精确匹配要求 HTTP `host` 头部字段与 `host` 字段值完全匹配。\n通配符匹配则要求 HTTP `host` 头部字段与通配符规则中的后缀部分相同。"}
{"en": "| Host         | Host header        | Match?                                              |\n| ------------ |--------------------| ----------------------------------------------------|\n| `*.foo.com`  | `bar.foo.com`      | Matches based on shared suffix                      |\n| `*.foo.com`  | `baz.bar.foo.com`  | No match, wildcard only covers a single DNS label   |\n| `*.foo.com`  | `foo.com`          | No match, wildcard only covers a single DNS label   |", "zh": "| 主机         | host 头部          | 匹配与否？                          |\n| ------------ |--------------------| ------------------------------------|\n| `*.foo.com`  | `bar.foo.com`      | 基于相同的后缀匹配                  |\n| `*.foo.com`  | `baz.bar.foo.com`  | 不匹配，通配符仅覆盖了一个 DNS 标签 |\n| `*.foo.com`  | `foo.com`          | 不匹配，通配符仅覆盖了一个 DNS 标签 |\n\n{{% code_sample file=\"service/networking/ingress-wildcard-host.yaml\" %}}"}
{"en": "## Ingress class\n\nIngresses can be implemented by different controllers, often with different\nconfiguration. Each Ingress should specify a class, a reference to an\nIngressClass resource that contains additional configuration including the name\nof the controller that should implement the class.", "zh": "## Ingress 类  {#ingress-class}\n\nIngress 可以由不同的控制器实现，通常使用不同的配置。\n每个 Ingress 应当指定一个类，也就是一个对 IngressClass 资源的引用。\nIngressClass 资源包含额外的配置，其中包括应当实现该类的控制器名称。\n\n{{% code_sample file=\"service/networking/external-lb.yaml\" %}}"}
{"en": "The `.spec.parameters` field of an IngressClass lets you reference another\nresource that provides configuration related to that IngressClass.\n\nThe specific type of parameters to use depends on the ingress controller\nthat you specify in the `.spec.controller` field of the IngressClass.", "zh": "IngressClass 中的 `.spec.parameters` 字段可用于引用其他资源以提供与该\nIngressClass 相关的配置。\n\n参数（`parameters`）的具体类型取决于你在 IngressClass 的 `.spec.controller`\n字段中指定的 Ingress 控制器。"}
{"en": "### IngressClass scope\n\nDepending on your ingress controller, you may be able to use parameters\nthat you set cluster-wide, or just for one namespace.", "zh": "### IngressClass 的作用域  {#ingressclass-scope}\n\n取决于你所使用的 Ingress 控制器，你可能可以使用集群作用域的参数或某个名字空间作用域的参数。\n\n{{< tabs name=\"tabs_ingressclass_parameter_scope\" >}}\n{{% tab name=\"集群作用域\" %}}"}
{"en": "The default scope for IngressClass parameters is cluster-wide.\n\nIf you set the `.spec.parameters` field and don't set\n`.spec.parameters.scope`, or if you set `.spec.parameters.scope` to\n`Cluster`, then the IngressClass refers to a cluster-scoped resource.\nThe `kind` (in combination the `apiGroup`) of the parameters\nrefers to a cluster-scoped API (possibly a custom resource), and\nthe `name` of the parameters identifies a specific cluster scoped\nresource for that API.\n\nFor example:", "zh": "IngressClass 参数的默认作用域是集群范围。\n\n如果你设置了 `.spec.parameters` 字段且未设置 `.spec.parameters.scope`\n字段，或是将 `.spec.parameters.scope` 字段设为了 `Cluster`，\n那么该 IngressClass 所引用的即是一个集群作用域的资源。\n参数的 `kind`（和 `apiGroup` 一起）指向一个集群作用域的 API 类型\n（可能是一个定制资源（Custom Resource）），而其 `name` 字段则进一步确定\n该 API 类型的一个具体的、集群作用域的资源。\n\n示例：\n\n```yaml\n---\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: external-lb-1\nspec:\n  controller: example.com/ingress-controller\n  parameters:\n    # 此 IngressClass 的配置定义在一个名为 “external-config-1” 的\n    # ClusterIngressParameter（API 组为 k8s.example.net）资源中。\n    # 这项定义告诉 Kubernetes 去寻找一个集群作用域的参数资源。\n    scope: Cluster\n    apiGroup: k8s.example.net\n    kind: ClusterIngressParameter\n    name: external-config-1\n```\n\n{{% /tab %}}\n{{% tab name=\"命名空间作用域\" %}}\n{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}"}
{"en": "If you set the `.spec.parameters` field and set\n`.spec.parameters.scope` to `Namespace`, then the IngressClass refers\nto a namespaced-scoped resource. You must also set the `namespace`\nfield within `.spec.parameters` to the namespace that contains\nthe parameters you want to use.\n\nThe `kind` (in combination the `apiGroup`) of the parameters\nrefers to a namespaced API (for example: ConfigMap), and\nthe `name` of the parameters identifies a specific resource\nin the namespace you specified in `namespace`.", "zh": "如果你设置了 `.spec.parameters` 字段且将 `.spec.parameters.scope`\n字段设为了 `Namespace`，那么该 IngressClass 将会引用一个名字空间作用域的资源。\n`.spec.parameters.namespace` 必须和此资源所处的名字空间相同。\n\n参数的 `kind`（和 `apiGroup` 一起）指向一个命名空间作用域的 API 类型\n（例如：ConfigMap），而其 `name` 则进一步确定指定 API 类型的、\n位于你指定的命名空间中的具体资源。"}
{"en": "Namespace-scoped parameters help the cluster operator delegate control over the\nconfiguration (for example: load balancer settings, API gateway definition)\nthat is used for a workload. If you used a cluster-scoped parameter then either:\n\n- the cluster operator team needs to approve a different team's changes every\n  time there's a new configuration change being applied.\n- the cluster operator must define specific access controls, such as\n  [RBAC](/docs/reference/access-authn-authz/rbac/) roles and bindings, that let\n  the application team make changes to the cluster-scoped parameters resource.", "zh": "名字空间作用域的参数帮助集群操作者将对工作负载所需的配置数据（比如：负载均衡设置、\nAPI 网关定义）的控制权力委派出去。如果你使用集群作用域的参数，那么你将面临以下情况之一：\n\n- 每次应用一项新的配置变更时，集群操作团队需要批准其他团队所作的修改。\n- 集群操作团队必须定义具体的准入控制规则，比如 [RBAC](/zh-cn/docs/reference/access-authn-authz/rbac/)\n  角色与角色绑定，以使得应用程序团队可以修改集群作用域的配置参数资源。"}
{"en": "The IngressClass API itself is always cluster-scoped.\n\nHere is an example of an IngressClass that refers to parameters that are\nnamespaced:", "zh": "IngressClass API 本身是集群作用域的。\n\n这里是一个引用名字空间作用域配置参数的 IngressClass 的示例：\n\n```yaml\n---\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: external-lb-2\nspec:\n  controller: example.com/ingress-controller\n  parameters:\n    # 此 IngressClass 的配置定义在一个名为 “external-config” 的\n    # IngressParameter（API 组为 k8s.example.com）资源中，\n    # 该资源位于 “external-configuration” 名字空间中。\n    scope: Namespace\n    apiGroup: k8s.example.com\n    kind: IngressParameter\n    namespace: external-configuration\n    name: external-config\n```\n\n{{% /tab %}}\n{{< /tabs >}}"}
{"en": "### Deprecated annotation\n\nBefore the IngressClass resource and `ingressClassName` field were added in\nKubernetes 1.18, Ingress classes were specified with a\n`kubernetes.io/ingress.class` annotation on the Ingress. This annotation was\nnever formally defined, but was widely supported by Ingress controllers.", "zh": "### 已废弃的注解  {#deprecated-annotation}\n\n在 Kubernetes 1.18 版本引入 IngressClass 资源和 `ingressClassName` 字段之前，\nIngress 类是通过 Ingress 中的一个 `kubernetes.io/ingress.class` 注解来指定的。\n这个注解从未被正式定义过，但是得到了 Ingress 控制器的广泛支持。"}
{"en": "The newer `ingressClassName` field on Ingresses is a replacement for that\nannotation, but is not a direct equivalent. While the annotation was generally\nused to reference the name of the Ingress controller that should implement the\nIngress, the field is a reference to an IngressClass resource that contains\nadditional Ingress configuration, including the name of the Ingress controller.", "zh": "Ingress 中新的 `ingressClassName` 字段用来替代该注解，但并非完全等价。\n注解通常用于引用实现该 Ingress 的控制器的名称，而这个新的字段则是对一个包含额外\nIngress 配置的 IngressClass 资源的引用，其中包括了 Ingress 控制器的名称。"}
{"en": "### Default IngressClass {#default-ingress-class}\n\nYou can mark a particular IngressClass as default for your cluster. Setting the\n`ingressclass.kubernetes.io/is-default-class` annotation to `true` on an\nIngressClass resource will ensure that new Ingresses without an\n`ingressClassName` field specified will be assigned this default IngressClass.", "zh": "### 默认 Ingress 类  {#default-ingress-class}\n\n你可以将一个特定的 IngressClass 标记为集群默认 Ingress 类。\n将某个 IngressClass 资源的 `ingressclass.kubernetes.io/is-default-class` 注解设置为\n`true` 将确保新的未指定 `ingressClassName` 字段的 Ingress 能够被赋予这一默认\nIngressClass.\n\n{{< caution >}}"}
{"en": "If you have more than one IngressClass marked as the default for your cluster,\nthe admission controller prevents creating new Ingress objects that don't have\nan `ingressClassName` specified. You can resolve this by ensuring that at most 1\nIngressClass is marked as default in your cluster.", "zh": "如果集群中有多个 IngressClass 被标记为默认，准入控制器将阻止创建新的未指定\n`ingressClassName` 的 Ingress 对象。\n解决这个问题需要确保集群中最多只能有一个 IngressClass 被标记为默认。\n{{< /caution >}}"}
{"en": "There are some ingress controllers, that work without the definition of a\ndefault `IngressClass`. For example, the Ingress-NGINX controller can be\nconfigured with a [flag](https://kubernetes.github.io/ingress-nginx/#what-is-the-flag-watch-ingress-without-class)\n`--watch-ingress-without-class`. It is [recommended](https://kubernetes.github.io/ingress-nginx/#i-have-only-one-instance-of-the-ingresss-nginx-controller-in-my-cluster-what-should-i-do)  though, to specify the\ndefault `IngressClass`:", "zh": "有一些 Ingress 控制器不需要定义默认的 `IngressClass`。比如：Ingress-NGINX\n控制器可以通过[参数](https://kubernetes.github.io/ingress-nginx/#what-is-the-flag-watch-ingress-without-class)\n`--watch-ingress-without-class` 来配置。\n不过仍然[推荐](https://kubernetes.github.io/ingress-nginx/#i-have-only-one-instance-of-the-ingresss-nginx-controller-in-my-cluster-what-should-i-do)\n设置默认的 `IngressClass`。\n\n{{% code_sample file=\"service/networking/default-ingressclass.yaml\" %}}"}
{"en": "## Types of Ingress\n\n### Ingress backed by a single Service {#single-service-ingress}\n\nThere are existing Kubernetes concepts that allow you to expose a single Service\n(see [alternatives](#alternatives)). You can also do this with an Ingress by specifying a\n*default backend* with no rules.", "zh": "## Ingress 类型  {#types-of-ingress}\n\n### 由单个 Service 来支持的 Ingress   {#single-service-ingress}\n\n现有的 Kubernetes 概念允许你暴露单个 Service（参见[替代方案](#alternatives)）。\n你也可以使用 Ingress 并设置无规则的**默认后端**来完成这类操作。\n\n{{% code_sample file=\"service/networking/test-ingress.yaml\" %}}"}
{"en": "If you create it using `kubectl apply -f` you should be able to view the state\nof the Ingress you added:", "zh": "如果使用 `kubectl apply -f` 创建此 Ingress，则应该能够查看刚刚添加的 Ingress 的状态：\n\n```shell\nkubectl get ingress test-ingress\n```\n\n```\nNAME           CLASS         HOSTS   ADDRESS         PORTS   AGE\ntest-ingress   external-lb   *       203.0.113.123   80      59s\n```"}
{"en": "Where `203.0.113.123` is the IP allocated by the Ingress controller to satisfy\nthis Ingress.", "zh": "其中 `203.0.113.123` 是由 Ingress 控制器分配的 IP，用以服务于此 Ingress。\n\n{{< note >}}"}
{"en": "Ingress controllers and load balancers may take a minute or two to allocate an IP address.\nUntil that time, you often see the address listed as `<pending>`.", "zh": "Ingress 控制器和负载平衡器的 IP 地址分配操作可能需要一两分钟。\n在此之前，你通常会看到地址字段的取值为 `<pending>`。\n{{< /note >}}"}
{"en": "### Simple fanout\n\nA fanout configuration routes traffic from a single IP address to more than one Service,\nbased on the HTTP URI being requested. An Ingress allows you to keep the number of load balancers\ndown to a minimum. For example, a setup like:", "zh": "### 简单扇出  {#simple-fanout}\n\n一个扇出（Fanout）配置根据请求的 HTTP URI 将来自同一 IP 地址的流量路由到多个 Service。\nIngress 允许你将负载均衡器的数量降至最低。例如，这样的设置：\n\n{{< figure src=\"/zh-cn/docs/images/ingressFanOut.svg\" alt=\"ingress-fanout-diagram\" class=\"diagram-large\" caption=\"图. Ingress 扇出\" link=\"https://mermaid.live/edit#pako:eNqNUk1v0zAY_iuWewEpyRKnjM5FPY0DEgfEjk0PTvxmtZbGke3woW03NDjuChNCRRyQkMYFidP4NyXlX5DMjroykLg4j_x8vM6j9xhnkgOm-FCxao4ePx0nJUJZIaA0d6ary48_33xvvnyd3fUD9Kg8VKC131wum_Oz5t0r9CBVE7T-9mF9dbV6_3q9XK7efkaBPxFWOXUOD0X3R8FeFEQkDqKYzK6HOJHvT052cilPNKhnIoNoemAB6i_okIThbU_KVO8hf3oIHYUj59F1an_u18VZ8-PTjRhLuyltZiV5NH0i-ewvBLlFEEvE_yKGGwJKbmtlWu9DjqqCiRLloijogHPuaaPkEdBBnucO-88FN3M6rF54mSykooMwDMdbIUcj7SJispvBvf9KabntlKyotQHlkjZWOkjTdDuGbGLsxE1S36jXl9YD4nWldsc1irtj2D39htdumy1l69q-zH3H2MMLUAsmeLuux50uwWYOC0gwbSGHnNWFSXBSnrbSuuLMwEMujFSY5qzQ4GFWG3nwsswwNaqGXrQvWLsgC6c6_Q0zxBrK\" >}}"}
{"en": "It would require an Ingress such as:", "zh": "这将需要一个如下所示的 Ingress：\n\n{{% code_sample file=\"service/networking/simple-fanout-example.yaml\" %}}"}
{"en": "When you create the Ingress with `kubectl apply -f`:", "zh": "当你使用 `kubectl apply -f` 创建 Ingress 时：\n\n```shell\nkubectl describe ingress simple-fanout-example\n```\n\n```\nName:             simple-fanout-example\nNamespace:        default\nAddress:          178.91.123.132\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\nRules:\n  Host         Path  Backends\n  ----         ----  --------\n  foo.bar.com\n               /foo   service1:4200 (10.8.0.90:4200)\n               /bar   service2:8080 (10.8.0.91:8080)\nAnnotations:\n  nginx.ingress.kubernetes.io/rewrite-target:  /\nEvents:\n  Type     Reason  Age                From                     Message\n  ----     ------  ----               ----                     -------\n  Normal   ADD     22s                loadbalancer-controller  default/test\n```"}
{"en": "The Ingress controller provisions an implementation-specific load balancer\nthat satisfies the Ingress, as long as the Services (`service1`, `service2`) exist.\nWhen it has done so, you can see the address of the load balancer at the\nAddress field.", "zh": "此 Ingress 控制器构造一个特定于实现的负载均衡器来供 Ingress 使用，\n前提是 Service （`service1`、`service2`）存在。\n当它完成负载均衡器的创建时，你会在 Address 字段看到负载均衡器的地址。\n\n{{< note >}}"}
{"en": "Depending on the [Ingress controller](/docs/concepts/services-networking/ingress-controllers/)\nyou are using, you may need to create a default-http-backend\n[Service](/docs/concepts/services-networking/service/).", "zh": "取决于你所使用的 [Ingress 控制器](/zh-cn/docs/concepts/services-networking/ingress-controllers/)，\n你可能需要创建默认 HTTP 后端[服务](/zh-cn/docs/concepts/services-networking/service/)。\n{{< /note >}}"}
{"en": "### Name based virtual hosting\n\nName-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.", "zh": "### 基于名称的虚拟主机服务   {#name-based-virtual-hosting}\n\n基于名称的虚拟主机支持将针对多个主机名的 HTTP 流量路由到同一 IP 地址上。\n\n{{< figure src=\"/zh-cn/docs/images/ingressNameBased.svg\" alt=\"ingress-namebase-diagram\" class=\"diagram-large\" caption=\"图. 基于名称实现虚拟托管的 Ingress\" link=\"https://mermaid.live/edit#pako:eNqNkk9v0zAYxr-K5V6GlESNU6B4qKdxQOKA2LHpwYnfrNaSOLId_mjbDQ2OXAdMUxEHJKRxQWLaND4NXcq3IJkT2gKTuDiv_Dzv73UevXs4lhwwxTuKFVP06MlmmCMUpwJyszGen364ev2t-vxlcsv10MN8R4HWbnU6q94cVm9fovuRGqHF15PF5eX8-NViNpsffUKeOxLWOW47HOTfHXr3fM8ngecHZHI9pDW57mj_x9nF1ftzihIpvYgpL5bZvgb1VMTgj7dtgboLOuzfCGiaG8gKgPwJIL8Buozsb_98d1h9_7jCtHI7sB5QSO6PH0s--YdA_hKIFYKbhMFSgJzbwJnWW5CgImUiR4lIU9rjnDvaKLkLtJckSVu7zwQ3UzoonjuxTKWivX6_v7kG2R3qFhGQOzHc_i9Kra1T4rTUBlRLWrbSXhRF6xiyxNiJS1KXqNOF1hXEaUJtjusqaI5B8_SVXruHNpS1a_uy9lsr2MEZqIwJXq_yXuMMsZlCBiGmdckhYWVqQhzmB7W1LDgz8IALIxWmCUs1OJiVRm6_yGNMjSqhM20JVq9I1roOfgEKNyn5\" >}}"}
{"en": "The following Ingress tells the backing load balancer to route requests based on\nthe [Host header](https://tools.ietf.org/html/rfc7230#section-5.4).", "zh": "以下 Ingress 让后台负载均衡器基于\n[host 头部字段](https://tools.ietf.org/html/rfc7230#section-5.4)来路由请求。\n\n{{% code_sample file=\"service/networking/name-virtual-host-ingress.yaml\" %}}"}
{"en": "If you create an Ingress resource without any hosts defined in the rules, then any\nweb traffic to the IP address of your Ingress controller can be matched without a name based\nvirtual host being required.", "zh": "如果你所创建的 Ingress 资源没有在 `rules` 中定义主机，则规则可以匹配指向\nIngress 控制器 IP 地址的所有网络流量，而无需基于名称的虚拟主机。"}
{"en": "For example, the following Ingress routes traffic\nrequested for `first.bar.com` to `service1`, `second.bar.com` to `service2`,\nand any traffic whose request host header doesn't match `first.bar.com`\nand `second.bar.com` to `service3`.", "zh": "例如，下面的 Ingress 对象会将请求 `first.bar.com` 的流量路由到 `service1`，将请求\n`second.bar.com` 的流量路由到 `service2`，而将所有其他流量路由到 `service3`。\n\n{{% code_sample file=\"service/networking/name-virtual-host-ingress-no-third-host.yaml\" %}}"}
{"en": "### TLS\n\nYou can secure an Ingress by specifying a {{< glossary_tooltip term_id=\"secret\" >}}\nthat contains a TLS private key and certificate. The Ingress resource only\nsupports a single TLS port, 443, and assumes TLS termination at the ingress point\n(traffic to the Service and its Pods is in plaintext).\nIf the TLS configuration section in an Ingress specifies different hosts, they are\nmultiplexed on the same port according to the hostname specified through the\nSNI TLS extension (provided the Ingress controller supports SNI). The TLS secret\nmust contain keys named `tls.crt` and `tls.key` that contain the certificate\nand private key to use for TLS. For example:", "zh": "### TLS\n\n你可以通过设定包含 TLS 私钥和证书的{{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}\n来保护 Ingress。\nIngress 资源只支持一个 TLS 端口 443，并假定 TLS 连接终止于 Ingress 节点\n（与 Service 及其 Pod 间的流量都以明文传输）。\n如果 Ingress 中的 TLS 配置部分指定了不同主机，那么它们将通过\nSNI TLS 扩展指定的主机名（如果 Ingress 控制器支持 SNI）在同一端口上进行复用。\nTLS Secret 的数据中必须包含键名为 `tls.crt` 的证书和键名为 `tls.key` 的私钥，\n才能用于 TLS 目的。例如：\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: testsecret-tls\n  namespace: default\ndata:\n  tls.crt: base64 编码的证书\n  tls.key: base64 编码的私钥\ntype: kubernetes.io/tls\n```"}
{"en": "Referencing this secret in an Ingress tells the Ingress controller to\nsecure the channel from the client to the load balancer using TLS. You need to make\nsure the TLS secret you created came from a certificate that contains a Common\nName (CN), also known as a Fully Qualified Domain Name (FQDN) for `https-example.foo.com`.", "zh": "在 Ingress 中引用此 Secret 将会告诉 Ingress 控制器使用 TLS 加密从客户端到负载均衡器的通道。\n你要确保所创建的 TLS Secret 创建自包含 `https-example.foo.com` 的公共名称\n（Common Name，CN）的证书。这里的公共名称也被称为全限定域名（Fully Qualified Domain Name，FQDN）。\n\n{{< note >}}"}
{"en": "Keep in mind that TLS will not work on the default rule because the\ncertificates would have to be issued for all the possible sub-domains. Therefore,\n`hosts` in the `tls` section need to explicitly match the `host` in the `rules`\nsection.", "zh": "注意，不能针对默认规则使用 TLS，因为这样做需要为所有可能的子域名签发证书。\n因此，`tls` 字段中的 `hosts` 的取值需要与 `rules` 字段中的 `host` 完全匹配。\n{{< /note >}}\n\n{{% code_sample file=\"service/networking/tls-example-ingress.yaml\" %}}\n\n{{< note >}}"}
{"en": "There is a gap between TLS features supported by various Ingress\ncontrollers. Please refer to documentation on\n[nginx](https://kubernetes.github.io/ingress-nginx/user-guide/tls/),\n[GCE](https://git.k8s.io/ingress-gce/README.md#frontend-https), or any other\nplatform specific Ingress controller to understand how TLS works in your environment.", "zh": "各种 Ingress 控制器在所支持的 TLS 特性上参差不齐。请参阅与\n[nginx](https://kubernetes.github.io/ingress-nginx/user-guide/tls/)、\n[GCE](https://git.k8s.io/ingress-gce/README.md#frontend-https)\n或者任何其他平台特定的 Ingress 控制器有关的文档，以了解 TLS 如何在你的环境中工作。\n{{< /note >}}"}
{"en": "### Load balancing {#load-balancing}\n\nAn Ingress controller is bootstrapped with some load balancing policy settings\nthat it applies to all Ingress, such as the load balancing algorithm, backend\nweight scheme, and others. More advanced load balancing concepts\n(e.g. persistent sessions, dynamic weights) are not yet exposed through the\nIngress. You can instead get these features through the load balancer used for\na Service.", "zh": "### 负载均衡  {#load-balancing}\n\nIngress 控制器启动引导时使用一些适用于所有 Ingress 的负载均衡策略设置，\n例如负载均衡算法、后端权重方案等。\n更高级的负载均衡概念（例如持久会话、动态权重）尚未通过 Ingress 公开。\n你可以通过用于 Service 的负载均衡器来获取这些功能。"}
{"en": "It's also worth noting that even though health checks are not exposed directly\nthrough the Ingress, there exist parallel concepts in Kubernetes such as\n[readiness probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)\nthat allow you to achieve the same end result. Please review the controller\nspecific documentation to see how they handle health checks (for example:\n[nginx](https://git.k8s.io/ingress-nginx/README.md), or\n[GCE](https://git.k8s.io/ingress-gce/README.md#health-checks)).", "zh": "值得注意的是，尽管健康检查不是通过 Ingress 直接暴露的，在 Kubernetes\n中存在[就绪态探针](/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)\n这类等价的概念，供你实现相同的目的。\n请查阅特定控制器的说明文档（例如：[nginx](https://git.k8s.io/ingress-nginx/README.md)、\n[GCE](https://git.k8s.io/ingress-gce/README.md#health-checks)）\n以了解它们是怎样处理健康检查的。"}
{"en": "## Updating an Ingress\n\nTo update an existing Ingress to add a new Host, you can update it by editing the resource:", "zh": "## 更新 Ingress   {#updating-an-ingress}\n\n要更新现有的 Ingress 以添加新的 Host，可以通过编辑资源来更新它：\n\n```shell\nkubectl describe ingress test\n```\n\n```\nName:             test\nNamespace:        default\nAddress:          178.91.123.132\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\nRules:\n  Host         Path  Backends\n  ----         ----  --------\n  foo.bar.com\n               /foo   service1:80 (10.8.0.90:80)\nAnnotations:\n  nginx.ingress.kubernetes.io/rewrite-target:  /\nEvents:\n  Type     Reason  Age                From                     Message\n  ----     ------  ----               ----                     -------\n  Normal   ADD     35s                loadbalancer-controller  default/test\n```\n\n```shell\nkubectl edit ingress test\n```"}
{"en": "This pops up an editor with the existing configuration in YAML format.\nModify it to include the new Host:", "zh": "这一命令将打开编辑器，允许你以 YAML 格式编辑现有配置。\n修改它来增加新的主机：\n\n```yaml\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: service1\n            port:\n              number: 80\n        path: /foo\n        pathType: Prefix\n  - host: bar.baz.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: service2\n            port:\n              number: 80\n        path: /foo\n        pathType: Prefix\n..\n```"}
{"en": "After you save your changes, kubectl updates the resource in the API server, which tells the\nIngress controller to reconfigure the load balancer.", "zh": "保存更改后，kubectl 将更新 API 服务器上的资源，该资源将告诉 Ingress 控制器重新配置负载均衡器。"}
{"en": "Verify this:", "zh": "验证：\n\n```shell\nkubectl describe ingress test\n```\n\n```\nName:             test\nNamespace:        default\nAddress:          178.91.123.132\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\nRules:\n  Host         Path  Backends\n  ----         ----  --------\n  foo.bar.com\n               /foo   service1:80 (10.8.0.90:80)\n  bar.baz.com\n               /foo   service2:80 (10.8.0.91:80)\nAnnotations:\n  nginx.ingress.kubernetes.io/rewrite-target:  /\nEvents:\n  Type     Reason  Age                From                     Message\n  ----     ------  ----               ----                     -------\n  Normal   ADD     45s                loadbalancer-controller  default/test\n```"}
{"en": "You can achieve the same outcome by invoking `kubectl replace -f` on a modified Ingress YAML file.", "zh": "你也可以针对修改后的 Ingress YAML 文件，通过 `kubectl replace -f` 命令获得同样结果。"}
{"en": "## Failing across availability zones\n\nTechniques for spreading traffic across failure domains differ between cloud providers.\nPlease check the documentation of the relevant [Ingress controller](/docs/concepts/services-networking/ingress-controllers) for details.", "zh": "## 跨可用区的失效  {#failing-across-availability-zones}\n\n不同的云厂商使用不同的技术来实现跨故障域的流量分布。\n请查看相关 [Ingress 控制器](/zh-cn/docs/concepts/services-networking/ingress-controllers)的文档以了解详细信息。"}
{"en": "## Alternatives\n\nYou can expose a Service in multiple ways that don't directly involve the Ingress resource:", "zh": "## 替代方案    {#alternatives}\n\n不直接使用 Ingress 资源，也有多种方法暴露 Service："}
{"en": "* Use [Service.Type=LoadBalancer](/docs/concepts/services-networking/service/#loadbalancer)\n* Use [Service.Type=NodePort](/docs/concepts/services-networking/service/#type-nodeport)", "zh": "* 使用 [Service.Type=LoadBalancer](/zh-cn/docs/concepts/services-networking/service/#loadbalancer)\n* 使用 [Service.Type=NodePort](/zh-cn/docs/concepts/services-networking/service/#type-nodeport)\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn about the [Ingress](/docs/reference/kubernetes-api/service-resources/ingress-v1/) API\n* Learn about [Ingress controllers](/docs/concepts/services-networking/ingress-controllers/)\n* [Set up Ingress on Minikube with the NGINX Controller](/docs/tasks/access-application-cluster/ingress-minikube/)", "zh": "* 进一步了解 [Ingress](/zh-cn/docs/reference/kubernetes-api/service-resources/ingress-v1/) API\n* 进一步了解 [Ingress 控制器](/zh-cn/docs/concepts/services-networking/ingress-controllers/)\n* [使用 NGINX 控制器在 Minikube 上安装 Ingress](/zh-cn/docs/tasks/access-application-cluster/ingress-minikube/)"}
{"en": "## The Kubernetes network model\n\nThe Kubernetes network model is built out of several pieces:\n\n* Each [pod](/docs/concepts/workloads/pods/) in a cluster gets its\n  own unique cluster-wide IP address.\n\n  * A pod has its own private network namespace which is shared by\n    all of the containers within the pod. Processes running in\n    different containers in the same pod can communicate with each\n    other over `localhost`.", "zh": "## Kubernetes 网络模型   {#the-kubernetes-network-model}\n\nKubernetes 网络模型由几个部分构成：\n\n* 集群中的每个 [Pod](/zh-cn/docs/concepts/workloads/pods/)\n  都会获得自己的、独一无二的集群范围 IP 地址。\n\n  * Pod 有自己的私有网络命名空间，Pod 内的所有容器共享这个命名空间。\n    运行在同一个 Pod 中的不同容器的进程彼此之间可以通过 `localhost` 进行通信。"}
{"en": "* The _pod network_ (also called a cluster network) handles communication\n  between pods. It ensures that (barring intentional network segmentation):\n\n  * All pods can communicate with all other pods, whether they are\n    on the same [node](/docs/concepts/architecture/nodes/) or on\n    different nodes. Pods can communicate with each other\n    directly, without the use of proxies or address translation (NAT).\n\n    On Windows, this rule does not apply to host-network pods.\n\n  * Agents on a node (such as system daemons, or kubelet) can\n    communicate with all pods on that node.", "zh": "* **Pod 网络**（也称为集群网络）处理 Pod 之间的通信。它确保（除非故意进行网络分段）：\n\n  * 所有 Pod 可以与所有其他 Pod 进行通信，\n    无论它们是在同一个[节点](/zh-cn/docs/concepts/architecture/nodes/)还是在不同的节点上。\n    Pod 可以直接相互通信，而无需使用代理或地址转换（NAT）。\n\n    在 Windows 上，这条规则不适用于主机网络 Pod。\n\n  * 节点上的代理（例如系统守护进程或 kubelet）可以与该节点上的所有 Pod 进行通信。"}
{"en": "* The [Service](/docs/concepts/services-networking/service/) API\n  lets you provide a stable (long lived) IP address or hostname for a service implemented\n  by one or more backend pods, where the individual pods making up\n  the service can change over time.\n\n  * Kubernetes automatically manages\n    [EndpointSlice](/docs/concepts/services-networking/endpoint-slices/)\n    objects to provide information about the pods currently backing a Service.\n\n  * A service proxy implementation monitors the set of Service and\n    EndpointSlice objects, and programs the data plane to route\n    service traffic to its backends, by using operating system or\n    cloud provider APIs to intercept or rewrite packets.", "zh": "* [Service](/zh-cn/docs/concepts/services-networking/service/) API\n  允许你为由一个或多个后端 Pod 实现的服务提供一个稳定（长效）的 IP 地址或主机名，\n  其中组成服务的各个 Pod 可以随时变化。\n\n  * Kubernetes 会自动管理\n    [EndpointSlice](/zh-cn/docs/concepts/services-networking/endpoint-slices/)\n    对象，以提供有关当前用来提供 Service 的 Pod 的信息。\n\n  * 服务代理实现通过使用操作系统或云平台 API 来拦截或重写数据包，\n    监视 Service 和 EndpointSlice 对象集，并在数据平面编程将服务流量路由到其后端。"}
{"en": "* The [Gateway](/docs/concepts/services-networking/gateway/) API\n  (or its predecessor, [Ingress](/docs/concepts/services-networking/ingress/))\n  allows you to make Services accessible to clients that are outside the cluster.\n\n  * A simpler, but less-configurable, mechanism for cluster\n    ingress is available via the Service API's\n    [`type: LoadBalancer`](/docs/concepts/services-networking/service/#loadbalancer),\n    when using a supported {{< glossary_tooltip term_id=\"cloud-provider\">}}.\n\n* [NetworkPolicy](/docs/concepts/services-networking/network-policies) is a built-in\n  Kubernetes API that allows you to control traffic between pods, or between pods and\n  the outside world.", "zh": "* [Gateway](/zh-cn/docs/concepts/services-networking/gateway/) API\n  （或其前身 [Ingress](/zh-cn/docs/concepts/services-networking/ingress/)\n  使得集群外部的客户端能够访问 Service。\n\n  * 当使用受支持的 {{< glossary_tooltip term_id=\"cloud-provider\">}} 时，通过 Service API 的\n    [`type: LoadBalancer`](/zh-cn/docs/concepts/services-networking/service/#loadbalancer)\n    可以使用一种更简单但可配置性较低的集群 Ingress 机制。\n\n* [NetworkPolicy](/zh-cn/docs/concepts/services-networking/network-policies)\n  是一个内置的 Kubernetes API，允许你控制 Pod 之间的流量或 Pod 与外部世界之间的流量。"}
{"en": "In older container systems, there was no automatic connectivity\nbetween containers on different hosts, and so it was often necessary\nto explicitly create links between containers, or to map container\nports to host ports to make them reachable by containers on other\nhosts. This is not needed in Kubernetes; Kubernetes's model is that\npods can be treated much like VMs or physical hosts from the\nperspectives of port allocation, naming, service discovery, load\nbalancing, application configuration, and migration.", "zh": "在早期的容器系统中，不同主机上的容器之间没有自动连通，\n因此通常需要显式创建容器之间的链路，或将容器端口映射到主机端口，以便其他主机上的容器能够访问。\n在 Kubernetes 中并不需要如此操作；在 Kubernetes 的网络模型中，\n从端口分配、命名、服务发现、负载均衡、应用配置和迁移的角度来看，Pod 可以被视作虚拟机或物理主机。"}
{"en": "Only a few parts of this model are implemented by Kubernetes itself.\nFor the other parts, Kubernetes defines the APIs, but the\ncorresponding functionality is provided by external components, some\nof which are optional:\n\n* Pod network namespace setup is handled by system-level software implementing the\n  [Container Runtime Interface](/docs/concepts/architecture/cri.md).", "zh": "这个模型只有少部分是由 Kubernetes 自身实现的。\n对于其他部分，Kubernetes 定义 API，但相应的功能由外部组件提供，其中一些是可选的：\n\n* Pod 网络命名空间的设置由实现[容器运行时接口（CRI）](/zh-cn/docs/concepts/architecture/cri.md)的系统层面软件处理。"}
{"en": "* The pod network itself is managed by a\n  [pod network implementation](/docs/concepts/cluster-administration/addons/#networking-and-network-policy).\n  On Linux, most container runtimes use the\n  {{< glossary_tooltip text=\"Container Networking Interface (CNI)\" term_id=\"cni\" >}}\n  to interact with the pod network implementation, so these\n  implementations are often called _CNI plugins_.\n\n* Kubernetes provides a default implementation of service proxying,\n  called {{< glossary_tooltip term_id=\"kube-proxy\">}}, but some pod\n  network implementations instead use their own service proxy that\n  is more tightly integrated with the rest of the implementation.", "zh": "* Pod 网络本身由\n  [Pod 网络实现](/zh-cn/docs/concepts/cluster-administration/addons/#networking-and-network-policy)管理。\n  在 Linux 上，大多数容器运行时使用{{< glossary_tooltip text=\"容器网络接口 (CNI)\" term_id=\"cni\" >}}\n  与 Pod 网络实现进行交互，因此这些实现通常被称为 **CNI 插件**。\n\n* Kubernetes 提供了一个默认的服务代理实现，称为 {{< glossary_tooltip term_id=\"kube-proxy\">}}，\n  但某些 Pod 网络实现使用其自己的服务代理，以便与实现的其余组件集成得更紧密。"}
{"en": "* NetworkPolicy is generally also implemented by the pod network\n  implementation. (Some simpler pod network implementations don't\n  implement NetworkPolicy, or an administrator may choose to\n  configure the pod network without NetworkPolicy support. In these\n  cases, the API will still be present, but it will have no effect.)\n\n* There are many [implementations of the Gateway API](https://gateway-api.sigs.k8s.io/implementations/),\n  some of which are specific to particular cloud environments, some more\n  focused on \"bare metal\" environments, and others more generic.", "zh": "* NetworkPolicy 通常也由 Pod 网络实现提供支持。\n  （某些更简单的 Pod 网络实现不支持 NetworkPolicy，或者管理员可能会选择在不支持 NetworkPolicy\n  的情况下配置 Pod 网络。在这些情况下，API 仍然存在，但将没有效果。）\n\n* [Gateway API 的实现](https://gateway-api.sigs.k8s.io/implementations/)有很多，\n  其中一些特定于某些云环境，还有一些更专注于“裸金属”环境，而其他一些则更加通用。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "The [Connecting Applications with Services](/docs/tutorials/services/connect-applications-service/)\ntutorial lets you learn about Services and Kubernetes networking with a hands-on example.\n\n[Cluster Networking](/docs/concepts/cluster-administration/networking/) explains how to set\nup networking for your cluster, and also provides an overview of the technologies involved.", "zh": "[使用 Service 连接到应用](/zh-cn/docs/tutorials/services/connect-applications-service/)教程通过一个实际的示例让你了解\nService 和 Kubernetes 如何联网。\n\n[集群网络](/zh-cn/docs/concepts/cluster-administration/networking/)解释了如何为集群设置网络，\n还概述了所涉及的技术。"}
{"en": "Kubernetes creates DNS records for Services and Pods. You can contact\nServices with consistent DNS names instead of IP addresses.", "zh": "Kubernetes 为 Service 和 Pod 创建 DNS 记录。\n你可以使用稳定的 DNS 名称而非 IP 地址访问 Service。"}
{"en": "Kubernetes publishes information about Pods and Services which is used\nto program DNS.  Kubelet configures Pods' DNS so that running containers\ncan lookup Services by name rather than IP.", "zh": "Kubernetes 发布有关 Pod 和 Service 的信息，用于配置 DNS。\nKubelet 配置 Pod 的 DNS，使运行中的容器可以通过名称而不是 IP 地址来查找服务。"}
{"en": "Services defined in the cluster are assigned DNS names. By default, a\nclient Pod's DNS search list includes the Pod's own namespace and the\ncluster's default domain.", "zh": "集群中定义的 Service 被赋予 DNS 名称。\n默认情况下，客户端 Pod 的 DNS 搜索列表包括 Pod 所在的命名空间和集群的默认域名。"}
{"en": "### Namespaces of Services \n\nA DNS query may return different results based on the namespace of the Pod making \nit. DNS queries that don't specify a namespace are limited to the Pod's \nnamespace. Access Services in other namespaces by specifying it in the DNS query. \n\nFor example, consider a Pod in a `test` namespace. A `data` Service is in \nthe `prod` namespace. \n\nA query for `data` returns no results, because it uses the Pod's `test` namespace. \n\nA query for `data.prod` returns the intended result, because it specifies the \nnamespace.", "zh": "### Service 的命名空间 {#namespaces-of-services}\n\nDNS 查询可能因为执行查询的 Pod 所在的命名空间而返回不同的结果。\n不指定命名空间的 DNS 查询会被限制在 Pod 所在的命名空间内。\n要访问其他命名空间中的 Service，需要在 DNS 查询中指定命名空间。\n\n例如，假定命名空间 `test` 中存在一个 Pod，`prod` 命名空间中存在一个服务 `data`。\n\nPod 查询 `data` 时没有返回结果，因为使用的是 Pod 所在的 `test` 命名空间。\n\nPod 查询 `data.prod` 时则会返回预期的结果，因为查询中指定了命名空间。"}
{"en": "DNS queries may be expanded using the Pod's `/etc/resolv.conf`. Kubelet \nconfigures this file for each Pod. For example, a query for just `data` may be \nexpanded to `data.test.svc.cluster.local`. The values of the `search` option \nare used to expand queries. To learn more about DNS queries, see \n[the `resolv.conf` manual page.](https://www.man7.org/linux/man-pages/man5/resolv.conf.5.html)", "zh": "DNS 查询可以使用 Pod 中的 `/etc/resolv.conf` 展开。\nKubelet 为每个 Pod 配置此文件。\n例如，对 `data` 的查询可能被扩展为 `data.test.svc.cluster.local`。\n`search` 选项的值用于扩展查询。要进一步了解 DNS 查询，可参阅\n[`resolv.conf` 手册页面](https://www.man7.org/linux/man-pages/man5/resolv.conf.5.html)。\n\n```\nnameserver 10.32.0.10\nsearch <namespace>.svc.cluster.local svc.cluster.local cluster.local\noptions ndots:5\n```"}
{"en": "In summary, a Pod in the _test_ namespace can successfully resolve either \n`data.prod` or `data.prod.svc.cluster.local`.", "zh": "概括起来，命名空间 _test_ 中的 Pod 可以成功地解析 `data.prod` 或者\n`data.prod.svc.cluster.local`。"}
{"en": "### DNS Records\n\nWhat objects get DNS records?", "zh": "### DNS 记录  {#dns-records}\n\n哪些对象会获得 DNS 记录呢？\n\n1. Services\n2. Pods"}
{"en": "The following sections detail the supported DNS record types and layout that is\nsupported.  Any other layout or names or queries that happen to work are\nconsidered implementation details and are subject to change without warning.\nFor more up-to-date specification, see\n[Kubernetes DNS-Based Service Discovery](https://github.com/kubernetes/dns/blob/master/docs/specification.md).", "zh": "以下各节详细介绍已支持的 DNS 记录类型和布局。\n其它布局、名称或者查询即使碰巧可以工作，也应视为实现细节，\n将来很可能被更改而且不会因此发出警告。\n有关最新规范请查看\n[Kubernetes 基于 DNS 的服务发现](https://github.com/kubernetes/dns/blob/master/docs/specification.md)。"}
{"en": "## Services\n\n### A/AAAA records\n\n\"Normal\" (not headless) Services are assigned DNS A and/or AAAA records,\ndepending on the IP family or families of the Service, with a name of the form\n`my-svc.my-namespace.svc.cluster-domain.example`.  This resolves to the cluster IP\nof the Service.\n\n[Headless Services](/docs/concepts/services-networking/service/#headless-services) \n(without a cluster IP) Services are also assigned DNS A and/or AAAA records,\nwith a name of the form `my-svc.my-namespace.svc.cluster-domain.example`.  Unlike normal\nServices, this resolves to the set of IPs of all of the Pods selected by the Service.\nClients are expected to consume the set or else use standard round-robin\nselection from the set.", "zh": "### Service\n\n#### A/AAAA 记录 {#a-aaaa-records}\n\n除了无头 Service 之外的 “普通” Service 会被赋予一个形如 `my-svc.my-namespace.svc.cluster-domain.example`\n的 DNS A 和/或 AAAA 记录，取决于 Service 的 IP 协议族（可能有多个）设置。\n该名称会解析成对应 Service 的集群 IP。\n\n没有集群 IP 的[无头 Service](/zh-cn/docs/concepts/services-networking/service/#headless-services)\n也会被赋予一个形如 `my-svc.my-namespace.svc.cluster-domain.example` 的 DNS A 和/或 AAAA 记录。\n与普通 Service 不同，这一记录会被解析成对应 Service 所选择的 Pod IP 的集合。\n客户端要能够使用这组 IP，或者使用标准的轮转策略从这组 IP 中进行选择。"}
{"en": "### SRV records\n\nSRV Records are created for named ports that are part of normal or headless\nservices.  For each named port, the SRV record has the form\n`_port-name._port-protocol.my-svc.my-namespace.svc.cluster-domain.example`.\nFor a regular Service, this resolves to the port number and the domain name:\n`my-svc.my-namespace.svc.cluster-domain.example`.\nFor a headless Service, this resolves to multiple answers, one for each Pod\nthat is backing the Service, and contains the port number and the domain name of the Pod\nof the form `hostname.my-svc.my-namespace.svc.cluster-domain.example`.", "zh": "#### SRV 记录  {#srv-records}\n\nKubernetes 根据普通 Service 或无头 Service\n中的命名端口创建 SRV 记录。每个命名端口，\nSRV 记录格式为 `_port-name._port-protocol.my-svc.my-namespace.svc.cluster-domain.example`。\n对于普通 Service，该记录会被解析成端口号和域名：`my-svc.my-namespace.svc.cluster-domain.example`。\n对于无头 Service，该记录会被解析成多个结果，及该服务的每个后端 Pod 各一个 SRV 记录，\n其中包含 Pod 端口号和格式为 `hostname.my-svc.my-namespace.svc.cluster-domain.example`\n的域名。"}
{"en": "## Pods", "zh": "## Pod"}
{"en": "### A/AAAA records\n\nKube-DNS versions, prior to the implementation of the [DNS specification](https://github.com/kubernetes/dns/blob/master/docs/specification.md), had the following DNS resolution:\n\n`pod-ipv4-address.my-namespace.pod.cluster-domain.example`.\n\nFor example, if a Pod in the `default` namespace has the IP address 172.17.0.3,\nand the domain name for your cluster is `cluster.local`, then the Pod has a DNS name:\n\n`172-17-0-3.default.pod.cluster.local`.\n\nAny Pods exposed by a Service have the following DNS resolution available:\n\n`pod-ipv4-address.service-name.my-namespace.svc.cluster-domain.example`.", "zh": "### A/AAAA 记录 {#a-aaaa-records}\n\n在实现 [DNS 规范](https://github.com/kubernetes/dns/blob/master/docs/specification.md)之前，\nKube-DNS 版本使用以下 DNS 解析：\n\n`pod-ipv4-address.my-namespace.pod.cluster-domain.example`\n\n例如，对于一个位于 `default` 命名空间，IP 地址为 172.17.0.3 的 Pod，\n如果集群的域名为 `cluster.local`，则 Pod 会对应 DNS 名称：\n\n`172-17-0-3.default.pod.cluster.local`\n\n通过 Service 暴露出来的所有 Pod 都会有如下 DNS 解析名称可用：\n\n`pod-ipv4-address.service-name.my-namespace.svc.cluster-domain.example`"}
{"en": "### Pod's hostname and subdomain fields\n\nCurrently when a Pod is created, its hostname (as observed from within the Pod)\nis the Pod's `metadata.name` value.", "zh": "### Pod 的 hostname 和 subdomain 字段 {#pod-s-hostname-and-subdomain-fields}\n\n当前，创建 Pod 时其主机名（从 Pod 内部观察）取自 Pod 的 `metadata.name` 值。"}
{"en": "The Pod spec has an optional `hostname` field, which can be used to specify a\ndifferent hostname. When specified, it takes precedence over the Pod's name to be\nthe hostname of the Pod (again, as observed from within the Pod). For example,\ngiven a Pod with `spec.hostname` set to `\"my-host\"`, the Pod will have its\nhostname set to `\"my-host\"`.", "zh": "Pod 规约中包含一个可选的 `hostname` 字段，可以用来指定一个不同的主机名。\n当这个字段被设置时，它将优先于 Pod 的名字成为该 Pod 的主机名（同样是从 Pod 内部观察）。\n举个例子，给定一个 `spec.hostname` 设置为 `“my-host”` 的 Pod，\n该 Pod 的主机名将被设置为 `“my-host”`。"}
{"en": "The Pod spec also has an optional `subdomain` field which can be used to indicate\nthat the pod is part of sub-group of the namespace. For example, a Pod with `spec.hostname`\nset to `\"foo\"`, and `spec.subdomain` set to `\"bar\"`, in namespace `\"my-namespace\"`, will\nhave its hostname set to `\"foo\"` and its fully qualified domain name (FQDN) set to\n`\"foo.bar.my-namespace.svc.cluster.local\"` (once more, as observed from within\nthe Pod).", "zh": "Pod 规约还有一个可选的 `subdomain` 字段，可以用来表明该 Pod 属于命名空间的一个子组。\n例如，某 Pod 的 `spec.hostname` 设置为 `“foo”`，`spec.subdomain` 设置为 `“bar”`，\n在命名空间 `“my-namespace”` 中，主机名称被设置成 `“foo”` 并且对应的完全限定域名（FQDN）为\n“`foo.bar.my-namespace.svc.cluster-domain.example`”（还是从 Pod 内部观察）。"}
{"en": "If there exists a headless Service in the same namespace as the Pod, with\nthe same name as the subdomain, the cluster's DNS Server also returns A and/or AAAA\nrecords for the Pod's fully qualified hostname.\nExample:", "zh": "如果 Pod 所在的命名空间中存在一个无头 Service，其名称与子域相同，\n则集群的 DNS 服务器还会为 Pod 的完全限定主机名返回 A 和/或 AAAA 记录。\n\n示例：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: busybox-subdomain\nspec:\n  selector:\n    name: busybox\n  clusterIP: None\n  ports:\n  - name: foo # 单个端口的 service 可以不指定 name\n    port: 1234\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox1\n  labels:\n    name: busybox\nspec:\n  hostname: busybox-1\n  subdomain: busybox-subdomain\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    name: busybox\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox2\n  labels:\n    name: busybox\nspec:\n  hostname: busybox-2\n  subdomain: busybox-subdomain\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    name: busybox\n```"}
{"en": "Given the above Service `\"busybox-subdomain\"` and the Pods which set `spec.subdomain`\nto `\"busybox-subdomain\"`, the first Pod will see its own FQDN as\n`\"busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example\"`. DNS serves\nA and/or AAAA records at that name, pointing to the Pod's IP. Both Pods \"`busybox1`\" and\n\"`busybox2`\" will have their own address records.", "zh": "鉴于上述服务 `“busybox-subdomain”` 和将 `spec.subdomain` 设置为 `“busybox-subdomain”` 的 Pod，\n第一个 Pod 将看到自己的 FQDN 为 `“busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example”`。\nDNS 会为此名字提供一个 A 记录和/或 AAAA 记录，指向该 Pod 的 IP。\nPod “`busybox1`” 和 “`busybox2`” 都将有自己的地址记录。"}
{"en": "An {{<glossary_tooltip term_id=\"endpoint-slice\" text=\"EndpointSlice\">}} can specify\nthe DNS hostname for any endpoint addresses, along with its IP.", "zh": "{{<glossary_tooltip term_id=\"endpoint-slice\" text=\"EndpointSlice\">}}\n对象可以为任何端点地址及其 IP 指定 `hostname`。"}
{"en": "Because A and AAAA records are not created for Pod names, `hostname` is required for the Pod's A or AAAA\nrecord to be created. A Pod with no `hostname` but with `subdomain` will only create the\nA or AAAA record for the headless Service (`busybox-subdomain.my-namespace.svc.cluster-domain.example`),\npointing to the Pods' IP addresses. Also, the Pod needs to be ready in order to have a\nrecord unless `publishNotReadyAddresses=True` is set on the Service.", "zh": "{{< note >}}\n由于 A 和 AAAA 记录不是基于 Pod 名称创建，因此需要设置了 `hostname` 才会生成 Pod 的 A 或 AAAA 记录。\n没有设置 `hostname` 但设置了 `subdomain` 的 Pod 只会为\n无头 Service 创建 A 或 AAAA 记录（`busybox-subdomain.my-namespace.svc.cluster-domain.example`）\n指向 Pod 的 IP 地址。\n另外，除非在服务上设置了 `publishNotReadyAddresses=True`，否则只有 Pod 准备就绪\n才会有与之对应的记录。\n{{< /note >}}"}
{"en": "### Pod's setHostnameAsFQDN field {#pod-sethostnameasfqdn-field}", "zh": "### Pod 的 setHostnameAsFQDN 字段  {#pod-sethostnameasfqdn-field}\n\n{{< feature-state for_k8s_version=\"v1.22\" state=\"stable\" >}}"}
{"en": "When a Pod is configured to have fully qualified domain name (FQDN), its\nhostname is the short hostname. For example, if you have a Pod with the fully\nqualified domain name `busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example`, \nthen by default the `hostname` command inside that Pod returns `busybox-1` and  the\n`hostname --fqdn` command returns the FQDN.\n\nWhen you set `setHostnameAsFQDN: true` in the Pod spec, the kubelet writes the Pod's FQDN into the hostname for that Pod's namespace. In this case, both `hostname` and `hostname --fqdn` return the Pod's FQDN.", "zh": "当 Pod 配置为具有全限定域名 (FQDN) 时，其主机名是短主机名。\n例如，如果你有一个具有完全限定域名 `busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example` 的 Pod，\n则默认情况下，该 Pod 内的 `hostname` 命令返回 `busybox-1`，而 `hostname --fqdn` 命令返回 FQDN。\n\n当你在 Pod 规约中设置了 `setHostnameAsFQDN: true` 时，kubelet 会将 Pod\n的全限定域名（FQDN）作为该 Pod 的主机名记录到 Pod 所在命名空间。\n在这种情况下，`hostname` 和 `hostname --fqdn` 都会返回 Pod 的全限定域名。\n\n{{< note >}}"}
{"en": "In Linux, the hostname field of the kernel (the `nodename` field of `struct utsname`) is limited to 64 characters.\n\nIf a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in `Pending` status (`ContainerCreating` as seen by `kubectl`) generating error events, such as Failed to construct FQDN from Pod hostname and cluster domain, FQDN `long-FQDN` is too long (64 characters is the max, 70 characters requested). One way of improving user experience for this scenario is to create an [admission webhook controller](/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks) to control FQDN size when users create top level objects, for example, Deployment.", "zh": "在 Linux 中，内核的主机名字段（`struct utsname` 的 `nodename` 字段）限定最多 64 个字符。\n\n如果 Pod 启用这一特性，而其 FQDN 超出 64 字符，Pod 的启动会失败。\nPod 会一直出于 `Pending` 状态（通过 `kubectl` 所看到的 `ContainerCreating`），\n并产生错误事件，例如\n\"Failed to construct FQDN from Pod hostname and cluster domain, FQDN\n`long-FQDN` is too long (64 characters is the max, 70 characters requested).\"\n（无法基于 Pod 主机名和集群域名构造 FQDN，FQDN `long-FQDN` 过长，至多 64 个字符，请求字符数为 70）。\n对于这种场景而言，改善用户体验的一种方式是创建一个\n[准入 Webhook 控制器](/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks)，\n在用户创建顶层对象（如 Deployment）的时候控制 FQDN 的长度。\n{{< /note >}}"}
{"en": "### Pod's DNS Policy\n\nDNS policies can be set on a per-Pod basis. Currently Kubernetes supports the\nfollowing Pod-specific DNS policies. These policies are specified in the\n`dnsPolicy` field of a Pod Spec.\n\n- \"`Default`\": The Pod inherits the name resolution configuration from the node\n  that the Pods run on.\n  See [related discussion](/docs/tasks/administer-cluster/dns-custom-nameservers)\n  for more details.\n- \"`ClusterFirst`\": Any DNS query that does not match the configured cluster\n  domain suffix, such as \"`www.kubernetes.io`\", is forwarded to an upstream\n  nameserver by the DNS server. Cluster administrators may have extra\n  stub-domain and upstream DNS servers configured.\n  See [related discussion](/docs/tasks/administer-cluster/dns-custom-nameservers)\n  for details on how DNS queries are handled in those cases.\n- \"`ClusterFirstWithHostNet`\": For Pods running with hostNetwork, you should\n  explicitly set its DNS policy to \"`ClusterFirstWithHostNet`\". Otherwise, Pods\n  running with hostNetwork and `\"ClusterFirst\"` will fallback to the behavior\n  of the `\"Default\"` policy.\n  - Note: This is not supported on Windows. See [below](#dns-windows) for details\n- \"`None`\": It allows a Pod to ignore DNS settings from the Kubernetes\n  environment. All DNS settings are supposed to be provided using the\n  `dnsConfig` field in the Pod Spec.\n  See [Pod's DNS config](#pod-dns-config) subsection below.", "zh": "### Pod 的 DNS 策略    {#pod-s-dns-policy}\n\nDNS 策略可以逐个 Pod 来设定。目前 Kubernetes 支持以下特定 Pod 的 DNS 策略。\n这些策略可以在 Pod 规约中的 `dnsPolicy` 字段设置：\n\n- \"`Default`\": Pod 从运行所在的节点继承域名解析配置。\n  参考[相关讨论](/zh-cn/docs/tasks/administer-cluster/dns-custom-nameservers)获取更多信息。\n- \"`ClusterFirst`\": 与配置的集群域后缀不匹配的任何 DNS 查询（例如 \"www.kubernetes.io\"）\n  都会由 DNS 服务器转发到上游域名服务器。集群管理员可能配置了额外的存根域和上游 DNS 服务器。\n  参阅[相关讨论](/zh-cn/docs/tasks/administer-cluster/dns-custom-nameservers)\n  了解在这些场景中如何处理 DNS 查询的信息。\n- \"`ClusterFirstWithHostNet`\": 对于以 hostNetwork 方式运行的 Pod，应将其 DNS 策略显式设置为\n  \"`ClusterFirstWithHostNet`\"。否则，以 hostNetwork 方式和 `\"ClusterFirst\"` 策略运行的\n  Pod 将会做出回退至 `\"Default\"` 策略的行为。\n  - 注意：这在 Windows 上不支持。 有关详细信息，请参见[下文](#dns-windows)。\n- \"`None`\": 此设置允许 Pod 忽略 Kubernetes 环境中的 DNS 设置。Pod 会使用其 `dnsConfig`\n  字段所提供的 DNS 设置。\n  参见 [Pod 的 DNS 配置](#pod-dns-config)节。\n\n{{< note >}}"}
{"en": "\"Default\" is not the default DNS policy. If `dnsPolicy` is not\nexplicitly specified, then \"ClusterFirst\" is used.", "zh": "\"Default\" 不是默认的 DNS 策略。如果未明确指定 `dnsPolicy`，则使用 \"ClusterFirst\"。\n{{< /note >}}"}
{"en": "The example below shows a Pod with its DNS policy set to\n\"`ClusterFirstWithHostNet`\" because it has `hostNetwork` set to `true`.", "zh": "下面的示例显示了一个 Pod，其 DNS 策略设置为 \"`ClusterFirstWithHostNet`\"，\n因为它已将 `hostNetwork` 设置为 `true`。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  restartPolicy: Always\n  hostNetwork: true\n  dnsPolicy: ClusterFirstWithHostNet\n```"}
{"en": "### Pod's DNS Config {#pod-dns-config}", "zh": "### Pod 的 DNS 配置  {#pod-dns-config}\n\n{{< feature-state for_k8s_version=\"v1.14\" state=\"stable\" >}}"}
{"en": "Pod's DNS Config allows users more control on the DNS settings for a Pod.\n\nThe `dnsConfig` field is optional and it can work with any `dnsPolicy` settings.\nHowever, when a Pod's `dnsPolicy` is set to \"`None`\", the `dnsConfig` field has\nto be specified.\n\nBelow are the properties a user can specify in the `dnsConfig` field:", "zh": "Pod 的 DNS 配置可让用户对 Pod 的 DNS 设置进行更多控制。\n\n`dnsConfig` 字段是可选的，它可以与任何 `dnsPolicy` 设置一起使用。\n但是，当 Pod 的 `dnsPolicy` 设置为 \"`None`\" 时，必须指定 `dnsConfig` 字段。\n\n用户可以在 `dnsConfig` 字段中指定以下属性："}
{"en": "- `nameservers`: a list of IP addresses that will be used as DNS servers for the\n  Pod. There can be at most 3 IP addresses specified. When the Pod's `dnsPolicy`\n  is set to \"`None`\", the list must contain at least one IP address, otherwise\n  this property is optional.\n  The servers listed will be combined to the base nameservers generated from the\n  specified DNS policy with duplicate addresses removed.\n- `searches`: a list of DNS search domains for hostname lookup in the Pod.\n  This property is optional. When specified, the provided list will be merged\n  into the base search domain names generated from the chosen DNS policy.\n  Duplicate domain names are removed.\n  Kubernetes allows up to 32 search domains.\n- `options`: an optional list of objects where each object may have a `name`\n  property (required) and a `value` property (optional). The contents in this\n  property will be merged to the options generated from the specified DNS policy.\n  Duplicate entries are removed.", "zh": "- `nameservers`：将用作于 Pod 的 DNS 服务器的 IP 地址列表。\n  最多可以指定 3 个 IP 地址。当 Pod 的 `dnsPolicy` 设置为 \"`None`\" 时，\n  列表必须至少包含一个 IP 地址，否则此属性是可选的。\n  所列出的服务器将合并到从指定的 DNS 策略生成的基本域名服务器，并删除重复的地址。\n\n- `searches`：用于在 Pod 中查找主机名的 DNS 搜索域的列表。此属性是可选的。\n  指定此属性时，所提供的列表将合并到根据所选 DNS 策略生成的基本搜索域名中。\n  重复的域名将被删除。Kubernetes 最多允许 32 个搜索域。\n\n- `options`：可选的对象列表，其中每个对象可能具有 `name` 属性（必需）和 `value` 属性（可选）。\n  此属性中的内容将合并到从指定的 DNS 策略生成的选项。\n  重复的条目将被删除。"}
{"en": "The following is an example Pod with custom DNS settings:", "zh": "以下是具有自定义 DNS 设置的 Pod 示例：\n\n{{% code_sample file=\"service/networking/custom-dns.yaml\" %}}"}
{"en": "When the Pod above is created, the container `test` gets the following contents\nin its `/etc/resolv.conf` file:", "zh": "创建上面的 Pod 后，容器 `test` 会在其 `/etc/resolv.conf` 文件中获取以下内容：\n\n```\nnameserver 192.0.2.1\nsearch ns1.svc.cluster-domain.example my.dns.search.suffix\noptions ndots:2 edns0\n```"}
{"en": "For IPv6 setup, search path and name server should be set up like this:", "zh": "对于 IPv6 设置，搜索路径和名称服务器应按以下方式设置：\n\n```shell\nkubectl exec -it dns-example -- cat /etc/resolv.conf\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nnameserver 2001:db8:30::a\nsearch default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example\noptions ndots:5\n```"}
{"en": "## DNS search domain list limits", "zh": "## DNS 搜索域列表限制  {#dns-search-domain-list-limits}\n\n{{< feature-state for_k8s_version=\"1.28\" state=\"stable\" >}}"}
{"en": "Kubernetes itself does not limit the DNS Config until the length of the search\ndomain list exceeds 32 or the total length of all search domains exceeds 2048.\nThis limit applies to the node's resolver configuration file, the Pod's DNS\nConfig, and the merged DNS Config respectively.", "zh": "Kubernetes 本身不限制 DNS 配置，最多可支持 32 个搜索域列表，所有搜索域的总长度不超过 2048。\n此限制分别适用于节点的解析器配置文件、Pod 的 DNS 配置和合并的 DNS 配置。\n\n{{< note >}}"}
{"en": "Some container runtimes of earlier versions may have their own restrictions on\nthe number of DNS search domains. Depending on the container runtime\nenvironment, the pods with a large number of DNS search domains may get stuck in\nthe pending state.\n\nIt is known that containerd v1.5.5 or earlier and CRI-O v1.21 or earlier have\nthis problem.", "zh": "早期版本的某些容器运行时可能对 DNS 搜索域的数量有自己的限制。\n根据容器运行环境，那些具有大量 DNS 搜索域的 Pod 可能会卡在 Pending 状态。\n\n众所周知 containerd v1.5.5 或更早版本和 CRI-O v1.21 或更早版本都有这个问题。\n{{< /note >}}"}
{"en": "## DNS resolution on Windows nodes {#dns-windows}\n\n- ClusterFirstWithHostNet is not supported for Pods that run on Windows nodes.\n  Windows treats all names with a `.` as a FQDN and skips FQDN resolution.\n- On Windows, there are multiple DNS resolvers that can be used. As these come with\n  slightly different behaviors, using the\n  [`Resolve-DNSName`](https://docs.microsoft.com/powershell/module/dnsclient/resolve-dnsname)\n  powershell cmdlet for name query resolutions is recommended.\n- On Linux, you have a DNS suffix list, which is used after resolution of a name as fully\n  qualified has failed.\n  On Windows, you can only have 1 DNS suffix, which is the DNS suffix associated with that\n  Pod's namespace (example: `mydns.svc.cluster.local`). Windows can resolve FQDNs, Services,\n  or network name which can be resolved with this single suffix. For example, a Pod spawned\n  in the `default` namespace, will have the DNS suffix `default.svc.cluster.local`.\n  Inside a Windows Pod, you can resolve both `kubernetes.default.svc.cluster.local`\n  and `kubernetes`, but not the partially qualified names (`kubernetes.default` or\n  `kubernetes.default.svc`).", "zh": "## Windows 节点上的 DNS 解析 {#dns-windows}\n\n- 在 Windows 节点上运行的 Pod 不支持 ClusterFirstWithHostNet。\n  Windows 将所有带有 `.` 的名称视为全限定域名（FQDN）并跳过全限定域名（FQDN）解析。\n- 在 Windows 上，可以使用的 DNS 解析器有很多。\n  由于这些解析器彼此之间会有轻微的行为差别，建议使用\n  [`Resolve-DNSName`](https://docs.microsoft.com/powershell/module/dnsclient/resolve-dnsname)\n  powershell cmdlet 进行名称查询解析。\n- 在 Linux 上，有一个 DNS 后缀列表，当解析全名失败时可以使用。\n  在 Windows 上，你只能有一个 DNS 后缀，\n  即与该 Pod 的命名空间相关联的 DNS 后缀（例如：`mydns.svc.cluster.local`）。\n  Windows 可以解析全限定域名（FQDN），和使用了该 DNS 后缀的 Services 或者网络名称。\n  例如，在 `default` 命名空间中生成一个 Pod，该 Pod 会获得的 DNS 后缀为 `default.svc.cluster.local`。\n  在 Windows 的 Pod 中，你可以解析 `kubernetes.default.svc.cluster.local` 和 `kubernetes`，\n  但是不能解析部分限定名称（`kubernetes.default` 和 `kubernetes.default.svc`）。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "For guidance on administering DNS configurations, check\n[Configure DNS Service](/docs/tasks/administer-cluster/dns-custom-nameservers/)", "zh": "有关管理 DNS 配置的指导，\n请查看[配置 DNS 服务](/zh-cn/docs/tasks/administer-cluster/dns-custom-nameservers/)"}
{"en": "You've deployed your application and exposed it via a Service. Now what? Kubernetes provides a\nnumber of tools to help you manage your application deployment, including scaling and updating.", "zh": "你已经部署了你的应用并且通过 Service 将其暴露出来。现在要做什么？\nKubernetes 提供了一系列的工具帮助你管理应用的部署，包括扩缩和更新。"}
{"en": "## Organizing resource configurations", "zh": "## 组织资源配置"}
{"en": "Many applications require multiple resources to be created, such as a Deployment along with a Service.\nManagement of multiple resources can be simplified by grouping them together in the same file\n(separated by `---` in YAML). For example:", "zh": "一些应用需要创建多个资源，例如 Deployment 和 Service。\n将多个资源归入同一个文件（在 YAML 中使用 `---` 分隔）可以简化对多个资源的管理。例如：\n\n{{% code_sample file=\"application/nginx-app.yaml\" %}}"}
{"en": "Multiple resources can be created the same way as a single resource:", "zh": "创建多个资源的方法与创建单个资源的方法相同：\n\n```shell\nkubectl apply -f https://k8s.io/examples/application/nginx-app.yaml\n```\n\n```none\nservice/my-nginx-svc created\ndeployment.apps/my-nginx created\n```"}
{"en": "The resources will be created in the order they appear in the manifest. Therefore, it's best to\nspecify the Service first, since that will ensure the scheduler can spread the pods associated\nwith the Service as they are created by the controller(s), such as Deployment.", "zh": "资源会按照在清单中出现的顺序创建。\n因此，最好先指定 Service，这样可以确保调度器能在控制器（如 Deployment）创建 Pod 时对\nService 相关的 Pod 作分布。"}
{"en": "`kubectl apply` also accepts multiple `-f` arguments:", "zh": "`kubectl apply` 还可以接收多个 `-f` 参数：\n\n```shell\nkubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml \\\n  -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml\n```"}
{"en": "It is a recommended practice to put resources related to the same microservice or application tier\ninto the same file, and to group all of the files associated with your application in the same\ndirectory. If the tiers of your application bind to each other using DNS, you can deploy all of\nthe components of your stack together.", "zh": "建议将同一个微服务或应用相关的资源放到同一个文件中，\n并将与应用相关的所有文件归类到同一目录中。\n如果应用各层使用 DNS 相互绑定，你可以同时部署工作栈中的所有组件。"}
{"en": "A URL can also be specified as a configuration source, which is handy for deploying directly from\nmanifests in your source control system:", "zh": "URL 链接也可以被指定为配置源，这对于直接基于源码控制系统的清单进行部署来说非常方便：\n\n```shell\nkubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml\n```\n\n```none\ndeployment.apps/my-nginx created\n```"}
{"en": "If you need to define more manifests, such as adding a ConfigMap, you can do that too.", "zh": "如果你需要定义更多清单，例如添加一个 ConfigMap，你也可以这样做。"}
{"en": "### External tools", "zh": "### 外部工具 {#external-tools}"}
{"en": "This section lists only the most common tools used for managing workloads on Kubernetes. To see a larger list, view\n[Application definition and image build](https://landscape.cncf.io/guide#app-definition-and-development--application-definition-image-build)\nin the {{< glossary_tooltip text=\"CNCF\" term_id=\"cncf\" >}} Landscape.", "zh": "这一节列出了在 Kubernetes 中管理工作负载最常用的一些工具。\n如果想要查看完整的清单，参阅 {{< glossary_tooltip text=\"CNCF\" term_id=\"cncf\" >}}\n文章 [Application definition and image build](https://landscape.cncf.io/guide#app-definition-and-development--application-definition-image-build)。\n\n#### Helm {#external-tool-helm}\n\n{{% thirdparty-content single=\"true\" %}}"}
{"en": "[Helm](https://helm.sh/) is a tool for managing packages of pre-configured\nKubernetes resources. These packages are known as _Helm charts_.", "zh": "[Helm](https://helm.sh/) 是一种管理预配置 Kubernetes 资源包的工具。这些资源包被称为 _Helm charts_。\n\n#### Kustomize {#external-tool-kustomize}"}
{"en": "[Kustomize](https://kustomize.io/) traverses a Kubernetes manifest to add, remove or update configuration options.\nIt is available both as a standalone binary and as a [native feature](/docs/tasks/manage-kubernetes-objects/kustomization/)\nof kubectl.", "zh": "[Kustomize](https://kustomize.io/) 遍历 Kubernetes 清单以添加、删除或更新配置选项。\n它既可以作为独立的二级制文件使用，也可以作为 kubectl 的[原生功能](/zh-cn/docs/tasks/manage-kubernetes-objects/kustomization/) 使用。"}
{"en": "## Bulk operations in kubectl", "zh": "## kubectl 中的批量操作 {#bulk-operations-in-kubectl}"}
{"en": "Resource creation isn't the only operation that `kubectl` can perform in bulk. It can also extract\nresource names from configuration files in order to perform other operations, in particular to\ndelete the same resources you created:", "zh": "资源创建并不是 `kubectl` 可以批量执行的唯一操作。\n它还能提取配置文件中的资源名称来执行其他操作，尤其是删除已经创建的相同资源：\n\n```shell\nkubectl delete -f https://k8s.io/examples/application/nginx-app.yaml\n```\n\n```none\ndeployment.apps \"my-nginx\" deleted\nservice \"my-nginx-svc\" deleted\n```"}
{"en": "In the case of two resources, you can specify both resources on the command line using the\nresource/name syntax:", "zh": "如果有两个资源，你可以使用 resource/name 语法在命令行中指定这两个资源：\n\n```shell\nkubectl delete deployments/my-nginx services/my-nginx-svc\n```"}
{"en": "For larger numbers of resources, you'll find it easier to specify the selector (label query)\nspecified using `-l` or `--selector`, to filter resources by their labels:", "zh": "对于数量众多的资源，使用 `-l` 或 `--selector` 指定选择算符（标签查询）会更方便，\n可以根据标签来过滤资源：\n\n```shell\nkubectl delete deployment,services -l app=nginx\n```\n\n```none\ndeployment.apps \"my-nginx\" deleted\nservice \"my-nginx-svc\" deleted\n```"}
{"en": "### Chaining and filtering", "zh": "### 链式操作和过滤 {#chaining-and-filtering}"}
{"en": "Because `kubectl` outputs resource names in the same syntax it accepts, you can chain operations\nusing `$()` or `xargs`:", "zh": "因为 `kubectl` 输出的资源名称与接收的语法相同，你可以使用 `$()` 或 `xargs` 进行链式操作：\n\n```shell\nkubectl get $(kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service/ )\nkubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service/ | xargs -i kubectl get '{}'\n```"}
{"en": "The output might be similar to:", "zh": "输出类似这样：\n\n```none\nNAME           TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)      AGE\nmy-nginx-svc   LoadBalancer   10.0.0.208   <pending>     80/TCP       0s\n```"}
{"en": "With the above commands, first you create resources under `examples/application/nginx/` and print\nthe resources created with `-o name` output format (print each resource as resource/name).\nThen you `grep` only the Service, and then print it with [`kubectl get`](/docs/reference/kubectl/generated/kubectl_get/).", "zh": "使用上面的命令，首先会创建 `examples/application/nginx/` 目录下的资源，\n然后使用 `-o name` 输出格式打印创建的资源（以 resource/name 格式打印）。\n然后 `grep` 筛选出 Service，再用 [`kubectl get`](/docs/reference/kubectl/generated/kubectl_get/) 打印。"}
{"en": "### Recursive operations on local files", "zh": "### 对本地文件的递归操作 {#recursive-operations-on-local-files}"}
{"en": "If you happen to organize your resources across several subdirectories within a particular\ndirectory, you can recursively perform the operations on the subdirectories also, by specifying\n`--recursive` or `-R` alongside the `--filename`/`-f` argument.", "zh": "如果你碰巧在一个特定目录下跨多个子目录中组织资源，\n你也可以通过在指定 `--filename`/`-f` 的同时指定 `--recursive` 或 `-R` 参数对子目录执行递归操作。"}
{"en": "For instance, assume there is a directory `project/k8s/development` that holds all of the\n{{< glossary_tooltip text=\"manifests\" term_id=\"manifest\" >}} needed for the development environment,\norganized by resource type:", "zh": "例如，假设有一个目录 `project/k8s/development` 包含了开发环境所需的所有{{< glossary_tooltip text=\"清单文件\" term_id=\"manifest\" >}}，\n并按资源类型进行了分类：\n\n```none\nproject/k8s/development\n├── configmap\n│   └── my-configmap.yaml\n├── deployment\n│   └── my-deployment.yaml\n└── pvc\n    └── my-pvc.yaml\n```"}
{"en": "By default, performing a bulk operation on `project/k8s/development` will stop at the first level\nof the directory, not processing any subdirectories. If you had tried to create the resources in\nthis directory using the following command, we would have encountered an error:", "zh": "默认情况下，在 `project/k8s/development` 下执行批量操作会在目录的第一层终止，不会处理任何子目录。\n如果你在这个目录下使用如下命令尝试创建资源，会得到如下错误：\n\n```shell\nkubectl apply -f project/k8s/development\n```\n\n```none\nerror: you must provide one or more resources by argument or filename (.json|.yaml|.yml|stdin)\n```"}
{"en": "Instead, specify the `--recursive` or `-R` command line argument along with the `--filename`/`-f` argument:", "zh": "在命令行参数中与 `--filename`/`-f` 一起指定 `--recursive` 或 `-R`：\n\n```shell\nkubectl apply -f project/k8s/development --recursive\n```\n\n```none\nconfigmap/my-config created\ndeployment.apps/my-deployment created\npersistentvolumeclaim/my-pvc created\n```"}
{"en": "The `--recursive` argument works with any operation that accepts the `--filename`/`-f` argument such as:\n`kubectl create`, `kubectl get`, `kubectl delete`, `kubectl describe`, or even `kubectl rollout`.\n\nThe `--recursive` argument also works when multiple `-f` arguments are provided:", "zh": "参数 `--recursive` 可以处理任何可以接收 `--filename`/`-f` 参数的操作，\n例如： `kubectl create`、`kubectl get`、`kubectl delete`、`kubectl describe`，甚至是 `kubectl rollout`。\n\n当指定了多个 `-f` 参数时，`--recursive` 仍然可以生效。\n\n```shell\nkubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive\n```\n\n```none\nnamespace/development created\nnamespace/staging created\nconfigmap/my-config created\ndeployment.apps/my-deployment created\npersistentvolumeclaim/my-pvc created\n```"}
{"en": "If you're interested in learning more about `kubectl`, go ahead and read\n[Command line tool (kubectl)](/docs/reference/kubectl/).", "zh": "如果你对了解更多 `kubectl` 有兴趣，请阅读[命令行工具 (kubectl)](/zh-cn/docs/reference/kubectl/)。"}
{"en": "## Updating your application without an outage", "zh": "## 无中断更新应用 {#updating-your-application-without-an-outage}"}
{"en": "At some point, you'll eventually need to update your deployed application, typically by specifying\na new image or image tag. `kubectl` supports several update operations, each of which is applicable\nto different scenarios.", "zh": "有时候，你需要更新你所部署的应用，通常是指定新的镜像或镜像标签。\n`kubectl` 支持多种更新操作，每一种都适用于不同的场景。"}
{"en": "You can run multiple copies of your app, and use a _rollout_ to gradually shift the traffic to\nnew healthy Pods. Eventually, all the running Pods would have the new software.", "zh": "你可以运行应用的多个副本，并使用 **上线（rollout）** 操作将流量逐渐转移到新的健康 Pod 上。\n最终，所有正在运行的 Pod 都将拥有新的应用。"}
{"en": "This section of the page guides you through how to create and update applications with Deployments.", "zh": "本节将指导你如何使用 Deployment 创建和更新应用。"}
{"en": "Let's say you were running version 1.14.2 of nginx:", "zh": "假设你运行了 Nginx 1.14.2 版本。\n\n```shell\nkubectl create deployment my-nginx --image=nginx:1.14.2\n```\n\n```none\ndeployment.apps/my-nginx created\n```"}
{"en": "Ensure that there is 1 replica:", "zh": "确保只有一个副本:\n\n```shell\nkubectl scale --replicas 1 deployments/my-nginx --subresource='scale' --type='merge' -p '{\"spec\":{\"replicas\": 1}}'\n```\n\n```none\ndeployment.apps/my-nginx scaled\n```"}
{"en": "and allow Kubernetes to add more temporary replicas during a rollout, by setting a _surge maximum_ of\n100%:", "zh": "允许 Kubernetes 在上线过程中添加更多的临时副本，方法是设置最大涨幅为 100%。\n\n```shell\nkubectl patch --type='merge' -p '{\"spec\":{\"strategy\":{\"rollingUpdate\":{\"maxSurge\": \"100%\" }}}}'\n```\n\n```none\ndeployment.apps/my-nginx patched\n```"}
{"en": "To update to version 1.16.1, change `.spec.template.spec.containers[0].image` from `nginx:1.14.2`\nto `nginx:1.16.1` using `kubectl edit`:", "zh": "要更新到版本 1.61.1，使用 `kubectl edit` 将 `.spec.template.spec.containers[0].image`\n值从 `nginx:1.14.2` 修改为 `nginx:1.16.1`。\n\n```shell\nkubectl edit deployment/my-nginx\n# 修改清单文件以使用新的容器镜像，然后保存你所作的更改\n```"}
{"en": "That's it! The Deployment will declaratively update the deployed nginx application progressively\nbehind the scene. It ensures that only a certain number of old replicas may be down while they are\nbeing updated, and only a certain number of new replicas may be created above the desired number\nof pods. To learn more details about how this happens,\nvisit [Deployment](/docs/concepts/workloads/controllers/deployment/).", "zh": "就是这样！Deployment 会逐步声明式地更新已部署的 Nginx 应用。\n它确保只有一定数量的旧副本会在更新时处于宕机状态，\n并且超过所需的 Pod 数量的新副本个数在创建期间可控。\n要了解更多关于如何实现的详细信息，参照 [Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/)。"}
{"en": "You can use rollouts with DaemonSets, Deployments, or StatefulSets.", "zh": "你可以使用 DaemonSet、Deployment 或 StatefulSet 来完成上线。"}
{"en": "### Managing rollouts", "zh": "### 管理上线 {#managing-rollouts}"}
{"en": "You can use [`kubectl rollout`](/docs/reference/kubectl/generated/kubectl_rollout/) to manage a\nprogressive update of an existing application.", "zh": "你可以使用 [`kubectl rollout`](/docs/reference/kubectl/generated/kubectl_rollout/) 管理现有应用的逐步更新。"}
{"en": "For example:", "zh": "例如："}
{"en": "```shell\nkubectl apply -f my-deployment.yaml\n\n# wait for rollout to finish\nkubectl rollout status deployment/my-deployment --timeout 10m # 10 minute timeout\n```", "zh": "```shell\nkubectl apply -f my-deployment.yaml\n\n# 等待上线完成\nkubectl rollout status deployment/my-deployment --timeout 10m # 超时时长为 10 分钟\n```"}
{"en": "or", "zh": "或者"}
{"en": "```shell\nkubectl apply -f backing-stateful-component.yaml\n\n# don't wait for rollout to finish, just check the status\nkubectl rollout status statefulsets/backing-stateful-component --watch=false\n```", "zh": "```shell\nkubectl apply -f backing-stateful-component.yaml\n\n# 不用等待上线完成，只需要检查状态\nkubectl rollout status statefulsets/backing-stateful-component --watch=false\n```"}
{"en": "You can also pause, resume or cancel a rollout.\nVisit [`kubectl rollout`](/docs/reference/kubectl/generated/kubectl_rollout/) to learn more.", "zh": "你也可以暂停、恢复或取消上线。\n参阅 [`kubectl rollout`](/docs/reference/kubectl/generated/kubectl_rollout/) 以深入了解。"}
{"en": "## Canary deployments", "zh": "## 金丝雀部署 {#canary-deployments}"}
{"en": "Another scenario where multiple labels are needed is to distinguish deployments of different\nreleases or configurations of the same component. It is common practice to deploy a *canary* of a\nnew application release (specified via image tag in the pod template) side by side with the\nprevious release so that the new release can receive live production traffic before fully rolling\nit out.", "zh": "另一种需要使用多个标签的情况是区分部署的是同一组件的不同版本或不同配置。\n通常的做法是将新应用版本的 **金丝雀**（在 Pod 模板中的镜像标签中指定）与之前发布的版本并排部署，\n这样新发布的版本可以在完全上线前接收实时生产流量。"}
{"en": "For instance, you can use a `track` label to differentiate different releases.\n\nThe primary, stable release would have a `track` label with value as `stable`:", "zh": "例如，你可以使用 `track` 标签来区分不同的版本。\n\n主版本、稳定版本会存在 `track` 标签，值为 `stable`。\n\n```none\nname: frontend\nreplicas: 3\n...\nlabels:\n   app: guestbook\n   tier: frontend\n   track: stable\n...\nimage: gb-frontend:v3\n```"}
{"en": "and then you can create a new release of the guestbook frontend that carries the `track` label\nwith different value (i.e. `canary`), so that two sets of pods would not overlap:", "zh": "然后你可以创建一个 guestbook 前端项目的新版本，该版本使用不同值的 `track` 标签（例如：`canary`），\n这样两组 Pod 就不会重叠。\n\n```none\nname: frontend-canary\nreplicas: 1\n...\nlabels:\n   app: guestbook\n   tier: frontend\n   track: canary\n...\nimage: gb-frontend:v4\n```"}
{"en": "The frontend service would span both sets of replicas by selecting the common subset of their\nlabels (i.e. omitting the `track` label), so that the traffic will be redirected to both\napplications:", "zh": "这个前端服务将通过选择标签的相同子集（例如：忽略 `track` 标签）来覆盖两套副本，\n这样，流量会被转发到两个应用：\n\n```yaml\nselector:\n   app: guestbook\n   tier: frontend\n```"}
{"en": "You can tweak the number of replicas of the stable and canary releases to determine the ratio of\neach release that will receive live production traffic (in this case, 3:1).\nOnce you're confident, you can update the stable track to the new application release and remove\nthe canary one.", "zh": "你可以调整稳定版本和金丝雀版本的副本数量，\n以确定每个版本接收实时生产流量的比例（本例中为 3：1）。\n一旦有把握，你可以更新所有 track 标签为 stable 的应用为新版本并且移除金丝雀标签。"}
{"en": "## Updating annotations", "zh": "## 更新注解 {#updating-annotations}"}
{"en": "Sometimes you would want to attach annotations to resources. Annotations are arbitrary\nnon-identifying metadata for retrieval by API clients such as tools or libraries.\nThis can be done with `kubectl annotate`. For example:", "zh": "有时候你想要为资源附加注解。\n注解是任意的非标识性元数据，供 API 客户端例如工具或库检索。\n这可以通过 `kubectl annotate` 来完成。例如：\n\n```shell\nkubectl annotate pods my-nginx-v4-9gw19 description='my frontend running nginx'\nkubectl get pods my-nginx-v4-9gw19 -o yaml\n```\n\n```yaml\napiVersion: v1\nkind: pod\nmetadata:\n  annotations:\n    description: my frontend running nginx\n...\n```"}
{"en": "For more information, see [annotations](/docs/concepts/overview/working-with-objects/annotations/)\nand [kubectl annotate](/docs/reference/kubectl/generated/kubectl_annotate/).", "zh": "更多信息，参阅[注解](/zh-cn/docs/concepts/overview/working-with-objects/annotations/)\n和 [kubectl annotate](/zh-cn/docs/reference/kubectl/generated/kubectl_annotate/)。"}
{"en": "## Scaling your application", "zh": "## 扩缩应用 {#scaling-your-application}"}
{"en": "When load on your application grows or shrinks, use `kubectl` to scale your application.\nFor instance, to decrease the number of nginx replicas from 3 to 1, do:", "zh": "当应用的负载增长或收缩时，使用 `kubectl` 扩缩你的应用。\n例如，将 Nginx 的副本数量从 3 减少到 1，这样做：\n\n```shell\nkubectl scale deployment/my-nginx --replicas=1\n```\n\n```none\ndeployment.apps/my-nginx scaled\n```"}
{"en": "Now you only have one pod managed by the deployment.", "zh": "现在，你的 Deployment 只管理一个 Pod。\n\n```shell\nkubectl get pods -l app=nginx\n```\n\n```none\nNAME                        READY     STATUS    RESTARTS   AGE\nmy-nginx-2035384211-j5fhi   1/1       Running   0          30m\n```"}
{"en": "To have the system automatically choose the number of nginx replicas as needed,\nranging from 1 to 3, do:", "zh": "为了让系统按需从 1 到 3 自动选择 Nginx 副本数量，这样做："}
{"en": "```shell\n# This requires an existing source of container and Pod metrics\nkubectl autoscale deployment/my-nginx --min=1 --max=3\n```", "zh": "```shell\n# 需要存在容器和 Pod 指标数据源\nkubectl autoscale deployment/my-nginx --min=1 --max=3\n```\n\n```none\nhorizontalpodautoscaler.autoscaling/my-nginx autoscaled\n```"}
{"en": "Now your nginx replicas will be scaled up and down as needed, automatically.", "zh": "现在你的 Nginx 副本数量将会按需自动扩缩。"}
{"en": "For more information, please see [kubectl scale](/docs/reference/kubectl/generated/kubectl_scale/),\n[kubectl autoscale](/docs/reference/kubectl/generated/kubectl_autoscale/) and\n[horizontal pod autoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/) document.", "zh": "更多信息请参阅文档 [kubectl scale](/docs/reference/kubectl/generated/kubectl_scale/)，\n[kubectl autoscale](/docs/reference/kubectl/generated/kubectl_autoscale/) 和\n[Pod 水平自动扩缩](/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale/) 。"}
{"en": "## In-place updates of resources", "zh": "## 就地更新资源 {#in-place-updates-of-resources}"}
{"en": "Sometimes it's necessary to make narrow, non-disruptive updates to resources you've created.", "zh": "有时需要对创建的资源进行小范围、非破坏性的更新。\n\n### kubectl apply"}
{"en": "It is suggested to maintain a set of configuration files in source control\n(see [configuration as code](https://martinfowler.com/bliki/InfrastructureAsCode.html)),\nso that they can be maintained and versioned along with the code for the resources they configure.\nThen, you can use [`kubectl apply`](/docs/reference/kubectl/generated/kubectl_apply/)\nto push your configuration changes to the cluster.", "zh": "建议参照 ([configuration as code](https://martinfowler.com/bliki/InfrastructureAsCode.html))，\n在源码控制系统中维护配置文件集合，\n这样它们就能与所配置资源的代码一起得到维护和版本控制。\n然后，你可以使用 [`kubectl apply`](/docs/reference/kubectl/generated/kubectl_apply/) \n将配置集更新推送到集群中。"}
{"en": "This command will compare the version of the configuration that you're pushing with the previous\nversion and apply the changes you've made, without overwriting any automated changes to properties\nyou haven't specified.", "zh": "这个命令会将你推送的配置的版本和之前的版本进行比较，并应用你所作的更改，\n而不会覆盖任何你没有指定的属性。\n\n```shell\nkubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml\n```\n\n```none\ndeployment.apps/my-nginx configured\n```"}
{"en": "To learn more about the underlying mechanism, read [server-side apply](/docs/reference/using-api/server-side-apply/).", "zh": "要进一步了解底层原理，参阅[服务器端应用](/zh-cn/docs/reference/using-api/server-side-apply/)。\n\n### kubectl edit"}
{"en": "Alternatively, you may also update resources with [`kubectl edit`](/docs/reference/kubectl/generated/kubectl_edit/):", "zh": "或者，你也可以使用 [`kubectl edit`](/docs/reference/kubectl/generated/kubectl_edit/) 来更新资源：\n\n```shell\nkubectl edit deployment/my-nginx\n```"}
{"en": "This is equivalent to first `get` the resource, edit it in text editor, and then `apply` the\nresource with the updated version:", "zh": "等价于先对资源进行 `get` 操作，在文本编辑器中进行编辑，\n然后对更新后的版本进行 `apply` 操作："}
{"en": "```shell\nkubectl get deployment my-nginx -o yaml > /tmp/nginx.yaml\nvi /tmp/nginx.yaml\n# do some edit, and then save the file\n\nkubectl apply -f /tmp/nginx.yaml\ndeployment.apps/my-nginx configured\n\nrm /tmp/nginx.yaml\n```", "zh": "```shell\nkubectl get deployment my-nginx -o yaml > /tmp/nginx.yaml\nvi /tmp/nginx.yaml\n# 编辑，然后保存\n\nkubectl apply -f /tmp/nginx.yaml\ndeployment.apps/my-nginx configured\n\nrm /tmp/nginx.yaml\n```"}
{"en": "This allows you to do more significant changes more easily. Note that you can specify the editor\nwith your `EDITOR` or `KUBE_EDITOR` environment variables.", "zh": "这样，你就可以轻松的进行更重要的修改。\n注意，你可以使用 `EDITOR` 或 `KUBE_EDITOR` 环境变量来指定编辑器。"}
{"en": "For more information, please see [kubectl edit](/docs/reference/kubectl/generated/kubectl_edit/).", "zh": "更多信息参阅 [kubectl edit](/docs/reference/kubectl/generated/kubectl_edit/)。\n\n### kubectl patch"}
{"en": "You can use [`kubectl patch`](/docs/reference/kubectl/generated/kubectl_patch/) to update API objects in place.\nThis subcommand supports JSON patch,\nJSON merge patch, and strategic merge patch.", "zh": "你可以使用 [`kubectl patch`](/docs/reference/kubectl/generated/kubectl_patch/) 来就地更新 API 对象。\n该子命令支持 JSON 补丁、JSON 合并补丁和策略合并补丁。"}
{"en": "See\n[Update API Objects in Place Using kubectl patch](/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/)\nfor more details.", "zh": "参阅[使用 kubectl patch 更新 API 对象](/zh-cn/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/)\n获取更多细节。"}
{"en": "## Disruptive updates", "zh": "## 破坏性更新 {#disruptive-updates}"}
{"en": "In some cases, you may need to update resource fields that cannot be updated once initialized, or\nyou may want to make a recursive change immediately, such as to fix broken pods created by a\nDeployment. To change such fields, use `replace --force`, which deletes and re-creates the\nresource. In this case, you can modify your original configuration file:", "zh": "某些场景下，你可能需要更新那些一旦被初始化就无法被更新的资源字段，\n或者希望立刻进行递归修改，例如修复被 Deployment 创建的异常 Pod。\n要更改此类字段，使用 `replace --force` 来删除并且重新创建资源。\n这种情况下，你可以修改原始配置文件。\n\n```shell\nkubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force\n```\n\n```none\ndeployment.apps/my-nginx deleted\ndeployment.apps/my-nginx replaced\n```\n\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Learn about [how to use `kubectl` for application introspection and debugging](/docs/tasks/debug/debug-application/debug-running-pod/).", "zh": "进一步学习[如何调试运行中的 Pod](/zh-cn/docs/tasks/debug/debug-application/debug-running-pod/)。"}
{"en": "In Kubernetes, you can _scale_ a workload depending on the current demand of resources.\nThis allows your cluster to react to changes in resource demand more elastically and efficiently.\n\nWhen you scale a workload, you can either increase or decrease the number of replicas managed by\nthe workload, or adjust the resources available to the replicas in-place.\n\nThe first approach is referred to as _horizontal scaling_, while the second is referred to as\n_vertical scaling_.\n\nThere are manual and automatic ways to scale your workloads, depending on your use case.", "zh": "在 Kubernetes 中，你可以根据当前的资源需求**扩缩**工作负载。\n这让你的集群可以更灵活、更高效地面对资源需求的变化。\n\n当你扩缩工作负载时，你可以增加或减少工作负载所管理的副本数量，或者就地调整副本的可用资源。\n\n第一种手段称为**水平扩缩**，第二种称为**垂直扩缩**。\n\n扩缩工作负载有手动和自动两种方式，这取决于你的使用情况。"}
{"en": "## Scaling workloads manually", "zh": "## 手动扩缩工作负载   {#scaling-workloads-manually}"}
{"en": "Kubernetes supports _manual scaling_ of workloads. Horizontal scaling can be done\nusing the `kubectl` CLI.\nFor vertical scaling, you need to _patch_ the resource definition of your workload.\n\nSee below for examples of both strategies.", "zh": "Kubernetes 支持工作负载的手动扩缩。水平扩缩可以使用 `kubectl` 命令行工具完成。\n对于垂直扩缩，你需要**更新**工作负载的资源定义。\n\n这两种策略的示例见下文。"}
{"en": "- **Horizontal scaling**: [Running multiple instances of your app](/docs/tutorials/kubernetes-basics/scale/scale-intro/)\n- **Vertical scaling**: [Resizing CPU and memory resources assigned to containers](/docs/tasks/configure-pod-container/resize-container-resources)", "zh": "- **水平扩缩**：[运行应用程序的多个实例](/docs/tutorials/kubernetes-basics/scale/scale-intro/)\n- **垂直扩缩**：[调整分配给容器的 CPU 和内存资源](/docs/tasks/configure-pod-container/resize-container-resources)"}
{"en": "## Scaling workloads automatically", "zh": "## 自动扩缩工作负载   {#scaling-workloads-automatically}"}
{"en": "Kubernetes also supports _automatic scaling_ of workloads, which is the focus of this page.", "zh": "Kubernetes 也支持工作负载的**自动扩缩**，这也是本页的重点。"}
{"en": "The concept of _Autoscaling_ in Kubernetes refers to the ability to automatically update an\nobject that manages a set of Pods (for example a\n{{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}).", "zh": "在 Kubernetes 中**自动扩缩**的概念是指自动更新管理一组 Pod 的能力（例如\n{{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}）。"}
{"en": "### Scaling workloads horizontally", "zh": "### 水平扩缩工作负载   {#scaling-workloads-horizontally}"}
{"en": "In Kubernetes, you can automatically scale a workload horizontally using a _HorizontalPodAutoscaler_ (HPA).", "zh": "在 Kubernetes 中，你可以使用 HorizontalPodAutoscaler (HPA) 实现工作负载的自动水平扩缩。"}
{"en": "It is implemented as a Kubernetes API resource and a {{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}\nand periodically adjusts the number of {{< glossary_tooltip text=\"replicas\" term_id=\"replica\" >}}\nin a workload to match observed resource utilization such as CPU or memory usage.", "zh": "它以 Kubernetes API 资源和{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}的方式实现，\n并定期调整工作负载中{{< glossary_tooltip text=\"副本\" term_id=\"replica\" >}}的数量\n以满足设置的资源利用率，如 CPU 或内存利用率。"}
{"en": "There is a [walkthrough tutorial](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough) of configuring a HorizontalPodAutoscaler for a Deployment.", "zh": "这是一个为 Deployment 部署配置 HorizontalPodAutoscaler 的[示例教程](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough)。"}
{"en": "### Scaling workloads vertically", "zh": "### 垂直扩缩工作负载   {#scaling-workloads-vertically}\n\n{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}"}
{"en": "You can automatically scale a workload vertically using a _VerticalPodAutoscaler_ (VPA).\nUnlike the HPA, the VPA doesn't come with Kubernetes by default, but is a separate project\nthat can be found [on GitHub](https://github.com/kubernetes/autoscaler/tree/9f87b78df0f1d6e142234bb32e8acbd71295585a/vertical-pod-autoscaler).", "zh": "你可以使用 VerticalPodAutoscaler (VPA) 实现工作负载的垂直扩缩。\n不同于 HPA，VPA 并非默认来源于 Kubernetes，而是一个独立的项目，\n参见 [on GitHub](https://github.com/kubernetes/autoscaler/tree/9f87b78df0f1d6e142234bb32e8acbd71295585a/vertical-pod-autoscaler)。"}
{"en": "Once installed, it allows you to create {{< glossary_tooltip text=\"CustomResourceDefinitions\" term_id=\"customresourcedefinition\" >}}\n(CRDs) for your workloads which define _how_ and _when_ to scale the resources of the managed replicas.", "zh": "安装后，你可以为工作负载创建 {{< glossary_tooltip text=\"CustomResourceDefinitions\" term_id=\"customresourcedefinition\" >}}(CRDs)，\n定义**如何**以及**何时**扩缩被管理副本的资源。\n\n{{< note >}}"}
{"en": "You will need to have the [Metrics Server](https://github.com/kubernetes-sigs/metrics-server)\ninstalled to your cluster for the HPA to work.", "zh": "你需要在集群中安装 [Metrics Server](https://github.com/kubernetes-sigs/metrics-server)，这样，你的 HPA 才能正常工作。\n{{< /note >}}"}
{"en": "At the moment, the VPA can operate in four different modes:", "zh": "目前，VPA 可以有四种不同的运行模式："}
{"en": "{{< table caption=\"Different modes of the VPA\" >}}\nMode | Description\n:----|:-----------\n`Auto` | Currently, `Recreate` might change to in-place updates in the future\n`Recreate` | The VPA assigns resource requests on pod creation as well as updates them on existing pods by evicting them when the requested resources differ significantly from the new recommendation\n`Initial` | The VPA only assigns resource requests on pod creation and never changes them later.\n`Off` | The VPA does not automatically change the resource requirements of the pods. The recommendations are calculated and can be inspected in the VPA object.\n{{< /table >}}", "zh": "{{< table caption=\"VPA 的不同模式\" >}}\n模式 | 描述\n:----|:-----------\n`Auto` | 目前是 `Recreate`，将来可能改为就地更新\n`Recreate` | VPA 会在创建 Pod 时分配资源请求，并且当请求的资源与新的建议值区别很大时通过驱逐 Pod 的方式来更新现存的 Pod\n`Initial` | VPA 只有在创建时分配资源请求，之后不做更改\n`Off` | VPA 不会自动更改 Pod 的资源需求，建议值仍会计算并可在 VPA 对象中查看\n{{< /table >}}"}
{"en": "#### Requirements for in-place resizing", "zh": "#### 就地调整的要求\n\n{{< feature-state for_k8s_version=\"v1.27\" state=\"alpha\" >}}"}
{"en": "Resizing a workload in-place **without** restarting the {{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}}\nor its {{< glossary_tooltip text=\"Containers\" term_id=\"container\" >}} requires Kubernetes version 1.27 or later.\nAdditionally, the `InPlaceVerticalScaling` feature gate needs to be enabled.", "zh": "在**不**重启 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 或其中{{< glossary_tooltip text=\"容器\" term_id=\"container\" >}}就地调整工作负载的情况下要求 Kubernetes 版本大于 1.27。\n此外，特性门控 `InPlaceVerticalScaling` 需要开启。\n\n{{< feature-gate-description name=\"InPlacePodVerticalScaling\" >}}"}
{"en": "### Autoscaling based on cluster size", "zh": "### 根据集群规模自动扩缩   {#autoscaling-based-on-cluster-size}"}
{"en": "For workloads that need to be scaled based on the size of the cluster (for example\n`cluster-dns` or other system components), you can use the\n[_Cluster Proportional Autoscaler_](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler).\nJust like the VPA, it is not part of the Kubernetes core, but hosted as its\nown project on GitHub.", "zh": "对于需要根据集群规模实现扩缩的工作负载（例如：`cluster-dns` 或者其他系统组件），\n你可以使用 [Cluster Proportional Autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler)。\n与 VPA 一样，这个项目不是 Kubernetes 核心项目的一部分，它在 GitHub 上有自己的项目。"}
{"en": "The Cluster Proportional Autoscaler watches the number of schedulable {{< glossary_tooltip text=\"nodes\" term_id=\"node\" >}}\nand cores and scales the number of replicas of the target workload accordingly.", "zh": "集群弹性伸缩器 (Cluster Proportional Autoscaler) 会观测可调度 {{< glossary_tooltip text=\"节点\" term_id=\"node\" >}} 和 内核数量，\n并调整目标工作负载的副本数量。"}
{"en": "If the number of replicas should stay the same, you can scale your workloads vertically according to the cluster size using\nthe [_Cluster Proportional Vertical Autoscaler_](https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler).\nThe project is **currently in beta** and can be found on GitHub.", "zh": "如果副本的数量需要保持一致，你可以使用 [Cluster Proportional Vertical Autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler) 来根据集群规模进行垂直扩缩。\n这个项目目前处于 **beta** 阶段，你可以在 GitHub 上找到它。"}
{"en": "While the Cluster Proportional Autoscaler scales the number of replicas of a workload, the Cluster Proportional Vertical Autoscaler\nadjusts the resource requests for a workload (for example a Deployment or DaemonSet) based on the number of nodes and/or cores\nin the cluster.", "zh": "集群弹性伸缩器会扩缩工作负载的副本数量，垂直集群弹性伸缩器 (Cluster Proportional Vertical Autoscaler) 会根据节点和/或核心的数量\n调整工作负载的资源请求（例如 Deployment 和 DaemonSet）。"}
{"en": "### Event driven Autoscaling", "zh": "### 事件驱动型自动扩缩   {#event-driven-autoscaling}"}
{"en": "It is also possible to scale workloads based on events, for example using the\n[_Kubernetes Event Driven Autoscaler_ (**KEDA**)](https://keda.sh/).", "zh": "通过事件驱动实现工作负载的扩缩也是可行的，\n例如使用 [Kubernetes Event Driven Autoscaler (**KEDA**)](https://keda.sh/)。"}
{"en": "KEDA is a CNCF graduated enabling you to scale your workloads based on the number\nof events to be processed, for example the amount of messages in a queue. There exists\na wide range of adapters for different event sources to choose from.", "zh": "KEDA 是 CNCF 的毕业项目，能让你根据要处理事件的数量对工作负载进行扩缩，例如队列中消息的数量。\n有多种针对不同事件源的适配可供选择。"}
{"en": "### Autoscaling based on schedules", "zh": "### 根据计划自动扩缩   {#autoscaling-based-on-schedules}"}
{"en": "Another strategy for scaling your workloads is to **schedule** the scaling operations, for example in order to\nreduce resource consumption during off-peak hours.", "zh": "扩缩工作负载的另一种策略是**计划**进行扩缩，例如在非高峰时段减少资源消耗。"}
{"en": "Similar to event driven autoscaling, such behavior can be achieved using KEDA in conjunction with\nits [`Cron` scaler](https://keda.sh/docs/2.13/scalers/cron/). The `Cron` scaler allows you to define schedules\n(and time zones) for scaling your workloads in or out.", "zh": "与事件驱动型自动扩缩相似，这种行为可以使用 KEDA 和 [`Cron` scaler](https://keda.sh/docs/2.13/scalers/cron/) 实现。\n你可以在计划扩缩器 (Cron scaler) 中定义计划来实现工作负载的横向扩缩。"}
{"en": "## Scaling cluster infrastructure", "zh": "## 扩缩集群基础设施   {#scaling-cluster-infrastructure}"}
{"en": "If scaling workloads isn't enough to meet your needs, you can also scale your cluster infrastructure itself.", "zh": "如果扩缩工作负载无法满足你的需求，你也可以扩缩集群基础设施本身。"}
{"en": "Scaling the cluster infrastructure normally means adding or removing {{< glossary_tooltip text=\"nodes\" term_id=\"node\" >}}.", "zh": "扩缩集群基础设施通常是指增加或移除{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}。"}
{"en": "Read [cluster autoscaling](/docs/concepts/cluster-administration/cluster-autoscaling/)\nfor more information.", "zh": "阅读[集群自动扩缩](/zh-cn/docs/concepts/cluster-administration/cluster-autoscaling/)了解更多信息。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Learn more about scaling horizontally\n  - [Scale a StatefulSet](/docs/tasks/run-application/scale-stateful-set/)\n  - [HorizontalPodAutoscaler Walkthrough](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)\n- [Resize Container Resources In-Place](/docs/tasks/configure-pod-container/resize-container-resources/)\n- [Autoscale the DNS Service in a Cluster](/docs/tasks/administer-cluster/dns-horizontal-autoscaling/)\n- Learn about [cluster autoscaling](/docs/concepts/cluster-administration/cluster-autoscaling/)", "zh": "- 了解有关横向扩缩的更多信息\n  - [扩缩 StatefulSet](/zh-cn/docs/tasks/run-application/scale-stateful-set/)\n  - [HorizontalPodAutoscaler 演练](/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)\n- [调整分配给容器的 CPU 和内存资源](/zh-cn/docs/tasks/configure-pod-container/resize-container-resources/)\n- [自动扩缩集群 DNS 服务](/zh-cn/docs/tasks/administer-cluster/dns-horizontal-autoscaling/)\n- 了解[集群自动扩缩]((/zh-cn/docs/concepts/cluster-administration/cluster-autoscaling/))"}
{"en": "title: \"Workloads\"\nweight: 55\ndescription: >\n  Understand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.\nno_list: true\ncard:\n  title: Workloads and Pods\n  name: concepts\n  weight: 60", "zh": "{{< glossary_definition term_id=\"workload\" length=\"short\" >}}"}
{"en": "Whether your workload is a single component or several that work together, on Kubernetes you run\nit inside a set of [_pods_](/docs/concepts/workloads/pods).\nIn Kubernetes, a Pod represents a set of running\n{{< glossary_tooltip text=\"containers\" term_id=\"container\" >}} on your cluster.", "zh": "在 Kubernetes 中，无论你的负载是由单个组件还是由多个一同工作的组件构成，\n你都可以在一组 [**Pod**](/zh-cn/docs/concepts/workloads/pods) 中运行它。\n在 Kubernetes 中，Pod 代表的是集群上处于运行状态的一组\n{{< glossary_tooltip text=\"容器\" term_id=\"container\" >}}的集合。"}
{"en": "Kubernetes pods have a [defined lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/).\nFor example, once a pod is running in your cluster then a critical fault on the\n{{< glossary_tooltip text=\"node\" term_id=\"node\" >}} where that pod is running means that\nall the pods on that node fail. Kubernetes treats that level of failure as final: you\nwould need to create a new Pod to recover, even if the node later becomes healthy.", "zh": "Kubernetes Pod 遵循[预定义的生命周期](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/)。\n例如，当在你的集群中运行了某个 Pod，但是 Pod 所在的\n{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}} 出现致命错误时，\n所有该节点上的 Pod 的状态都会变成失败。Kubernetes 将这类失败视为最终状态：\n即使该节点后来恢复正常运行，你也需要创建新的 Pod 以恢复应用。"}
{"en": "However, to make life considerably easier, you don't need to manage each `Pod` directly.\nInstead, you can use _workload resources_ that manage a set of pods on your behalf.\nThese resources configure {{< glossary_tooltip term_id=\"controller\" text=\"controllers\" >}}\nthat make sure the right number of the right kind of pod are running, to match the state\nyou specified.\n\nKubernetes provides several built-in workload resources:", "zh": "不过，为了减轻用户的使用负担，通常不需要用户直接管理每个 `Pod`。\n而是使用**负载资源**来替用户管理一组 Pod。\n这些负载资源通过配置 {{< glossary_tooltip term_id=\"controller\" text=\"控制器\" >}}\n来确保正确类型的、处于运行状态的 Pod 个数是正确的，与用户所指定的状态相一致。\n\nKubernetes 提供若干种内置的工作负载资源："}
{"en": "* [Deployment](/docs/concepts/workloads/controllers/deployment/) and [`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/)\n  (replacing the legacy resource\n  {{< glossary_tooltip text=\"ReplicationController\" term_id=\"replication-controller\" >}}).\n  Deployment is a good fit for managing a stateless application workload on your cluster,\n  where any Pod in the Deployment is interchangeable and can be replaced if needed.\n* [StatefulSet](/docs/concepts/workloads/controllers/statefulset/) lets you\n  run one or more related Pods that do track state somehow. For example, if your workload\n  records data persistently, you can run a StatefulSet that matches each Pod with a\n  [PersistentVolume](/docs/concepts/storage/persistent-volumes/). Your code, running in the\n  Pods for that StatefulSet, can replicate data to other Pods in the same StatefulSet\n  to improve overall resilience.", "zh": "* [Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/) 和\n  [ReplicaSet](/zh-cn/docs/concepts/workloads/controllers/replicaset/)\n  （替换原来的资源 {{< glossary_tooltip text=\"ReplicationController\" term_id=\"replication-controller\" >}}）。\n  Deployment 很适合用来管理你的集群上的无状态应用，Deployment 中的所有\n  Pod 都是相互等价的，并且在需要的时候被替换。\n* [StatefulSet](/zh-cn/docs/concepts/workloads/controllers/statefulset/)\n  让你能够运行一个或者多个以某种方式跟踪应用状态的 Pod。\n  例如，如果你的负载会将数据作持久存储，你可以运行一个 StatefulSet，将每个\n  Pod 与某个 [PersistentVolume](/zh-cn/docs/concepts/storage/persistent-volumes/)\n  对应起来。你在 StatefulSet 中各个 Pod 内运行的代码可以将数据复制到同一\n  StatefulSet 中的其它 Pod 中以提高整体的服务可靠性。"}
{"en": "* [DaemonSet](/docs/concepts/workloads/controllers/daemonset/) defines Pods that provide\n  node-local facilities. These might be fundamental to the operation of your cluster, such\n  as a networking helper tool, or be part of an\n  {{< glossary_tooltip text=\"add-on\" term_id=\"addons\" >}}.\n  Every time you add a node to your cluster that matches the specification in a DaemonSet,\n  the control plane schedules a Pod for that DaemonSet onto the new node.\n* [Job](/docs/concepts/workloads/controllers/job/) and\n  [CronJob](/docs/concepts/workloads/controllers/cron-jobs/)\n  define tasks that run to completion and then stop.\n  You can use a [Job](/docs/concepts/workloads/controllers/job/) to\n  define a task that runs to completion, just once. You can use a\n  [CronJob](/docs/concepts/workloads/controllers/cron-jobs/) to run\n  the same Job multiple times according a schedule.", "zh": "* [DaemonSet](/zh-cn/docs/concepts/workloads/controllers/daemonset/)\n  定义提供节点本地支撑设施的 Pod。这些 Pod 可能对于你的集群的运维是\n  非常重要的，例如作为网络链接的辅助工具或者作为网络\n  {{< glossary_tooltip text=\"插件\" term_id=\"addons\" >}}\n  的一部分等等。每次你向集群中添加一个新节点时，如果该节点与某 `DaemonSet`\n  的规约匹配，则控制平面会为该 DaemonSet 调度一个 Pod 到该新节点上运行。\n* [Job](/zh-cn/docs/concepts/workloads/controllers/job/) 和\n  [CronJob](/zh-cn/docs/concepts/workloads/controllers/cron-jobs/)。\n  定义一些一直运行到结束并停止的任务。\n  你可以使用 [Job](/zh-cn/docs/concepts/workloads/controllers/job/)\n  来定义只需要执行一次并且执行后即视为完成的任务。你可以使用\n  [CronJob](/zh-cn/docs/concepts/workloads/controllers/cron-jobs/)\n  来根据某个排期表来多次运行同一个 Job。"}
{"en": "In the wider Kubernetes ecosystem, you can find third-party workload resources that provide\nadditional behaviors. Using a\n[custom resource definition](/docs/concepts/extend-kubernetes/api-extension/custom-resources/),\nyou can add in a third-party workload resource if you want a specific behavior that's not part\nof Kubernetes' core. For example, if you wanted to run a group of Pods for your application but\nstop work unless _all_ the Pods are available (perhaps for some high-throughput distributed task),\nthen you can implement or install an extension that does provide that feature.", "zh": "在庞大的 Kubernetes 生态系统中，你还可以找到一些提供额外操作的第三方工作负载相关的资源。\n通过使用[定制资源定义（CRD）](/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources/)，\n你可以添加第三方工作负载资源，以完成原本不是 Kubernetes 核心功能的工作。\n例如，如果你希望运行一组 Pod，但要求**所有** Pod 都可用时才执行操作\n（比如针对某种高吞吐量的分布式任务），你可以基于定制资源实现一个能够满足这一需求的扩展，\n并将其安装到集群中运行。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "As well as reading about each resource, you can learn about specific tasks that relate to them:\n\n* [Run a stateless application using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/)\n* Run a stateful application either as a [single instance](/docs/tasks/run-application/run-single-instance-stateful-application/)\n  or as a [replicated set](/docs/tasks/run-application/run-replicated-stateful-application/)\n* [Run automated tasks with a CronJob](/docs/tasks/job/automated-tasks-with-cron-jobs/)", "zh": "除了阅读了解每类资源外，你还可以了解与这些资源相关的任务：\n\n* [使用 Deployment 运行一个无状态的应用](/zh-cn/docs/tasks/run-application/run-stateless-application-deployment/)\n* 以[单实例](/zh-cn/docs/tasks/run-application/run-single-instance-stateful-application/)或者[多副本集合](/zh-cn/docs/tasks/run-application/run-replicated-stateful-application/)\n  的形式运行有状态的应用；\n* [使用 CronJob 运行自动化的任务](/zh-cn/docs/tasks/job/automated-tasks-with-cron-jobs/)"}
{"en": "To learn about Kubernetes' mechanisms for separating code from configuration,\nvisit [Configuration](/docs/concepts/configuration/).", "zh": "要了解 Kubernetes 将代码与配置分离的实现机制，可参阅[配置](/zh-cn/docs/concepts/configuration/)节。"}
{"en": "There are two supporting concepts that provide backgrounds about how Kubernetes manages pods\nfor applications:\n* [Garbage collection](/docs/concepts/architecture/garbage-collection/) tidies up objects\n  from your cluster after their _owning resource_ has been removed.\n* The [_time-to-live after finished_ controller](/docs/concepts/workloads/controllers/ttlafterfinished/)\n  removes Jobs once a defined time has passed since they completed.", "zh": "关于 Kubernetes 如何为应用管理 Pod，还有两个支撑概念能够提供相关背景信息：\n\n* [垃圾收集](/zh-cn/docs/concepts/architecture/garbage-collection/)机制负责在\n  对象的**属主资源**被删除时在集群中清理这些对象。\n* [**Time-to-Live** 控制器](/zh-cn/docs/concepts/workloads/controllers/ttlafterfinished/)会在 Job\n  结束之后的指定时间间隔之后删除它们。"}
{"en": "Once your application is running, you might want to make it available on the internet as\na [Service](/docs/concepts/services-networking/service/) or, for web application only,\nusing an [Ingress](/docs/concepts/services-networking/ingress).", "zh": "一旦你的应用处于运行状态，你就可能想要以\n[Service](/zh-cn/docs/concepts/services-networking/service/)\n的形式使之可在互联网上访问；或者对于 Web 应用而言，使用\n[Ingress](/zh-cn/docs/concepts/services-networking/ingress) 资源将其暴露到互联网上。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.30\" state=\"beta\" >}}"}
{"en": "This page explains how user namespaces are used in Kubernetes pods. A user\nnamespace isolates the user running inside the container from the one\nin the host.\n\nA process running as root in a container can run as a different (non-root) user\nin the host; in other words, the process has full privileges for operations\ninside the user namespace, but is unprivileged for operations outside the\nnamespace.", "zh": "本页解释了在 Kubernetes Pod 中如何使用用户命名空间。\n用户命名空间将容器内运行的用户与主机中的用户隔离开来。\n\n在容器中以 root 身份运行的进程可以在主机中以不同的（非 root）用户身份运行；\n换句话说，该进程在用户命名空间内的操作具有完全的权限，\n但在命名空间外的操作是无特权的。"}
{"en": "You can use this feature to reduce the damage a compromised container can do to\nthe host or other pods in the same node. There are [several security\nvulnerabilities][KEP-vulns] rated either **HIGH** or **CRITICAL** that were not\nexploitable when user namespaces is active. It is expected user namespace will\nmitigate some future vulnerabilities too.", "zh": "你可以使用这个功能来减少被破坏的容器对主机或同一节点中的其他 Pod 的破坏。\n有[几个安全漏洞][KEP-vulns]被评为 **高** 或 **重要**，\n当用户命名空间处于激活状态时，这些漏洞是无法被利用的。\n预计用户命名空间也会减轻一些未来的漏洞。\n\n[KEP-vulns]: https://github.com/kubernetes/enhancements/tree/217d790720c5aef09b8bd4d6ca96284a0affe6c2/keps/sig-node/127-user-namespaces#motivation"}
{"en": "body", "zh": "## {{% heading \"prerequisites\" %}}\n\n{{% thirdparty-content %}}"}
{"en": "This is a Linux-only feature and support is needed in Linux for idmap mounts on\nthe filesystems used. This means:\n\n* On the node, the filesystem you use for `/var/lib/kubelet/pods/`, or the\n  custom directory you configure for this, needs idmap mount support.\n* All the filesystems used in the pod's volumes must support idmap mounts.\n\nIn practice this means you need at least Linux 6.3, as tmpfs started supporting\nidmap mounts in that version. This is usually needed as several Kubernetes\nfeatures use tmpfs (the service account token that is mounted by default uses a\ntmpfs, Secrets use a tmpfs, etc.)\n\nSome popular filesystems that support idmap mounts in Linux 6.3 are: btrfs,\next4, xfs, fat, tmpfs, overlayfs.", "zh": "这是一个只对 Linux 有效的功能特性，且需要 Linux 支持在所用文件系统上挂载 idmap。\n这意味着：\n\n* 在节点上，你用于 `/var/lib/kubelet/pods/` 的文件系统，或你为此配置的自定义目录，\n  需要支持 idmap 挂载。\n* Pod 卷中使用的所有文件系统都必须支持 idmap 挂载。\n\n在实践中，这意味着你最低需要 Linux 6.3，因为 tmpfs 在该版本中开始支持 idmap 挂载。\n这通常是需要的，因为有几个 Kubernetes 功能特性使用 tmpfs\n（默认情况下挂载的服务账号令牌使用 tmpfs、Secret 使用 tmpfs 等等）。\n\nLinux 6.3 中支持 idmap 挂载的一些比较流行的文件系统是：btrfs、ext4、xfs、fat、\ntmpfs、overlayfs。"}
{"en": "In addition, the container runtime and its underlying OCI runtime must support\nuser namespaces. The following OCI runtimes offer support:\n\n* [crun](https://github.com/containers/crun) version 1.9 or greater (it's recommend version 1.13+).", "zh": "此外，容器运行时及其底层 OCI 运行时必须支持用户命名空间。以下 OCI 运行时提供支持：\n\n* [crun](https://github.com/containers/crun) 1.9 或更高版本（推荐 1.13+ 版本）。"}
{"en": "ideally, update this if a newer minor release of runc comes out, whether or not it includes the idmap support", "zh": "{{< note >}}"}
{"en": "Many OCI runtimes do not include the support needed for using user namespaces in\nLinux pods. If you use a managed Kubernetes, or have downloaded it from packages\nand set it up, it's likely that nodes in your cluster use a runtime that doesn't\ninclude this support. For example, the most widely used OCI runtime is `runc`,\nand version `1.1.z` of runc doesn't support all the features needed by the\nKubernetes implementation of user namespaces.", "zh": "许多 OCI 运行时不包含在 Linux Pod 中使用用户命名空间所需的支持。\n如果你使用托管 Kubernetes，或者使用软件包下载并安装 Kubernetes 集群，\n则集群中的节点可能使用不包含支持此特性的运行时。\n例如，最广泛使用的 OCI 运行时是 `runc`，而 runc 的 `1.1.z`\n版本不支持 Kubernetes 实现用户命名空间所需的所有特性。"}
{"en": "If there is a newer release of runc than 1.1 available for use, check its\ndocumentation and release notes for compatibility (look for idmap mounts support\nin particular, because that is the missing feature).", "zh": "如果有比 1.1 更新的 runc 版本可供使用，请检查其文档和发行说明以了解兼容性\n（特别寻找 idmap 挂载支持，因为这一特性是缺失的）。\n{{< /note >}}"}
{"en": "To use user namespaces with Kubernetes, you also need to use a CRI\n {{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}\n to use this feature with Kubernetes pods:\n\n* CRI-O: version 1.25 (and later) supports user namespaces for containers.", "zh": "此外，需要在{{< glossary_tooltip text=\"容器运行时\" term_id=\"container-runtime\" >}}提供支持，\n才能在 Kubernetes Pod 中使用这一功能：\n\n* CRI-O：1.25（及更高）版本支持配置容器的用户命名空间。"}
{"en": "containerd v1.7 is not compatible with the userns support in Kubernetes v1.27 to v{{< skew latestVersion >}}.\nKubernetes v1.25 and v1.26 used an earlier implementation that **is** compatible with containerd v1.7,\nin terms of userns support.\nIf you are using a version of Kubernetes other than {{< skew currentVersion >}},\ncheck the documentation for that version of Kubernetes for the most relevant information.\nIf there is a newer release of containerd than v1.7 available for use, also check the containerd\ndocumentation for compatibility information.\n\nYou can see the status of user namespaces support in cri-dockerd tracked in an [issue][CRI-dockerd-issue]\non GitHub.", "zh": "containerd v1.7 与 Kubernetes v1.27 至 v{{< skew currentVersion >}}\n版本中的用户命名空间不兼容。\nKubernetes v1.25 和 v1.26 使用了早期的实现，在用户命名空间方面与 containerd v1.7 兼容。\n如果你使用的 Kubernetes 版本不是 {{< skew currentVersion >}}，请查看该版本 Kubernetes\n的文档以获取更准确的信息。\n如果有比 v1.7 更新的 containerd 版本可供使用，请检查 containerd 文档以获取兼容性信息。\n\n你可以在 GitHub 上的 [issue][CRI-dockerd-issue] 中查看 cri-dockerd\n中用户命名空间支持的状态。"}
{"en": "## Introduction", "zh": "## 介绍 {#introduction}"}
{"en": "User namespaces is a Linux feature that allows to map users in the container to\ndifferent users in the host. Furthermore, the capabilities granted to a pod in\na user namespace are valid only in the namespace and void outside of it.\n\nA pod can opt-in to use user namespaces by setting the `pod.spec.hostUsers` field\nto `false`.", "zh": "用户命名空间是一个 Linux 功能，允许将容器中的用户映射到主机中的不同用户。\n此外，在某用户命名空间中授予 Pod 的权能只在该命名空间中有效，在该命名空间之外无效。\n\n一个 Pod 可以通过将 `pod.spec.hostUsers` 字段设置为 `false` 来选择使用用户命名空间。"}
{"en": "The kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way\nto guarantee that no two pods on the same node use the same mapping.\n\nThe `runAsUser`, `runAsGroup`, `fsGroup`, etc. fields in the `pod.spec` always\nrefer to the user inside the container.\n\nThe valid UIDs/GIDs when this feature is enabled is the range 0-65535. This\napplies to files and processes (`runAsUser`, `runAsGroup`, etc.).", "zh": "kubelet 将挑选 Pod 所映射的主机 UID/GID，\n并以此保证同一节点上没有两个 Pod 使用相同的方式进行映射。\n\n`pod.spec` 中的 `runAsUser`、`runAsGroup`、`fsGroup` 等字段总是指的是容器内的用户。\n启用该功能时，有效的 UID/GID 在 0-65535 范围内。这以限制适用于文件和进程（`runAsUser`、`runAsGroup` 等）。"}
{"en": "Files using a UID/GID outside this range will be seen as belonging to the\noverflow ID, usually 65534 (configured in `/proc/sys/kernel/overflowuid` and\n`/proc/sys/kernel/overflowgid`). However, it is not possible to modify those\nfiles, even by running as the 65534 user/group.\n\nMost applications that need to run as root but don't access other host\nnamespaces or resources, should continue to run fine without any changes needed\nif user namespaces is activated.", "zh": "使用这个范围之外的 UID/GID 的文件将被视为属于溢出 ID，\n通常是 65534（配置在 `/proc/sys/kernel/overflowuid和/proc/sys/kernel/overflowgid`）。\n然而，即使以 65534 用户/组的身份运行，也不可能修改这些文件。\n\n大多数需要以 root 身份运行但不访问其他主机命名空间或资源的应用程序，\n在用户命名空间被启用时，应该可以继续正常运行，不需要做任何改变。"}
{"en": "## Understanding user namespaces for pods {#pods-and-userns}", "zh": "## 了解 Pod 的用户命名空间 {#pods-and-userns}"}
{"en": "Several container runtimes with their default configuration (like Docker Engine,\ncontainerd, CRI-O) use Linux namespaces for isolation. Other technologies exist\nand can be used with those runtimes too (e.g. Kata Containers uses VMs instead of\nLinux namespaces). This page is applicable for container runtimes using Linux\nnamespaces for isolation.\n\nWhen creating a pod, by default, several new namespaces are used for isolation:\na network namespace to isolate the network of the container, a PID namespace to\nisolate the view of processes, etc. If a user namespace is used, this will\nisolate the users in the container from the users in the node.", "zh": "一些容器运行时的默认配置（如 Docker Engine、containerd、CRI-O）使用 Linux 命名空间进行隔离。\n其他技术也存在，也可以与这些运行时（例如，Kata Containers 使用虚拟机而不是 Linux 命名空间）结合使用。\n本页适用于使用 Linux 命名空间进行隔离的容器运行时。\n\n在创建 Pod 时，默认情况下会使用几个新的命名空间进行隔离：\n一个网络命名空间来隔离容器网络，一个 PID 命名空间来隔离进程视图等等。\n如果使用了一个用户命名空间，这将把容器中的用户与节点中的用户隔离开来。"}
{"en": "This means containers can run as root and be mapped to a non-root user on the\nhost. Inside the container the process will think it is running as root (and\ntherefore tools like `apt`, `yum`, etc. work fine), while in reality the process\ndoesn't have privileges on the host. You can verify this, for example, if you\ncheck which user the container process is running by executing `ps aux` from\nthe host. The user `ps` shows is not the same as the user you see if you\nexecute inside the container the command `id`.\n\nThis abstraction limits what can happen, for example, if the container manages\nto escape to the host. Given that the container is running as a non-privileged\nuser on the host, it is limited what it can do to the host.", "zh": "这意味着容器可以以 root 身份运行，并将该身份映射到主机上的一个非 root 用户。\n在容器内，进程会认为它是以 root 身份运行的（因此像 `apt`、`yum` 等工具可以正常工作），\n而实际上该进程在主机上没有权限。\n你可以验证这一点，例如，如果你从主机上执行 `ps aux` 来检查容器进程是以哪个用户运行的。\n`ps` 显示的用户与你在容器内执行 `id` 命令时看到的用户是不一样的。\n\n这种抽象限制了可能发生的情况，例如，容器设法逃逸到主机上时的后果。\n鉴于容器是作为主机上的一个非特权用户运行的，它能对主机做的事情是有限的。"}
{"en": "Furthermore, as users on each pod will be mapped to different non-overlapping\nusers in the host, it is limited what they can do to other pods too.\n\nCapabilities granted to a pod are also limited to the pod user namespace and\nmostly invalid out of it, some are even completely void. Here are two examples:\n- `CAP_SYS_MODULE` does not have any effect if granted to a pod using user\nnamespaces, the pod isn't able to load kernel modules.\n- `CAP_SYS_ADMIN` is limited to the pod's user namespace and invalid outside\nof it.", "zh": "此外，由于每个 Pod 上的用户将被映射到主机中不同的非重叠用户，\n他们对其他 Pod 可以执行的操作也是有限的。\n\n授予一个 Pod 的权能也被限制在 Pod 的用户命名空间内，\n并且在这一命名空间之外大多无效，有些甚至完全无效。这里有两个例子：\n\n- `CAP_SYS_MODULE` 若被授予一个使用用户命名空间的 Pod 则没有任何效果，这个 Pod 不能加载内核模块。\n- `CAP_SYS_ADMIN` 只限于 Pod 所在的用户命名空间，在该命名空间之外无效。"}
{"en": "Without using a user namespace a container running as root, in the case of a\ncontainer breakout, has root privileges on the node. And if some capability were\ngranted to the container, the capabilities are valid on the host too. None of\nthis is true when we use user namespaces.\n\nIf you want to know more details about what changes when user namespaces are in\nuse, see `man 7 user_namespaces`.", "zh": "在不使用用户命名空间的情况下，以 root 账号运行的容器，在容器逃逸时，在节点上有 root 权限。\n而且如果某些权能被授予了某容器，这些权能在宿主机上也是有效的。\n当我们使用用户命名空间时，这些都不再成立。\n\n如果你想知道关于使用用户命名空间时的更多变化细节，请参见 `man 7 user_namespaces`。"}
{"en": "## Set up a node to support user namespaces", "zh": "## 设置一个节点以支持用户命名空间 {#set-up-a-node-to-support-user-namespaces}"}
{"en": "By default, the kubelet assigns pods UIDs/GIDs above the range 0-65535, based on\nthe assumption that the host's files and processes use UIDs/GIDs within this\nrange, which is standard for most Linux distributions. This approach prevents\nany overlap between the UIDs/GIDs of the host and those of the pods.", "zh": "默认情况下，kubelet 会分配 0-65535 范围以上的 Pod UID/GID，\n这是基于主机的文件和进程使用此范围内的 UID/GID 的假设，也是大多数 Linux 发行版的标准。\n此方法可防止主机的 UID/GID 与 Pod 的 UID/GID 之间出现重叠。"}
{"en": "Avoiding the overlap is important to mitigate the impact of vulnerabilities such\nas [CVE-2021-25741][CVE-2021-25741], where a pod can potentially read arbitrary\nfiles in the host. If the UIDs/GIDs of the pod and the host don't overlap, it is\nlimited what a pod would be able to do: the pod UID/GID won't match the host's\nfile owner/group.", "zh": "避免重叠对于减轻 [CVE-2021-25741][CVE-2021-25741] 等漏洞的影响非常重要，\n其中 Pod 可能会读取主机中的任意文件。\n如果 Pod 和主机的 UID/GID 不重叠，则 Pod 的功能将受到限制：\nPod UID/GID 将与主机的文件所有者/组不匹配。"}
{"en": "The kubelet can use a custom range for user IDs and group IDs for pods. To\nconfigure a custom range, the node needs to have:\n\n * A user `kubelet` in the system (you cannot use any other username here)\n * The binary `getsubids` installed (part of [shadow-utils][shadow-utils]) and\n   in the `PATH` for the kubelet binary.\n * A configuration of subordinate UIDs/GIDs for the `kubelet` user (see\n   [`man 5 subuid`](https://man7.org/linux/man-pages/man5/subuid.5.html) and\n   [`man 5 subgid`](https://man7.org/linux/man-pages/man5/subgid.5.html)).", "zh": "kubelet 可以对 Pod 的用户 ID 和组 ID 使用自定义范围。要配置自定义范围，节点需要具有：\n* 系统中的用户 `kubelet`（此处不能使用任何其他用户名）。\n* 已安装二进制文件 `getsubids`（[shadow-utils][shadow-utils] 的一部分）并位于 kubelet 二进制文件的 `PATH` 中。\n* `kubelet` 用户的从属 UID/GID 配置\n  （请参阅 [`man 5 subuid`](https://man7.org/linux/man-pages/man5/subuid.5.html) 和\n  [`man 5 subgid`](https://man7.org/linux/man-pages/man5/subgid.5.html)）"}
{"en": "This setting only gathers the UID/GID range configuration and does not change\nthe user executing the `kubelet`.\n\nYou must follow some constraints for the subordinate ID range that you assign\nto the `kubelet` user:", "zh": "此设置仅收集 UID/GID 范围配置，不会更改执行 `kubelet` 的用户。\n\n对于分配给 `kubelet` 用户的从属 ID 范围， 你必须遵循一些限制："}
{"en": "* The subordinate user ID, that starts the UID range for Pods, **must** be a\n  multiple of 65536 and must also be greater than or equal to 65536. In other\n  words, you cannot use any ID from the range 0-65535 for Pods; the kubelet\n  imposes this restriction to make it difficult to create an accidentally insecure\n  configuration.", "zh": "* 启动 Pod 的 UID 范围的从属用户 ID **必须**是 65536 的倍数，并且还必须大于或等于 65536。\n  换句话说，Pod 不能使用 0-65535 范围内的任何 ID；kubelet 施加此限制是为了使创建意外不安全的配置变得困难。"}
{"en": "* The subordinate ID count must be a multiple of 65536\n\n* The subordinate ID count must be at least `65536 x <maxPods>` where `<maxPods>`\n  is the maximum number of pods that can run on the node.\n\n* You must assign the same range for both user IDs and for group IDs, It doesn't\n  matter if other users have user ID ranges that don't align with the group ID\n  ranges.", "zh": "* 从属 ID 计数必须是 65536 的倍数\n\n* 从属 ID 计数必须至少为 `65536 x <maxPods>`，其中 `<maxPods>` 是节点上可以运行的最大 Pod 数量。\n\n* 你必须为用户 ID 和组 ID 分配相同的范围。如果其他用户的用户 ID 范围与组 ID 范围不一致也没关系。"}
{"en": "* None of the assigned ranges should overlap with any other assignment.\n\n* The subordinate configuration must be only one line. In other words, you can't\n  have multiple ranges.\n\nFor example, you could define `/etc/subuid` and `/etc/subgid` to both have\nthese entries for the `kubelet` user:", "zh": "* 所分配的范围不得与任何其他分配重叠。\n\n* 从属配置必须只有一行。换句话说，你不能有多个范围。\n\n例如，你可以定义 `/etc/subuid` 和 `/etc/subgid` 来为 `kubelet` 用户定义以下条目："}
{"en": "```\n# The format is\n#   name:firstID:count of IDs\n# where\n# - firstID is 65536 (the minimum value possible)\n# - count of IDs is 110 (default limit for number of) * 65536\n```", "zh": "```\n# 格式为：\n#   name:firstID:count of IDs\n# 在哪里：\n# - firstID 是 65536 （可能的最小值）\n# - IDs 的数量是 110（默认数量限制）* 65536\nkubelet:65536:7208960\n```\n\n[CVE-2021-25741]: https://github.com/kubernetes/kubernetes/issues/104980\n[shadow-utils]: https://github.com/shadow-maint/shadow"}
{"en": "## Integration with Pod security admission checks", "zh": "## 与 Pod 安全准入检查的集成   {#integration-with-pod-security-admission-checks}\n\n{{< feature-state state=\"alpha\" for_k8s_version=\"v1.29\" >}}"}
{"en": "For Linux Pods that enable user namespaces, Kubernetes relaxes the application of\n[Pod Security Standards](/docs/concepts/security/pod-security-standards) in a controlled way.\nThis behavior can be controlled by the [feature\ngate](/docs/reference/command-line-tools-reference/feature-gates/)\n`UserNamespacesPodSecurityStandards`, which allows an early opt-in for end\nusers. Admins have to ensure that user namespaces are enabled by all nodes\nwithin the cluster if using the feature gate.", "zh": "对于启用了用户命名空间的 Linux Pod，Kubernetes 会以受控方式放宽\n[Pod 安全性标准](/zh-cn/docs/concepts/security/pod-security-standards)的应用。\n这种行为可以通过[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/) \n`UserNamespacesPodSecurityStandards` 进行控制，可以让最终用户提前尝试此特性。\n如果管理员启用此特性门控，必须确保群集中的所有节点都启用了用户命名空间。"}
{"en": "If you enable the associated feature gate and create a Pod that uses user\nnamespaces, the following fields won't be constrained even in contexts that enforce the\n_Baseline_ or _Restricted_ pod security standard. This behavior does not\npresent a security concern because `root` inside a Pod with user namespaces\nactually refers to the user inside the container, that is never mapped to a\nprivileged user on the host. Here's the list of fields that are **not** checks for Pods in those\ncircumstances:", "zh": "如果你启用相关特性门控并创建了使用用户命名空间的 Pod，以下的字段不会被限制，\n即使在执行了 _Baseline_ 或 _Restricted_ Pod 安全性标准的上下文中。这种行为不会带来安全问题，\n因为带有用户命名空间的 Pod 内的 `root` 实际上指的是容器内的用户，绝不会映射到主机上的特权用户。\n以下是在这种情况下**不进行**检查的 Pod 字段列表：\n\n- `spec.securityContext.runAsNonRoot`\n- `spec.containers[*].securityContext.runAsNonRoot`\n- `spec.initContainers[*].securityContext.runAsNonRoot`\n- `spec.ephemeralContainers[*].securityContext.runAsNonRoot`\n- `spec.securityContext.runAsUser`\n- `spec.containers[*].securityContext.runAsUser`\n- `spec.initContainers[*].securityContext.runAsUser`"}
{"en": "## Limitations", "zh": "## 限制 {#limitations}"}
{"en": "When using a user namespace for the pod, it is disallowed to use other host\nnamespaces. In particular, if you set `hostUsers: false` then you are not\nallowed to set any of:\n\n * `hostNetwork: true`\n * `hostIPC: true`\n * `hostPID: true`", "zh": "当 Pod 使用用户命名空间时，不允许 Pod 使用其他主机命名空间。\n特别是，如果你设置了 `hostUsers: false`，那么你就不可以设置如下属性：\n\n* `hostNetwork: true`\n* `hostIPC: true`\n* `hostPID: true`\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Take a look at [Use a User Namespace With a Pod](/docs/tasks/configure-pod-container/user-namespaces/)", "zh": "* 查阅[为 Pod 配置用户命名空间](/zh-cn/docs/tasks/configure-pod-container/user-namespaces/)"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}"}
{"en": "Sidecar containers are the secondary containers that run along with the main\napplication container within the same {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}.\nThese containers are used to enhance or to extend the functionality of the primary _app\ncontainer_ by providing additional services, or functionality such as logging, monitoring,\nsecurity, or data synchronization, without directly altering the primary application code.", "zh": "边车容器是与**主应用容器**在同一个 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 中运行的辅助容器。\n这些容器通过提供额外的服务或功能（如日志记录、监控、安全性或数据同步）来增强或扩展主应用容器的功能，\n而无需直接修改主应用代码。"}
{"en": "Typically, you only have one app container in a Pod. For example, if you have a web\napplication that requires a local webserver, the local webserver is a sidecar and the\nweb application itself is the app container.", "zh": "通常，一个 Pod 中只有一个应用容器。\n例如，如果你有一个需要本地 Web 服务器的 Web 应用，\n则本地 Web 服务器以边车容器形式运行，而 Web 应用本身以应用容器形式运行。"}
{"en": "## Sidecar containers in Kubernetes {#pod-sidecar-containers}\n\nKubernetes implements sidecar containers as a special case of\n[init containers](/docs/concepts/workloads/pods/init-containers/); sidecar containers remain\nrunning after Pod startup. This document uses the term _regular init containers_ to clearly\nrefer to containers that only run during Pod startup.", "zh": "## Kubernetes 中的边车容器   {#pod-sidecar-containers}\n\nKubernetes 将边车容器作为\n[Init 容器](/zh-cn/docs/concepts/workloads/pods/init-containers/)的一个特例来实现，\nPod 启动后，边车容器仍保持运行状态。\n本文档使用术语\"常规 Init 容器\"来明确指代仅在 Pod 启动期间运行的容器。"}
{"en": "Provided that your cluster has the `SidecarContainers`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) enabled\n(the feature is active by default since Kubernetes v1.29), you can specify a `restartPolicy`\nfor containers listed in a Pod's `initContainers` field.\nThese restartable _sidecar_ containers are independent from other init containers and from\nthe main application container(s) within the same pod.\nThese can be started, stopped, or restarted without effecting the main application container\nand other init containers.", "zh": "如果你的集群启用了 `SidecarContainers`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)\n（该特性自 Kubernetes v1.29 起默认启用），你可以为 Pod 的 `initContainers`\n字段中列出的容器指定 `restartPolicy`。\n这些可重新启动的**边车（Sidecar）** 容器独立于其他 Init 容器以及同一 Pod 内的主应用容器，\n这些容器可以启动、停止和重新启动，而不会影响主应用容器和其他 Init 容器。"}
{"en": "You can also run a Pod with multiple containers that are not marked as init or sidecar\ncontainers. This is appropriate if the containers within the Pod are required for the\nPod to work overall, but you don't need to control which containers start or stop first.\nYou could also do this if you need to support older versions of Kubernetes that don't\nsupport a container-level `restartPolicy` field.", "zh": "你还可以运行包含多个未标记为 Init 或边车容器的 Pod。\n如果作为一个整体而言，某个 Pod 中的所有容器都要运行，但你不需要控制哪些容器先启动或停止，那么这种设置是合适的。\n如果你使用的是不支持容器级 `restartPolicy` 字段的旧版本 Kubernetes，你也可以这样做。"}
{"en": "### Example application {#sidecar-example}\n\nHere's an example of a Deployment with two containers, one of which is a sidecar:", "zh": "### 应用示例   {#sidecar-example}\n\n下面是一个包含两个容器的 Deployment 示例，其中一个容器是边车形式：\n\n{{% code_sample language=\"yaml\" file=\"application/deployment-sidecar.yaml\" %}}"}
{"en": "## Sidecar containers and Pod lifecycle\n\nIf an init container is created with its `restartPolicy` set to `Always`, it will\nstart and remain running during the entire life of the Pod. This can be helpful for\nrunning supporting services separated from the main application containers.", "zh": "## 边车容器和 Pod 生命周期   {#sidecar-containers-and-pod-lifecyle}\n\n如果创建 Init 容器时将 `restartPolicy` 设置为 `Always`，\n则它将在整个 Pod 的生命周期内启动并持续运行。这对于运行与主应用容器分离的支持服务非常有帮助。"}
{"en": "If a `readinessProbe` is specified for this init container, its result will be used\nto determine the `ready` state of the Pod.\n\nSince these containers are defined as init containers, they benefit from the same\nordering and sequential guarantees as other init containers, allowing them to mix\nsidecar containers with regular init containers for complex Pod initialization flows.", "zh": "如果为此 Init 容器指定了 `readinessProbe`，其结果将用于确定 Pod 的 `ready` 状态。\n\n由于这些容器被定义为 Init 容器，所以它们享有与其他 Init 容器相同的顺序和按序执行保证，\n从而允许将边车容器与常规 Init 容器混合使用，支持复杂的 Pod 初始化流程。"}
{"en": "Compared to regular init containers, sidecars defined within `initContainers` continue to\nrun after they have started. This is important when there is more than one entry inside\n`.spec.initContainers` for a Pod. After a sidecar-style init container is running (the kubelet\nhas set the `started` status for that init container to true), the kubelet then starts the\nnext init container from the ordered `.spec.initContainers` list.\nThat status either becomes true because there is a process running in the\ncontainer and no startup probe defined, or as a result of its `startupProbe` succeeding.", "zh": "与常规 Init 容器相比，在 `initContainers` 中定义的边车容器在启动后继续运行。\n当 Pod 的 `.spec.initContainers` 中有多个条目时，这一点非常重要。\n在边车风格的 Init 容器运行后（kubelet 将该 Init 容器的 `started` 状态设置为 true），\nkubelet 启动 `.spec.initContainers` 这一有序列表中的下一个 Init 容器。\n该状态要么因为容器中有一个正在运行的进程且没有定义启动探针而变为 true，\n要么是其 `startupProbe` 成功而返回的结果。"}
{"en": "Upon Pod [termination](/docs/concepts/workloads/pods/pod-lifecycle/#termination-with-sidecars),\nthe kubelet postpones terminating sidecar containers until the main application container has fully stopped.\nThe sidecar containers are then shut down in the opposite order of their appearance in the Pod specification.\nThis approach ensures that the sidecars remain operational, supporting other containers within the Pod,\nuntil their service is no longer required.", "zh": "在 Pod [终止](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#termination-with-sidecars)时，\nkubelet 会推迟终止边车容器，直到主应用容器已完全停止。边车容器随后将按照它们在 Pod 规约中出现的相反顺序被关闭。\n这种方法确保了在不再需要边车服务之前这些边车继续发挥作用，以支持 Pod 内的其他容器。"}
{"en": "### Jobs with sidecar containers", "zh": "### 带边车容器的 Job {#jobs-with-sidecar-containers}"}
{"en": "If you define a Job that uses sidecar using Kubernetes-style init containers,\nthe sidecar container in each Pod does not prevent the Job from completing after the\nmain container has finished.\n\nHere's an example of a Job with two containers, one of which is a sidecar:", "zh": "如果你定义 Job 时使用基于 Kubernetes 风格 Init 容器的边车容器，\n各个 Pod 中的边车容器不会阻止 Job 在主容器结束后进入完成状态。\n\n以下是一个具有两个容器的 Job 示例，其中一个是边车：\n\n{{% code_sample language=\"yaml\" file=\"application/job/job-sidecar.yaml\" %}}"}
{"en": "## Differences from application containers\n\nSidecar containers run alongside _app containers_ in the same pod. However, they do not\nexecute the primary application logic; instead, they provide supporting functionality to\nthe main application.", "zh": "## 与应用容器的区别   {#differences-from-application-containers}\n\n边车容器与同一 Pod 中的**应用容器**并行运行。不过边车容器不执行主应用逻辑，而是为主应用提供支持功能。"}
{"en": "Sidecar containers have their own independent lifecycles. They can be started, stopped,\nand restarted independently of app containers. This means you can update, scale, or\nmaintain sidecar containers without affecting the primary application.\n\nSidecar containers share the same network and storage namespaces with the primary\ncontainer. This co-location allows them to interact closely and share resources.", "zh": "边车容器具有独立的生命周期。它们可以独立于应用容器启动、停止和重启。\n这意味着你可以更新、扩展或维护边车容器，而不影响主应用。\n\n边车容器与主容器共享相同的网络和存储命名空间。这种共存使它们能够紧密交互并共享资源。"}
{"en": "## Differences from init containers\n\nSidecar containers work alongside the main container, extending its functionality and\nproviding additional services.", "zh": "## 与 Init 容器的区别   {#differences-from-init-containers}\n\n边车容器与主容器并行工作，扩展其功能并提供附加服务。"}
{"en": "Sidecar containers run concurrently with the main application container. They are active\nthroughout the lifecycle of the pod and can be started and stopped independently of the\nmain container. Unlike [init containers](/docs/concepts/workloads/pods/init-containers/),\nsidecar containers support [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.", "zh": "边车容器与主应用容器同时运行。它们在整个 Pod 的生命周期中都处于活动状态，并且可以独立于主容器启动和停止。\n与 [Init 容器](/zh-cn/docs/concepts/workloads/pods/init-containers/)不同，\n边车容器支持[探针](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe)来控制其生命周期。"}
{"en": "Sidecar containers can interact directly with the main application containers, because\nlike init containers they always share the same network, and can optionally also share\nvolumes (filesystems).\n\nInit containers stop before the main containers start up, so init containers cannot\nexchange messages with the app container in a Pod. Any data passing is one-way\n(for example, an init container can put information inside an `emptyDir` volume).\n\n## Resource sharing within containers", "zh": "边车容器可以直接与主应用容器交互，因为与 Init 容器一样，\n它们总是与应用容器共享相同的网络，并且还可以选择共享卷（文件系统）。\n\nInit 容器在主容器启动之前停止，因此 Init 容器无法与 Pod 中的应用容器交换消息。\n所有数据传递都是单向的（例如，Init 容器可以将信息放入 `emptyDir` 卷中）。\n\n## 容器内的资源共享   {#resource-sharing-within-containers}\n\n{{< comment >}}"}
{"en": "This section is also present in the [init containers](/docs/concepts/workloads/pods/init-containers/) page.\nIf you're editing this section, change both places.", "zh": "这部分内容也出现在 [Init 容器](/zh-cn/docs/concepts/workloads/pods/init-containers/)页面上。\n如果你正在编辑这部分内容，请同时修改两处。\n{{< /comment >}}"}
{"en": "Given the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:", "zh": "假如执行顺序为 Init 容器、边车容器和应用容器，则关于资源用量适用以下规则："}
{"en": "* The highest of any particular resource request or limit defined on all init\n  containers is the *effective init request/limit*. If any resource has no\n  resource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the sum of\n[pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/) and the higher of:\n  * the sum of all non-init containers(app and sidecar containers) request/limit for a\n  resource\n  * the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\n  init containers can reserve resources for initialization that are not used\n  during the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\n  QoS tier for all init, sidecar and app containers alike.", "zh": "* 所有 Init 容器上定义的任何特定资源的 limit 或 request 的最大值，作为\n  Pod **有效初始 request/limit**。\n  如果任何资源没有指定资源限制，则被视为最高限制。\n* Pod 对资源的 **有效 limit/request** 是如下两者中的较大者：\n  * 所有应用容器对某个资源的 limit/request 之和\n  * Init 容器中对某个资源的有效 limit/request\n* 系统基于有效的 limit/request 完成调度，这意味着 Init 容器能够为初始化过程预留资源，\n  而这些资源在 Pod 的生命周期中不再被使用。\n* Pod 的 **有效 QoS 级别**，对于 Init 容器和应用容器而言是相同的。"}
{"en": "Quota and limits are applied based on the effective Pod request and\nlimit.", "zh": "配额和限制适用于 Pod 的有效请求和限制值。"}
{"en": "### Sidecar containers and Linux cgroups {#cgroups}\n\nOn Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.", "zh": "### 边车容器和 Linux Cgroup   {#cgroups}\n\n在 Linux 上，Pod Cgroup 的资源分配基于 Pod 级别的有效资源请求和限制，这一点与调度器相同。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn how to [Adopt Sidecar Containers](/docs/tutorials/configuration/pod-sidecar-containers/)\n* Read a blog post on [native sidecar containers](/blog/2023/08/25/native-sidecar-containers/).\n* Read about [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* Learn about the [types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* Learn about [pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/).", "zh": "* 了解如何[采用边车容器](/zh-cn/docs/tutorials/configuration/pod-sidecar-containers/)。\n* 阅读关于[原生边车容器](/zh-cn/blog/2023/08/25/native-sidecar-containers/)的博文。\n* 阅读[如何创建具有 Init 容器的 Pod](/zh-cn/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container)。\n* 了解[探针类型](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe)：\n  存活态探针、就绪态探针、启动探针。\n* 了解 [Pod 开销](/zh-cn/docs/concepts/scheduling-eviction/pod-overhead/)。"}
{"en": "It is sometimes useful for a container to have information about itself, without\nbeing overly coupled to Kubernetes. The _downward API_ allows containers to consume\ninformation about themselves or the cluster without using the Kubernetes client\nor API server.", "zh": "对于容器来说，在不与 Kubernetes 过度耦合的情况下，拥有关于自身的信息有时是很有用的。\n**Downward API** 允许容器在不使用 Kubernetes 客户端或 API 服务器的情况下获得自己或集群的信息。"}
{"en": "An example is an existing application that assumes a particular well-known\nenvironment variable holds a unique identifier. One possibility is to wrap the\napplication, but that is tedious and error-prone, and it violates the goal of low\ncoupling. A better option would be to use the Pod's name as an identifier, and\ninject the Pod's name into the well-known environment variable.", "zh": "例如，现有应用程序假设某特定的周知的环境变量是存在的，其中包含唯一标识符。\n一种方法是对应用程序进行封装，但这很繁琐且容易出错，并且违背了低耦合的目标。\n更好的选择是使用 Pod 名称作为标识符，并将 Pod 名称注入到周知的环境变量中。"}
{"en": "In Kubernetes, there are two ways to expose Pod and container fields to a running container:\n\n* as [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* as [files in a `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)", "zh": "在 Kubernetes 中，有两种方法可以将 Pod 和容器字段暴露给运行中的容器：\n\n* 作为[环境变量](/zh-cn/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* 作为 [`downwardAPI` 卷中的文件](/zh-cn/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)"}
{"en": "Together, these two ways of exposing Pod and container fields are called the\n_downward API_.", "zh": "这两种暴露 Pod 和容器字段的方式统称为 **Downward API**。"}
{"en": "## Available fields\n\nOnly some Kubernetes API fields are available through the downward API. This\nsection lists which fields you can make available.", "zh": "## 可用字段  {#available-fields}\n\n只有部分 Kubernetes API 字段可以通过 Downward API 使用。本节列出了你可以使用的字段。"}
{"en": "You can pass information from available Pod-level fields using `fieldRef`.\nAt the API level, the `spec` for a Pod always defines at least one\n[Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container).\nYou can pass information from available Container-level fields using\n`resourceFieldRef`.", "zh": "你可以使用 `fieldRef` 传递来自可用的 Pod 级字段的信息。在 API 层面，一个 Pod 的\n`spec` 总是定义了至少一个 [Container](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container)。\n你可以使用 `resourceFieldRef` 传递来自可用的 Container 级字段的信息。"}
{"en": "### Information available via `fieldRef` {#downwardapi-fieldRef}\n\nFor some Pod-level fields, you can provide them to a container either as\nan environment variable or using a `downwardAPI` volume. The fields available\nvia either mechanism are:", "zh": "### 可通过 `fieldRef` 获得的信息  {#downwardapi-fieldRef}\n\n对于某些 Pod 级别的字段，你可以将它们作为环境变量或使用 `downwardAPI` 卷提供给容器。\n通过这两种机制可用的字段有："}
{"en": "`metadata.name`\n: the pod's name", "zh": "`metadata.name`\n: Pod 的名称"}
{"en": "`metadata.namespace`\n: the pod's {{< glossary_tooltip text=\"namespace\" term_id=\"namespace\" >}}", "zh": "`metadata.namespace`\n: Pod 的{{< glossary_tooltip text=\"命名空间\" term_id=\"namespace\" >}}"}
{"en": "`metadata.uid`\n: the pod's unique ID", "zh": "`metadata.uid`\n: Pod 的唯一 ID"}
{"en": "`metadata.annotations['<KEY>']`\n: the value of the pod's {{< glossary_tooltip text=\"annotation\" term_id=\"annotation\" >}} named `<KEY>` (for example, `metadata.annotations['myannotation']`)", "zh": "`metadata.annotations['<KEY>']`\n: Pod 的{{< glossary_tooltip text=\"注解\" term_id=\"annotation\" >}} `<KEY>` 的值（例如：`metadata.annotations['myannotation']`）"}
{"en": "`metadata.labels['<KEY>']`\n: the text value of the pod's {{< glossary_tooltip text=\"label\" term_id=\"label\" >}} named `<KEY>` (for example, `metadata.labels['mylabel']`)", "zh": "`metadata.labels['<KEY>']`\n: Pod 的{{< glossary_tooltip text=\"标签\" term_id=\"label\" >}} `<KEY>` 的值（例如：`metadata.labels['mylabel']`）"}
{"en": "The following information is available through environment variables\n**but not as a downwardAPI volume fieldRef**:", "zh": "以下信息可以通过环境变量获得，但**不能作为 `downwardAPI` 卷 `fieldRef`** 获得："}
{"en": "`spec.serviceAccountName`\n: the name of the pod's {{< glossary_tooltip text=\"service account\" term_id=\"service-account\" >}}", "zh": "`spec.serviceAccountName`\n: Pod 的{{< glossary_tooltip text=\"服务账号\" term_id=\"service-account\" >}}名称"}
{"en": "`spec.nodeName`\n: the name of the {{< glossary_tooltip term_id=\"node\" text=\"node\">}} where the Pod is executing", "zh": "`spec.nodeName`\n: Pod 运行时所处的{{< glossary_tooltip term_id=\"node\" text=\"节点\">}}名称"}
{"en": "`status.hostIP`\n: the primary IP address of the node to which the Pod is assigned", "zh": "`status.hostIP`\n: Pod 所在节点的主 IP 地址"}
{"en": "`status.hostIPs`\n: the IP addresses is a dual-stack version of `status.hostIP`, the first is always the same as `status.hostIP`.", "zh": "`status.hostIPs`\n: 这组 IP 地址是 `status.hostIP` 的双协议栈版本，第一个 IP 始终与 `status.hostIP` 相同。"}
{"en": "`status.podIP`\n: the pod's primary IP address (usually, its IPv4 address)", "zh": "`status.podIP`\n: Pod 的主 IP 地址（通常是其 IPv4 地址）"}
{"en": "`status.podIPs`\n: the IP addresses is a dual-stack version of `status.podIP`, the first is always the same as `status.podIP`", "zh": "`status.podIPs`\n: 这组 IP 地址是 `status.podIP` 的双协议栈版本, 第一个 IP 始终与 `status.podIP` 相同。"}
{"en": "The following information is available through a `downwardAPI` volume \n`fieldRef`, **but not as environment variables**:", "zh": "以下信息可以通过 `downwardAPI` 卷 `fieldRef` 获得，但**不能作为环境变量**获得："}
{"en": "`metadata.labels`\n: all of the pod's labels, formatted as `label-key=\"escaped-label-value\"` with one label per line", "zh": "`metadata.labels`\n: Pod 的所有标签，格式为 `标签键名=\"转义后的标签值\"`，每行一个标签"}
{"en": "`metadata.annotations`\n: all of the pod's annotations, formatted as `annotation-key=\"escaped-annotation-value\"` with one annotation per line", "zh": "`metadata.annotations`\n: Pod 的全部注解，格式为 `注解键名=\"转义后的注解值\"`，每行一个注解"}
{"en": "### Information available via `resourceFieldRef` {#downwardapi-resourceFieldRef}\n\nThese container-level fields allow you to provide information about\n[requests and limits](/docs/concepts/configuration/manage-resources-containers/#requests-and-limits)\nfor resources such as CPU and memory.", "zh": "### 可通过 `resourceFieldRef` 获得的信息  {#downwardapi-resourceFieldRef}"}
{"en": "`resource: limits.cpu`\n: A container's CPU limit", "zh": "`resource: limits.cpu`\n: 容器的 CPU 限制值"}
{"en": "`resource: requests.cpu`\n: A container's CPU request", "zh": "`resource: requests.cpu`\n: 容器的 CPU 请求值"}
{"en": "`resource: limits.memory`\n: A container's memory limit", "zh": "`resource: limits.memory`\n: 容器的内存限制值"}
{"en": "`resource: requests.memory`\n: A container's memory request", "zh": "`resource: requests.memory`\n: 容器的内存请求值"}
{"en": "`resource: limits.hugepages-*`\n: A container's hugepages limit", "zh": "`resource: limits.hugepages-*`\n: 容器的巨页限制值"}
{"en": "`resource: requests.hugepages-*`\n: A container's hugepages request", "zh": "`resource: requests.hugepages-*`\n: 容器的巨页请求值"}
{"en": "`resource: limits.ephemeral-storage`\n: A container's ephemeral-storage limit", "zh": "`resource: limits.ephemeral-storage`\n: 容器的临时存储的限制值"}
{"en": "`resource: requests.ephemeral-storage`\n: A container's ephemeral-storage request", "zh": "`resource: requests.ephemeral-storage`\n: 容器的临时存储的请求值"}
{"en": "#### Fallback information for resource limits\n\nIf CPU and memory limits are not specified for a container, and you use the\ndownward API to try to expose that information, then the\nkubelet defaults to exposing the maximum allocatable value for CPU and memory\nbased on the [node allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\ncalculation.", "zh": "#### 资源限制的后备信息  {#fallback-information-for-resource-limits}\n\n如果没有为容器指定 CPU 和内存限制时尝试使用 Downward API 暴露该信息，那么 kubelet 默认会根据\n[节点可分配资源](/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\n计算并暴露 CPU 和内存的最大可分配值。\n\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "You can read about [`downwardAPI` volumes](/docs/concepts/storage/volumes/#downwardapi).\n\nYou can try using the downward API to expose container- or Pod-level information:\n* as [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* as [files in `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)", "zh": "你可以阅读有关 [`downwardAPI` 卷](/zh-cn/docs/concepts/storage/volumes/#downwardapi)的内容。\n\n你可以尝试使用 Downward API 暴露容器或 Pod 级别的信息：\n* 作为[环境变量](/zh-cn/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* 作为 [`downwardAPI` 卷中的文件](/zh-cn/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)"}
{"en": "This page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting\nin the `Pending` [phase](#pod-phase), moving through `Running` if at least one\nof its primary containers starts OK, and then through either the `Succeeded` or\n`Failed` phases depending on whether any container in the Pod terminated in failure.", "zh": "本页面讲述 Pod 的生命周期。\nPod 遵循预定义的生命周期，起始于 `Pending` [阶段](#pod-phase)，\n如果至少其中有一个主要容器正常启动，则进入 `Running`，之后取决于 Pod\n中是否有容器以失败状态结束而进入 `Succeeded` 或者 `Failed` 阶段。"}
{"en": "Like individual application containers, Pods are considered to be relatively\nephemeral (rather than durable) entities. Pods are created, assigned a unique\nID ([UID](/docs/concepts/overview/working-with-objects/names/#uids)), and scheduled\nto run on nodes where they remain until termination (according to restart policy) or\ndeletion.\nIf a {{< glossary_tooltip term_id=\"node\" >}} dies, the Pods running on (or scheduled\nto run on) that node are [marked for deletion](#pod-garbage-collection). The control\nplane marks the Pods for removal after a timeout period.", "zh": "和一个个独立的应用容器一样，Pod 也被认为是相对临时性（而不是长期存在）的实体。\nPod 会被创建、赋予一个唯一的\nID（[UID](/zh-cn/docs/concepts/overview/working-with-objects/names/#uids)），\n并被调度到节点，并在终止（根据重启策略）或删除之前一直运行在该节点。\n如果一个{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}死掉了，调度到该节点的\nPod 也被计划在给定超时期限结束后[删除](#pod-garbage-collection)。"}
{"en": "## Pod lifetime\n\nWhilst a Pod is running, the kubelet is able to restart containers to handle some\nkind of faults. Within a Pod, Kubernetes tracks different container\n[states](#container-states) and determines what action to take to make the Pod\nhealthy again.", "zh": "## Pod 生命期   {#pod-lifetime}\n\n在 Pod 运行期间，`kubelet` 能够重启容器以处理一些失效场景。\n在 Pod 内部，Kubernetes 跟踪不同容器的[状态](#container-states)并确定使\nPod 重新变得健康所需要采取的动作。"}
{"en": "In the Kubernetes API, Pods have both a specification and an actual status. The\nstatus for a Pod object consists of a set of [Pod conditions](#pod-conditions).\nYou can also inject [custom readiness information](#pod-readiness-gate) into the\ncondition data for a Pod, if that is useful to your application.", "zh": "在 Kubernetes API 中，Pod 包含规约部分和实际状态部分。\nPod 对象的状态包含了一组 [Pod 状况（Conditions）](#pod-conditions)。\n如果应用需要的话，你也可以向其中注入[自定义的就绪态信息](#pod-readiness-gate)。"}
{"en": "Pods are only [scheduled](/docs/concepts/scheduling-eviction/) once in their lifetime;\nassigning a Pod to a specific node is called _binding_, and the process of selecting\nwhich node to use is called _scheduling_.\nOnce a Pod has been scheduled and is bound to a node, Kubernetes tries\nto run that Pod on the node. The Pod runs on that node until it stops, or until the Pod\nis [terminated](#pod-termination); if Kubernetes isn't able start the Pod on the selected\nnode (for example, if the node crashes before the Pod starts), then that particular Pod\nnever starts.", "zh": "Pod 在其生命周期中只会被[调度](/zh-cn/docs/concepts/scheduling-eviction/)一次。\n将 Pod 分配到特定节点的过程称为**绑定**，而选择使用哪个节点的过程称为**调度**。\n一旦 Pod 被调度并绑定到某个节点，Kubernetes 会尝试在该节点上运行 Pod。\nPod 会在该节点上运行，直到 Pod 停止或者被[终止](#pod-termination)；\n如果 Kubernetes 无法在选定的节点上启动 Pod（例如，如果节点在 Pod 启动前崩溃），\n那么特定的 Pod 将永远不会启动。"}
{"en": "You can use [Pod Scheduling Readiness](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/)\nto delay scheduling for a Pod until all its _scheduling gates_ are removed. For example,\nyou might want to define a set of Pods but only trigger scheduling once all the Pods\nhave been created.", "zh": "你可以使用 [Pod 调度就绪态](/zh-cn/docs/concepts/scheduling-eviction/pod-scheduling-readiness/)来延迟\nPod 的调度，直到所有的**调度门控**都被移除。\n例如，你可能想要定义一组 Pod，但只有在所有 Pod 都被创建完成后才会触发调度。"}
{"en": "### Pods and fault recovery {#pod-fault-recovery}\n\nIf one of the containers in the Pod fails, then Kubernetes may try to restart that\nspecific container.\nRead [How Pods handle problems with containers](#container-restarts) to learn more.", "zh": "### Pod 和故障恢复   {#pod-fault-recovery}\n\n如果 Pod 中的某个容器失败，Kubernetes 可能会尝试重启特定的容器。\n有关细节参阅 [Pod 如何处理容器问题](#container-restarts)。"}
{"en": "Pods can however fail in a way that the cluster cannot recover from, and in that case\nKubernetes does not attempt to heal the Pod further; instead, Kubernetes deletes the\nPod and relies on other components to provide automatic healing.\n\nIf a Pod is scheduled to a {{< glossary_tooltip text=\"node\" term_id=\"node\" >}} and that\nnode then fails, the Pod is treated as unhealthy and Kubernetes eventually deletes the Pod.\nA Pod won't survive an {{< glossary_tooltip text=\"eviction\" term_id=\"eviction\" >}} due to\na lack of resources or Node maintenance.", "zh": "然而，Pod 也可能以集群无法恢复的方式失败，在这种情况下，Kubernetes 不会进一步尝试修复 Pod；\n相反，Kubernetes 会删除 Pod 并依赖其他组件提供自动修复。\n\n如果 Pod 被调度到某个{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}而该节点之后失效，\nPod 会被视为不健康，最终 Kubernetes 会删除 Pod。\nPod 无法在因节点资源耗尽或者节点维护而被{{< glossary_tooltip text=\"驱逐\" term_id=\"eviction\" >}}期间继续存活。"}
{"en": "Kubernetes uses a higher-level abstraction, called a\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}}, that handles the work of\nmanaging the relatively disposable Pod instances.", "zh": "Kubernetes 使用一种高级抽象来管理这些相对而言可随时丢弃的 Pod 实例，\n称作{{< glossary_tooltip term_id=\"controller\" text=\"控制器\" >}}。"}
{"en": "A given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead,\nthat Pod can be replaced by a new, near-identical Pod. If you make a replacement Pod, it can\neven have same name (as in `.metadata.name`) that the old Pod had, but the replacement\nwould have a different `.metadata.uid` from the old Pod.\n\nKubernetes does not guarantee that a replacement for an existing Pod would be scheduled to\nthe same node as the old Pod that was being replaced.", "zh": "任何给定的 Pod （由 UID 定义）从不会被“重新调度（rescheduled）”到不同的节点；\n相反，这一 Pod 可以被一个新的、几乎完全相同的 Pod 替换掉。\n如果你创建一个替换 Pod，它甚至可以拥有与旧 Pod 相同的名称（如 `.metadata.name`），\n但替换 Pod 将具有与旧 Pod 不同的 `.metadata.uid`。\n\nKubernetes 不保证现有 Pod 的替换 Pod 会被调度到与被替换的旧 Pod 相同的节点。"}
{"en": "### Associated lifetimes\n\nWhen something is said to have the same lifetime as a Pod, such as a\n{{< glossary_tooltip term_id=\"volume\" text=\"volume\" >}},\nthat means that the thing exists as long as that specific Pod (with that exact UID)\nexists. If that Pod is deleted for any reason, and even if an identical replacement\nis created, the related thing (a volume, in this example) is also destroyed and\ncreated anew.", "zh": "### 关联的生命期    {#associated-lifetimes}\n\n如果某物声称其生命期与某 Pod 相同，例如存储{{< glossary_tooltip term_id=\"volume\" text=\"卷\" >}}，\n这就意味着该对象在此 Pod （UID 亦相同）存在期间也一直存在。\n如果 Pod 因为任何原因被删除，甚至某完全相同的替代 Pod 被创建时，\n这个相关的对象（例如这里的卷）也会被删除并重建。"}
{"en": "{{< figure src=\"/images/docs/pod.svg\" title=\"Figure 1.\" class=\"diagram-medium\" caption=\"A multi-container Pod that contains a file puller [sidecar](/docs/concepts/workloads/pods/sidecar-containers/) and a web server. The Pod uses an [ephemeral `emptyDir` volume](/docs/concepts/storage/volumes/#emptydir) for shared storage between the containers.\" >}}", "zh": "{{< figure src=\"/images/docs/pod.svg\" title=\"图 1\" class=\"diagram-medium\" caption=\"一个包含文件拉取程序 [Sidecar（边车）](/zh-cn/docs/concepts/workloads/pods/sidecar-containers/) 和 Web 服务器的多容器 Pod。此 Pod 使用[临时 `emptyDir` 卷](/zh-cn/docs/concepts/storage/volumes/#emptydir)作为容器之间的共享存储。\" >}}"}
{"en": "## Pod phase\n\nA Pod's `status` field is a\n[PodStatus](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podstatus-v1-core)\nobject, which has a `phase` field.\n\nThe phase of a Pod is a simple, high-level summary of where the Pod is in its\nlifecycle. The phase is not intended to be a comprehensive rollup of observations\nof container or Pod state, nor is it intended to be a comprehensive state machine.", "zh": "## Pod 阶段     {#pod-phase}\n\nPod 的 `status` 字段是一个\n[PodStatus](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podstatus-v1-core)\n对象，其中包含一个 `phase` 字段。\n\nPod 的阶段（Phase）是 Pod 在其生命周期中所处位置的简单宏观概述。\n该阶段并不是对容器或 Pod 状态的综合汇总，也不是为了成为完整的状态机。"}
{"en": "The number and meanings of Pod phase values are tightly guarded.\nOther than what is documented here, nothing should be assumed about Pods that\nhave a given `phase` value.\n\nHere are the possible values for `phase`:", "zh": "Pod 阶段的数量和含义是严格定义的。\n除了本文档中列举的内容外，不应该再假定 Pod 有其他的 `phase` 值。\n\n下面是 `phase` 可能的值："}
{"en": "Value       | Description\n:-----------|:-----------\n`Pending`   | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.\n`Running`   | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.\n`Succeeded` | All containers in the Pod have terminated in success, and will not be restarted.\n`Failed`    | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting.\n`Unknown`   | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.", "zh": "取值 | 描述\n:-----|:-----------\n`Pending`（悬决）| Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间。\n`Running`（运行中） | Pod 已经绑定到了某个节点，Pod 中所有的容器都已被创建。至少有一个容器仍在运行，或者正处于启动或重启状态。\n`Succeeded`（成功） | Pod 中的所有容器都已成功终止，并且不会再重启。\n`Failed`（失败） | Pod 中的所有容器都已终止，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止，且未被设置为自动重启。\n`Unknown`（未知） | 因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。\n\n{{< note >}}"}
{"en": "When a pod is failing to start repeatedly, `CrashLoopBackOff` may appear in the `Status` field of some kubectl commands. Similarly, when a pod is being deleted, `Terminating` may appear in the `Status` field of some kubectl commands. \n\nMake sure not to confuse _Status_, a kubectl display field for user intuition, with the pod's `phase`.\nPod phase is an explicit part of the Kubernetes data model and of the\n[Pod API](/docs/reference/kubernetes-api/workload-resources/pod-v1/).", "zh": "当 Pod 反复启动失败时，某些 kubectl 命令的 `Status` 字段中可能会出现 `CrashLoopBackOff`。\n同样，当 Pod 被删除时，某些 kubectl 命令的 `Status` 字段中可能会出现 `Terminating`。\n\n确保不要将 **Status**（kubectl 用于用户直觉的显示字段）与 Pod 的 `phase` 混淆。\nPod 阶段（phase）是 Kubernetes 数据模型和\n[Pod API](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/)\n的一个明确的部分。\n\n```console\nNAMESPACE               NAME               READY   STATUS             RESTARTS   AGE\nalessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h\n```\n\n---"}
{"en": "A Pod is granted a term to terminate gracefully, which defaults to 30 seconds.\nYou can use the flag `--force` to [terminate a Pod by force](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced).", "zh": "Pod 被赋予一个可以体面终止的期限，默认为 30 秒。\n你可以使用 `--force` 参数来[强制终止 Pod](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced)。\n{{< /note >}}"}
{"en": "Since Kubernetes 1.27, the kubelet transitions deleted Pods, except for\n[static Pods](/docs/tasks/configure-pod-container/static-pod/) and\n[force-deleted Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced)\nwithout a finalizer, to a terminal phase (`Failed` or `Succeeded` depending on\nthe exit statuses of the pod containers) before their deletion from the API server.", "zh": "从 Kubernetes 1.27 开始，除了[静态 Pod](/zh-cn/docs/tasks/configure-pod-container/static-pod/)\n和没有 Finalizer 的[强制终止 Pod](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced)\n之外，`kubelet` 会将已删除的 Pod 转换到终止阶段\n（`Failed` 或 `Succeeded` 具体取决于 Pod 容器的退出状态），然后再从 API 服务器中删除。"}
{"en": "If a node dies or is disconnected from the rest of the cluster, Kubernetes\napplies a policy for setting the `phase` of all Pods on the lost node to Failed.", "zh": "如果某节点死掉或者与集群中其他节点失联，Kubernetes\n会实施一种策略，将失去的节点上运行的所有 Pod 的 `phase` 设置为 `Failed`。"}
{"en": "## Container states\n\nAs well as the [phase](#pod-phase) of the Pod overall, Kubernetes tracks the state of\neach container inside a Pod. You can use\n[container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/) to\ntrigger events to run at certain points in a container's lifecycle.\n\nOnce the {{< glossary_tooltip text=\"scheduler\" term_id=\"kube-scheduler\" >}}\nassigns a Pod to a Node, the kubelet starts creating containers for that Pod\nusing a {{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}.\nThere are three possible container states: `Waiting`, `Running`, and `Terminated`.", "zh": "## 容器状态  {#container-states}\n\nKubernetes 会跟踪 Pod 中每个容器的状态，就像它跟踪 Pod 总体上的[阶段](#pod-phase)一样。\n你可以使用[容器生命周期回调](/zh-cn/docs/concepts/containers/container-lifecycle-hooks/)\n来在容器生命周期中的特定时间点触发事件。\n\n一旦{{< glossary_tooltip text=\"调度器\" term_id=\"kube-scheduler\" >}}将 Pod\n分派给某个节点，`kubelet`\n就通过{{< glossary_tooltip text=\"容器运行时\" term_id=\"container-runtime\" >}}开始为\nPod 创建容器。容器的状态有三种：`Waiting`（等待）、`Running`（运行中）和\n`Terminated`（已终止）。"}
{"en": "To check the state of a Pod's containers, you can use\n`kubectl describe pod <name-of-pod>`. The output shows the state for each container\nwithin that Pod.\n\nEach state has a specific meaning:", "zh": "要检查 Pod 中容器的状态，你可以使用 `kubectl describe pod <pod 名称>`。\n其输出中包含 Pod 中每个容器的状态。\n\n每种状态都有特定的含义："}
{"en": "### `Waiting` {#container-state-waiting}\n\nIf a container is not in either the `Running` or `Terminated` state, it is `Waiting`.\nA container in the `Waiting` state is still running the operations it requires in\norder to complete start up: for example, pulling the container image from a container\nimage registry, or applying {{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}\ndata.\nWhen you use `kubectl` to query a Pod with a container that is `Waiting`, you also see\na Reason field to summarize why the container is in that state.", "zh": "### `Waiting` （等待）  {#container-state-waiting}\n\n如果容器并不处在 `Running` 或 `Terminated` 状态之一，它就处在 `Waiting` 状态。\n处于 `Waiting` 状态的容器仍在运行它完成启动所需要的操作：例如，\n从某个容器镜像仓库拉取容器镜像，或者向容器应用 {{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}\n数据等等。\n当你使用 `kubectl` 来查询包含 `Waiting` 状态的容器的 Pod 时，你也会看到一个\nReason 字段，其中给出了容器处于等待状态的原因。"}
{"en": "### `Running` {#container-state-running}\n\nThe `Running` status indicates that a container is executing without issues. If there\nwas a `postStart` hook configured, it has already executed and finished. When you use\n`kubectl` to query a Pod with a container that is `Running`, you also see information\nabout when the container entered the `Running` state.", "zh": "### `Running`（运行中）     {#container-state-running}\n\n`Running` 状态表明容器正在执行状态并且没有问题发生。\n如果配置了 `postStart` 回调，那么该回调已经执行且已完成。\n如果你使用 `kubectl` 来查询包含 `Running` 状态的容器的 Pod 时，\n你也会看到关于容器进入 `Running` 状态的信息。"}
{"en": "### `Terminated` {#container-state-terminated}\n\nA container in the `Terminated` state began execution and then either ran to\ncompletion or failed for some reason. When you use `kubectl` to query a Pod with\na container that is `Terminated`, you see a reason, an exit code, and the start and\nfinish time for that container's period of execution.\n\nIf a container has a `preStop` hook configured, this hook runs before the container enters\nthe `Terminated` state.", "zh": "### `Terminated`（已终止）   {#container-state-terminated}\n\n处于 `Terminated` 状态的容器开始执行后，或者运行至正常结束或者因为某些原因失败。\n如果你使用 `kubectl` 来查询包含 `Terminated` 状态的容器的 Pod 时，\n你会看到容器进入此状态的原因、退出代码以及容器执行期间的起止时间。\n\n如果容器配置了 `preStop` 回调，则该回调会在容器进入 `Terminated`\n状态之前执行。"}
{"en": "## How Pods handle problems with containers {#container-restarts}\n\nKubernetes manages container failures within Pods using a [`restartPolicy`](#restart-policy) defined in the Pod `spec`. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:", "zh": "## Pod 如何处理容器问题 {#container-restarts}\n\nKubernetes 通过在 Pod `spec` 中定义的 [`restartPolicy`](#restart-policy) 管理 Pod 内容器出现的失效。 \n该策略决定了 Kubernetes 如何对由于错误或其他原因而退出的容器做出反应，其顺序如下："}
{"en": "1. **Initial crash**: Kubernetes attempts an immediate restart based on the Pod `restartPolicy`.\n1. **Repeated crashes**: After the initial crash Kubernetes applies an exponential\n   backoff delay for subsequent restarts, described in [`restartPolicy`](#restart-policy).\n   This prevents rapid, repeated restart attempts from overloading the system.\n1. **CrashLoopBackOff state**: This indicates that the backoff delay mechanism is currently\n   in effect for a given container that is in a crash loop, failing and restarting repeatedly.\n1. **Backoff reset**: If a container runs successfully for a certain duration\n   (e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash\n   as the first one.", "zh": "1. **最初的崩溃**：Kubernetes 尝试根据 Pod 的 `restartPolicy` 立即重新启动。\n1. **反复的崩溃**：在最初的崩溃之后，Kubernetes 对于后续重新启动的容器采用指数级回退延迟机制，\n    如 [`restartPolicy`](#restart-policy) 中所述。\n    这一机制可以防止快速、重复的重新启动尝试导致系统过载。\n1. **CrashLoopBackOff 状态**：这一状态表明，对于一个给定的、处于崩溃循环、反复失效并重启的容器，\n    回退延迟机制目前正在生效。\n1. **回退重置**：如果容器成功运行了一定时间（如 10 分钟），\n  Kubernetes 会重置回退延迟机制，将新的崩溃视为第一次崩溃。"}
{"en": "In practice, a `CrashLoopBackOff` is a condition or event that might be seen as output\nfrom the `kubectl` command, while describing or listing Pods, when a container in the Pod\nfails to start properly and then continually tries and fails in a loop.", "zh": "在实际部署中，`CrashLoopBackOff` 是在描述或列出 Pod 时从 `kubectl` 命令输出的一种状况或事件。\n当 Pod 中的容器无法正常启动，并反复进入尝试与失败的循环时就会出现。"}
{"en": "In other words, when a container enters the crash loop, Kubernetes applies the\nexponential backoff delay mentioned in the [Container restart policy](#restart-policy).\nThis mechanism prevents a faulty container from overwhelming the system with continuous\nfailed start attempts.", "zh": "换句话说，当容器进入崩溃循环时，Kubernetes 会应用[容器重启策略](#restart-policy) \n中提到的指数级回退延迟机制。这种机制可以防止有问题的容器因不断进行启动失败尝试而导致系统不堪重负。"}
{"en": "The `CrashLoopBackOff` can be caused by issues like the following:\n\n* Application errors that cause the container to exit.\n* Configuration errors, such as incorrect environment variables or missing\n  configuration files.\n* Resource constraints, where the container might not have enough memory or CPU\n  to start properly.\n* Health checks failing if the application doesn't start serving within the\n  expected time.\n* Container liveness probes or startup probes returning a `Failure` result\n  as mentioned in the [probes section](#container-probes).", "zh": "下列问题可以导致 `CrashLoopBackOff`：\n\n* 应用程序错误导致的容器退出。\n* 配置错误，如环境变量不正确或配置文件丢失。\n* 资源限制，容器可能没有足够的内存或 CPU 正常启动。\n* 如果应用程序没有在预期时间内启动服务，健康检查就会失败。\n* 容器的存活探针或者启动探针返回 `失败` 结果，如[探针部分](#container-probes)所述。"}
{"en": "To investigate the root cause of a `CrashLoopBackOff` issue, a user can:\n\n1. **Check logs**: Use `kubectl logs <name-of-pod>` to check the logs of the container.\n   This is often the most direct way to diagnose the issue causing the crashes.\n1. **Inspect events**: Use `kubectl describe pod <name-of-pod>` to see events\n   for the Pod, which can provide hints about configuration or resource issues.\n1. **Review configuration**: Ensure that the Pod configuration, including\n   environment variables and mounted volumes, is correct and that all required\n   external resources are available.\n1. **Check resource limits**: Make sure that the container has enough CPU\n   and memory allocated. Sometimes, increasing the resources in the Pod definition\n   can resolve the issue.\n1. **Debug application**: There might exist bugs or misconfigurations in the\n   application code. Running this container image locally or in a development\n   environment can help diagnose application specific issues.", "zh": "要调查 `CrashLoopBackOff` 问题的根本原因，用户可以：\n\n1. **检查日志**：使用 `kubectl logs <pod名称>` 检查容器的日志。\n    这通常是诊断导致崩溃的问题的最直接方法。\n1. **检查事件**：使用 `kubectl describe pod <pod名称>` 查看 Pod 的事件，\n    这可以提供有关配置或资源问题的提示。\n1. **审查配置**：确保 Pod 配置正确无误，包括环境变量和挂载卷，并且所有必需的外部资源都可用。\n1. **检查资源限制**： 确保容器被分配了足够的 CPU 和内存。有时，增加 Pod 定义中的资源可以解决问题。\n1. **调试应用程序**：应用程序代码中可能存在错误或配置不当。\n    在本地或开发环境中运行此容器镜像有助于诊断应用程序的特定问题。"}
{"en": "### Container restart policy {#restart-policy}\n\nThe `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure,\nand Never. The default value is Always.\n\nThe `restartPolicy` applies to {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}\nin the Pod and to regular [init containers](/docs/concepts/workloads/pods/init-containers/).\n[Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nignore the Pod-level `restartPolicy` field: in Kubernetes, a sidecar is defined as an\nentry inside `initContainers` that has its container-level `restartPolicy` set to `Always`.\nFor init containers that exit with an error, the kubelet restarts the init container if\nthe Pod level `restartPolicy` is either `OnFailure` or `Always`.\n\n* `Always`: Automatically restarts the container after any termination.\n* `OnFailure`: Only restarts the container if it exits with an error (non-zero exit status).\n* `Never`: Does not automatically restart the terminated container.", "zh": "### 容器重启策略 {#restart-policy}\n\nPod 的 `spec` 中包含一个 `restartPolicy` 字段，其可能取值包括\nAlways、OnFailure 和 Never。默认值是 Always。\n\n`restartPolicy` 应用于 Pod\n中的{{< glossary_tooltip text=\"应用容器\" term_id=\"app-container\" >}}和常规的\n[Init 容器](/zh-cn/docs/concepts/workloads/pods/init-containers/)。\n[Sidecar 容器](/zh-cn/docs/concepts/workloads/pods/sidecar-containers/)忽略\nPod 级别的 `restartPolicy` 字段：在 Kubernetes 中，Sidecar 被定义为\n`initContainers` 内的一个条目，其容器级别的 `restartPolicy` 被设置为 `Always`。\n对于因错误而退出的 Init 容器，如果 Pod 级别 `restartPolicy` 为 `OnFailure` 或 `Always`，\n则 kubelet 会重新启动 Init 容器。\n\n* `Always`：只要容器终止就自动重启容器。\n* `OnFailure`：只有在容器错误退出（退出状态非零）时才重新启动容器。\n* `Never`：不会自动重启已终止的容器。"}
{"en": "When the kubelet is handling container restarts according to the configured restart\npolicy, that only applies to restarts that make replacement containers inside the\nsame Pod and running on the same node. After containers in a Pod exit, the kubelet\nrestarts them with an exponential backoff delay (10s, 20s, 40s, …), that is capped at\n300 seconds (5 minutes). Once a container has executed for 10 minutes without any\nproblems, the kubelet resets the restart backoff timer for that container.\n[Sidecar containers and Pod lifecycle](/docs/concepts/workloads/pods/sidecar-containers/#sidecar-containers-and-pod-lifecycle)\nexplains the behaviour of `init containers` when specify `restartpolicy` field on it.", "zh": "当 kubelet 根据配置的重启策略处理容器重启时，仅适用于同一 Pod\n内替换容器并在同一节点上运行的重启。当 Pod 中的容器退出时，`kubelet`\n会以指数级回退延迟机制（10 秒、20 秒、40 秒......）重启容器，\n上限为 300 秒（5 分钟）。一旦容器顺利执行了 10 分钟，\nkubelet 就会重置该容器的重启延迟计时器。\n[Sidecar 容器和 Pod 生命周期](/zh-cn/docs/concepts/workloads/pods/sidecar-containers/#sidecar-containers-and-pod-lifecycle)中解释了\n`init containers` 在指定 `restartpolicy` 字段时的行为。"}
{"en": "## Pod conditions\n\nA Pod has a PodStatus, which has an array of\n[PodConditions](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podcondition-v1-core)\nthrough which the Pod has or has not passed. Kubelet manages the following\nPodConditions:", "zh": "## Pod 状况  {#pod-conditions}\n\nPod 有一个 PodStatus 对象，其中包含一个\n[PodConditions](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podcondition-v1-core)\n数组。Pod 可能通过也可能未通过其中的一些状况测试。\nKubelet 管理以下 PodCondition："}
{"en": "* `PodScheduled`: the Pod has been scheduled to a node.\n* `PodReadyToStartContainers`: (beta feature; enabled by [default](#pod-has-network)) the\n  Pod sandbox has been successfully created and networking configured.\n* `ContainersReady`: all containers in the Pod are ready.\n* `Initialized`: all [init containers](/docs/concepts/workloads/pods/init-containers/)\n  have completed successfully.\n* `Ready`: the Pod is able to serve requests and should be added to the load\n  balancing pools of all matching Services.", "zh": "* `PodScheduled`：Pod 已经被调度到某节点；\n* `PodReadyToStartContainers`：Pod 沙箱被成功创建并且配置了网络（Beta 特性，[默认](#pod-has-network)启用）；\n* `ContainersReady`：Pod 中所有容器都已就绪；\n* `Initialized`：所有的 [Init 容器](/zh-cn/docs/concepts/workloads/pods/init-containers/)都已成功完成；\n* `Ready`：Pod 可以为请求提供服务，并且应该被添加到对应服务的负载均衡池中。"}
{"en": "Field name           | Description\n:--------------------|:-----------\n`type`               | Name of this Pod condition.\n`status`             | Indicates whether that condition is applicable, with possible values \"`True`\", \"`False`\", or \"`Unknown`\".\n`lastProbeTime`      | Timestamp of when the Pod condition was last probed.\n`lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another.\n`reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.\n`message`            | Human-readable message indicating details about the last status transition.", "zh": "字段名称             | 描述\n:--------------------|:-----------\n`type`               | Pod 状况的名称\n`status`             | 表明该状况是否适用，可能的取值有 \"`True`\"、\"`False`\" 或 \"`Unknown`\"\n`lastProbeTime`      | 上次探测 Pod 状况时的时间戳\n`lastTransitionTime` | Pod 上次从一种状态转换到另一种状态时的时间戳\n`reason`             | 机器可读的、驼峰编码（UpperCamelCase）的文字，表述上次状况变化的原因\n`message`            | 人类可读的消息，给出上次状态转换的详细信息"}
{"en": "### Pod readiness {#pod-readiness-gate}\n\nYour application can inject extra feedback or signals into PodStatus:\n_Pod readiness_. To use this, set `readinessGates` in the Pod's `spec` to\nspecify a list of additional conditions that the kubelet evaluates for Pod readiness.", "zh": "### Pod 就绪态        {#pod-readiness-gate}\n\n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}\n\n你的应用可以向 PodStatus 中注入额外的反馈或者信号：**Pod Readiness（Pod 就绪态）**。\n要使用这一特性，可以设置 Pod 规约中的 `readinessGates` 列表，为 kubelet\n提供一组额外的状况供其评估 Pod 就绪态时使用。"}
{"en": "Readiness gates are determined by the current state of `status.condition`\nfields for the Pod. If Kubernetes cannot find such a condition in the\n`status.conditions` field of a Pod, the status of the condition\nis defaulted to \"`False`\".\n\nHere is an example:", "zh": "就绪态门控基于 Pod 的 `status.conditions` 字段的当前值来做决定。\n如果 Kubernetes 无法在 `status.conditions` 字段中找到某状况，\n则该状况的状态值默认为 \"`False`\"。\n\n这里是一个例子：\n\n```yaml\nkind: Pod\n...\nspec:\n  readinessGates:\n    - conditionType: \"www.example.com/feature-1\"\nstatus:\n  conditions:\n    - type: Ready                              # 内置的 Pod 状况\n      status: \"False\"\n      lastProbeTime: null\n      lastTransitionTime: 2018-01-01T00:00:00Z\n    - type: \"www.example.com/feature-1\"        # 额外的 Pod 状况\n      status: \"False\"\n      lastProbeTime: null\n      lastTransitionTime: 2018-01-01T00:00:00Z\n  containerStatuses:\n    - containerID: docker://abcd...\n      ready: true\n...\n```"}
{"en": "The Pod conditions you add must have names that meet the Kubernetes\n[label key format](/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set).", "zh": "你所添加的 Pod 状况名称必须满足 Kubernetes\n[标签键名格式](/zh-cn/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set)。"}
{"en": "### Status for Pod readiness {#pod-readiness-status}\n\nThe `kubectl patch` command does not support patching object status.\nTo set these `status.conditions` for the Pod, applications and\n{{< glossary_tooltip term_id=\"operator-pattern\" text=\"operators\">}} should use\nthe `PATCH` action.\nYou can use a [Kubernetes client library](/docs/reference/using-api/client-libraries/) to\nwrite code that sets custom Pod conditions for Pod readiness.", "zh": "### Pod 就绪态的状态 {#pod-readiness-status}\n\n命令 `kubectl patch` 不支持修改对象的状态。\n如果需要设置 Pod 的 `status.conditions`，应用或者\n{{< glossary_tooltip term_id=\"operator-pattern\" text=\"Operators\">}}\n需要使用 `PATCH` 操作。你可以使用\n[Kubernetes 客户端库](/zh-cn/docs/reference/using-api/client-libraries/)之一来编写代码，\n针对 Pod 就绪态设置定制的 Pod 状况。"}
{"en": "For a Pod that uses custom conditions, that Pod is evaluated to be ready **only**\nwhen both the following statements apply:\n\n* All containers in the Pod are ready.\n* All conditions specified in `readinessGates` are `True`.\n\nWhen a Pod's containers are Ready but at least one custom condition is missing or\n`False`, the kubelet sets the Pod's [condition](#pod-conditions) to `ContainersReady`.", "zh": "对于使用定制状况的 Pod 而言，只有当下面的陈述都适用时，该 Pod 才会被评估为就绪：\n\n* Pod 中所有容器都已就绪；\n* `readinessGates` 中的所有状况都为 `True` 值。\n\n当 Pod 的容器都已就绪，但至少一个定制状况没有取值或者取值为 `False`，\n`kubelet` 将 Pod 的[状况](#pod-conditions)设置为 `ContainersReady`。"}
{"en": "### Pod network readiness {#pod-has-network}", "zh": "### Pod 网络就绪 {#pod-has-network}\n\n{{< feature-state for_k8s_version=\"v1.25\" state=\"alpha\" >}}\n\n{{< note >}}"}
{"en": "During its early development, this condition was named `PodHasNetwork`.", "zh": "在其早期开发过程中，这种状况被命名为 `PodHasNetwork`。\n{{< /note >}}"}
{"en": "After a Pod gets scheduled on a node, it needs to be admitted by the kubelet and\nto have any required storage volumes mounted. Once these phases are complete,\nthe Kubelet works with\na container runtime (using {{< glossary_tooltip term_id=\"cri\" >}}) to set up a\nruntime sandbox and configure networking for the Pod. If the\n`PodReadyToStartContainersCondition`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled\n(it is enabled by default for Kubernetes {{< skew currentVersion >}}), the\n`PodReadyToStartContainers` condition will be added to the `status.conditions` field of a Pod.", "zh": "在 Pod 被调度到某节点后，它需要被 kubelet 接受并且挂载所需的存储卷。\n一旦这些阶段完成，Kubelet 将与容器运行时（使用{{< glossary_tooltip term_id=\"cri\" >}}）\n一起为 Pod 生成运行时沙箱并配置网络。如果启用了 `PodReadyToStartContainersCondition` \n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)\n（Kubernetes {{< skew currentVersion >}} 版本中默认启用），\n`PodReadyToStartContainers` 状况会被添加到 Pod 的 `status.conditions` 字段中。"}
{"en": "The `PodReadyToStartContainers` condition is set to `False` by the Kubelet when it detects a\nPod does not have a runtime sandbox with networking configured. This occurs in\nthe following scenarios:", "zh": "当 kubelet 检测到 Pod 不具备配置了网络的运行时沙箱时，`PodReadyToStartContainers`\n状况将被设置为 `False`。以下场景中将会发生这种状况："}
{"en": "- Early in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for\n  the Pod using the container runtime.\n- Later in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:\n  - the node rebooting, without the Pod getting evicted\n  - for container runtimes that use virtual machines for isolation, the Pod\n    sandbox virtual machine rebooting, which then requires creating a new sandbox and\n    fresh container network configuration.", "zh": "* 在 Pod 生命周期的早期阶段，kubelet 还没有开始使用容器运行时为 Pod 设置沙箱时。\n* 在 Pod 生命周期的末期阶段，Pod 的沙箱由于以下原因被销毁时：\n  * 节点重启时 Pod 没有被驱逐\n  * 对于使用虚拟机进行隔离的容器运行时，Pod 沙箱虚拟机重启时，需要创建一个新的沙箱和全新的容器网络配置。"}
{"en": "The `PodReadyToStartContainers` condition is set to `True` by the kubelet after the\nsuccessful completion of sandbox creation and network configuration for the Pod\nby the runtime plugin. The kubelet can start pulling container images and create\ncontainers after `PodReadyToStartContainers` condition has been set to `True`.", "zh": "在运行时插件成功完成 Pod 的沙箱创建和网络配置后，\nkubelet 会将 `PodReadyToStartContainers` 状况设置为 `True`。\n当 `PodReadyToStartContainers` 状况设置为 `True` 后，\nKubelet 可以开始拉取容器镜像和创建容器。"}
{"en": "For a Pod with init containers, the kubelet sets the `Initialized` condition to\n`True` after the init containers have successfully completed (which happens\nafter successful sandbox creation and network configuration by the runtime\nplugin). For a Pod without init containers, the kubelet sets the `Initialized`\ncondition to `True` before sandbox creation and network configuration starts.", "zh": "对于带有 Init 容器的 Pod，kubelet 会在 Init 容器成功完成后将 `Initialized` 状况设置为 `True`\n（这发生在运行时成功创建沙箱和配置网络之后），\n对于没有 Init 容器的 Pod，kubelet 会在创建沙箱和网络配置开始之前将\n`Initialized` 状况设置为 `True`。"}
{"en": "## Container probes\n\nA _probe_ is a diagnostic performed periodically by the [kubelet](/docs/reference/command-line-tools-reference/kubelet/)\non a container. To perform a diagnostic, the kubelet either executes code within the container,\nor makes a network request.", "zh": "## 容器探针    {#container-probes}\n\n**probe** 是由 [kubelet](/zh-cn/docs/reference/command-line-tools-reference/kubelet/) 对容器执行的定期诊断。\n要执行诊断，kubelet 既可以在容器内执行代码，也可以发出一个网络请求。"}
{"en": "### Check mechanisms {#probe-check-methods}\n\nThere are four different ways to check a container using a probe.\nEach probe must define exactly one of these four mechanisms:\n\n`exec`\n: Executes a specified command inside the container. The diagnostic\n  is considered successful if the command exits with a status code of 0.\n\n`grpc`\n: Performs a remote procedure call using [gRPC](https://grpc.io/).\n  The target should implement\n  [gRPC health checks](https://grpc.io/grpc/core/md_doc_health-checking.html).\n  The diagnostic is considered successful if the `status`\n  of the response is `SERVING`.\n\n`httpGet`\n: Performs an HTTP `GET` request against the Pod's IP\n  address on a specified port and path. The diagnostic is\n  considered successful if the response has a status code\n  greater than or equal to 200 and less than 400.\n\n`tcpSocket`\n: Performs a TCP check against the Pod's IP address on\n  a specified port. The diagnostic is considered successful if\n  the port is open. If the remote system (the container) closes\n  the connection immediately after it opens, this counts as healthy.", "zh": "### 检查机制    {#probe-check-methods}\n\n使用探针来检查容器有四种不同的方法。\n每个探针都必须准确定义为这四种机制中的一种：\n\n`exec`\n: 在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。\n\n`grpc`\n: 使用 [gRPC](https://grpc.io/) 执行一个远程过程调用。\n  目标应该实现\n  [gRPC 健康检查](https://grpc.io/grpc/core/md_doc_health-checking.html)。\n  如果响应的状态是 \"SERVING\"，则认为诊断成功。\n\n`httpGet`\n: 对容器的 IP 地址上指定端口和路径执行 HTTP `GET` 请求。如果响应的状态码大于等于 200\n  且小于 400，则诊断被认为是成功的。\n\n`tcpSocket`\n: 对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。\n  如果远程系统（容器）在打开连接后立即将其关闭，这算作是健康的。"}
{"en": "{{< caution >}} Unlike the other mechanisms, `exec` probe's implementation involves the creation/forking of multiple processes each time when executed.\nAs a result, in case of the clusters having higher pod densities, lower intervals of `initialDelaySeconds`, `periodSeconds`, configuring any probe with exec mechanism might introduce an overhead on the cpu usage of the node.\nIn such scenarios, consider using the alternative probe mechanisms to avoid the overhead.{{< /caution >}}", "zh": "{{< caution >}}\n和其他机制不同，`exec` 探针的实现涉及每次执行时创建/复制多个进程。\n因此，在集群中具有较高 pod 密度、较低的 `initialDelaySeconds` 和 `periodSeconds` 时长的时候，\n配置任何使用 exec 机制的探针可能会增加节点的 CPU 负载。\n这种场景下，请考虑使用其他探针机制以避免额外的开销。\n{{< /caution >}}"}
{"en": "### Probe outcome\nEach probe has one of three results:\n\n`Success`\n: The container passed the diagnostic.\n\n`Failure`\n: The container failed the diagnostic.\n\n`Unknown`\n: The diagnostic failed (no action should be taken, and the kubelet\n  will make further checks).", "zh": "### 探测结果    {#probe-outcome}\n\n每次探测都将获得以下三种结果之一：\n\n`Success`（成功）\n: 容器通过了诊断。\n\n`Failure`（失败）\n: 容器未通过诊断。\n\n`Unknown`（未知）\n: 诊断失败，因此不会采取任何行动。"}
{"en": "### Types of probe\n\nThe kubelet can optionally perform and react to three kinds of probes on running\ncontainers:", "zh": "### 探测类型    {#types-of-probe}\n\n针对运行中的容器，`kubelet` 可以选择是否执行以下三种探针，以及如何针对探测结果作出反应："}
{"en": "`livenessProbe`\n: Indicates whether the container is running. If\n  the liveness probe fails, the kubelet kills the container, and the container\n  is subjected to its [restart policy](#restart-policy). If a container does not\n  provide a liveness probe, the default state is `Success`.\n\n`readinessProbe`\n: Indicates whether the container is ready to respond to requests.\n  If the readiness probe fails, the endpoints controller removes the Pod's IP\n  address from the endpoints of all Services that match the Pod. The default\n  state of readiness before the initial delay is `Failure`. If a container does\n  not provide a readiness probe, the default state is `Success`.\n\n`startupProbe`\n: Indicates whether the application within the container is started.\n  All other probes are disabled if a startup probe is provided, until it succeeds.\n  If the startup probe fails, the kubelet kills the container, and the container\n  is subjected to its [restart policy](#restart-policy). If a container does not\n  provide a startup probe, the default state is `Success`.", "zh": "`livenessProbe`\n: 指示容器是否正在运行。如果存活态探测失败，则 kubelet 会杀死容器，\n  并且容器将根据其[重启策略](#restart-policy)决定未来。如果容器不提供存活探针，\n  则默认状态为 `Success`。\n\n`readinessProbe`\n: 指示容器是否准备好为请求提供服务。如果就绪态探测失败，\n  端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。\n  初始延迟之前的就绪态的状态值默认为 `Failure`。\n  如果容器不提供就绪态探针，则默认状态为 `Success`。\n\n`startupProbe`\n: 指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被\n  禁用，直到此探针成功为止。如果启动探测失败，`kubelet` 将杀死容器，\n  而容器依其[重启策略](#restart-policy)进行重启。\n  如果容器没有提供启动探测，则默认状态为 `Success`。"}
{"en": "For more information about how to set up a liveness, readiness, or startup probe,\nsee [Configure Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).", "zh": "如欲了解如何设置存活态、就绪态和启动探针的进一步细节，\n可以参阅[配置存活态、就绪态和启动探针](/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)。"}
{"en": "#### When should you use a liveness probe?", "zh": "#### 何时该使用存活态探针?    {#when-should-you-use-a-liveness-probe}"}
{"en": "If the process in your container is able to crash on its own whenever it\nencounters an issue or becomes unhealthy, you do not necessarily need a liveness\nprobe; the kubelet will automatically perform the correct action in accordance\nwith the Pod's `restartPolicy`.\n\nIf you'd like your container to be killed and restarted if a probe fails, then\nspecify a liveness probe, and specify a `restartPolicy` of Always or OnFailure.", "zh": "如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活态探针；\n`kubelet` 将根据 Pod 的 `restartPolicy` 自动执行修复操作。\n\n如果你希望容器在探测失败时被杀死并重新启动，那么请指定一个存活态探针，\n并指定 `restartPolicy` 为 \"`Always`\" 或 \"`OnFailure`\"。"}
{"en": "#### When should you use a readiness probe?", "zh": "#### 何时该使用就绪态探针？      {#when-should-you-use-a-readiness-probe}"}
{"en": "If you'd like to start sending traffic to a Pod only when a probe succeeds,\nspecify a readiness probe. In this case, the readiness probe might be the same\nas the liveness probe, but the existence of the readiness probe in the spec means\nthat the Pod will start without receiving any traffic and only start receiving\ntraffic after the probe starts succeeding.", "zh": "如果要仅在探测成功时才开始向 Pod 发送请求流量，请指定就绪态探针。\n在这种情况下，就绪态探针可能与存活态探针相同，但是规约中的就绪态探针的存在意味着\nPod 将在启动阶段不接收任何数据，并且只有在探针探测成功后才开始接收数据。"}
{"en": "If you want your container to be able to take itself down for maintenance, you\ncan specify a readiness probe that checks an endpoint specific to readiness that\nis different from the liveness probe.", "zh": "如果你希望容器能够自行进入维护状态，也可以指定一个就绪态探针，\n检查某个特定于就绪态的因此不同于存活态探测的端点。"}
{"en": "If your app has a strict dependency on back-end services, you can implement both\na liveness and a readiness probe. The liveness probe passes when the app itself\nis healthy, but the readiness probe additionally checks that each required\nback-end service is available. This helps you avoid directing traffic to Pods\nthat can only respond with error messages.\n\nIf your container needs to work on loading large data, configuration files, or\nmigrations during startup, you can use a\n[startup probe](#when-should-you-use-a-startup-probe). However, if you want to\ndetect the difference between an app that has failed and an app that is still\nprocessing its startup data, you might prefer a readiness probe.", "zh": "如果你的应用程序对后端服务有严格的依赖性，你可以同时实现存活态和就绪态探针。\n当应用程序本身是健康的，存活态探针检测通过后，就绪态探针会额外检查每个所需的后端服务是否可用。\n这可以帮助你避免将流量导向只能返回错误信息的 Pod。\n\n如果你的容器需要在启动期间加载大型数据、配置文件或执行迁移，\n你可以使用[启动探针](#when-should-you-use-a-startup-probe)。\n然而，如果你想区分已经失败的应用和仍在处理其启动数据的应用，你可能更倾向于使用就绪探针。\n\n{{< note >}}"}
{"en": "If you want to be able to drain requests when the Pod is deleted, you do not\nnecessarily need a readiness probe; on deletion, the Pod automatically puts itself\ninto an unready state regardless of whether the readiness probe exists.\nThe Pod remains in the unready state while it waits for the containers in the Pod\nto stop.", "zh": "请注意，如果你只是想在 Pod 被删除时能够排空请求，则不一定需要使用就绪态探针；\n在删除 Pod 时，Pod 会自动将自身置于未就绪状态，无论就绪态探针是否存在。\n等待 Pod 中的容器停止期间，Pod 会一直处于未就绪状态。\n{{< /note >}}"}
{"en": "#### When should you use a startup probe?", "zh": "#### 何时该使用启动探针？   {#when-should-you-use-a-startup-probe}"}
{"en": "Startup probes are useful for Pods that have containers that take a long time to\ncome into service. Rather than set a long liveness interval, you can configure\na separate configuration for probing the container as it starts up, allowing\na time longer than the liveness interval would allow.", "zh": "对于所包含的容器需要较长时间才能启动就绪的 Pod 而言，启动探针是有用的。\n你不再需要配置一个较长的存活态探测时间间隔，只需要设置另一个独立的配置选定，\n对启动期间的容器执行探测，从而允许使用远远超出存活态时间间隔所允许的时长。"}
{"en": "If your container usually starts in more than\n`initialDelaySeconds + failureThreshold × periodSeconds`, you should specify a\nstartup probe that checks the same endpoint as the liveness probe. The default for\n`periodSeconds` is 10s. You should then set its `failureThreshold` high enough to\nallow the container to start, without changing the default values of the liveness\nprobe. This helps to protect against deadlocks.", "zh": "如果你的容器启动时间通常超出 `initialDelaySeconds + failureThreshold × periodSeconds`\n总值，你应该设置一个启动探测，对存活态探针所使用的同一端点执行检查。\n`periodSeconds` 的默认值是 10 秒。你应该将其 `failureThreshold` 设置得足够高，\n以便容器有充足的时间完成启动，并且避免更改存活态探针所使用的默认值。\n这一设置有助于减少死锁状况的发生。"}
{"en": "## Termination of Pods {#pod-termination}\n\nBecause Pods represent processes running on nodes in the cluster, it is important to\nallow those processes to gracefully terminate when they are no longer needed (rather\nthan being abruptly stopped with a `KILL` signal and having no chance to clean up).", "zh": "## Pod 的终止    {#pod-termination}\n\n由于 Pod 所代表的是在集群中节点上运行的进程，当不再需要这些进程时允许其体面地终止是很重要的。\n一般不应武断地使用 `KILL` 信号终止它们，导致这些进程没有机会完成清理操作。"}
{"en": "The design aim is for you to be able to request deletion and know when processes\nterminate, but also be able to ensure that deletes eventually complete.\nWhen you request deletion of a Pod, the cluster records and tracks the intended grace period\nbefore the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in\nplace, the {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} attempts graceful\nshutdown.", "zh": "设计的目标是令你能够请求删除进程，并且知道进程何时被终止，同时也能够确保删除操作终将完成。\n当你请求删除某个 Pod 时，集群会记录并跟踪 Pod 的体面终止周期，\n而不是直接强制地杀死 Pod。在存在强制关闭设施的前提下，\n{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} 会尝试体面地终止\nPod。"}
{"en": "Typically, with this graceful termination of the pod, kubelet makes requests to the container runtime to attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM) signal, with a grace period timeout, to the main process in each container. The requests to stop the containers are processed by the container runtime asynchronously. There is no guarantee to the order of processing for these requests. Many container runtimes respect the `STOPSIGNAL` value defined in the container image and, if different, send the container image configured STOPSIGNAL instead of TERM.\nOnce the grace period has expired, the KILL signal is sent to any remaining\nprocesses, and the Pod is then deleted from the\n{{< glossary_tooltip text=\"API Server\" term_id=\"kube-apiserver\" >}}. If the kubelet or the\ncontainer runtime's management service is restarted while waiting for processes to terminate, the\ncluster retries from the start including the full original grace period.", "zh": "通常 Pod 体面终止的过程为：kubelet 先发送一个带有体面超时限期的 TERM（又名 SIGTERM）\n信号到每个容器中的主进程，将请求发送到容器运行时来尝试停止 Pod 中的容器。\n停止容器的这些请求由容器运行时以异步方式处理。\n这些请求的处理顺序无法被保证。许多容器运行时遵循容器镜像内定义的 `STOPSIGNAL` 值，\n如果不同，则发送容器镜像中配置的 STOPSIGNAL，而不是 TERM 信号。\n一旦超出了体面终止限期，容器运行时会向所有剩余进程发送 KILL 信号，之后\nPod 就会被从 {{< glossary_tooltip text=\"API 服务器\" term_id=\"kube-apiserver\" >}}上移除。\n如果 `kubelet` 或者容器运行时的管理服务在等待进程终止期间被重启，\n集群会从头开始重试，赋予 Pod 完整的体面终止限期。"}
{"en": "Pod termination flow, illustrated with an example:\n\n1. You use the `kubectl` tool to manually delete a specific Pod, with the default grace period\n   (30 seconds).\n\n1. The Pod in the API server is updated with the time beyond which the Pod is considered \"dead\"\n   along with the grace period.\n   If you use `kubectl describe` to check the Pod you're deleting, that Pod shows up as \"Terminating\".\n   On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked\n   as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod\n   shutdown process.", "zh": "Pod 终止流程，如下例所示：\n\n1. 你使用 `kubectl` 工具手动删除某个特定的 Pod，而该 Pod 的体面终止限期是默认值（30 秒）。\n\n2. API 服务器中的 Pod 对象被更新，记录涵盖体面终止限期在内 Pod\n   的最终死期，超出所计算时间点则认为 Pod 已死（dead）。\n   如果你使用 `kubectl describe` 来查验你正在删除的 Pod，该 Pod 会显示为\n   \"Terminating\" （正在终止）。\n   在 Pod 运行所在的节点上：`kubelet` 一旦看到 Pod\n   被标记为正在终止（已经设置了体面终止限期），`kubelet` 即开始本地的 Pod 关闭过程。"}
{"en": "1. If one of the Pod's containers has defined a `preStop`\n      [hook](/docs/concepts/containers/container-lifecycle-hooks) and the `terminationGracePeriodSeconds`\n      in the Pod spec is not set to 0, the kubelet runs that hook inside of the container.\n      The default `terminationGracePeriodSeconds` setting is 30 seconds.\n\n      If the `preStop` hook is still running after the grace period expires, the kubelet requests\n      a small, one-off grace period extension of 2 seconds.", "zh": "1. 如果 Pod 中的容器之一定义了 `preStop`\n      [回调](/zh-cn/docs/concepts/containers/container-lifecycle-hooks)，\n      `kubelet` 开始在容器内运行该回调逻辑。如果超出体面终止限期时，\n      `preStop` 回调逻辑仍在运行，`kubelet` 会请求给予该 Pod 的宽限期一次性增加 2 秒钟。\n\n      如果 `preStop` 回调在体面期结束后仍在运行，kubelet 将请求短暂的、一次性的体面期延长 2 秒。"}
{"en": "If the `preStop` hook needs longer to complete than the default grace period allows,\n   you must modify `terminationGracePeriodSeconds` to suit this.", "zh": "{{< note >}}\n   如果 `preStop` 回调所需要的时间长于默认的体面终止限期，你必须修改\n   `terminationGracePeriodSeconds` 属性值来使其正常工作。\n   {{< /note >}}"}
{"en": "1. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each\n      container.", "zh": "2. `kubelet` 接下来触发容器运行时发送 TERM 信号给每个容器中的进程 1。"}
{"en": "There is [special ordering](#termination-with-sidecars) if the Pod has any\n      {{< glossary_tooltip text=\"sidecar containers\" term_id=\"sidecar-container\" >}} defined.\n      Otherwise, the containers in the Pod receive the TERM signal at different times and in\n      an arbitrary order. If the order of shutdowns matters, consider using a `preStop` hook\n      to synchronize (or switch to using sidecar containers).", "zh": "如果 Pod 中定义了{{< glossary_tooltip text=\"Sidecar 容器\" term_id=\"sidecar-container\" >}}，\n      则存在[特殊排序](#termination-with-sidecars)。否则，Pod 中的容器会在不同的时间和任意的顺序接收\n      TERM 信号。如果关闭顺序很重要，考虑使用 `preStop` 钩子进行同步（或者切换为使用 Sidecar 容器）。"}
{"en": "1. At the same time as the kubelet is starting graceful shutdown of the Pod, the control plane\n   evaluates whether to remove that shutting-down Pod from EndpointSlice (and Endpoints) objects,\n   where those objects represent a {{< glossary_tooltip term_id=\"service\" text=\"Service\" >}}\n   with a configured {{< glossary_tooltip text=\"selector\" term_id=\"selector\" >}}.\n   {{< glossary_tooltip text=\"ReplicaSets\" term_id=\"replica-set\" >}} and other workload resources\n   no longer treat the shutting-down Pod as a valid, in-service replica.\n\n   Pods that shut down slowly should not continue to serve regular traffic and should start\n   terminating and finish processing open connections.  Some applications need to go beyond\n   finishing open connections and need more graceful termination, for example, session draining\n   and completion.", "zh": "3. 在 `kubelet` 启动 Pod 的体面关闭逻辑的同时，控制平面会评估是否将关闭的\n   Pod 从对应的 EndpointSlice（和端点）对象中移除，过滤条件是 Pod\n   被对应的{{< glossary_tooltip term_id=\"service\" text=\"服务\" >}}以某\n   {{< glossary_tooltip text=\"选择算符\" term_id=\"selector\" >}}选定。\n   {{< glossary_tooltip text=\"ReplicaSet\" term_id=\"replica-set\" >}}\n   和其他工作负载资源不再将关闭进程中的 Pod 视为合法的、能够提供服务的副本。\n\n   关闭动作很慢的 Pod 不应继续处理常规服务请求，而应开始终止并完成对打开的连接的处理。\n   一些应用程序不仅需要完成对打开的连接的处理，还需要更进一步的体面终止逻辑 -\n   比如：排空和完成会话。"}
{"en": "Any endpoints that represent the terminating Pods are not immediately removed from\n   EndpointSlices, and a status indicating [terminating state](/docs/concepts/services-networking/endpoint-slices/#conditions)\n   is exposed from the EndpointSlice API (and the legacy Endpoints API).\n   Terminating endpoints always have their `ready` status as `false` (for backward compatibility\n   with versions before 1.26), so load balancers will not use it for regular traffic.\n\n   If traffic draining on terminating Pod is needed, the actual readiness can be checked as a\n   condition `serving`.  You can find more details on how to implement connections draining in the\n   tutorial [Pods And Endpoints Termination Flow](/docs/tutorials/services/pods-and-endpoint-termination-flow/)", "zh": "任何正在终止的 Pod 所对应的端点都不会立即从 EndpointSlice\n   中被删除，EndpointSlice API（以及传统的 Endpoints API）会公开一个状态来指示其处于\n   [终止状态](/zh-cn/docs/concepts/services-networking/endpoint-slices/#conditions)。\n   正在终止的端点始终将其 `ready` 状态设置为 `false`（为了向后兼容 1.26 之前的版本），\n   因此负载均衡器不会将其用于常规流量。\n\n   如果需要排空正被终止的 Pod 上的流量，可以将 `serving` 状况作为实际的就绪状态。你可以在教程\n   [探索 Pod 及其端点的终止行为](/zh-cn/docs/tutorials/services/pods-and-endpoint-termination-flow/)\n   中找到有关如何实现连接排空的更多详细信息。\n\n   <a id=\"pod-termination-beyond-grace-period\" />"}
{"en": "1. The kubelet ensures the Pod is shut down and terminated\n   1. When the grace period expires, if there is still any container running in the Pod, the\n      kubelet triggers forcible shutdown.\n      The container runtime sends `SIGKILL` to any processes still running in any container in the Pod.\n      The kubelet also cleans up a hidden `pause` container if that container runtime uses one.\n   1. The kubelet transitions the Pod into a terminal phase (`Failed` or `Succeeded` depending on\n      the end state of its containers).\n   1. The kubelet triggers forcible removal of the Pod object from the API server, by setting grace period\n      to 0 (immediate deletion).\n   1. The API server deletes the Pod's API object, which is then no longer visible from any client.", "zh": "4. kubelet 确保 Pod 被关闭和终止\n\n   1. 超出终止宽限期限时，如果 Pod 中仍有容器在运行，kubelet 会触发强制关闭过程。\n      容器运行时会向 Pod 中所有容器内仍在运行的进程发送 `SIGKILL` 信号。\n      `kubelet` 也会清理隐藏的 `pause` 容器，如果容器运行时使用了这种容器的话。\n\n   1. `kubelet` 将 Pod 转换到终止阶段（`Failed` 或 `Succeeded`，具体取决于其容器的结束状态）。\n\n   1. kubelet 通过将宽限期设置为 0（立即删除），触发从 API 服务器强制移除 Pod 对象的操作。\n\n   1. API 服务器删除 Pod 的 API 对象，从任何客户端都无法再看到该对象。"}
{"en": "### Forced Pod termination {#pod-termination-forced}", "zh": "### 强制终止 Pod     {#pod-termination-forced}\n\n{{< caution >}}"}
{"en": "Forced deletions can be potentially disruptive for some workloads and their Pods.", "zh": "对于某些工作负载及其 Pod 而言，强制删除很可能会带来某种破坏。\n{{< /caution >}}"}
{"en": "By default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports\nthe `--grace-period=<seconds>` option which allows you to override the default and specify your\nown value.", "zh": "默认情况下，所有的删除操作都会附有 30 秒钟的宽限期限。\n`kubectl delete` 命令支持 `--grace-period=<seconds>` 选项，允许你重载默认值，\n设定自己希望的期限值。"}
{"en": "Setting the grace period to `0` forcibly and immediately deletes the Pod from the API\nserver. If the Pod was still running on a node, that forcible deletion triggers the kubelet to\nbegin immediate cleanup.", "zh": "将宽限期限强制设置为 `0` 意味着立即从 API 服务器删除 Pod。\n如果 Pod 仍然运行于某节点上，强制删除操作会触发 `kubelet` 立即执行清理操作。"}
{"en": "Using kubectl, You must specify an additional flag `--force` along with `--grace-period=0`\nin order to perform force deletions.", "zh": "使用 kubectl 时，你必须在设置 `--grace-period=0` 的同时额外设置 `--force` 参数才能发起强制删除请求。"}
{"en": "When a force deletion is performed, the API server does not wait for confirmation\nfrom the kubelet that the Pod has been terminated on the node it was running on. It\nremoves the Pod in the API immediately so a new Pod can be created with the same\nname. On the node, Pods that are set to terminate immediately will still be given\na small grace period before being force killed.", "zh": "执行强制删除操作时，API 服务器不再等待来自 `kubelet` 的、关于 Pod\n已经在原来运行的节点上终止执行的确认消息。\nAPI 服务器直接删除 Pod 对象，这样新的与之同名的 Pod 即可以被创建。\n在节点侧，被设置为立即终止的 Pod 仍然会在被强行杀死之前获得一点点的宽限时间。\n\n{{< caution >}}"}
{"en": "Immediate deletion does not wait for confirmation that the running resource has been terminated.\nThe resource may continue to run on the cluster indefinitely.", "zh": "马上删除时不等待确认正在运行的资源已被终止。这些资源可能会无限期地继续在集群上运行。\n{{< /caution >}}"}
{"en": "If you need to force-delete Pods that are part of a StatefulSet, refer to the task\ndocumentation for\n[deleting Pods from a StatefulSet](/docs/tasks/run-application/force-delete-stateful-set-pod/).", "zh": "如果你需要强制删除 StatefulSet 的 Pod，\n请参阅[从 StatefulSet 中删除 Pod](/zh-cn/docs/tasks/run-application/force-delete-stateful-set-pod/)\n的任务文档。"}
{"en": "### Pod shutdown and sidecar containers {##termination-with-sidecars}\n\nIf your Pod includes one or more\n[sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\n(init containers with an Always restart policy), the kubelet will delay sending\nthe TERM signal to these sidecar containers until the last main container has fully terminated.\nThe sidecar containers will be terminated in the reverse order they are defined in the Pod spec.\nThis ensures that sidecar containers continue serving the other containers in the Pod until they\nare no longer needed.", "zh": "### Pod 关闭和 Sidecar 容器 {#termination-with-sidecars}\n\n如果你的 Pod 包含一个或多个 [Sidecar 容器](/zh-cn/docs/concepts/workloads/pods/sidecar-containers/)\n（重启策略为 Always 的 Init 容器），kubelet 将延迟向这些 Sidecar 容器发送 TERM 信号，\n直到最后一个主容器已完全终止。Sidecar 容器将按照它们在 Pod 规约中被定义的相反顺序被终止。\n这样确保了 Sidecar 容器继续为 Pod 中的其他容器提供服务，直到完全不再需要为止。"}
{"en": "This means that slow termination of a main container will also delay the termination of the sidecar containers.\nIf the grace period expires before the termination process is complete, the Pod may enter [forced termination](#pod-termination-beyond-grace-period).\nIn this case, all remaining containers in the Pod will be terminated simultaneously with a short grace period.\n\nSimilarly, if the Pod has a `preStop` hook that exceeds the termination grace period, emergency termination may occur.\nIn general, if you have used `preStop` hooks to control the termination order without sidecar containers, you can now\nremove them and allow the kubelet to manage sidecar termination automatically.", "zh": "这意味着主容器的慢终止也会延迟 Sidecar 容器的终止。\n如果在终止过程完成之前宽限期已到，Pod 可能会进入[强制终止](#pod-termination-beyond-grace-period)阶段。\n在这种情况下，Pod 中所有剩余的容器将在某个短宽限期内被同时终止。\n\n同样地，如果 Pod 有一个 `preStop` 钩子超过了终止宽限期，可能会发生紧急终止。\n总体而言，如果你以前使用 `preStop` 钩子来控制没有 Sidecar 的 Pod 中容器的终止顺序，\n你现在可以移除这些钩子，允许 kubelet 自动管理 Sidecar 的终止。"}
{"en": "### Garbage collection of Pods {#pod-garbage-collection}\n\nFor failed Pods, the API objects remain in the cluster's API until a human or\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}} process\nexplicitly removes them.\n\nThe Pod garbage collector (PodGC), which is a controller in the control plane, cleans up\nterminated Pods (with a phase of `Succeeded` or `Failed`), when the number of Pods exceeds the\nconfigured threshold (determined by `terminated-pod-gc-threshold` in the kube-controller-manager).\nThis avoids a resource leak as Pods are created and terminated over time.", "zh": "### Pod 的垃圾收集    {#pod-garbage-collection}\n\n对于已失败的 Pod 而言，对应的 API 对象仍然会保留在集群的 API 服务器上，\n直到用户或者{{< glossary_tooltip term_id=\"controller\" text=\"控制器\" >}}进程显式地将其删除。\n\nPod 的垃圾收集器（PodGC）是控制平面的控制器，它会在 Pod 个数超出所配置的阈值\n（根据 `kube-controller-manager` 的 `terminated-pod-gc-threshold` 设置）时删除已终止的\nPod（阶段值为 `Succeeded` 或 `Failed`）。\n这一行为会避免随着时间演进不断创建和终止 Pod 而引起的资源泄露问题。"}
{"en": "Additionally, PodGC cleans up any Pods which satisfy any of the following conditions:\n\n1. are orphan Pods - bound to a node which no longer exists,\n1. are unscheduled terminating Pods,\n1. are terminating Pods, bound to a non-ready node tainted with\n   [`node.kubernetes.io/out-of-service`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-out-of-service),\n   when the `NodeOutOfServiceVolumeDetach` feature gate is enabled.\n\nAlong with cleaning up the Pods, PodGC will also mark them as failed if they are in a non-terminal\nphase. Also, PodGC adds a Pod disruption condition when cleaning up an orphan Pod.\nSee [Pod disruption conditions](/docs/concepts/workloads/pods/disruptions#pod-disruption-conditions)\nfor more details.", "zh": "此外，PodGC 会清理满足以下任一条件的所有 Pod：\n\n1. 孤儿 Pod - 绑定到不再存在的节点，\n2. 计划外终止的 Pod\n3. 终止过程中的 Pod，当启用 `NodeOutOfServiceVolumeDetach` 特性门控时，\n   绑定到有 [`node.kubernetes.io/out-of-service`](/zh-cn/docs/reference/labels-annotations-taints/#node-kubernetes-io-out-of-service)\n   污点的未就绪节点。\n\n在清理 Pod 的同时，如果它们处于非终止状态阶段，PodGC 也会将它们标记为失败。\n此外，PodGC 在清理孤儿 Pod 时会添加 Pod 干扰状况。参阅\n[Pod 干扰状况](/zh-cn/docs/concepts/workloads/pods/disruptions#pod-disruption-conditions) 了解更多详情。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Get hands-on experience\n  [attaching handlers to container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).\n\n* Get hands-on experience\n  [configuring Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).\n\n* Learn more about [container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/).\n\n* Learn more about [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/).\n\n* For detailed information about Pod and container status in the API, see\n  the API reference documentation covering\n  [`status`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus) for Pod.", "zh": "* 动手实践[为容器生命周期时间关联处理程序](/zh-cn/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/)。\n* 动手实践[配置存活态、就绪态和启动探针](/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)。\n* 进一步了解[容器生命周期回调](/zh-cn/docs/concepts/containers/container-lifecycle-hooks/)。\n* 进一步了解 [Sidecar 容器](/zh-cn/docs/concepts/workloads/pods/sidecar-containers/)。\n* 关于 API 中定义的有关 Pod 和容器状态的详细规范信息，\n  可参阅 API 参考文档中 Pod 的 [`status`](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus) 字段。"}
{"en": "This guide is for application owners who want to build\nhighly available applications, and thus need to understand\nwhat types of disruptions can happen to Pods.", "zh": "本指南针对的是希望构建高可用性应用的应用所有者，他们有必要了解可能发生在 Pod 上的干扰类型。"}
{"en": "It is also for cluster administrators who want to perform automated\ncluster actions, like upgrading and autoscaling clusters.", "zh": "文档同样适用于想要执行自动化集群操作（例如升级和自动扩展集群）的集群管理员。"}
{"en": "## Voluntary and involuntary disruptions\n\nPods do not disappear until someone (a person or a controller) destroys them, or\nthere is an unavoidable hardware or system software error.", "zh": "## 自愿干扰和非自愿干扰     {#voluntary-and-involuntary-disruptions}\n\nPod 不会消失，除非有人（用户或控制器）将其销毁，或者出现了不可避免的硬件或软件系统错误。"}
{"en": "We call these unavoidable cases *involuntary disruptions* to\nan application.  Examples are:", "zh": "我们把这些不可避免的情况称为应用的**非自愿干扰（Involuntary Disruptions）**。例如："}
{"en": "- a hardware failure of the physical machine backing the node\n- cluster administrator deletes VM (instance) by mistake\n- cloud provider or hypervisor failure makes VM disappear\n- a kernel panic\n- the node disappears from the cluster due to cluster network partition\n- eviction of a pod due to the node being [out-of-resources](/docs/concepts/scheduling-eviction/node-pressure-eviction/).", "zh": "- 节点下层物理机的硬件故障\n- 集群管理员错误地删除虚拟机（实例）\n- 云提供商或虚拟机管理程序中的故障导致的虚拟机消失\n- 内核错误\n- 节点由于集群网络隔离从集群中消失\n- 由于节点[资源不足](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)导致 pod 被驱逐。"}
{"en": "Except for the out-of-resources condition, all these conditions\nshould be familiar to most users; they are not specific\nto Kubernetes.", "zh": "除了资源不足的情况，大多数用户应该都熟悉这些情况；它们不是特定于 Kubernetes 的。"}
{"en": "We call other cases *voluntary disruptions*.  These include both\nactions initiated by the application owner and those initiated by a Cluster\nAdministrator.  Typical application owner actions include:", "zh": "我们称其他情况为**自愿干扰（Voluntary Disruptions）**。\n包括由应用所有者发起的操作和由集群管理员发起的操作。\n典型的应用所有者的操作包括："}
{"en": "- deleting the deployment or other controller that manages the pod\n- updating a deployment's pod template causing a restart\n- directly deleting a pod (e.g. by accident)", "zh": "- 删除 Deployment 或其他管理 Pod 的控制器\n- 更新了 Deployment 的 Pod 模板导致 Pod 重启\n- 直接删除 Pod（例如，因为误操作）"}
{"en": "Cluster administrator actions include:\n\n- [Draining a node](/docs/tasks/administer-cluster/safely-drain-node/) for repair or upgrade.\n- Draining a node from a cluster to scale the cluster down (learn about\n[Cluster Autoscaling](https://github.com/kubernetes/autoscaler/#readme)).\n- Removing a pod from a node to permit something else to fit on that node.", "zh": "集群管理员操作包括：\n\n- [排空（drain）节点](/zh-cn/docs/tasks/administer-cluster/safely-drain-node/)进行修复或升级。\n- 从集群中排空节点以缩小集群（了解[集群自动扩缩](https://github.com/kubernetes/autoscaler/#readme)）。\n- 从节点中移除一个 Pod，以允许其他 Pod 使用该节点。"}
{"en": "These actions might be taken directly by the cluster administrator, or by automation\nrun by the cluster administrator, or by your cluster hosting provider.", "zh": "这些操作可能由集群管理员直接执行，也可能由集群管理员所使用的自动化工具执行，或者由集群托管提供商自动执行。"}
{"en": "Ask your cluster administrator or consult your cloud provider or distribution documentation\nto determine if any sources of voluntary disruptions are enabled for your cluster.\nIf none are enabled, you can skip creating Pod Disruption Budgets.", "zh": "咨询集群管理员或联系云提供商，或者查询发布文档，以确定是否为集群启用了任何资源干扰源。\n如果没有启用，可以不用创建 Pod Disruption Budgets（Pod 干扰预算）\n\n{{< caution >}}"}
{"en": "Not all voluntary disruptions are constrained by Pod Disruption Budgets. For example,\ndeleting deployments or pods bypasses Pod Disruption Budgets.", "zh": "并非所有的自愿干扰都会受到 Pod 干扰预算的限制。\n例如，删除 Deployment 或 Pod 的删除操作就会跳过 Pod 干扰预算检查。\n{{< /caution >}}"}
{"en": "## Dealing with disruptions\n\nHere are some ways to mitigate involuntary disruptions:", "zh": "## 处理干扰   {#dealing-with-disruptions}\n\n以下是减轻非自愿干扰的一些方法："}
{"en": "- Ensure your pod [requests the resources](/docs/tasks/configure-pod-container/assign-memory-resource) it needs.\n- Replicate your application if you need higher availability.  (Learn about running replicated\n  [stateless](/docs/tasks/run-application/run-stateless-application-deployment/)\n  and [stateful](/docs/tasks/run-application/run-replicated-stateful-application/) applications.)\n- For even higher availability when running replicated applications,\n  spread applications across racks (using\n  [anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity))\n  or across zones (if using a\n  [multi-zone cluster](/docs/setup/multiple-zones).)", "zh": "- 确保 Pod 在请求中给出[所需资源](/zh-cn/docs/tasks/configure-pod-container/assign-memory-resource/)。\n- 如果需要更高的可用性，请复制应用。\n  （了解有关运行多副本的[无状态](/zh-cn/docs/tasks/run-application/run-stateless-application-deployment/)\n  和[有状态](/zh-cn/docs/tasks/run-application/run-replicated-stateful-application/)应用的信息。）\n- 为了在运行复制应用时获得更高的可用性，请跨机架（使用\n  [反亲和性](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)）\n  或跨区域（如果使用[多区域集群](/zh-cn/docs/setup/best-practices/multiple-zones/)）扩展应用。"}
{"en": "The frequency of voluntary disruptions varies.  On a basic Kubernetes cluster, there are\nno automated voluntary disruptions (only user-triggered ones).  However, your cluster administrator or hosting provider\nmay run some additional services which cause voluntary disruptions. For example,\nrolling out node software updates can cause voluntary disruptions. Also, some implementations\nof cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.\nYour cluster administrator or hosting provider should have documented what level of voluntary\ndisruptions, if any, to expect. Certain configuration options, such as\n[using PriorityClasses](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\nin your pod spec can also cause voluntary (and involuntary) disruptions.", "zh": "自愿干扰的频率各不相同。在一个基本的 Kubernetes 集群中，没有自愿干扰（只有用户触发的干扰）。\n然而，集群管理员或托管提供商可能运行一些可能导致自愿干扰的额外服务。例如，节点软\n更新可能导致自愿干扰。另外，集群（节点）自动缩放的某些\n实现可能导致碎片整理和紧缩节点的自愿干扰。集群\n管理员或托管提供商应该已经记录了各级别的自愿干扰（如果有的话）。\n有些配置选项，例如在 pod spec 中\n[使用 PriorityClasses](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/)\n也会产生自愿（和非自愿）的干扰。"}
{"en": "## Pod disruption budgets\n\nKubernetes offers features to help you run highly available applications even when you\nintroduce frequent voluntary disruptions.\n\nAs an application owner, you can create a PodDisruptionBudget (PDB) for each application.\nA PDB limits the number of Pods of a replicated application that are down simultaneously from\nvoluntary disruptions. For example, a quorum-based application would\nlike to ensure that the number of replicas running is never brought below the\nnumber needed for a quorum. A web front end might want to\nensure that the number of replicas serving load never falls below a certain\npercentage of the total.", "zh": "## 干扰预算   {#pod-disruption-budgets}\n\n{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}\n\n即使你会经常引入自愿性干扰，Kubernetes 提供的功能也能够支持你运行高度可用的应用。\n\n作为一个应用的所有者，你可以为每个应用创建一个 `PodDisruptionBudget`（PDB）。\nPDB 将限制在同一时间因自愿干扰导致的多副本应用中发生宕机的 Pod 数量。\n例如，基于票选机制的应用希望确保运行中的副本数永远不会低于票选所需的数量。\nWeb 前端可能希望确保提供负载的副本数量永远不会低于总数的某个百分比。"}
{"en": "Cluster managers and hosting providers should use tools which\nrespect PodDisruptionBudgets by calling the [Eviction API](/docs/tasks/administer-cluster/safely-drain-node/#eviction-api)\ninstead of directly deleting pods or deployments.", "zh": "集群管理员和托管提供商应该使用遵循 PodDisruptionBudgets 的接口\n（通过调用[Eviction API](/zh-cn/docs/tasks/administer-cluster/safely-drain-node/#the-eviction-api)），\n而不是直接删除 Pod 或 Deployment。"}
{"en": "For example, the `kubectl drain` subcommand lets you mark a node as going out of\nservice. When you run `kubectl drain`, the tool tries to evict all of the Pods on\nthe Node you're taking out of service. The eviction request that `kubectl` submits on\nyour behalf may be temporarily rejected, so the tool periodically retries all failed\nrequests until all Pods on the target node are terminated, or until a configurable timeout\nis reached.", "zh": "例如，`kubectl drain` 命令可以用来标记某个节点即将停止服务。\n运行 `kubectl drain` 命令时，工具会尝试驱逐你所停服的节点上的所有 Pod。\n`kubectl` 代表你所提交的驱逐请求可能会暂时被拒绝，\n所以该工具会周期性地重试所有失败的请求，\n直到目标节点上的所有的 Pod 都被终止，或者达到配置的超时时间。"}
{"en": "A PDB specifies the number of replicas that an application can tolerate having, relative to how\nmany it is intended to have.  For example, a Deployment which has a `.spec.replicas: 5` is\nsupposed to have 5 pods at any given time.  If its PDB allows for there to be 4 at a time,\nthen the Eviction API will allow voluntary disruption of one (but not two) pods at a time.", "zh": "PDB 指定应用可以容忍的副本数量（相当于应该有多少副本）。\n例如，具有 `.spec.replicas: 5` 的 Deployment 在任何时间都应该有 5 个 Pod。\n如果 PDB 允许其在某一时刻有 4 个副本，那么驱逐 API 将允许同一时刻仅有一个（而不是两个）Pod 自愿干扰。"}
{"en": "The group of pods that comprise the application is specified using a label selector, the same\nas the one used by the application's controller (deployment, stateful-set, etc).", "zh": "使用标签选择器来指定构成应用的一组 Pod，这与应用的控制器（Deployment、StatefulSet 等）\n选择 Pod 的逻辑一样。"}
{"en": "The \"intended\" number of pods is computed from the `.spec.replicas` of the workload resource\nthat is managing those pods. The control plane discovers the owning workload resource by\nexamining the `.metadata.ownerReferences` of the Pod.", "zh": "Pod 的“预期”数量由管理这些 Pod 的工作负载资源的 `.spec.replicas` 参数计算出来的。\n控制平面通过检查 Pod 的\n`.metadata.ownerReferences` 来发现关联的工作负载资源。"}
{"en": "[Involuntary disruptions](#voluntary-and-involuntary-disruptions) cannot be prevented by PDBs; however they\ndo count against the budget.", "zh": "PDB 无法防止[非自愿干扰](#voluntary-and-involuntary-disruptions)；\n但它们确实计入预算。"}
{"en": "Pods which are deleted or unavailable due to a rolling upgrade to an application do count\nagainst the disruption budget, but workload resources (such as Deployment and StatefulSet)\nare not limited by PDBs when doing rolling upgrades. Instead, the handling of failures\nduring application updates is configured in the spec for the specific workload resource.", "zh": "由于应用的滚动升级而被删除或不可用的 Pod 确实会计入干扰预算，\n但是工作负载资源（如 Deployment 和 StatefulSet）\n在进行滚动升级时不受 PDB 的限制。\n应用更新期间的故障处理方式是在对应的工作负载资源的 `spec` 中配置的。"}
{"en": "It is recommended to set `AlwaysAllow` [Unhealthy Pod Eviction Policy](/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy)\nto your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain.\nThe default behavior is to wait for the application pods to become [healthy](/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod)\nbefore the drain can proceed.", "zh": "建议在你的 PodDisruptionBudget 中将\n[不健康 Pod 驱逐策略](/zh-cn/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy)\n设置为 `AlwaysAllow` 以支持在节点腾空期间驱逐行为不当的应用程序。\n默认行为是等待应用程序 Pod 变得\n[健康](/zh-cn/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod)，然后才能继续执行腾空。"}
{"en": "When a pod is evicted using the eviction API, it is gracefully\n[terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination), honoring the\n`terminationGracePeriodSeconds` setting in its [PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core).", "zh": "当使用驱逐 API 驱逐 Pod 时，Pod 会被体面地\n[终止](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)，期间会\n参考 [PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core)\n中的 `terminationGracePeriodSeconds` 配置值。"}
{"en": "## PodDisruptionBudget example {#pdb-example}\n\nConsider a cluster with 3 nodes, `node-1` through `node-3`.\nThe cluster is running several applications.  One of them has 3 replicas initially called\n`pod-a`, `pod-b`, and `pod-c`.  Another, unrelated pod without a PDB, called `pod-x`, is also shown.\nInitially, the pods are laid out as follows:", "zh": "## PodDisruptionBudget 例子   {#pdb-example}\n\n假设集群有 3 个节点，`node-1` 到 `node-3`。集群上运行了一些应用。\n其中一个应用有 3 个副本，分别是 `pod-a`，`pod-b` 和 `pod-c`。\n另外，还有一个不带 PDB 的无关 pod `pod-x` 也同样显示出来。\n最初，所有的 Pod 分布如下：\n\n|       node-1         |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *available*   | pod-b *available*   | pod-c *available*  |\n| pod-x  *available*   |                     |                    |"}
{"en": "All 3 pods are part of a deployment, and they collectively have a PDB which requires\nthere be at least 2 of the 3 pods to be available at all times.", "zh": "3 个 Pod 都是 deployment 的一部分，并且共同拥有同一个 PDB，要求 3 个 Pod 中至少有 2 个 Pod 始终处于可用状态。"}
{"en": "For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.\nThe cluster administrator first tries to drain `node-1` using the `kubectl drain` command.\nThat tool tries to evict `pod-a` and `pod-x`.  This succeeds immediately.\nBoth pods go into the `terminating` state at the same time.\nThis puts the cluster in this state:", "zh": "例如，假设集群管理员想要重启系统，升级内核版本来修复内核中的缺陷。\n集群管理员首先使用 `kubectl drain` 命令尝试腾空 `node-1` 节点。\n命令尝试驱逐 `pod-a` 和 `pod-x`。操作立即就成功了。\n两个 Pod 同时进入 `terminating` 状态。这时的集群处于下面的状态：\n\n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* |                     |                    |"}
{"en": "The deployment notices that one of the pods is terminating, so it creates a replacement\ncalled `pod-d`.  Since `node-1` is cordoned, it lands on another node.  Something has\nalso created `pod-y` as a replacement for `pod-x`.", "zh": "Deployment 控制器观察到其中一个 Pod 正在终止，因此它创建了一个替代 Pod `pod-d`。\n由于 `node-1` 被封锁（cordon），`pod-d` 落在另一个节点上。\n同样其他控制器也创建了 `pod-y` 作为 `pod-x` 的替代品。"}
{"en": "(Note: for a StatefulSet, `pod-a`, which would be called something like `pod-0`, would need\nto terminate completely before its replacement, which is also called `pod-0` but has a\ndifferent UID, could be created.  Otherwise, the example applies to a StatefulSet as well.)", "zh": "（注意：对于 StatefulSet 来说，`pod-a`（也称为 `pod-0`）需要在替换 Pod 创建之前完全终止，\n替代它的也称为 `pod-0`，但是具有不同的 UID。除此之外，此示例也适用于 StatefulSet。）"}
{"en": "Now the cluster is in this state:", "zh": "当前集群的状态如下：\n\n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* | pod-d *starting*    | pod-y              |"}
{"en": "At some point, the pods terminate, and the cluster looks like this:", "zh": "在某一时刻，Pod 被终止，集群如下所示：\n\n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *starting*    | pod-y              |"}
{"en": "At this point, if an impatient cluster administrator tries to drain `node-2` or\n`node-3`, the drain command will block, because there are only 2 available\npods for the deployment, and its PDB requires at least 2.  After some time passes, `pod-d` becomes available.", "zh": "此时，如果一个急躁的集群管理员试图排空（drain）`node-2` 或 `node-3`，drain 命令将被阻塞，\n因为对于 Deployment 来说只有 2 个可用的 Pod，并且它的 PDB 至少需要 2 个。\n经过一段时间，`pod-d` 变得可用。"}
{"en": "The cluster state now looks like this:", "zh": "集群状态如下所示：\n\n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *available*   | pod-y              |"}
{"en": "Now, the cluster administrator tries to drain `node-2`.\nThe drain command will try to evict the two pods in some order, say\n`pod-b` first and then `pod-d`.  It will succeed at evicting `pod-b`.\nBut, when it tries to evict `pod-d`, it will be refused because that would leave only\none pod available for the deployment.", "zh": "现在，集群管理员试图排空（drain）`node-2`。\ndrain 命令将尝试按照某种顺序驱逐两个 Pod，假设先是 `pod-b`，然后是 `pod-d`。\n命令成功驱逐 `pod-b`，但是当它尝试驱逐 `pod-d`时将被拒绝，因为对于\nDeployment 来说只剩一个可用的 Pod 了。"}
{"en": "The deployment creates a replacement for `pod-b` called `pod-e`.\nBecause there are not enough resources in the cluster to schedule\n`pod-e` the drain will again block.  The cluster may end up in this\nstate:", "zh": "Deployment 创建 `pod-b` 的替代 Pod `pod-e`。\n因为集群中没有足够的资源来调度 `pod-e`，drain 命令再次阻塞。集群最终将是下面这种状态：\n\n|    node-1 *drained*  |       node-2        |       node-3       | *no node*          |\n|:--------------------:|:-------------------:|:------------------:|:------------------:|\n|                      | pod-b *terminating* | pod-c *available*  | pod-e *pending*    |\n|                      | pod-d *available*   | pod-y              |                    |"}
{"en": "At this point, the cluster administrator needs to\nadd a node back to the cluster to proceed with the upgrade.", "zh": "此时，集群管理员需要增加一个节点到集群中以继续升级操作。"}
{"en": "You can see how Kubernetes varies the rate at which disruptions\ncan happen, according to:", "zh": "可以看到 Kubernetes 如何改变干扰发生的速率，根据："}
{"en": "- how many replicas an application needs\n- how long it takes to gracefully shutdown an instance\n- how long it takes a new instance to start up\n- the type of controller\n- the cluster's resource capacity", "zh": "- 应用需要多少个副本\n- 优雅关闭应用实例需要多长时间\n- 启动应用新实例需要多长时间\n- 控制器的类型\n- 集群的资源能力"}
{"en": "## Pod disruption conditions {#pod-disruption-conditions}", "zh": "## Pod 干扰状况 {#pod-disruption-conditions}\n\n{{< feature-state feature_gate_name=\"PodDisruptionConditions\" >}}"}
{"en": "A dedicated Pod `DisruptionTarget` [condition](/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions)\nis added to indicate\nthat the Pod is about to be deleted due to a {{<glossary_tooltip term_id=\"disruption\" text=\"disruption\">}}.\nThe `reason` field of the condition additionally\nindicates one of the following reasons for the Pod termination:", "zh": "Pod 会被添加一个 `DisruptionTarget`\n[状况](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions)，\n用来表明该 Pod 因为发生{{<glossary_tooltip term_id=\"disruption\" text=\"干扰\">}}而被删除。\n状况中的 `reason` 字段进一步给出 Pod 终止的原因，如下："}
{"en": "`PreemptionByScheduler`\n: Pod is due to be {{<glossary_tooltip term_id=\"preemption\" text=\"preempted\">}} by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see [Pod priority preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).", "zh": "`PreemptionByScheduler`\n: Pod 将被调度器{{<glossary_tooltip term_id=\"preemption\" text=\"抢占\">}}，\n目的是接受优先级更高的新 Pod。\n要了解更多的相关信息，请参阅 [Pod 优先级和抢占](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/)。"}
{"en": "`DeletionByTaintManager`\n: Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within `kube-controller-manager`) due to a `NoExecute` taint that the Pod does not tolerate; see {{<glossary_tooltip term_id=\"taint\" text=\"taint\">}}-based evictions.", "zh": "`DeletionByTaintManager`\n: 由于 Pod 不能容忍 `NoExecute` 污点，Pod 将被\nTaint Manager（`kube-controller-manager` 中节点生命周期控制器的一部分）删除；\n请参阅基于{{<glossary_tooltip term_id=\"taint\" text=\"污点\">}}的驱逐。"}
{"en": "`EvictionByEvictionAPI`\n: Pod has been marked for {{<glossary_tooltip term_id=\"api-eviction\" text=\"eviction using the Kubernetes API\">}}.", "zh": "`EvictionByEvictionAPI`\n: Pod 已被标记为{{<glossary_tooltip term_id=\"api-eviction\" text=\"通过 Kubernetes API 驱逐\">}}。"}
{"en": "`DeletionByPodGC`\n: Pod, that is bound to a no longer existing Node, is due to be deleted by [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection).", "zh": "`DeletionByPodGC`\n: 绑定到一个不再存在的 Node 上的 Pod 将被\n[Pod 垃圾收集](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)删除。"}
{"en": "`TerminationByKubelet`\n: Pod has been terminated by the kubelet, because of either {{<glossary_tooltip term_id=\"node-pressure-eviction\" text=\"node pressure eviction\">}},\n  the [graceful node shutdown](/docs/concepts/architecture/nodes/#graceful-node-shutdown),\n  or preemption for [system critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).", "zh": "`TerminationByKubelet`\n: Pod\n由于{{<glossary_tooltip term_id=\"node-pressure-eviction\" text=\"节点压力驱逐\">}}、\n[节点体面关闭](/zh-cn/docs/concepts/architecture/nodes/#graceful-node-shutdown)\n或[系统关键 Pod](/zh-cn/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/)的抢占而被\nkubelet 终止。"}
{"en": "In all other disruption scenarios, like eviction due to exceeding\n[Pod container limits](/docs/concepts/configuration/manage-resources-containers/),\nPods don't receive the `DisruptionTarget` condition because the disruptions were\nprobably caused by the Pod and would reoccur on retry.", "zh": "在所有其他中断场景中，例如由于超出\n[Pod 容器限制]而被驱逐，`DisruptionTarget` 状况不会被添加到 Pod 上，\n因为中断可能是由 Pod 引起的，并且会在重试时再次发生。\n\n{{< note >}}"}
{"en": "A Pod disruption might be interrupted. The control plane might re-attempt to\ncontinue the disruption of the same Pod, but it is not guaranteed. As a result,\nthe `DisruptionTarget` condition might be added to a Pod, but that Pod might then not actually be\ndeleted. In such a situation, after some time, the\nPod disruption condition will be cleared.", "zh": "Pod 的干扰可能会被中断。控制平面可能会重新尝试继续干扰同一个 Pod，但这没办法保证。\n因此，`DisruptionTarget` 状况可能会被添加到 Pod 上，\n但该 Pod 实际上可能不会被删除。\n在这种情况下，一段时间后，Pod 干扰状况将被清除。\n{{< /note >}}"}
{"en": "Along with cleaning up the pods, the Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal\nphase (see also [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)).", "zh": "在清理 Pod 的同时，如果这些 Pod 处于非终止阶段，\n则 Pod 垃圾回收器 (PodGC) 也会将这些 Pod 标记为失效\n（另见 [Pod 垃圾回收](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)）。"}
{"en": "When using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's\n[Pod failure policy](/docs/concepts/workloads/controllers/job#pod-failure-policy).", "zh": "使用 Job（或 CronJob）时，你可能希望将这些 Pod 干扰状况作为 Job\n[Pod 失效策略](/zh-cn/docs/concepts/workloads/controllers/job#pod-failure-policy)的一部分。"}
{"en": "## Separating Cluster Owner and Application Owner Roles\n\nOften, it is useful to think of the Cluster Manager\nand Application Owner as separate roles with limited knowledge\nof each other.   This separation of responsibilities\nmay make sense in these scenarios:", "zh": "## 分离集群所有者和应用所有者角色   {#separating-cluster-owner-and-application-owner-roles}\n\n通常，将集群管理者和应用所有者视为彼此了解有限的独立角色是很有用的。这种责任分离在下面这些场景下是有意义的："}
{"en": "- when there are many application teams sharing a Kubernetes cluster, and\n  there is natural specialization of roles\n- when third-party tools or services are used to automate cluster management", "zh": "- 当有许多应用团队共用一个 Kubernetes 集群，并且有自然的专业角色\n- 当第三方工具或服务用于集群自动化管理"}
{"en": "Pod Disruption Budgets support this separation of roles by providing an\ninterface between the roles.", "zh": "Pod 干扰预算通过在角色之间提供接口来支持这种分离。"}
{"en": "If you do not have such a separation of responsibilities in your organization,\nyou may not need to use Pod Disruption Budgets.", "zh": "如果你的组织中没有这样的责任分离，则可能不需要使用 Pod 干扰预算。"}
{"en": "## How to perform Disruptive Actions on your Cluster\n\nIf you are a Cluster Administrator, and you need to perform a disruptive action on all\nthe nodes in your cluster, such as a node or system software upgrade, here are some options:", "zh": "## 如何在集群上执行干扰性操作   {#how-to-perform-disruptive-actions-on-your-cluster}\n\n如果你是集群管理员，并且需要对集群中的所有节点执行干扰操作，例如节点或系统软件升级，则可以使用以下选项"}
{"en": "- Accept downtime during the upgrade.\n- Failover to another complete replica cluster.\n   -  No downtime, but may be costly both for the duplicated nodes\n     and for human effort to orchestrate the switchover.\n- Write disruption tolerant applications and use PDBs.\n   - No downtime.\n   - Minimal resource duplication.\n   - Allows more automation of cluster administration.\n   - Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary\n     disruptions largely overlaps with work to support autoscaling and tolerating\n     involuntary disruptions.", "zh": "- 接受升级期间的停机时间。\n- 故障转移到另一个完整的副本集群。\n  - 没有停机时间，但是对于重复的节点和人工协调成本可能是昂贵的。\n- 编写可容忍干扰的应用和使用 PDB。\n  - 不停机。\n  - 最小的资源重复。\n  - 允许更多的集群管理自动化。\n  - 编写可容忍干扰的应用是棘手的，但对于支持容忍自愿干扰所做的工作，和支持自动扩缩和容忍非\n    自愿干扰所做工作相比，有大量的重叠\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).\n\n* Learn more about [draining nodes](/docs/tasks/administer-cluster/safely-drain-node/)\n\n* Learn about [updating a deployment](/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)\n  including steps to maintain its availability during the rollout.", "zh": "* 参考[配置 Pod 干扰预算](/zh-cn/docs/tasks/run-application/configure-pdb/)中的方法来保护你的应用。\n\n* 进一步了解[排空节点](/zh-cn/docs/tasks/administer-cluster/safely-drain-node/)的信息。\n\n* 了解[更新 Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)\n  的过程，包括如何在其进程中维持应用的可用性"}
{"en": "This page introduces _Quality of Service (QoS) classes_ in Kubernetes, and explains\nhow Kubernetes assigns a QoS class to each Pod as a consequence of the resource\nconstraints that you specify for the containers in that Pod. Kubernetes relies on this\nclassification to make decisions about which Pods to evict when there are not enough\navailable resources on a Node.", "zh": "本页介绍 Kubernetes 中的 **服务质量（Quality of Service，QoS）** 类，\n阐述 Kubernetes 如何根据为 Pod 中的容器指定的资源约束为每个 Pod 设置 QoS 类。\nKubernetes 依赖这种分类来决定当 Node 上没有足够可用资源时要驱逐哪些 Pod。"}
{"en": "## Quality of Service classes", "zh": "## QoS 类   {#qos-class}"}
{"en": "Kubernetes classifies the Pods that you run and allocates each Pod into a specific\n_quality of service (QoS) class_. Kubernetes uses that classification to influence how different\npods are handled. Kubernetes does this classification based on the\n[resource requests](/docs/concepts/configuration/manage-resources-containers/)\nof the {{< glossary_tooltip text=\"Containers\" term_id=\"container\" >}} in that Pod, along with\nhow those requests relate to resource limits.\nThis is known as {{< glossary_tooltip text=\"Quality of Service\" term_id=\"qos-class\" >}}\n(QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requests\nand limits of its component Containers. QoS classes are used by Kubernetes to decide\nwhich Pods to evict from a Node experiencing\n[Node Pressure](/docs/concepts/scheduling-eviction/node-pressure-eviction/). The possible\nQoS classes are `Guaranteed`, `Burstable`, and `BestEffort`. When a Node runs out of resources,\nKubernetes will first evict `BestEffort` Pods running on that Node, followed by `Burstable` and\nfinally `Guaranteed` Pods. When this eviction is due to resource pressure, only Pods exceeding\nresource requests are candidates for eviction.", "zh": "Kubernetes 对你运行的 Pod 进行分类，并将每个 Pod 分配到特定的 **QoS 类**中。\nKubernetes 使用这种分类来影响不同 Pod 被处理的方式。Kubernetes 基于 Pod\n中{{< glossary_tooltip text=\"容器\" term_id=\"container\" >}}的[资源请求](/zh-cn/docs/concepts/configuration/manage-resources-containers/)进行分类，\n同时确定这些请求如何与资源限制相关。\n这称为{{< glossary_tooltip text=\"服务质量\" term_id=\"qos-class\" >}} (QoS) 类。\nKubernetes 基于每个 Pod 中容器的资源请求和限制为 Pod 设置 QoS 类。Kubernetes 使用 QoS\n类来决定从遇到[节点压力](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)的\nNode 中驱逐哪些 Pod。可选的 QoS 类有 `Guaranteed`、`Burstable` 和 `BestEffort`。\n当一个 Node 耗尽资源时，Kubernetes 将首先驱逐在该 Node 上运行的 `BestEffort` Pod，\n然后是 `Burstable` Pod，最后是 `Guaranteed` Pod。当这种驱逐是由于资源压力时，\n只有超出资源请求的 Pod 才是被驱逐的候选对象。\n\n### Guaranteed"}
{"en": "Pods that are `Guaranteed` have the strictest resource limits and are least likely\nto face eviction. They are guaranteed not to be killed until they exceed their limits\nor there are no lower-priority Pods that can be preempted from the Node. They may\nnot acquire resources beyond their specified limits. These Pods can also make\nuse of exclusive CPUs using the\n[`static`](/docs/tasks/administer-cluster/cpu-management-policies/#static-policy) CPU management policy.", "zh": "`Guaranteed` Pod 具有最严格的资源限制，并且最不可能面临驱逐。\n在这些 Pod 超过其自身的限制或者没有可以从 Node 抢占的低优先级 Pod 之前，\n这些 Pod 保证不会被杀死。这些 Pod 不可以获得超出其指定 limit 的资源。这些 Pod 也可以使用\n[`static`](/zh-cn/docs/tasks/administer-cluster/cpu-management-policies/#static-policy)\nCPU 管理策略来使用独占的 CPU。"}
{"en": "#### Criteria\n\nFor a Pod to be given a QoS class of `Guaranteed`:", "zh": "#### 判据   {#criteria}\n\nPod 被赋予 `Guaranteed` QoS 类的几个判据："}
{"en": "* Every Container in the Pod must have a memory limit and a memory request.\n* For every Container in the Pod, the memory limit must equal the memory request.\n* Every Container in the Pod must have a CPU limit and a CPU request.\n* For every Container in the Pod, the CPU limit must equal the CPU request.", "zh": "* Pod 中的每个容器必须有内存 limit 和内存 request。\n* 对于 Pod 中的每个容器，内存 limit 必须等于内存 request。\n* Pod 中的每个容器必须有 CPU limit 和 CPU request。\n* 对于 Pod 中的每个容器，CPU limit 必须等于 CPU request。\n\n### Burstable"}
{"en": "Pods that are `Burstable` have some lower-bound resource guarantees based on the request, but\ndo not require a specific limit. If a limit is not specified, it defaults to a\nlimit equivalent to the capacity of the Node, which allows the Pods to flexibly increase\ntheir resources if resources are available. In the event of Pod eviction due to Node\nresource pressure, these Pods are evicted only after all `BestEffort` Pods are evicted.\nBecause a `Burstable` Pod can include a Container that has no resource limits or requests, a Pod\nthat is `Burstable` can try to use any amount of node resources.", "zh": "`Burstable` Pod 有一些基于 request 的资源下限保证，但不需要特定的 limit。\n如果未指定 limit，则默认为其 limit 等于 Node 容量，这允许 Pod 在资源可用时灵活地增加其资源。\n在由于 Node 资源压力导致 Pod 被驱逐的情况下，只有在所有 `BestEffort` Pod 被驱逐后\n这些 Pod 才会被驱逐。因为 `Burstable` Pod 可以包括没有资源 limit 或资源 request 的容器，\n所以 `Burstable` Pod 可以尝试使用任意数量的节点资源。"}
{"en": "#### Criteria\n\nA Pod is given a QoS class of `Burstable` if:\n\n* The Pod does not meet the criteria for QoS class `Guaranteed`.\n* At least one Container in the Pod has a memory or CPU request or limit.", "zh": "#### 判据   {#criteria-1}\n\nPod 被赋予 `Burstable` QoS 类的几个判据：\n\n* Pod 不满足针对 QoS 类 `Guaranteed` 的判据。\n* Pod 中至少一个容器有内存或 CPU 的 request 或 limit。\n\n### BestEffort"}
{"en": "Pods in the `BestEffort` QoS class can use node resources that aren't specifically assigned\nto Pods in other QoS classes. For example, if you have a node with 16 CPU cores available to the\nkubelet, and you assign 4 CPU cores to a `Guaranteed` Pod, then a Pod in the `BestEffort`\nQoS class can try to use any amount of the remaining 12 CPU cores.\n\nThe kubelet prefers to evict `BestEffort` Pods if the node comes under resource pressure.", "zh": "`BestEffort` QoS 类中的 Pod 可以使用未专门分配给其他 QoS 类中的 Pod 的节点资源。\n例如若你有一个节点有 16 核 CPU 可供 kubelet 使用，并且你将 4 核 CPU 分配给一个 `Guaranteed` Pod，\n那么 `BestEffort` QoS 类中的 Pod 可以尝试任意使用剩余的 12 核 CPU。\n\n如果节点遇到资源压力，kubelet 将优先驱逐 `BestEffort` Pod。"}
{"en": "#### Criteria\n\nA Pod has a QoS class of `BestEffort` if it doesn't meet the criteria for either `Guaranteed`\nor `Burstable`. In other words, a Pod is `BestEffort` only if none of the Containers in the Pod have a\nmemory limit or a memory request, and none of the Containers in the Pod have a\nCPU limit or a CPU request.\nContainers in a Pod can request other resources (not CPU or memory) and still be classified as\n`BestEffort`.", "zh": "#### 判据   {#criteria-2}\n\n如果 Pod 不满足 `Guaranteed` 或 `Burstable` 的判据，则它的 QoS 类为 `BestEffort`。\n换言之，只有当 Pod 中的所有容器没有内存 limit 或内存 request，也没有 CPU limit 或\nCPU request 时，Pod 才是 `BestEffort`。Pod 中的容器可以请求（除 CPU 或内存之外的）\n其他资源并且仍然被归类为 `BestEffort`。"}
{"en": "## Memory QoS with cgroup v2", "zh": "## 使用 cgroup v2 的内存 QoS   {#memory-qos-with-cgroup-v2}\n\n{{< feature-state feature_gate_name=\"MemoryQoS\" >}}"}
{"en": "Memory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes.\nMemory requests and limits of containers in pod are used to set specific interfaces `memory.min`\nand `memory.high` provided by the memory controller. When `memory.min` is set to memory requests,\nmemory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures\nmemory availability for Kubernetes pods. And if memory limits are set in the container,\nthis means that the system needs to limit container memory usage; Memory QoS uses `memory.high`\nto throttle workload approaching its memory limit, ensuring that the system is not overwhelmed\nby instantaneous memory allocation.", "zh": "内存 QoS 使用 cgroup v2 的内存控制器来保证 Kubernetes 中的内存资源。\nPod 中容器的内存请求和限制用于设置由内存控制器所提供的特定接口 `memory.min` 和 `memory.high`。\n当 `memory.min` 被设置为内存请求时，内存资源被保留并且永远不会被内核回收；\n这就是内存 QoS 确保 Kubernetes Pod 的内存可用性的方式。而如果容器中设置了内存限制，\n这意味着系统需要限制容器内存的使用；内存 QoS 使用 `memory.high` 来限制接近其内存限制的工作负载，\n确保系统不会因瞬时内存分配而不堪重负。"}
{"en": "Memory QoS relies on QoS class to determine which settings to apply; however, these are different\nmechanisms that both provide controls over quality of service.", "zh": "内存 QoS 依赖于 QoS 类来确定应用哪些设置；它们的机制不同，但都提供对服务质量的控制。"}
{"en": "## Some behavior is independent of QoS class {#class-independent-behavior}\n\nCertain behavior is independent of the QoS class assigned by Kubernetes. For example:", "zh": "## 某些行为独立于 QoS 类 {#class-independent-behavior}\n\n某些行为独立于 Kubernetes 分配的 QoS 类。例如："}
{"en": "* Any Container exceeding a resource limit will be killed and restarted by the kubelet without\n  affecting other Containers in that Pod.\n* If a Container exceeds its resource request and the node it runs on faces\n  resource pressure, the Pod it is in becomes a candidate for [eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n  If this occurs, all Containers in the Pod will be terminated. Kubernetes may create a\n  replacement Pod, usually on a different node.", "zh": "* 所有超过资源 limit 的容器都将被 kubelet 杀死并重启，而不会影响该 Pod 中的其他容器。\n* 如果一个容器超出了自身的资源 request，且该容器运行的节点面临资源压力，则该容器所在的 Pod\n  就会成为被[驱逐](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)的候选对象。\n  如果出现这种情况，Pod 中的所有容器都将被终止。Kubernetes 通常会在不同的节点上创建一个替代的 Pod。"}
{"en": "* The resource request of a Pod is equal to the sum of the resource requests of\n  its component Containers, and the resource limit of a Pod is equal to the sum of\n  the resource limits of its component Containers.\n* The kube-scheduler does not consider QoS class when selecting which Pods to\n  [preempt](/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption).\n  Preemption can occur when a cluster does not have enough resources to run all the Pods\n  you defined.", "zh": "* Pod 的资源 request 等于其成员容器的资源 request 之和，Pod 的资源 limit 等于其成员容器的资源 limit 之和。\n* kube-scheduler 在选择要[抢占](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption)的\n  Pod 时不考虑 QoS 类。当集群没有足够的资源来运行你所定义的所有 Pod 时，就会发生抢占。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn about [resource management for Pods and Containers](/docs/concepts/configuration/manage-resources-containers/).\n* Learn about [Node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n* Learn about [Pod priority and preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).\n* Learn about [Pod disruptions](/docs/concepts/workloads/pods/disruptions/).\n* Learn how to [assign memory resources to containers and pods](/docs/tasks/configure-pod-container/assign-memory-resource/).\n* Learn how to [assign CPU resources to containers and pods](/docs/tasks/configure-pod-container/assign-cpu-resource/).\n* Learn how to [configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/).", "zh": "* 进一步了解[为 Pod 和容器管理资源](/zh-cn/docs/concepts/configuration/manage-resources-containers/)。\n* 进一步了解[节点压力驱逐](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)。\n* 进一步了解 [Pod 优先级和抢占](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/)。\n* 进一步了解 [Pod 干扰](/zh-cn/docs/concepts/workloads/pods/disruptions/)。\n* 进一步了解如何[为容器和 Pod 分配内存资源](/zh-cn/docs/tasks/configure-pod-container/assign-memory-resource/)。\n* 进一步了解如何[为容器和 Pod 分配 CPU 资源](/zh-cn/docs/tasks/configure-pod-container/assign-cpu-resource/)。\n* 进一步了解如何[配置 Pod 的服务质量](/zh-cn/docs/tasks/configure-pod-container/quality-service-pod/)。"}
{"en": "This page provides an overview of init containers: specialized containers that run\nbefore app containers in a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}.\nInit containers can contain utilities or setup scripts not present in an app image.", "zh": "本页提供了 Init 容器的概览。Init 容器是一种特殊容器，在 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}\n内的应用容器启动之前运行。Init 容器可以包括一些应用镜像中不存在的实用工具和安装脚本。"}
{"en": "You can specify init containers in the Pod specification alongside the `containers`\narray (which describes app containers).", "zh": "你可以在 Pod 的规约中与用来描述应用容器的 `containers` 数组平行的位置指定\nInit 容器。"}
{"en": "In Kubernetes, a [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/) is a container that\nstarts before the main application container and _continues to run_. This document is about init containers:\ncontainers that run to completion during Pod initialization.", "zh": "在 Kubernetes 中，[边车容器](/zh-cn/docs/concepts/workloads/pods/sidecar-containers/)\n是在主应用容器之前启动并**持续运行**的容器。本文介绍 Init 容器：在 Pod 初始化期间完成运行的容器。"}
{"en": "## Understanding init containers\n\nA {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} can have multiple containers\nrunning apps within it, but it can also have one or more init containers, which are run\nbefore the app containers are started.", "zh": "## 理解 Init 容器   {#understanding-init-containers}\n\n每个 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 中可以包含多个容器，\n应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。"}
{"en": "Init containers are exactly like regular containers, except:\n\n* Init containers always run to completion.\n* Each init container must complete successfully before the next one starts.", "zh": "Init 容器与普通的容器非常像，除了如下两点：\n\n* 它们总是运行到完成。\n* 每个都必须在下一个启动之前成功完成。"}
{"en": "If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.\nHowever, if the Pod has a `restartPolicy` of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.", "zh": "如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。\n然而，如果 Pod 对应的 `restartPolicy` 值为 \"Never\"，并且 Pod 的 Init 容器失败，\n则 Kubernetes 会将整个 Pod 状态设置为失败。"}
{"en": "To specify an init container for a Pod, add the `initContainers` field into\nthe [Pod specification](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec),\nas an array of `container` items (similar to the app `containers` field and its contents).\nSee [Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container) in the\nAPI reference for more details.\n\nThe status of the init containers is returned in `.status.initContainerStatuses`\nfield as an array of the container statuses (similar to the `.status.containerStatuses`\nfield).", "zh": "为 Pod 设置 Init 容器需要在\n[Pod 规约](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec)中添加 `initContainers` 字段，\n该字段以 [Container](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#container-v1-core)\n类型对象数组的形式组织，和应用的 `containers` 数组同级相邻。\n参阅 API 参考的[容器](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container)章节了解详情。\n\nInit 容器的状态在 `status.initContainerStatuses` 字段中以容器状态数组的格式返回\n（类似 `status.containerStatuses` 字段）。"}
{"en": "### Differences from regular containers\n\nInit containers support all the fields and features of app containers,\nincluding resource limits, [volumes](/docs/concepts/storage/volumes/), and security settings. However, the\nresource requests and limits for an init container are handled differently,\nas documented in [Resource sharing within containers](#resource-sharing-within-containers).", "zh": "### 与普通容器的不同之处   {#differences-from-regular-containers}\n\nInit 容器支持应用容器的全部字段和特性，包括资源限制、\n[数据卷](/zh-cn/docs/concepts/storage/volumes/)和安全设置。\n然而，Init 容器对资源请求和限制的处理稍有不同，\n在下面[容器内的资源共享](#resource-sharing-within-containers)节有说明。"}
{"en": "Regular init containers (in other words: excluding sidecar containers) do not support the\n`lifecycle`, `livenessProbe`, `readinessProbe`, or `startupProbe` fields. Init containers\nmust run to completion before the Pod can be ready; sidecar containers continue running\nduring a Pod's lifetime, and _do_ support some probes. See [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nfor further details about sidecar containers.", "zh": "常规的 Init 容器（即不包括边车容器）不支持 `lifecycle`、`livenessProbe`、`readinessProbe` 或\n`startupProbe` 字段。Init 容器必须在 Pod 准备就绪之前完成运行；而边车容器在 Pod 的生命周期内继续运行，\n它支持一些探针。有关边车容器的细节请参阅[边车容器](/zh-cn/docs/concepts/workloads/pods/sidecar-containers/)。"}
{"en": "If you specify multiple init containers for a Pod, kubelet runs each init\ncontainer sequentially. Each init container must succeed before the next can run.\nWhen all of the init containers have run to completion, kubelet initializes\nthe application containers for the Pod and runs them as usual.", "zh": "如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。\n每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时，\nKubernetes 才会为 Pod 初始化应用容器并像平常一样运行。"}
{"en": "### Differences from sidecar containers\n\nInit containers run and complete their tasks before the main application container starts.\nUnlike [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers),\ninit containers are not continuously running alongside the main containers.", "zh": "### 与边车容器的不同之处   {#differences-from-sidecar-containers}\n\nInit 容器在主应用容器启动之前运行并完成其任务。\n与[边车容器](/zh-cn/docs/concepts/workloads/pods/sidecar-containers)不同，\nInit 容器不会持续与主容器一起运行。"}
{"en": "Init containers run to completion sequentially, and the main container does not start\nuntil all the init containers have successfully completed.\n\ninit containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or\n`startupProbe` whereas sidecar containers support all these [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.", "zh": "Init 容器按顺序完成运行，等到所有 Init 容器成功完成之后，主容器才会启动。\n\nInit 容器不支持 `lifecycle`、`livenessProbe`、`readinessProbe` 或 `startupProbe`，\n而边车容器支持所有这些[探针](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe)以控制其生命周期。"}
{"en": "Init containers share the same resources (CPU, memory, network) with the main application\ncontainers but do not interact directly with them. They can, however, use shared volumes\nfor data exchange.", "zh": "Init 容器与主应用容器共享资源（CPU、内存、网络），但不直接与主应用容器进行交互。\n不过这些容器可以使用共享卷进行数据交换。"}
{"en": "## Using init containers\n\nBecause init containers have separate images from app containers, they\nhave some advantages for start-up related code:\n\n* Init containers can contain utilities or custom code for setup that are not present in an app\n  image. For example, there is no need to make an image `FROM` another image just to use a tool like\n  `sed`, `awk`, `python`, or `dig` during setup.\n* The application image builder and deployer roles can work independently without\n  the need to jointly build a single app image.", "zh": "## 使用 Init 容器   {#using-init-containers}\n\n因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势：\n\n* Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。\n  例如，没有必要仅为了在安装过程中使用类似 `sed`、`awk`、`python` 或 `dig`\n  这样的工具而去 `FROM` 一个镜像来生成一个新的镜像。\n\n* 应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。"}
{"en": "* Init containers can run with a different view of the filesystem than app containers in the\n  same Pod. Consequently, they can be given access to\n  {{< glossary_tooltip text=\"Secrets\" term_id=\"secret\" >}} that app containers cannot access.\n* Because init containers run to completion before any app containers start, init containers offer\n  a mechanism to block or delay app container startup until a set of preconditions are met. Once\n  preconditions are met, all of the app containers in a Pod can start in parallel.\n* Init containers can securely run utilities or custom code that would otherwise make an app\n  container image less secure. By keeping unnecessary tools separate you can limit the attack\n  surface of your app container image.", "zh": "* 与同一 Pod 中的多个应用容器相比，Init 容器能以不同的文件系统视图运行。因此，Init\n  容器可以被赋予访问应用容器不能访问的 {{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}} 的权限。\n\n* 由于 Init 容器必须在应用容器启动之前运行完成，因此 Init\n  容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。\n  一旦前置条件满足，Pod 内的所有的应用容器会并行启动。\n\n* Init 容器可以安全地运行实用程序或自定义代码，而在其他方式下运行这些实用程序或自定义代码可能会降低应用容器镜像的安全性。\n  通过将不必要的工具分开，你可以限制应用容器镜像的被攻击范围。"}
{"en": "### Examples\n\nHere are some ideas for how to use init containers:\n\n* Wait for a {{< glossary_tooltip text=\"Service\" term_id=\"service\">}} to\n  be created, using a shell one-line command like:", "zh": "### 示例  {#examples}\n\n下面是一些如何使用 Init 容器的想法：\n\n* 等待一个 Service 完成创建，通过类似如下 Shell 命令：\n\n  ```shell\n  for i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1\n  ```"}
{"en": "* Register this Pod with a remote server from the downward API with a command like:", "zh": "* 注册这个 Pod 到远程服务器，通过在命令中调用 API，类似如下：\n\n  ```shell\n  curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'\n  ```"}
{"en": "* Wait for some time before starting the app container with a command like", "zh": "* 在启动应用容器之前等一段时间，使用类似命令：\n\n  ```shell\n  sleep 60\n  ```"}
{"en": "* Clone a Git repository into a {{< glossary_tooltip text=\"Volume\" term_id=\"volume\" >}}\n\n* Place values into a configuration file and run a template tool to dynamically\n  generate a configuration file for the main app container. For example,\n  place the `POD_IP` value in a configuration and generate the main app\n  configuration file using Jinja.", "zh": "* 克隆 Git 仓库到{{< glossary_tooltip text=\"卷\" term_id=\"volume\" >}}中。\n\n* 将配置值放到配置文件中，运行模板工具为主应用容器动态地生成配置文件。\n  例如，在配置文件中存放 `POD_IP` 值，并使用 Jinja 生成主应用配置文件。"}
{"en": "#### Init containers in use\n\nThis example defines a simple Pod that has two init containers.\nThe first waits for `myservice`, and the second waits for `mydb`. Once both\ninit containers complete, the Pod runs the app container from its `spec` section.", "zh": "### 使用 Init 容器的情况   {#init-containers-in-use}\n\n下面的例子定义了一个具有 2 个 Init 容器的简单 Pod。 第一个等待 `myservice` 启动，\n第二个等待 `mydb` 启动。 一旦这两个 Init 容器都启动完成，Pod 将启动 `spec` 节中的应用容器。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app.kubernetes.io/name: MyApp\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! && sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox:1.28\n    command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"]\n  - name: init-mydb\n    image: busybox:1.28\n    command: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"]\n```"}
{"en": "You can start this Pod by running:", "zh": "你通过运行下面的命令启动 Pod：\n\n```shell\nkubectl apply -f myapp.yaml\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\npod/myapp-pod created\n```"}
{"en": "And check on its status with:", "zh": "使用下面的命令检查其状态：\n\n```shell\nkubectl get -f myapp.yaml\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nNAME        READY     STATUS     RESTARTS   AGE\nmyapp-pod   0/1       Init:0/2   0          6m\n```"}
{"en": "or for more details:", "zh": "或者查看更多详细信息：\n\n```shell\nkubectl describe -f myapp.yaml\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nName:          myapp-pod\nNamespace:     default\n[...]\nLabels:        app.kubernetes.io/name=MyApp\nStatus:        Pending\n[...]\nInit Containers:\n  init-myservice:\n[...]\n    State:         Running\n[...]\n  init-mydb:\n[...]\n    State:         Waiting\n      Reason:      PodInitializing\n    Ready:         False\n[...]\nContainers:\n  myapp-container:\n[...]\n    State:         Waiting\n      Reason:      PodInitializing\n    Ready:         False\n[...]\nEvents:\n  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message\n  ---------    --------    -----    ----                      -------------                           --------      ------        -------\n  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201\n  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image \"busybox\"\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image \"busybox\"\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container init-myservice\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container init-myservice\n```"}
{"en": "To see logs for the init containers in this Pod, run:", "zh": "如需查看 Pod 内 Init 容器的日志，请执行：\n\n```shell\nkubectl logs myapp-pod -c init-myservice # 查看第一个 Init 容器\nkubectl logs myapp-pod -c init-mydb      # 查看第二个 Init 容器\n```"}
{"en": "At this point, those init containers will be waiting to discover {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}} named\n`mydb` and `myservice`.\n\nHere's a configuration you can use to make those Services appear:", "zh": "在这一刻，Init 容器将会等待至发现名称为 `mydb` 和 `myservice`\n的{{< glossary_tooltip text=\"服务\" term_id=\"service\" >}}。\n\n如下为创建这些 Service 的配置文件：\n\n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: myservice\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 9376\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mydb\nspec:\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 9377\n```"}
{"en": "To create the `mydb` and `myservice` services:", "zh": "创建 `mydb` 和 `myservice` 服务的命令：\n\n```shell\nkubectl apply -f services.yaml\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nservice/myservice created\nservice/mydb created\n```"}
{"en": "You'll then see that those init containers complete, and that the `myapp-pod`\nPod moves into the Running state:", "zh": "这样你将能看到这些 Init 容器执行完毕，随后 `my-app` 的 Pod 进入 `Running` 状态：\n\n```shell\nkubectl get -f myapp.yaml\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nNAME        READY     STATUS    RESTARTS   AGE\nmyapp-pod   1/1       Running   0          9m\n```"}
{"en": "This simple example should provide some inspiration for you to create your own\ninit containers. [What's next](#what-s-next) contains a link to a more detailed example.", "zh": "这个简单例子应该能为你创建自己的 Init 容器提供一些启发。\n[接下来](#what-s-next)节提供了更详细例子的链接。"}
{"en": "## Detailed behavior\n\nDuring Pod startup, the kubelet delays running init containers until the networking\nand storage are ready. Then the kubelet runs the Pod's init containers in the order\nthey appear in the Pod's spec.", "zh": "## 具体行为 {#detailed-behavior}\n\n在 Pod 启动过程中，每个 Init 容器会在网络和数据卷初始化之后按顺序启动。\nkubelet 运行依据 Init 容器在 Pod 规约中的出现顺序依次运行之。"}
{"en": "Each init container must exit successfully before\nthe next container starts. If a container fails to start due to the runtime or\nexits with failure, it is retried according to the Pod `restartPolicy`. However,\nif the Pod `restartPolicy` is set to Always, the init containers use\n`restartPolicy` OnFailure.", "zh": "每个 Init 容器成功退出后才会启动下一个 Init 容器。\n如果某容器因为容器运行时的原因无法启动，或以错误状态退出，kubelet 会根据\nPod 的 `restartPolicy` 策略进行重试。\n然而，如果 Pod 的 `restartPolicy` 设置为 \"Always\"，Init 容器失败时会使用\n`restartPolicy` 的 \"OnFailure\" 策略。"}
{"en": "A Pod cannot be `Ready` until all init containers have succeeded. The ports on an\ninit container are not aggregated under a Service. A Pod that is initializing\nis in the `Pending` state but should have a condition `Initialized` set to false.\n\nIf the Pod [restarts](#pod-restart-reasons), or is restarted, all init containers\nmust execute again.", "zh": "在所有的 Init 容器没有成功之前，Pod 将不会变成 `Ready` 状态。\nInit 容器的端口将不会在 Service 中进行聚集。正在初始化中的 Pod 处于 `Pending` 状态，\n但会将状况 `Initializing` 设置为 false。\n\n如果 Pod [重启](#pod-restart-reasons)，所有 Init 容器必须重新执行。"}
{"en": "Changes to the init container spec are limited to the container image field.\nDirectly altering the `image` field of  an init container does _not_ restart the\nPod or trigger its recreation. If the Pod has yet to start, that change may\nhave an effect on how the Pod boots up.\n\nFor a [pod template](/docs/concepts/workloads/pods/#pod-templates)\nyou can typically change any field for an init container; the impact of making\nthat change depends on where the pod template is used.\n\nBecause init containers can be restarted, retried, or re-executed, init container\ncode should be idempotent. In particular, code that writes into any `emptyDir` volume\nshould be prepared for the possibility that an output file already exists.", "zh": "对 Init 容器规约的修改仅限于容器的 `image` 字段。\n直接更改 Init 容器的 `image` 字段**不会**重启该 Pod 或触发其重新创建。如果该 Pod 尚未启动，则该更改可能会影响 Pod 的启动方式。\n\n对于 [Pod 模板](/zh-cn/docs/concepts/workloads/pods/#pod-templates)，你通常可以更改 Init 容器的任何字段；更改的影响取决于 Pod 模板的使用位置。\n\n因为 Init 容器可能会被重启、重试或者重新执行，所以 Init 容器的代码应该是幂等的。\n特别地，向任何 `emptyDir` 卷写入数据的代码应该对输出文件可能已经存在做好准备。"}
{"en": "Init containers have all of the fields of an app container. However, Kubernetes\nprohibits `readinessProbe` from being used because init containers cannot\ndefine readiness distinct from completion. This is enforced during validation.", "zh": "Init 容器具有应用容器的所有字段。然而 Kubernetes 禁止使用 `readinessProbe`，\n因为 Init 容器不能定义不同于完成态（Completion）的就绪态（Readiness）。\nKubernetes 会在校验时强制执行此检查。"}
{"en": "Use `activeDeadlineSeconds` on the Pod to prevent init containers from failing forever.\nThe active deadline includes init containers.\nHowever it is recommended to use `activeDeadlineSeconds` only if teams deploy their application\nas a Job, because `activeDeadlineSeconds` has an effect even after initContainer finished.\nThe Pod which is already running correctly would be killed by `activeDeadlineSeconds` if you set.\n\nThe name of each app and init container in a Pod must be unique; a\nvalidation error is thrown for any container sharing a name with another.", "zh": "在 Pod 上使用 `activeDeadlineSeconds` 和在容器上使用 `livenessProbe` 可以避免\nInit 容器一直重复失败。\n`activeDeadlineSeconds` 时间包含了 Init 容器启动的时间。\n但建议仅在团队将其应用程序部署为 Job 时才使用 `activeDeadlineSeconds`，\n因为 `activeDeadlineSeconds` 在 Init 容器结束后仍有效果。\n如果你设置了 `activeDeadlineSeconds`，已经在正常运行的 Pod 会被杀死。\n\n在 Pod 中的每个应用容器和 Init 容器的名称必须唯一；\n与任何其它容器共享同一个名称，会在校验时抛出错误。"}
{"en": "#### Resource sharing within containers\n\nGiven the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:", "zh": "#### 容器内的资源共享   {#resource-sharing-within-containers}\n\n在给定的 Init、边车和应用容器执行顺序下，资源使用适用于如下规则："}
{"en": "* The highest of any particular resource request or limit defined on all init\n  containers is the *effective init request/limit*. If any resource has no\n  resource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the higher of:\n  * the sum of all app containers request/limit for a resource\n  * the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\n  init containers can reserve resources for initialization that are not used\n  during the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\n  QoS tier for init containers and app containers alike.", "zh": "* 所有 Init 容器上定义的任何特定资源的 limit 或 request 的最大值，作为\n  Pod **有效初始 request/limit**。\n  如果任何资源没有指定资源限制，这被视为最高限制。\n* Pod 对资源的 **有效 limit/request** 是如下两者中的较大者：\n  * 所有应用容器对某个资源的 limit/request 之和\n  * 对某个资源的有效初始 limit/request\n* 基于有效 limit/request 完成调度，这意味着 Init 容器能够为初始化过程预留资源，\n  这些资源在 Pod 生命周期过程中并没有被使用。\n* Pod 的 **有效 QoS 层**，与 Init 容器和应用容器的一样。"}
{"en": "Quota and limits are applied based on the effective Pod request and\nlimit.", "zh": "配额和限制适用于有效 Pod 的请求和限制值。"}
{"en": "### Init containers and Linux cgroups {#cgroups}\n\nOn Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.", "zh": "### Init 容器和 Linux cgroup    {#cgroups}\n\n在 Linux 上，Pod 级别的 CGroup 资源分配基于 Pod 的有效请求和限制值，与调度程序相同。"}
{"en": "### Pod restart reasons\n\nA Pod can restart, causing re-execution of init containers, for the following\nreasons:", "zh": "### Pod 重启的原因  {#pod-restart-reasons}\n\nPod 重启会导致 Init 容器重新执行，主要有如下几个原因：\n\n{{< comment >}}"}
{"en": "This section also present under [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/) page.\nIf you're editing this section, change both places.", "zh": "这部分内容也出现在[边车容器](/zh-cn/docs/concepts/workloads/pods/sidecar-containers/)页面上。\n如果你正在编辑这部分内容，请同时修改两处。\n{{< /comment >}}"}
{"en": "* The Pod infrastructure container is restarted. This is uncommon and would\n  have to be done by someone with root access to nodes.\n* All containers in a Pod are terminated while `restartPolicy` is set to Always,\n  forcing a restart, and the init container completion record has been lost due\n  to {{< glossary_tooltip text=\"garbage collection\" term_id=\"garbage-collection\" >}}.", "zh": "* Pod 的基础设施容器 (译者注：如 `pause` 容器) 被重启。这种情况不多见，\n  必须由具备 root 权限访问节点的人员来完成。\n\n* 当 `restartPolicy` 设置为 `Always`，Pod 中所有容器会终止而强制重启。\n  由于{{< glossary_tooltip text=\"垃圾回收\" term_id=\"garbage-collection\" >}}机制的原因，\n  Init 容器的完成记录将会丢失。"}
{"en": "The Pod will not be restarted when the init container image is changed, or the\ninit container completion record has been lost due to garbage collection. This\napplies for Kubernetes v1.20 and later. If you are using an earlier version of\nKubernetes, consult the documentation for the version you are using.", "zh": "当 Init 容器的镜像发生改变或者 Init 容器的完成记录因为垃圾收集等原因被丢失时，\nPod 不会被重启。这一行为适用于 Kubernetes v1.20 及更新版本。\n如果你在使用较早版本的 Kubernetes，可查阅你所使用的版本对应的文档。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "Learn more about the following:\n* [Creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* [Debug init containers](/docs/tasks/debug/debug-application/debug-init-containers/).\n* Overview of [kubelet](/docs/reference/command-line-tools-reference/kubelet/) and [kubectl](/docs/reference/kubectl/).\n* [Types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* [Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers).", "zh": "进一步了解以下内容：\n\n* [创建包含 Init 容器的 Pod](/zh-cn/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container)\n* [调试 Init 容器](/zh-cn/docs/tasks/debug/debug-application/debug-init-containers/)\n* [kubelet](/zh-cn/docs/reference/command-line-tools-reference/kubelet/) 和\n  [kubectl](/zh-cn/docs/reference/kubectl/) 的概述。\n* [探针类型](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe)：\n  存活态探针、就绪态探针、启动探针。\n* [边车容器](/zh-cn/docs/concepts/workloads/pods/sidecar-containers)。"}
{"en": "overview", "zh": "{{< feature-state state=\"stable\" for_k8s_version=\"v1.25\" >}}"}
{"en": "This page provides an overview of ephemeral containers: a special type of container\nthat runs temporarily in an existing {{< glossary_tooltip term_id=\"pod\" >}} to\naccomplish user-initiated actions such as troubleshooting. You use ephemeral\ncontainers to inspect services rather than to build applications.", "zh": "本页面概述了临时容器：一种特殊的容器，该容器在现有\n{{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}\n中临时运行，以便完成用户发起的操作，例如故障排查。\n你会使用临时容器来检查服务，而不是用它来构建应用程序。"}
{"en": "## Understanding ephemeral containers\n\n{{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} are the fundamental building\nblock of Kubernetes applications. Since Pods are intended to be disposable and\nreplaceable, you cannot add a container to a Pod once it has been created.\nInstead, you usually delete and replace Pods in a controlled fashion using\n{{< glossary_tooltip text=\"deployments\" term_id=\"deployment\" >}}.", "zh": "## 了解临时容器   {#understanding-ephemeral-containers}\n\n{{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 是 Kubernetes 应用程序的基本构建块。\n由于 Pod 是一次性且可替换的，因此一旦 Pod 创建，就无法将容器加入到 Pod 中。\n取而代之的是，通常使用 {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}\n以受控的方式来删除并替换 Pod。"}
{"en": "Sometimes it's necessary to inspect the state of an existing Pod, however, for\nexample to troubleshoot a hard-to-reproduce bug. In these cases you can run\nan ephemeral container in an existing Pod to inspect its state and run\narbitrary commands.", "zh": "有时有必要检查现有 Pod 的状态。例如，对于难以复现的故障进行排查。\n在这些场景中，可以在现有 Pod 中运行临时容器来检查其状态并运行任意命令。"}
{"en": "### What is an ephemeral container?\n\nEphemeral containers differ from other containers in that they lack guarantees\nfor resources or execution, and they will never be automatically restarted, so\nthey are not appropriate for building applications.  Ephemeral containers are\ndescribed using the same `ContainerSpec` as regular containers, but many fields\nare incompatible and disallowed for ephemeral containers.", "zh": "### 什么是临时容器？    {#what-is-an-ephemeral-container}\n\n临时容器与其他容器的不同之处在于，它们缺少对资源或执行的保证，并且永远不会自动重启，\n因此不适用于构建应用程序。\n临时容器使用与常规容器相同的 `ContainerSpec` 节来描述，但许多字段是不兼容和不允许的。"}
{"en": "- Ephemeral containers may not have ports, so fields such as `ports`,\n  `livenessProbe`, `readinessProbe` are disallowed.\n- Pod resource allocations are immutable, so setting `resources` is disallowed.\n- For a complete list of allowed fields, see the [EphemeralContainer reference\n  documentation](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#ephemeralcontainer-v1-core).", "zh": "- 临时容器没有端口配置，因此像 `ports`、`livenessProbe`、`readinessProbe`\n  这样的字段是不允许的。\n- Pod 资源分配是不可变的，因此 `resources` 配置是不允许的。\n- 有关允许字段的完整列表，请参见\n  [EphemeralContainer 参考文档](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#ephemeralcontainer-v1-core)。"}
{"en": "Ephemeral containers are created using a special `ephemeralcontainers` handler\nin the API rather than by adding them directly to `pod.spec`, so it's not\npossible to add an ephemeral container using `kubectl edit`.", "zh": "临时容器是使用 API 中的一种特殊的 `ephemeralcontainers` 处理器进行创建的，\n而不是直接添加到 `pod.spec` 段，因此无法使用 `kubectl edit` 来添加一个临时容器。"}
{"en": "Like regular containers, you may not change or remove an ephemeral container\nafter you have added it to a Pod.", "zh": "与常规容器一样，将临时容器添加到 Pod 后，将不能更改或删除临时容器。\n\n{{< note >}}"}
{"en": "Ephemeral containers are not supported by [static pods](/docs/tasks/configure-pod-container/static-pod/).", "zh": "临时容器不被[静态 Pod](/zh-cn/docs/tasks/configure-pod-container/static-pod/) 支持。\n{{< /note >}}"}
{"en": "## Uses for ephemeral containers\n\nEphemeral containers are useful for interactive troubleshooting when `kubectl\nexec` is insufficient because a container has crashed or a container image\ndoesn't include debugging utilities.", "zh": "## 临时容器的用途   {#uses-for-ephemeral-containers}\n\n当由于容器崩溃或容器镜像不包含调试工具而导致 `kubectl exec` 无用时，\n临时容器对于交互式故障排查很有用。"}
{"en": "In particular, [distroless images](https://github.com/GoogleContainerTools/distroless)\nenable you to deploy minimal container images that reduce attack surface\nand exposure to bugs and vulnerabilities. Since distroless images do not include a\nshell or any debugging utilities, it's difficult to troubleshoot distroless\nimages using `kubectl exec` alone.", "zh": "尤其是，[Distroless 镜像](https://github.com/GoogleContainerTools/distroless)\n允许用户部署最小的容器镜像，从而减少攻击面并减少故障和漏洞的暴露。\n由于 distroless 镜像不包含 Shell 或任何的调试工具，因此很难单独使用\n`kubectl exec` 命令进行故障排查。"}
{"en": "When using ephemeral containers, it's helpful to enable [process namespace\nsharing](/docs/tasks/configure-pod-container/share-process-namespace/) so\nyou can view processes in other containers.", "zh": "使用临时容器时，\n启用[进程名字空间共享](/zh-cn/docs/tasks/configure-pod-container/share-process-namespace/)很有帮助，\n可以查看其他容器中的进程。\n\n{{% heading \"whatsnext\" %}}"}
{"en": "* Learn how to [debug pods using ephemeral containers](/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container).", "zh": "* 了解如何[使用临时调试容器来进行调试](/zh-cn/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container)"}
{"en": "_Pods_ are the smallest deployable units of computing that you can create and manage in Kubernetes.\n\nA _Pod_ (as in a pod of whales or pea pod) is a group of one or more\n{{< glossary_tooltip text=\"containers\" term_id=\"container\" >}}, with shared storage and network resources,\nand a specification for how to run the containers. A Pod's contents are always co-located and\nco-scheduled, and run in a shared context. A Pod models an\napplication-specific \"logical host\": it contains one or more application\ncontainers which are relatively tightly coupled.\nIn non-cloud contexts, applications executed on the same physical or virtual machine are\nanalogous to cloud applications executed on the same logical host.", "zh": "**Pod** 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。\n\n**Pod**（就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个）\n{{< glossary_tooltip text=\"容器\" term_id=\"container\" >}}；\n这些容器共享存储、网络、以及怎样运行这些容器的规约。\nPod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。\nPod 所建模的是特定于应用的 “逻辑主机”，其中包含一个或多个应用容器，\n这些容器相对紧密地耦合在一起。\n在非云环境中，在相同的物理机或虚拟机上运行的应用类似于在同一逻辑主机上运行的云应用。"}
{"en": "As well as application containers, a Pod can contain\n{{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}} that run\nduring Pod startup. You can also inject\n{{< glossary_tooltip text=\"ephemeral containers\" term_id=\"ephemeral-container\" >}}\nfor debugging a running Pod.", "zh": "除了应用容器，Pod 还可以包含在 Pod 启动期间运行的\n{{< glossary_tooltip text=\"Init 容器\" term_id=\"init-container\" >}}。\n你也可以注入{{< glossary_tooltip text=\"临时性容器\" term_id=\"ephemeral-container\" >}}来调试正在运行的 Pod。"}
{"en": "## What is a Pod?", "zh": "## 什么是 Pod？   {#what-is-a-pod}\n\n{{< note >}}"}
{"en": "You need to install a [container runtime](/docs/setup/production-environment/container-runtimes/)\ninto each node in the cluster so that Pods can run there.", "zh": "为了运行 Pod，你需要提前在每个节点安装好[容器运行时](/zh-cn/docs/setup/production-environment/container-runtimes/)。\n{{< /note >}}"}
{"en": "The shared context of a Pod is a set of Linux namespaces, cgroups, and\npotentially other facets of isolation - the same things that isolate a {{< glossary_tooltip text=\"container\" term_id=\"container\" >}}.\nWithin a Pod's context, the individual applications may have\nfurther sub-isolations applied.\n\nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.", "zh": "Pod 的共享上下文包括一组 Linux 名字空间、控制组（cgroup）和可能一些其他的隔离方面，\n即用来隔离{{< glossary_tooltip text=\"容器\" term_id=\"container\" >}}的技术。\n在 Pod 的上下文中，每个独立的应用可能会进一步实施隔离。\n\nPod 类似于共享名字空间并共享文件系统卷的一组容器。"}
{"en": "Pods in a Kubernetes cluster are used in two main ways:\n\n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\n  most common Kubernetes use case; in this case, you can think of a Pod as a\n  wrapper around a single container; Kubernetes manages Pods rather than managing\n  the containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\n  encapsulate an application composed of\n  [multiple co-located containers](#how-pods-manage-multiple-containers) that are\n  tightly coupled and need to share resources. These co-located containers\n  form a single cohesive unit.", "zh": "Kubernetes 集群中的 Pod 主要有两种用法：\n\n* **运行单个容器的 Pod**。\"每个 Pod 一个容器\"模型是最常见的 Kubernetes 用例；\n  在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。\n* **运行多个协同工作的容器的 Pod**。\n  Pod 可以封装由紧密耦合且需要共享资源的[多个并置容器](#how-pods-manage-multiple-containers)组成的应用。\n  这些位于同一位置的容器构成一个内聚单元。"}
{"en": "Grouping multiple co-located and co-managed containers in a single Pod is a\n  relatively advanced use case. You should use this pattern only in specific\n  instances in which your containers are tightly coupled.\n  \n  You don't need to run multiple containers to provide replication (for resilience\n  or capacity); if you need multiple replicas, see\n  [Workload management](/docs/concepts/workloads/controllers/).", "zh": "将多个并置、同管的容器组织到一个 Pod 中是一种相对高级的使用场景。\n  只有在一些场景中，容器之间紧密关联时你才应该使用这种模式。\n\n  你不需要运行多个容器来扩展副本（为了弹性或容量）；\n  如果你需要多个副本，请参阅[工作负载管理](/zh-cn/docs/concepts/workloads/controllers/)。"}
{"en": "## Using Pods\n\nThe following is an example of a Pod which consists of a container running the image `nginx:1.14.2`.", "zh": "## 使用 Pod   {#using-pods}\n\n下面是一个 Pod 示例，它由一个运行镜像 `nginx:1.14.2` 的容器组成。\n\n{{% code_sample file=\"pods/simple-pod.yaml\" %}}"}
{"en": "To create the Pod shown above, run the following command:", "zh": "要创建上面显示的 Pod，请运行以下命令：\n\n```shell\nkubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml\n```"}
{"en": "Pods are generally not created directly and are created using workload resources.\nSee [Working with Pods](#working-with-pods) for more information on how Pods are used\nwith workload resources.\n\n### Workload resources for managing pods", "zh": "Pod 通常不是直接创建的，而是使用工作负载资源创建的。\n有关如何将 Pod 用于工作负载资源的更多信息，请参阅[使用 Pod](#working-with-pods)。\n\n### 用于管理 Pod 的工作负载资源   {#workload-resources-for-managing-pods}"}
{"en": "Usually you don't need to create Pods directly, even singleton Pods. Instead,\ncreate them using workload resources such as {{< glossary_tooltip text=\"Deployment\"\nterm_id=\"deployment\" >}} or {{< glossary_tooltip text=\"Job\" term_id=\"job\" >}}.\nIf your Pods need to track state, consider the\n{{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}} resource.", "zh": "通常你不需要直接创建 Pod，甚至单实例 Pod。相反，你会使用诸如\n{{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}} 或\n{{< glossary_tooltip text=\"Job\" term_id=\"job\" >}} 这类工作负载资源来创建 Pod。\n如果 Pod 需要跟踪状态，可以考虑\n{{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}} 资源。"}
{"en": "Each Pod is meant to run a single instance of a given application. If you want to\nscale your application horizontally (to provide more overall resources by running\nmore instances), you should use multiple Pods, one for each instance. In\nKubernetes, this is typically referred to as _replication_.\nReplicated Pods are usually created and managed as a group by a workload resource\nand its {{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}.\n\nSee [Pods and controllers](#pods-and-controllers) for more information on how\nKubernetes uses workload resources, and their controllers, to implement application\nscaling and auto-healing.", "zh": "每个 Pod 都旨在运行给定应用程序的单个实例。如果希望横向扩展应用程序\n（例如，运行多个实例以提供更多的资源），则应该使用多个 Pod，每个实例使用一个 Pod。\n在 Kubernetes 中，这通常被称为**副本（Replication）**。\n通常使用一种工作负载资源及其{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}来创建和管理一组 Pod 副本。\n\n参见 [Pod 和控制器](#pods-and-controllers)以了解 Kubernetes\n如何使用工作负载资源及其控制器以实现应用的扩缩和自动修复。"}
{"en": "Pods natively provide two kinds of shared resources for their constituent containers:\n[networking](#pod-networking) and [storage](#pod-storage).", "zh": "Pod 天生地为其成员容器提供了两种共享资源：[网络](#pod-networking)和[存储](#pod-storage)。"}
{"en": "## Working with Pods\n\nYou'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This\nis because Pods are designed as relatively ephemeral, disposable entities. When\na Pod gets created (directly by you, or indirectly by a\n{{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}), the new Pod is\nscheduled to run on a {{< glossary_tooltip term_id=\"node\" >}} in your cluster.\nThe Pod remains on that node until the Pod finishes execution, the Pod object is deleted,\nthe Pod is *evicted* for lack of resources, or the node fails.", "zh": "## 使用 Pod   {#working-with-pods}\n\n你很少在 Kubernetes 中直接创建一个个的 Pod，甚至是单实例（Singleton）的 Pod。\n这是因为 Pod 被设计成了相对临时性的、用后即抛的一次性实体。\n当 Pod 由你或者间接地由{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}\n创建时，它被调度在集群中的{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}上运行。\nPod 会保持在该节点上运行，直到 Pod 结束执行、Pod 对象被删除、Pod 因资源不足而被**驱逐**或者节点失效为止。\n\n{{< note >}}"}
{"en": "Restarting a container in a Pod should not be confused with restarting a Pod. A Pod\nis not a process, but an environment for running container(s). A Pod persists until\nit is deleted.", "zh": "重启 Pod 中的容器不应与重启 Pod 混淆。\nPod 不是进程，而是容器运行的环境。\n在被删除之前，Pod 会一直存在。\n{{< /note >}}"}
{"en": "The name of a Pod must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostname.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).", "zh": "Pod 的名称必须是一个合法的\n[DNS 子域](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)值，\n但这可能对 Pod 的主机名产生意外的结果。为获得最佳兼容性，名称应遵循更严格的\n[DNS 标签](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-label-names)规则。"}
{"en": "### Pod OS", "zh": "### Pod 操作系统   {#pod-os}\n\n{{< feature-state state=\"stable\" for_k8s_version=\"v1.25\" >}}"}
{"en": "You should set the `.spec.os.name` field to either `windows` or `linux` to indicate the OS on\nwhich you want the pod to run. These two are the only operating systems supported for now by\nKubernetes. In the future, this list may be expanded.\n\nIn Kubernetes v{{< skew currentVersion >}}, the value of `.spec.os.name` does not affect\nhow the {{< glossary_tooltip text=\"kube-scheduler\" term_id=\"kube-scheduler\" >}}\npicks a node for the Pod to run on. In any cluster where there is more than one operating system for\nrunning nodes, you should set the\n[kubernetes.io/os](/docs/reference/labels-annotations-taints/#kubernetes-io-os)\nlabel correctly on each node, and define pods with a `nodeSelector` based on the operating system\nlabel. The kube-scheduler assigns your pod to a node based on other criteria and may or may not\nsucceed in picking a suitable node placement where the node OS is right for the containers in that Pod.\nThe [Pod security standards](/docs/concepts/security/pod-security-standards/) also use this\nfield to avoid enforcing policies that aren't relevant to the operating system.", "zh": "你应该将 `.spec.os.name` 字段设置为 `windows` 或 `linux` 以表示你希望 Pod 运行在哪个操作系统之上。\n这两个是 Kubernetes 目前支持的操作系统。将来，这个列表可能会被扩充。\n\n在 Kubernetes v{{< skew currentVersion >}} 中，`.spec.os.name` 的值对\n{{< glossary_tooltip text=\"kube-scheduler\" term_id=\"kube-scheduler\" >}}\n如何选择要运行 Pod 的节点没有影响。在任何有多种操作系统运行节点的集群中，你应该在每个节点上正确设置\n[kubernetes.io/os](/zh-cn/docs/reference/labels-annotations-taints/#kubernetes-io-os)\n标签，并根据操作系统标签为 Pod 设置 `nodeSelector` 字段。\nkube-scheduler 将根据其他标准将你的 Pod 分配到节点，\n并且可能会也可能不会成功选择合适的节点位置，其中节点操作系统适合该 Pod 中的容器。\n[Pod 安全标准](/zh-cn/docs/concepts/security/pod-security-standards/)也使用这个字段来避免强制执行与该操作系统无关的策略。"}
{"en": "### Pods and controllers\n\nYou can use workload resources to create and manage multiple Pods for you. A controller\nfor the resource handles replication and rollout and automatic healing in case of\nPod failure. For example, if a Node fails, a controller notices that Pods on that\nNode have stopped working and creates a replacement Pod. The scheduler places the\nreplacement Pod onto a healthy Node.\n\nHere are some examples of workload resources that manage one or more Pods:", "zh": "### Pod 和控制器    {#pods-and-controllers}\n\n你可以使用工作负载资源来创建和管理多个 Pod。\n资源的控制器能够处理副本的管理、上线，并在 Pod 失效时提供自愈能力。\n例如，如果一个节点失败，控制器注意到该节点上的 Pod 已经停止工作，\n就可以创建替换性的 Pod。调度器会将替身 Pod 调度到一个健康的节点执行。\n\n下面是一些管理一个或者多个 Pod 的工作负载资源的示例：\n\n* {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}\n* {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}\n* {{< glossary_tooltip text=\"DaemonSet\" term_id=\"daemonset\" >}}"}
{"en": "### Pod templates\n\nControllers for {{< glossary_tooltip text=\"workload\" term_id=\"workload\" >}} resources create Pods\nfrom a _pod template_ and manage those Pods on your behalf.\n\nPodTemplates are specifications for creating Pods, and are included in workload resources such as\n[Deployments](/docs/concepts/workloads/controllers/deployment/),\n[Jobs](/docs/concepts/workloads/controllers/job/), and\n[DaemonSets](/docs/concepts/workloads/controllers/daemonset/).", "zh": "### Pod 模板    {#pod-templates}\n\n{{< glossary_tooltip text=\"工作负载\" term_id=\"workload\" >}}资源的控制器通常使用\n**Pod 模板（Pod Template）** 来替你创建 Pod 并管理它们。\n\nPod 模板是包含在工作负载对象中的规范，用来创建 Pod。这类负载资源包括\n[Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/)、\n[Job](/zh-cn/docs/concepts/workloads/controllers/job/) 和\n[DaemonSet](/zh-cn/docs/concepts/workloads/controllers/daemonset/) 等。"}
{"en": "Each controller for a workload resource uses the `PodTemplate` inside the workload\nobject to make actual Pods. The `PodTemplate` is part of the desired state of whatever\nworkload resource you used to run your app.\n\nWhen you create a Pod, you can include\n[environment variables](/docs/tasks/inject-data-application/define-environment-variable-container/)\nin the Pod template for the containers that run in the Pod.\n\nThe sample below is a manifest for a simple Job with a `template` that starts one\ncontainer. The container in that Pod prints a message then pauses.", "zh": "工作负载的控制器会使用负载对象中的 `PodTemplate` 来生成实际的 Pod。\n`PodTemplate` 是你用来运行应用时指定的负载资源的目标状态的一部分。\n\n创建 Pod 时，你可以在 Pod 模板中包含 Pod\n中运行的容器的[环境变量](/zh-cn/docs/tasks/inject-data-application/define-environment-variable-container/)。\n\n下面的示例是一个简单的 Job 的清单，其中的 `template` 指示启动一个容器。\n该 Pod 中的容器会打印一条消息之后暂停。"}
{"en": "# This is the pod template\n# The pod template ends here", "zh": "```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hello\nspec:\n  template:\n    # 这里是 Pod 模板\n    spec:\n      containers:\n      - name: hello\n        image: busybox:1.28\n        command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600']\n      restartPolicy: OnFailure\n    # 以上为 Pod 模板\n```"}
{"en": "Modifying the pod template or switching to a new pod template has no direct effect\non the Pods that already exist. If you change the pod template for a workload\nresource, that resource needs to create replacement Pods that use the updated template.\n\nFor example, the StatefulSet controller ensures that the running Pods match the current\npod template for each StatefulSet object. If you edit the StatefulSet to change its pod\ntemplate, the StatefulSet starts to create new Pods based on the updated template.\nEventually, all of the old Pods are replaced with new Pods, and the update is complete.\n\nEach workload resource implements its own rules for handling changes to the Pod template.\nIf you want to read more about StatefulSet specifically, read\n[Update strategy](/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets) in the StatefulSet Basics tutorial.", "zh": "修改 Pod 模板或者切换到新的 Pod 模板都不会对已经存在的 Pod 直接起作用。\n如果改变工作负载资源的 Pod 模板，工作负载资源需要使用更新后的模板来创建 Pod，\n并使用新创建的 Pod 替换旧的 Pod。\n\n例如，StatefulSet 控制器针对每个 StatefulSet 对象确保运行中的 Pod 与当前的 Pod\n模板匹配。如果编辑 StatefulSet 以更改其 Pod 模板，\nStatefulSet 将开始基于更新后的模板创建新的 Pod。\n\n每个工作负载资源都实现了自己的规则，用来处理对 Pod 模板的更新。\n如果你想了解更多关于 StatefulSet 的具体信息，\n请阅读 StatefulSet 基础教程中的[更新策略](/zh-cn/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets)。"}
{"en": "On Nodes, the {{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}} does not\ndirectly observe or manage any of the details around pod templates and updates; those\ndetails are abstracted away. That abstraction and separation of concerns simplifies\nsystem semantics, and makes it feasible to extend the cluster's behavior without\nchanging existing code.", "zh": "在节点上，{{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}} 并不直接监测或管理与\nPod 模板相关的细节或模板的更新，这些细节都被抽象出来。\n这种抽象和关注点分离简化了整个系统的语义，\n并且使得用户可以在不改变现有代码的前提下就能扩展集群的行为。"}
{"en": "## Pod update and replacement\n\nAs mentioned in the previous section, when the Pod template for a workload\nresource is changed, the controller creates new Pods based on the updated\ntemplate instead of updating or patching the existing Pods.", "zh": "## Pod 更新与替换   {#pod-update-and-replacement}\n\n正如前面章节所述，当某工作负载的 Pod 模板被改变时，\n控制器会基于更新的模板创建新的 Pod 对象而不是对现有 Pod 执行更新或者修补操作。"}
{"en": "Kubernetes doesn't prevent you from managing Pods directly. It is possible to\nupdate some fields of a running Pod, in place. However, Pod update operations\nlike \n[`patch`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#patch-pod-v1-core), and\n[`replace`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replace-pod-v1-core)\nhave some limitations:", "zh": "Kubernetes 并不禁止你直接管理 Pod。对运行中的 Pod 的某些字段执行就地更新操作还是可能的。不过，类似\n[`patch`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#patch-pod-v1-core) 和\n[`replace`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replace-pod-v1-core)\n这类更新操作有一些限制："}
{"en": "- Most of the metadata about a Pod is immutable. For example, you cannot\n  change the `namespace`, `name`, `uid`, or `creationTimestamp` fields;\n  the `generation` field is unique. It only accepts updates that increment the\n  field's current value.\n- If the `metadata.deletionTimestamp` is set, no new entry can be added to the\n  `metadata.finalizers` list.\n- Pod updates may not change fields other than `spec.containers[*].image`,\n  `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or\n  `spec.tolerations`. For `spec.tolerations`, you can only add new entries.\n- When updating the `spec.activeDeadlineSeconds` field, two types of updates\n  are allowed:\n\n  1. setting the unassigned field to a positive number; \n  1. updating the field from a positive number to a smaller, non-negative\n     number.", "zh": "- Pod 的绝大多数元数据都是不可变的。例如，你不可以改变其 `namespace`、`name`、\n  `uid` 或者 `creationTimestamp` 字段；`generation` 字段是比较特别的，\n  如果更新该字段，只能增加字段取值而不能减少。\n- 如果 `metadata.deletionTimestamp` 已经被设置，则不可以向 `metadata.finalizers`\n  列表中添加新的条目。\n- Pod 更新不可以改变除 `spec.containers[*].image`、`spec.initContainers[*].image`、\n  `spec.activeDeadlineSeconds` 或 `spec.tolerations` 之外的字段。\n  对于 `spec.tolerations`，你只被允许添加新的条目到其中。\n- 在更新 `spec.activeDeadlineSeconds` 字段时，以下两种更新操作是被允许的：\n\n  1. 如果该字段尚未设置，可以将其设置为一个正数；\n  1. 如果该字段已经设置为一个正数，可以将其设置为一个更小的、非负的整数。"}
{"en": "## Resource sharing and communication\n\nPods enable data sharing and communication among their constituent\ncontainers.", "zh": "### 资源共享和通信 {#resource-sharing-and-communication}\n\nPod 使它的成员容器间能够进行数据共享和通信。"}
{"en": "### Storage in Pods {#pod-storage}\n\nA Pod can specify a set of shared storage\n{{< glossary_tooltip text=\"volumes\" term_id=\"volume\" >}}. All containers\nin the Pod can access the shared volumes, allowing those containers to\nshare data. Volumes also allow persistent data in a Pod to survive\nin case one of the containers within needs to be restarted. See\n[Storage](/docs/concepts/storage/) for more information on how\nKubernetes implements shared storage and makes it available to Pods.", "zh": "### Pod 中的存储 {#pod-storage}\n\n一个 Pod 可以设置一组共享的存储{{< glossary_tooltip text=\"卷\" term_id=\"volume\" >}}。\nPod 中的所有容器都可以访问该共享卷，从而允许这些容器共享数据。\n卷还允许 Pod 中的持久数据保留下来，即使其中的容器需要重新启动。\n有关 Kubernetes 如何在 Pod 中实现共享存储并将其提供给 Pod 的更多信息，\n请参考[存储](/zh-cn/docs/concepts/storage/)。"}
{"en": "### Pod networking\n\nEach Pod is assigned a unique IP address for each address family. Every\ncontainer in a Pod shares the network namespace, including the IP address and\nnetwork ports. Inside a Pod (and **only** then), the containers that belong to the Pod\ncan communicate with one another using `localhost`. When containers in a Pod communicate\nwith entities *outside the Pod*,\nthey must coordinate how they use the shared network resources (such as ports).", "zh": "### Pod 联网    {#pod-networking}\n\n每个 Pod 都在每个地址族中获得一个唯一的 IP 地址。\nPod 中的每个容器共享网络名字空间，包括 IP 地址和网络端口。\n**Pod 内**的容器可以使用 `localhost` 互相通信。\n当 Pod 中的容器与 **Pod 之外**的实体通信时，它们必须协调如何使用共享的网络资源（例如端口）。"}
{"en": "Within a Pod, containers share an IP address and port space, and\ncan find each other via `localhost`. The containers in a Pod can also communicate\nwith each other using standard inter-process communications like SystemV semaphores\nor POSIX shared memory.  Containers in different Pods have distinct IP addresses\nand can not communicate by OS-level IPC without special configuration.\nContainers that want to interact with a container running in a different Pod can\nuse IP networking to communicate.", "zh": "在同一个 Pod 内，所有容器共享一个 IP 地址和端口空间，并且可以通过 `localhost` 发现对方。\n他们也能通过如 SystemV 信号量或 POSIX 共享内存这类标准的进程间通信方式互相通信。\n不同 Pod 中的容器的 IP 地址互不相同，如果没有特殊配置，就无法通过 OS 级 IPC 进行通信。\n如果某容器希望与运行于其他 Pod 中的容器通信，可以通过 IP 联网的方式实现。"}
{"en": "Containers within the Pod see the system hostname as being the same as the configured\n`name` for the Pod. There's more about this in the [networking](/docs/concepts/cluster-administration/networking/)\nsection.", "zh": "Pod 中的容器所看到的系统主机名与为 Pod 配置的 `name` 属性值相同。\n[网络](/zh-cn/docs/concepts/cluster-administration/networking/)部分提供了更多有关此内容的信息。"}
{"en": "## Pod security settings {#pod-security}", "zh": "## Pod 安全设置     {#pod-security}"}
{"en": "To set security constraints on Pods and containers, you use the\n`securityContext` field in the Pod specification. This field gives you\ngranular control over what a Pod or individual containers can do. For example:", "zh": "要对 Pod 和容器设置安全约束，请使用 Pod 规约中的 `securityContext` 字段。\n该字段使你可以精细控制 Pod 或单个容器可以执行的操作。例如："}
{"en": "* Drop specific Linux capabilities to avoid the impact of a CVE.\n* Force all processes in the Pod to run as a non-root user or as a specific\n  user or group ID.\n* Set a specific seccomp profile.\n* Set Windows security options, such as whether containers run as HostProcess.", "zh": "* 放弃特定的 Linux 权能（Capability）以避免受到某 CVE 的影响。\n* 强制 Pod 中的所有进程以非 root 用户或特定用户或组 ID 的身份运行。\n* 设置特定的 seccomp 配置文件。\n* 设置 Windows 安全选项，例如容器是否作为 HostProcess 运行。\n\n{{< caution >}}"}
{"en": "You can also use the Pod securityContext to enable\n[_privileged mode_](/docs/concepts/security/linux-kernel-security-constraints/#privileged-containers)\nin Linux containers. Privileged mode overrides many of the other security\nsettings in the securityContext. Avoid using this setting unless you can't grant\nthe equivalent permissions by using other fields in the securityContext.\nIn Kubernetes 1.26 and later, you can run Windows containers in a similarly\nprivileged mode by setting the `windowsOptions.hostProcess` flag on the\nsecurity context of the Pod spec. For details and instructions, see\n[Create a Windows HostProcess Pod](/docs/tasks/configure-pod-container/create-hostprocess-pod/).", "zh": "你还可以使用 Pod securityContext 在 Linux 容器中启用[**特权模式**](/zh-cn/docs/concepts/security/linux-kernel-security-constraints/#privileged-containers)。\n特权模式会覆盖 securityContext 中的许多其他安全设置。\n请避免使用此设置，除非你无法通过使用 securityContext 中的其他字段授予等效权限。\n在 Kubernetes 1.26 及更高版本中，你可以通过在 Pod 规约的安全上下文中设置\n`windowsOptions.hostProcess` 标志，以类似的特权模式运行 Windows 容器。\n有关详细信息和说明，请参阅[创建 Windows HostProcess Pod](/zh-cn/docs/tasks/configure-pod-container/create-hostprocess-pod/)。\n{{< /caution >}}"}
{"en": "* To learn about kernel-level security constraints that you can use,\n  see [Linux kernel security constraints for Pods and containers](/docs/concepts/security/linux-kernel-security-constraints).\n* To learn more about the Pod security context, see\n  [Configure a Security Context for a Pod or Container](/docs/tasks/configure-pod-container/security-context/).", "zh": "* 要了解可以使用的内核级安全约束，请参阅 [Pod 和容器的 Linux 内核安全约束](/zh-cn/docs/concepts/security/linux-kernel-security-constraints)。\n* 要了解有关 Pod 安全上下文的更多信息，请参阅[为 Pod 或容器配置安全上下文](/zh-cn/docs/tasks/configure-pod-container/security-context/)。"}
{"en": "## Static Pods\n\n_Static Pods_ are managed directly by the kubelet daemon on a specific node,\nwithout the {{< glossary_tooltip text=\"API server\" term_id=\"kube-apiserver\" >}}\nobserving them.\nWhereas most Pods are managed by the control plane (for example, a\n{{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}), for static\nPods, the kubelet directly supervises each static Pod (and restarts it if it fails).", "zh": "## 静态 Pod    {#static-pods}\n\n**静态 Pod（Static Pod）** 直接由特定节点上的 `kubelet` 守护进程管理，\n不需要 {{< glossary_tooltip text=\"API 服务器\" term_id=\"kube-apiserver\" >}}看到它们。\n尽管大多数 Pod 都是通过控制面（例如，{{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}）\n来管理的，对于静态 Pod 而言，`kubelet` 直接监控每个 Pod，并在其失效时重启之。"}
{"en": "Static Pods are always bound to one {{< glossary_tooltip term_id=\"kubelet\" >}} on a specific node.\nThe main use for static Pods is to run a self-hosted control plane: in other words,\nusing the kubelet to supervise the individual [control plane components](/docs/concepts/architecture/#control-plane-components).\n\nThe kubelet automatically tries to create a {{< glossary_tooltip text=\"mirror Pod\" term_id=\"mirror-pod\" >}}\non the Kubernetes API server for each static Pod.\nThis means that the Pods running on a node are visible on the API server,\nbut cannot be controlled from there. See the guide [Create static Pods](/docs/tasks/configure-pod-container/static-pod) for more information.", "zh": "静态 Pod 通常绑定到某个节点上的 {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}}。\n其主要用途是运行自托管的控制面。\n在自托管场景中，使用 `kubelet`\n来管理各个独立的[控制面组件](/zh-cn/docs/concepts/architecture/#control-plane-components)。\n\n`kubelet` 自动尝试为每个静态 Pod 在 Kubernetes API\n服务器上创建一个{{< glossary_tooltip text=\"镜像 Pod\" term_id=\"mirror-pod\" >}}。\n这意味着在节点上运行的 Pod 在 API 服务器上是可见的，但不可以通过 API 服务器来控制。\n有关更多信息，请参阅[创建静态 Pod](/zh-cn/docs/tasks/configure-pod-container/static-pod) 的指南。\n\n{{< note >}}"}
{"en": "The `spec` of a static Pod cannot refer to other API objects\n(e.g., {{< glossary_tooltip text=\"ServiceAccount\" term_id=\"service-account\" >}},\n{{< glossary_tooltip text=\"ConfigMap\" term_id=\"configmap\" >}},\n{{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}, etc).", "zh": "静态 Pod 的 `spec` 不能引用其他的 API 对象（例如：\n{{< glossary_tooltip text=\"ServiceAccount\" term_id=\"service-account\" >}}、\n{{< glossary_tooltip text=\"ConfigMap\" term_id=\"configmap\" >}}、\n{{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}} 等）。\n{{< /note >}}"}
{"en": "### Pods manage multiple containers  {#how-pods-manage-multiple-containers}\n\nPods are designed to support multiple cooperating processes (as containers) that form\na cohesive unit of service. The containers in a Pod are automatically co-located and\nco-scheduled on the same physical or virtual machine in the cluster. The containers\ncan share resources and dependencies, communicate with one another, and coordinate\nwhen and how they are terminated.", "zh": "### Pod 管理多个容器   {#how-pods-manage-multiple-containers}\n\nPod 被设计成支持构造内聚的服务单元的多个协作进程（形式为容器）。\nPod 中的容器被自动并置到集群中的同一物理机或虚拟机上，并可以一起进行调度。\n容器之间可以共享资源和依赖、彼此通信、协调何时以及何种方式终止自身。"}
{"en": "Pods in a Kubernetes cluster are used in two main ways:\n\n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\n  most common Kubernetes use case; in this case, you can think of a Pod as a\n  wrapper around a single container; Kubernetes manages Pods rather than managing\n  the containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\n  encapsulate an application composed of\n  multiple co-located containers that are\n  tightly coupled and need to share resources. These co-located containers\n  form a single cohesive unit of service—for example, one container serving data\n  stored in a shared volume to the public, while a separate\n  {{< glossary_tooltip text=\"sidecar container\" term_id=\"sidecar-container\" >}}\n  refreshes or updates those files.\n  The Pod wraps these containers, storage resources, and an ephemeral network\n  identity together as a single unit.", "zh": "Kubernetes 集群中的 Pod 主要有两种用法：\n\n* **运行单个容器的 Pod**。\"每个 Pod 一个容器\" 模型是最常见的 Kubernetes 用例；\n  在这种情况下，可以将 Pod 看作单个容器的包装器。Kubernetes 直接管理 Pod，而不是容器。\n* **运行多个需要协同工作的容器的 Pod**。\n  Pod 可以封装由多个紧密耦合且需要共享资源的并置容器组成的应用。\n  这些位于同一位置的容器可能形成单个内聚的服务单元 —— 一个容器将文件从共享卷提供给公众，\n  而另一个单独的{{< glossary_tooltip text=\"边车容器\" term_id=\"sidecar-container\" >}}则刷新或更新这些文件。\n  Pod 将这些容器和存储资源打包为一个可管理的实体。"}
{"en": "For example, you might have a container that\nacts as a web server for files in a shared volume, and a separate\n[sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nthat updates those files from a remote source, as in the following diagram:", "zh": "例如，你可能有一个容器，为共享卷中的文件提供 Web 服务器支持，以及一个单独的\n[边车（Sidercar）](/zh-cn/docs/concepts/workloads/pods/sidecar-containers/)\n容器负责从远端更新这些文件，如下图所示：\n\n{{< figure src=\"/zh-cn/docs/images/pod.svg\" alt=\"Pod 创建示意图\" class=\"diagram-medium\" >}}"}
{"en": "Some Pods have {{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}}\nas well as {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}.\nBy default, init containers run and complete before the app containers are started.", "zh": "有些 Pod 具有 {{< glossary_tooltip text=\"Init 容器\" term_id=\"init-container\" >}}和\n{{< glossary_tooltip text=\"应用容器\" term_id=\"app-container\" >}}。\nInit 容器默认会在启动应用容器之前运行并完成。"}
{"en": "You can also have [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nthat provide auxiliary services to the main application Pod (for example: a service mesh).", "zh": "你还可以拥有为主应用 Pod 提供辅助服务的\n[边车容器](/zh-cn/docs/concepts/workloads/pods/sidecar-containers/)（例如：服务网格）。\n\n\n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}"}
{"en": "Enabled by default, the `SidecarContainers` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nallows you to specify `restartPolicy: Always` for init containers.\nSetting the `Always` restart policy ensures that the init containers where you set it are\ntreated as _sidecars_ that are kept running during the entire lifetime of the Pod.\nSee [Sidecar containers and restartPolicy](/docs/concepts/workloads/pods/init-containers/#sidecar-containers-and-restartpolicy)\nfor more details.", "zh": "启用 `SidecarContainers` [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)（默认启用）允许你为\nInit 容器指定 `restartPolicy: Always`。设置重启策略为 `Always` 会确保设置的 Init 容器被视为**边车**，\n并在 Pod 的整个生命周期内保持运行。\n更多细节参阅[边车容器和重启策略](/zh-cn/docs/concepts/workloads/pods/init-containers/#sidecar-containers-and-restartpolicy)"}
{"en": "## Container probes\n\nA _probe_ is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke different actions:\n\n- `ExecAction` (performed with the help of the container runtime)\n- `TCPSocketAction` (checked directly by the kubelet)\n- `HTTPGetAction` (checked directly by the kubelet)\n\nYou can read more about [probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes) \nin the Pod Lifecycle documentation.", "zh": "## 容器探针   {#container-probes}\n\n**Probe** 是由 kubelet 对容器执行的定期诊断。要执行诊断，kubelet 可以执行三种动作：\n    \n- `ExecAction`（借助容器运行时执行）\n- `TCPSocketAction`（由 kubelet 直接检测）\n- `HTTPGetAction`（由 kubelet 直接检测）\n\n你可以参阅 Pod 的生命周期文档中的[探针](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)部分。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn about the [lifecycle of a Pod](/docs/concepts/workloads/pods/pod-lifecycle/).\n* Learn about [RuntimeClass](/docs/concepts/containers/runtime-class/) and how you can use it to\n  configure different Pods with different container runtime configurations.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how you can use it to manage application availability during disruptions.\n* Pod is a top-level resource in the Kubernetes REST API.\n  The {{< api-reference page=\"workload-resources/pod-v1\" >}}\n  object definition describes the object in detail.\n* [The Distributed System Toolkit: Patterns for Composite Containers](/blog/2015/06/the-distributed-system-toolkit-patterns/) explains common layouts for Pods with more than one container.\n* Read about [Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)", "zh": "* 了解 [Pod 生命周期](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/)。\n* 了解 [RuntimeClass](/zh-cn/docs/concepts/containers/runtime-class/)，\n  以及如何使用它来配置不同的 Pod 使用不同的容器运行时配置。\n* 了解 [PodDisruptionBudget](/zh-cn/docs/concepts/workloads/pods/disruptions/)，\n  以及你可以如何利用它在出现干扰因素时管理应用的可用性。\n* Pod 在 Kubernetes REST API 中是一个顶层资源。\n  {{< api-reference page=\"workload-resources/pod-v1\" >}}\n  对象的定义中包含了更多的细节信息。\n* 博客[分布式系统工具箱：复合容器模式](/blog/2015/06/the-distributed-system-toolkit-patterns/)中解释了在同一\n  Pod 中包含多个容器时的几种常见布局。\n* 了解 [Pod 拓扑分布约束](/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/)。"}
{"en": "To understand the context for why Kubernetes wraps a common Pod API in other resources (such as {{< glossary_tooltip text=\"StatefulSets\" term_id=\"statefulset\" >}} or {{< glossary_tooltip text=\"Deployments\" term_id=\"deployment\" >}}), you can read about the prior art, including:", "zh": "要了解为什么 Kubernetes 会在其他资源\n（如 {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}\n或 {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}）\n封装通用的 Pod API，相关的背景信息可以在前人的研究中找到。具体包括："}
{"en": "* [Aurora](https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema)\n* [Borg](https://research.google.com/pubs/pub43438.html)\n* [Marathon](https://mesosphere.github.io/marathon/docs/rest-api.html)\n* [Omega](https://research.google/pubs/pub41684/)\n* [Tupperware](https://engineering.fb.com/data-center-engineering/tupperware/).", "zh": "* [Aurora](https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema)\n* [Borg](https://research.google.com/pubs/pub43438.html)\n* [Marathon](https://mesosphere.github.io/marathon/docs/rest-api.html)\n* [Omega](https://research.google/pubs/pub41684/)\n* [Tupperware](https://engineering.fb.com/data-center-engineering/tupperware/)。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}"}
{"en": "When your Job has finished, it's useful to keep that Job in the API (and not immediately delete the Job)\nso that you can tell whether the Job succeeded or failed.\n\nKubernetes' TTL-after-finished {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} provides a\nTTL (time to live) mechanism to limit the lifetime of Job objects that\nhave finished execution.", "zh": "当你的 Job 已结束时，将 Job 保留在 API 中（而不是立即删除 Job）很有用，\n这样你就可以判断 Job 是成功还是失败。\n\nKubernetes TTL-after-finished {{<glossary_tooltip text=\"控制器\" term_id=\"controller\">}}提供了一种\nTTL 机制来限制已完成执行的 Job 对象的生命期。"}
{"en": "## Cleanup for finished Jobs\n\nThe TTL-after-finished controller is only supported for Jobs. You can use this mechanism to clean\nup finished Jobs (either `Complete` or `Failed`) automatically by specifying the\n`.spec.ttlSecondsAfterFinished` field of a Job, as in this\n[example](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).", "zh": "## 清理已完成的 Job   {#cleanup-for-finished-jobs}\n\nTTL-after-finished 控制器只支持 Job。你可以通过指定 Job 的 `.spec.ttlSecondsAfterFinished`\n字段来自动清理已结束的 Job（`Complete` 或 `Failed`），\n如[示例](/zh-cn/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically)所示。"}
{"en": "The TTL-after-finished controller assumes that a Job is eligible to be cleaned up\nTTL seconds after the Job has finished. The timer starts once the\nstatus condition of the Job changes to show that the Job is either `Complete` or `Failed`; once the TTL has\nexpired, that Job becomes eligible for\n[cascading](/docs/concepts/architecture/garbage-collection/#cascading-deletion) removal. When the\nTTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete\nits dependent objects together with it.", "zh": "TTL-after-finished 控制器假设 Job 能在执行完成后的 TTL 秒内被清理。一旦 Job\n的状态条件发生变化表明该 Job 是 `Complete` 或 `Failed`，计时器就会启动；一旦 TTL 已过期，该 Job\n就能被[级联删除](/zh-cn/docs/concepts/architecture/garbage-collection/#cascading-deletion)。\n当 TTL 控制器清理作业时，它将做级联删除操作，即删除 Job 的同时也删除其依赖对象。"}
{"en": "Kubernetes honors object lifecycle guarantees on the Job, such as waiting for\n[finalizers](/docs/concepts/overview/working-with-objects/finalizers/).\n\nYou can set the TTL seconds at any time. Here are some examples for setting the\n`.spec.ttlSecondsAfterFinished` field of a Job:", "zh": "Kubernetes 尊重 Job 对象的生命周期保证，例如等待\n[Finalizer](/zh-cn/docs/concepts/overview/working-with-objects/finalizers/)。\n\n你可以随时设置 TTL 秒。以下是设置 Job 的 `.spec.ttlSecondsAfterFinished` 字段的一些示例："}
{"en": "* Specify this field in the Job manifest, so that a Job can be cleaned up\n  automatically some time after it finishes.\n* Manually set this field of existing, already finished Jobs, so that they become eligible\n  for cleanup.\n* Use a\n  [mutating admission webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\n  to set this field dynamically at Job creation time. Cluster administrators can\n  use this to enforce a TTL policy for finished jobs.", "zh": "* 在 Job 清单（manifest）中指定此字段，以便 Job 在完成后的某个时间被自动清理。\n* 手动设置现有的、已完成的 Job 的此字段，以便这些 Job 可被清理。\n* 在创建 Job 时使用[修改性质的准入 Webhook](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\n  动态设置该字段。集群管理员可以使用它对已完成的作业强制执行 TTL 策略。"}
{"en": "* Use a\n  [mutating admission webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\n  to set this field dynamically after the Job has finished, and choose\n  different TTL values based on job status, labels. For this case, the webhook needs\n  to detect changes to the `.status` of the Job and only set a TTL when the Job\n  is being marked as completed.\n* Write your own controller to manage the cleanup TTL for Jobs that match a particular\n  {{< glossary_tooltip term_id=\"selector\" text=\"selector\" >}}.", "zh": "* 使用[修改性质的准入 Webhook](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\n  在 Job 完成后动态设置该字段，并根据 Job 状态、标签等选择不同的 TTL 值。\n  对于这种情况，Webhook 需要检测 Job 的 `.status` 变化，并且仅在 Job 被标记为已完成时设置 TTL。\n* 编写你自己的控制器来管理与特定{{< glossary_tooltip term_id=\"selector\" text=\"选择算符\" >}}匹配的\n  Job 的清理 TTL。"}
{"en": "## Caveats\n\n### Updating TTL for finished Jobs\n\nYou can modify the TTL period, e.g. `.spec.ttlSecondsAfterFinished` field of Jobs,\nafter the job is created or has finished. If you extend the TTL period after the\nexisting `ttlSecondsAfterFinished` period has expired, Kubernetes doesn't guarantee\nto retain that Job, even if an update to extend the TTL returns a successful API\nresponse.", "zh": "## 警告  {#caveats}\n\n### 更新已完成 Job 的 TTL  {#updating-ttl-for-finished-jobs}\n\n在创建 Job 或已经执行结束后，你仍可以修改其 TTL 周期，例如 Job 的\n`.spec.ttlSecondsAfterFinished` 字段。\n如果你在当前 `ttlSecondsAfterFinished` 时长已过期后延长 TTL 周期，\n即使延长 TTL 的更新得到了成功的 API 响应，Kubernetes 也不保证保留此 Job，"}
{"en": "### Time skew\n\nBecause the TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to\ndetermine whether the TTL has expired or not, this feature is sensitive to time\nskew in your cluster, which may cause the control plane to clean up Job objects\nat the wrong time.", "zh": "### 时间偏差  {#time-skew}\n\n由于 TTL-after-finished 控制器使用存储在 Kubernetes Job 中的时间戳来确定 TTL 是否已过期，\n因此该功能对集群中的时间偏差很敏感，这可能导致控制平面在错误的时间清理 Job 对象。"}
{"en": "Clocks aren't always correct, but the difference should be\nvery small. Please be aware of this risk when setting a non-zero TTL.", "zh": "时钟并不总是如此正确，但差异应该很小。\n设置非零 TTL 时请注意避免这种风险。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read [Clean up Jobs automatically](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically)\n\n* Refer to the [Kubernetes Enhancement Proposal](https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md)\n  (KEP) for adding this mechanism.", "zh": "* 阅读[自动清理 Job](/zh-cn/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically)\n\n* 参阅 [Kubernetes 增强提案](https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md)\n  (KEP) 了解此机制的演进过程。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}"}
{"en": "A _CronJob_ creates {{< glossary_tooltip term_id=\"job\" text=\"Jobs\" >}} on a repeating schedule.\n\nCronJob is meant for performing regular scheduled actions such as backups, report generation,\nand so on. One CronJob object is like one line of a _crontab_ (cron table) file on a\nUnix system. It runs a Job periodically on a given schedule, written in\n[Cron](https://en.wikipedia.org/wiki/Cron) format.", "zh": "**CronJob** 创建基于时隔重复调度的 {{< glossary_tooltip term_id=\"job\" text=\"Job\" >}}。\n\nCronJob 用于执行排期操作，例如备份、生成报告等。\n一个 CronJob 对象就像 Unix 系统上的 **crontab**（cron table）文件中的一行。\n它用 [Cron](https://zh.wikipedia.org/wiki/Cron) 格式进行编写，\n并周期性地在给定的调度时间执行 Job。"}
{"en": "CronJobs have limitations and idiosyncrasies.\nFor example, in certain circumstances, a single CronJob can create multiple concurrent Jobs. See the [limitations](#cron-job-limitations) below.", "zh": "CronJob 有所限制，也比较特殊。\n例如在某些情况下，单个 CronJob 可以创建多个并发任务。\n请参阅下面的[限制](#cron-job-limitations)。"}
{"en": "When the control plane creates new Jobs and (indirectly) Pods for a CronJob, the `.metadata.name`\nof the CronJob is part of the basis for naming those Pods.  The name of a CronJob must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\nEven when the name is a DNS subdomain, the name must be no longer than 52\ncharacters.  This is because the CronJob controller will automatically append\n11 characters to the name you provide and there is a constraint that the\nlength of a Job name is no more than 63 characters.", "zh": "当控制平面为 CronJob 创建新的 Job 和（间接）Pod 时，CronJob 的 `.metadata.name` 是命名这些 Pod 的部分基础。\nCronJob 的名称必须是一个合法的\n[DNS 子域](/zh-cn/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names)值，\n但这会对 Pod 的主机名产生意外的结果。为获得最佳兼容性，名称应遵循更严格的\n[DNS 标签](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-label-names)规则。\n即使名称是一个 DNS 子域，它也不能超过 52 个字符。这是因为 CronJob 控制器将自动在你所提供的 Job 名称后附加\n11 个字符，并且存在 Job 名称的最大长度不能超过 63 个字符的限制。"}
{"en": "## Example\n\nThis example CronJob manifest prints the current time and a hello message every minute:", "zh": "## 示例    {#example}\n\n下面的 CronJob 示例清单会在每分钟打印出当前时间和问候消息：\n\n{{% code_sample file=\"application/job/cronjob.yaml\" %}}"}
{"en": "([Running Automated Tasks with a CronJob](/docs/tasks/job/automated-tasks-with-cron-jobs/)\ntakes you through this example in more detail).", "zh": "[使用 CronJob 运行自动化任务](/zh-cn/docs/tasks/job/automated-tasks-with-cron-jobs/)一文会为你详细讲解此例。"}
{"en": "## Writing a CronJob spec\n### Schedule syntax\nThe `.spec.schedule` field is required. The value of that field follows the [Cron](https://en.wikipedia.org/wiki/Cron) syntax:", "zh": "## 编写 CronJob 声明信息   {#writing-a-cronjob-spec}\n\n### Cron 时间表语法    {#cron-schedule-syntax}\n\n`.spec.schedule` 字段是必需的。该字段的值遵循 [Cron](https://zh.wikipedia.org/wiki/Cron) 语法："}
{"en": "```\n# ┌───────────── minute (0 - 59)\n# │ ┌───────────── hour (0 - 23)\n# │ │ ┌───────────── day of the month (1 - 31)\n# │ │ │ ┌───────────── month (1 - 12)\n# │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday)\n# │ │ │ │ │                                   OR sun, mon, tue, wed, thu, fri, sat\n# │ │ │ │ │ \n# │ │ │ │ │\n# * * * * *\n```", "zh": "```\n# ┌───────────── 分钟 (0 - 59)\n# │ ┌───────────── 小时 (0 - 23)\n# │ │ ┌───────────── 月的某天 (1 - 31)\n# │ │ │ ┌───────────── 月份 (1 - 12)\n# │ │ │ │ ┌───────────── 周的某天 (0 - 6)（周日到周六）\n# │ │ │ │ │                          或者是 sun，mon，tue，web，thu，fri，sat\n# │ │ │ │ │\n# │ │ │ │ │\n# * * * * *\n```"}
{"en": "For example, `0 3 * * 1` means this task is scheduled to run weekly on a Monday at 3 AM.", "zh": "例如 `0 0 13 * 5` 表示此任务计划于每周一凌晨 3 点运行。"}
{"en": "The format also includes extended \"Vixie cron\" step values. As explained in the\n[FreeBSD manual](https://www.freebsd.org/cgi/man.cgi?crontab%285%29):", "zh": "该格式也包含了扩展的 “Vixie cron” 步长值。\n[FreeBSD 手册](https://www.freebsd.org/cgi/man.cgi?crontab%285%29)中解释如下:"}
{"en": "> Step values can be used in conjunction with ranges. Following a range\n> with `/<number>` specifies skips of the number's value through the\n> range. For example, `0-23/2` can be used in the hours field to specify\n> command execution every other hour (the alternative in the V7 standard is\n> `0,2,4,6,8,10,12,14,16,18,20,22`). Steps are also permitted after an\n> asterisk, so if you want to say \"every two hours\", just use `*/2`.", "zh": "> 步长可被用于范围组合。范围后面带有 `/<数字>` 可以声明范围内的步幅数值。\n> 例如，`0-23/2` 可被用在小时字段来声明命令在其他数值的小时数执行\n> （V7 标准中对应的方法是 `0,2,4,6,8,10,12,14,16,18,20,22`）。\n> 步长也可以放在通配符后面，因此如果你想表达 “每两小时”，就用 `*/2` 。\n\n{{< note >}}"}
{"en": "A question mark (`?`) in the schedule has the same meaning as an asterisk `*`, that is,\nit stands for any of available value for a given field.", "zh": "时间表中的问号 (`?`) 和星号 `*` 含义相同，它们用来表示给定字段的任何可用值。\n{{< /note >}}"}
{"en": "Other than the standard syntax, some macros like `@monthly` can also be used:", "zh": "除了标准语法，还可以使用一些类似 `@monthly` 的宏："}
{"en": "| Entry \t\t\t\t\t\t\t\t\t\t| Description\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t| Equivalent to |\n| ------------- \t\t\t\t\t\t| ------------- \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t|-------------  |\n| @yearly (or @annually)\t\t| Run once a year at midnight of 1 January\t\t\t\t\t\t\t\t\t\t| 0 0 1 1 * \t\t|\n| @monthly \t\t\t\t\t\t\t\t\t| Run once a month at midnight of the first day of the month\t| 0 0 1 * * \t\t|\n| @weekly \t\t\t\t\t\t\t\t\t| Run once a week at midnight on Sunday morning\t\t\t\t\t\t\t\t| 0 0 * * 0 \t\t|\n| @daily (or @midnight)\t\t\t| Run once a day at midnight\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t| 0 0 * * * \t\t|\n| @hourly \t\t\t\t\t\t\t\t\t| Run once an hour at the beginning of the hour\t\t\t\t\t\t\t\t| 0 * * * * \t\t|", "zh": "| 输入                   | 描述                      | 相当于        |\n| ---------------------- | ------------------------ | ------------ |\n| @yearly (或 @annually) | 每年 1 月 1 日的午夜运行一次 | 0 0 1 1 *    |\n| @monthly               | 每月第一天的午夜运行一次     | 0 0 1 * *    |\n| @weekly                | 每周的周日午夜运行一次       | 0 0 * * 0    |\n| @daily (或 @midnight)  | 每天午夜运行一次            | 0 0 * * *    |\n| @hourly                | 每小时的开始一次            | 0 * * * *    |"}
{"en": "To generate CronJob schedule expressions, you can also use web tools like [crontab.guru](https://crontab.guru/).", "zh": "为了生成 CronJob 时间表的表达式，你还可以使用 [crontab.guru](https://crontab.guru/) 这类 Web 工具。"}
{"en": "### Job template\n\nThe `.spec.jobTemplate` defines a template for the Jobs that the CronJob creates, and it is required.\nIt has exactly the same schema as a [Job](/docs/concepts/workloads/controllers/job/), except that\nit is nested and does not have an `apiVersion` or `kind`.\nYou can specify common metadata for the templated Jobs, such as\n{{< glossary_tooltip text=\"labels\" term_id=\"label\" >}} or\n{{< glossary_tooltip text=\"annotations\" term_id=\"annotation\" >}}.\nFor information about writing a Job `.spec`, see [Writing a Job Spec](/docs/concepts/workloads/controllers/job/#writing-a-job-spec).", "zh": "### 任务模板   {#job-template}\n\n`.spec.jobTemplate`为 CronJob 创建的 Job 定义模板，它是必需的。它和\n[Job](/zh-cn/docs/concepts/workloads/controllers/job/) 的语法完全一样，\n只不过它是嵌套的，没有 `apiVersion` 和 `kind`。\n你可以为模板化的 Job 指定通用的元数据，\n例如{{< glossary_tooltip text=\"标签\" term_id=\"label\" >}}或{{< glossary_tooltip text=\"注解\" term_id=\"annotation\" >}}。\n有关如何编写一个 Job 的 `.spec`，\n请参考[编写 Job 规约](/zh-cn/docs/concepts/workloads/controllers/job/#writing-a-job-spec)。"}
{"en": "### Deadline for delayed Job start {#starting-deadline}\n\nThe `.spec.startingDeadlineSeconds` field is optional.\nThis field defines a deadline (in whole seconds) for starting the Job, if that Job misses its scheduled time\nfor any reason.\n\nAfter missing the deadline, the CronJob skips that instance of the Job (future occurrences are still scheduled).\nFor example, if you have a backup Job that runs twice a day, you might allow it to start up to 8 hours late,\nbut no later, because a backup taken any later wouldn't be useful: you would instead prefer to wait for\nthe next scheduled run.", "zh": "### Job 延迟开始的最后期限   {#starting-deadline}\n\n`.spec.startingDeadlineSeconds` 字段是可选的。\n它表示 Job 如果由于某种原因错过了调度时间，开始该 Job 的截止时间的秒数。\n\n过了截止时间，CronJob 就不会开始该 Job 的实例（未来的 Job 仍在调度之中）。\n例如，如果你有一个每天运行两次的备份 Job，你可能会允许它最多延迟 8 小时开始，但不能更晚，\n因为更晚进行的备份将变得没有意义：你宁愿等待下一次计划的运行。"}
{"en": "For Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs.\nIf you don't specify `startingDeadlineSeconds` for a CronJob, the Job occurrences have no deadline.\n\nIf the `.spec.startingDeadlineSeconds` field is set (not null), the CronJob\ncontroller measures the time between when a Job is expected to be created and\nnow. If the difference is higher than that limit, it will skip this execution.\n\nFor example, if it is set to `200`, it allows a Job to be created for up to 200\nseconds after the actual schedule.", "zh": "对于错过已配置的最后期限的 Job，Kubernetes 将其视为失败的 Job。\n如果你没有为 CronJob 指定 `startingDeadlineSeconds`，那 Job 就没有最后期限。\n\n如果 `.spec.startingDeadlineSeconds` 字段被设置（非空），\nCronJob 控制器将会计算从预期创建 Job 到当前时间的时间差。\n如果时间差大于该限制，则跳过此次执行。\n\n例如，如果将其设置为 `200`，则 Job 控制器允许在实际调度之后最多 200 秒内创建 Job。"}
{"en": "### Concurrency policy\n\nThe `.spec.concurrencyPolicy` field is also optional.\nIt specifies how to treat concurrent executions of a Job that is created by this CronJob.\nThe spec may specify only one of the following concurrency policies:", "zh": "### 并发性规则   {#concurrency-policy}\n\n`.spec.concurrencyPolicy` 字段也是可选的。它声明了 CronJob 创建的 Job 执行时发生重叠如何处理。\nspec 仅能声明下列规则中的一种："}
{"en": "* `Allow` (default): The CronJob allows concurrently running Jobs\n* `Forbid`: The CronJob does not allow concurrent runs; if it is time for a new Job run and the\n  previous Job run hasn't finished yet, the CronJob skips the new Job run. Also note that when the\n  previous Job run finishes, `.spec.startingDeadlineSeconds` is still taken into account and may\n  result in a new Job run.\n* `Replace`: If it is time for a new Job run and the previous Job run hasn't finished yet, the\n  CronJob replaces the currently running Job run with a new Job run", "zh": "* `Allow`（默认）：CronJob 允许并发 Job 执行。\n* `Forbid`：CronJob 不允许并发执行；如果新 Job 的执行时间到了而老 Job 没有执行完，CronJob 会忽略新 Job 的执行。\n  另请注意，当老 Job 执行完成时，仍然会考虑 `.spec.startingDeadlineSeconds`，可能会导致新的 Job 执行。\n* `Replace`：如果新 Job 的执行时间到了而老 Job 没有执行完，CronJob 会用新 Job 替换当前正在运行的 Job。"}
{"en": "Note that concurrency policy only applies to the Jobs created by the same CronJob.\nIf there are multiple CronJobs, their respective Jobs are always allowed to run concurrently.", "zh": "请注意，并发性规则仅适用于相同 CronJob 创建的 Job。如果有多个 CronJob，它们相应的 Job 总是允许并发执行的。"}
{"en": "### Schedule suspension\n\nYou can suspend execution of Jobs for a CronJob, by setting the optional `.spec.suspend` field\nto true. The field defaults to false.\n\nThis setting does _not_ affect Jobs that the CronJob has already started.", "zh": "### 调度挂起   {#schedule-suspension}\n\n通过将可选的 `.spec.suspend` 字段设置为 `true`，可以挂起针对 CronJob 执行的任务。\n\n这个设置**不**会影响 CronJob 已经开始的任务。"}
{"en": "If you do set that field to true, all subsequent executions are suspended (they remain\nscheduled, but the CronJob controller does not start the Jobs to run the tasks) until\nyou unsuspend the CronJob.", "zh": "如果你将此字段设置为 `true`，后续发生的执行都会被挂起\n（这些任务仍然在调度中，但 CronJob 控制器不会启动这些 Job 来运行任务），直到你取消挂起 CronJob 为止。\n\n{{< caution >}}"}
{"en": "Executions that are suspended during their scheduled time count as missed Jobs.\nWhen `.spec.suspend` changes from `true` to `false` on an existing CronJob without a\n[starting deadline](#starting-deadline), the missed Jobs are scheduled immediately.", "zh": "在调度时间内挂起的执行都会被统计为错过的 Job。当现有的 CronJob 将 `.spec.suspend` 从 `true` 改为 `false` 时，\n且没有[开始的最后期限](#starting-deadline)，错过的 Job 会被立即调度。\n{{< /caution >}}"}
{"en": "### Jobs history limits\n\nThe `.spec.successfulJobsHistoryLimit` and `.spec.failedJobsHistoryLimit` fields specify\nhow many completed and failed Jobs should be kept. Both fields are optional.\n\n* `.spec.successfulJobsHistoryLimit`: This field specifies the number of successful finished\njobs to keep. The default value is `3`. Setting this field to `0` will not keep any successful jobs.\n\n* `.spec.failedJobsHistoryLimit`: This field specifies the number of failed finished jobs to keep.\nThe default value is `1`. Setting this field to `0` will not keep any failed jobs.\n\nFor another way to clean up Jobs automatically, see\n[Clean up finished Jobs automatically](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).", "zh": "### 任务历史限制   {#jobs-history-limits}\n\n`.spec.successfulJobsHistoryLimit` 和 `.spec.failedJobsHistoryLimit`\n字段指定应保留多少已完成和失败的 Job。这两个字段都是可选的。\n\n* `.spec.successfulJobsHistoryLimit`：此字段指定要保留多少成功完成的 Job。默认值为 `3`。\n 将此字段设置为 `0` 意味着不会保留任何成功的 Job。\n\n* `.spec.failedJobsHistoryLimit`：此字段指定要保留多少失败完成的 Job。默认值为 `1`。\n 将此字段设置为 `0` 意味着不会保留任何失败的 Job。\n\n有关自动清理 Job 的其他方式，\n请参见[自动清理完成的 Job](/zh-cn/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically)。"}
{"en": "### Time zones", "zh": "## 时区    {#time-zones}\n\n{{< feature-state for_k8s_version=\"v1.27\" state=\"stable\" >}}"}
{"en": "For CronJobs with no time zone specified, the {{< glossary_tooltip term_id=\"kube-controller-manager\" text=\"kube-controller-manager\" >}}\ninterprets schedules relative to its local time zone.", "zh": "对于没有指定时区的 CronJob，\n{{< glossary_tooltip term_id=\"kube-controller-manager\" text=\"kube-controller-manager\" >}}\n基于本地时区解释排期表（Schedule）。"}
{"en": "You can specify a time zone for a CronJob by setting `.spec.timeZone` to the name\nof a valid [time zone](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).\nFor example, setting `.spec.timeZone: \"Etc/UTC\"` instructs Kubernetes to interpret\nthe schedule relative to Coordinated Universal Time.", "zh": "你可以通过将 `.spec.timeZone`\n设置为一个有效[时区](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)的名称，\n为 CronJob 指定一个时区。例如设置 `.spec.timeZone: \"Etc/UTC\"` 将告诉\nKubernetes 基于世界标准时间解读排期表。"}
{"en": "A time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is not available on the system.", "zh": "Go 标准库中的时区数据库包含在二进制文件中，并用作备用数据库，以防系统上没有可用的外部数据库。"}
{"en": "## CronJob limitations {#cron-job-limitations}\n\n### Unsupported TimeZone specification", "zh": "## CronJob 的限制    {#cron-job-limitations}\n\n### 不支持的时区规范   {#unsupported-timezone-spec}"}
{"en": "Specifying a timezone using `CRON_TZ` or `TZ` variables inside `.spec.schedule`\nis **not officially supported** (and never has been).", "zh": "在 `.spec.schedule` 中通过 `CRON_TZ` 或 `TZ` 变量来指定时区**并未得到官方支持**（而且从未支持过）。"}
{"en": "Starting with Kubernetes 1.29 if you try to set a schedule that includes `TZ` or `CRON_TZ`\ntimezone specification, Kubernetes will fail to create the resource with a validation\nerror.\nUpdates to CronJobs already using `TZ` or `CRON_TZ` will continue to report a\n[warning](/blog/2020/09/03/warnings/) to the client.", "zh": "从 Kubernetes 1.29 版本开始，如果你尝试设定包含 `TZ` 或 `CRON_TZ` 时区规范的排期表，\nKubernetes 将无法创建该资源，并会报告验证错误。\n对已经设置 `TZ` 或 `CRON_TZ` 的 CronJob 进行更新时，\n系统会继续向客户端发送[警告](/zh-cn/blog/2020/09/03/warnings/)。"}
{"en": "### Modifying a CronJob\n\nBy design, a CronJob contains a template for _new_ Jobs.\nIf you modify an existing CronJob, the changes you make will apply to new Jobs that\nstart to run after your modification is complete. Jobs (and their Pods) that have already\nstarted continue to run without changes.\nThat is, the CronJob does _not_ update existing Jobs, even if those remain running.", "zh": "### 修改 CronJob   {#modifying-a-cronjob}\n\n按照设计，CronJob 包含一个用于**新** Job 的模板。\n如果你修改现有的 CronJob，你所做的更改将应用于修改完成后开始运行的新任务。\n已经开始的任务（及其 Pod）将继续运行而不会发生任何变化。\n也就是说，CronJob **不** 会更新现有任务，即使这些任务仍在运行。"}
{"en": "### Job creation\n\nA CronJob creates a Job object approximately once per execution time of its schedule.\nThe scheduling is approximate because there\nare certain circumstances where two Jobs might be created, or no Job might be created.\nKubernetes tries to avoid those situations, but does not completely prevent them. Therefore,\nthe Jobs that you define should be _idempotent_.", "zh": "### Job 创建  {#job-creation}\n\nCronJob 根据其计划编排，在每次该执行任务的时候大约会创建一个 Job。\n我们之所以说 \"大约\"，是因为在某些情况下，可能会创建两个 Job，或者不会创建任何 Job。\n我们试图使这些情况尽量少发生，但不能完全杜绝。因此，Job 应该是 **幂等的**。"}
{"en": "If `startingDeadlineSeconds` is set to a large value or left unset (the default)\nand if `concurrencyPolicy` is set to `Allow`, the Jobs will always run\nat least once.", "zh": "如果 `startingDeadlineSeconds` 设置为很大的数值或未设置（默认），并且\n`concurrencyPolicy` 设置为 `Allow`，则 Job 将始终至少运行一次。\n\n{{< caution >}}"}
{"en": "If `startingDeadlineSeconds` is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds.", "zh": "如果 `startingDeadlineSeconds` 的设置值低于 10 秒钟，CronJob 可能无法被调度。\n这是因为 CronJob 控制器每 10 秒钟执行一次检查。\n{{< /caution >}}"}
{"en": "For every CronJob, the CronJob {{< glossary_tooltip term_id=\"controller\" >}} checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the Job and logs the error.", "zh": "对于每个 CronJob，CronJob {{< glossary_tooltip term_text=\"控制器\" term_id=\"controller\" >}}\n检查从上一次调度的时间点到现在所错过了调度次数。如果错过的调度次数超过 100 次，\n那么它就不会启动这个 Job，并记录这个错误:\n\n```\nCannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.\n```"}
{"en": "It is important to note that if the `startingDeadlineSeconds` field is set (not `nil`), the controller counts how many missed Jobs occurred from the value of `startingDeadlineSeconds` until now rather than from the last scheduled time until now. For example, if `startingDeadlineSeconds` is `200`, the controller counts how many missed Jobs occurred in the last 200 seconds.", "zh": "需要注意的是，如果 `startingDeadlineSeconds` 字段非空，则控制器会统计从\n`startingDeadlineSeconds` 设置的值到现在而不是从上一个计划时间到现在错过了多少次 Job。\n例如，如果 `startingDeadlineSeconds` 是 `200`，则控制器会统计在过去 200 秒中错过了多少次 Job。"}
{"en": "A CronJob is counted as missed if it has failed to be created at its scheduled time. For example, if `concurrencyPolicy` is set to `Forbid` and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.", "zh": "如果未能在调度时间内创建 CronJob，则计为错过。\n例如，如果 `concurrencyPolicy` 被设置为 `Forbid`，并且当前有一个调度仍在运行的情况下，\n试图调度的 CronJob 将被计算为错过。"}
{"en": "For example, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its\n`startingDeadlineSeconds` field is not set. If the CronJob controller happens to\nbe down from `08:29:00` to `10:21:00`, the Job will not start as the number of missed Jobs which missed their schedule is greater than 100.", "zh": "例如，假设一个 CronJob 被设置为从 `08:30:00` 开始每隔一分钟创建一个新的 Job，\n并且它的 `startingDeadlineSeconds` 字段未被设置。如果 CronJob 控制器从\n`08:29:00` 到 `10:21:00` 终止运行，则该 Job 将不会启动，\n因为其错过的调度次数超过了 100。"}
{"en": "To illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its\n`startingDeadlineSeconds` is set to 200 seconds. If the CronJob controller happens to\nbe down for the same period as the previous example (`08:29:00` to `10:21:00`,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than from the last scheduled time until now.", "zh": "为了进一步阐述这个概念，假设将 CronJob 设置为从 `08:30:00` 开始每隔一分钟创建一个新的 Job，\n并将其 `startingDeadlineSeconds` 字段设置为 200 秒。\n如果 CronJob 控制器恰好在与上一个示例相同的时间段（`08:29:00` 到 `10:21:00`）终止运行，\n则 Job 仍将从 `10:22:00` 开始。\n造成这种情况的原因是控制器现在检查在最近 200 秒（即 3 个错过的调度）中发生了多少次错过的\nJob 调度，而不是从现在为止的最后一个调度时间开始。"}
{"en": "The CronJob is only responsible for creating Jobs that match its schedule, and\nthe Job in turn is responsible for the management of the Pods it represents.", "zh": "CronJob 仅负责创建与其调度时间相匹配的 Job，而 Job 又负责管理其代表的 Pod。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn about [Pods](/docs/concepts/workloads/pods/) and\n  [Jobs](/docs/concepts/workloads/controllers/job/), two concepts\n  that CronJobs rely upon.\n* Read about the detailed [format](https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format)\n  of CronJob `.spec.schedule` fields.\n* For instructions on creating and working with CronJobs, and for an example\n  of a CronJob manifest,\n  see [Running automated tasks with CronJobs](/docs/tasks/job/automated-tasks-with-cron-jobs/).\n* `CronJob` is part of the Kubernetes REST API.\n  Read the {{< api-reference page=\"workload-resources/cron-job-v1\" >}}\n  API reference for more details.", "zh": "* 了解 CronJob 所依赖的 [Pod](/zh-cn/docs/concepts/workloads/pods/) 与\n  [Job](/zh-cn/docs/concepts/workloads/controllers/job/) 的概念。\n* 阅读 CronJob `.spec.schedule` 字段的详细[格式](https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format)。\n* 有关创建和使用 CronJob 的说明及 CronJob 清单的示例，\n  请参见[使用 CronJob 运行自动化任务](/zh-cn/docs/tasks/job/automated-tasks-with-cron-jobs/)。\n* `CronJob` 是 Kubernetes REST API 的一部分，\n  阅读 {{< api-reference page=\"workload-resources/cron-job-v1\" >}} API 参考了解更多细节。"}
{"en": "A _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the\ncluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage\ncollected.  Deleting a DaemonSet will clean up the Pods it created.\n-", "zh": "**DaemonSet** 确保全部（或者某些）节点上运行一个 Pod 的副本。\n当有节点加入集群时， 也会为他们新增一个 Pod 。\n当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。"}
{"en": "Some typical uses of a DaemonSet are:\n\n- running a cluster storage daemon on every node\n- running a logs collection daemon on every node\n- running a node monitoring daemon on every node", "zh": "DaemonSet 的一些典型用法：\n\n- 在每个节点上运行集群守护进程\n- 在每个节点上运行日志收集守护进程\n- 在每个节点上运行监控守护进程"}
{"en": "In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\ndifferent flags and/or different memory and cpu requests for different hardware types.", "zh": "一种简单的用法是为每种类型的守护进程在所有的节点上都启动一个 DaemonSet。\n一个稍微复杂的用法是为同一种守护进程部署多个 DaemonSet；每个具有不同的标志，\n并且对不同硬件类型具有不同的内存、CPU 要求。"}
{"en": "## Writing a DaemonSet Spec\n\n### Create a DaemonSet", "zh": "## 编写 DaemonSet Spec   {#writing-a-daemon-set-spec}\n\n### 创建 DaemonSet   {#create-a-daemon-set}"}
{"en": "You can describe a DaemonSet in a YAML file. For example, the `daemonset.yaml` file below\ndescribes a DaemonSet that runs the fluentd-elasticsearch Docker image:", "zh": "你可以在 YAML 文件中描述 DaemonSet。\n例如，下面的 daemonset.yaml 文件描述了一个运行 fluentd-elasticsearch Docker 镜像的 DaemonSet：\n\n{{% code_sample file=\"controllers/daemonset.yaml\" %}}"}
{"en": "Create a DaemonSet based on the YAML file:", "zh": "基于 YAML 文件创建 DaemonSet：\n\n```\nkubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml\n```"}
{"en": "### Required Fields\n\nAs with all other Kubernetes config, a DaemonSet needs `apiVersion`, `kind`, and `metadata` fields.  For\ngeneral information about working with config files, see\n[running stateless applications](/docs/tasks/run-application/run-stateless-application-deployment/)\nand [object management using kubectl](/docs/concepts/overview/working-with-objects/object-management/).\n\nThe name of a DaemonSet object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).\n\nA DaemonSet also needs a\n[`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)\nsection.", "zh": "### 必需字段   {#required-fields}\n\n与所有其他 Kubernetes 配置一样，DaemonSet 也需要 `apiVersion`、`kind` 和 `metadata` 字段。\n有关使用这些配置文件的通用信息，\n参见[运行无状态应用](/zh-cn/docs/tasks/run-application/run-stateless-application-deployment/)和[使用 kubectl 管理对象](/zh-cn/docs/concepts/overview/working-with-objects/object-management/)。\n\nDaemonSet 对象的名称必须是一个合法的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。\n\nDaemonSet 也需要 [`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status) 节区。"}
{"en": "### Pod Template\n\nThe `.spec.template` is one of the required fields in `.spec`.\n\nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates).\nIt has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}},\nexcept it is nested and does not have an `apiVersion` or `kind`.\n\nIn addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate\nlabels (see [pod selector](#pod-selector)).\n\nA Pod Template in a DaemonSet must have a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\n equal to `Always`, or be unspecified, which defaults to `Always`.", "zh": "### Pod 模板   {#pod-template}\n\n`.spec` 中唯一必需的字段是 `.spec.template`。\n\n`.spec.template` 是一个 [Pod 模板](/zh-cn/docs/concepts/workloads/pods/#pod-templates)。\n除了它是嵌套的，因而不具有 `apiVersion` 或 `kind` 字段之外，它与\n{{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 具有相同的 schema。\n\n除了 Pod 必需字段外，在 DaemonSet 中的 Pod 模板必须指定合理的标签（查看 [Pod 选择算符](#pod-selector)）。\n\n在 DaemonSet 中的 Pod 模板必须具有一个值为 `Always` 的\n[`RestartPolicy`](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)。\n当该值未指定时，默认是 `Always`。"}
{"en": "### Pod Selector\n\nThe `.spec.selector` field is a pod selector.  It works the same as the `.spec.selector` of\na [Job](/docs/concepts/workloads/controllers/job/).\n\nYou must specify a pod selector that matches the labels of the\n`.spec.template`.\nAlso, once a DaemonSet is created,\nits `.spec.selector` can not be mutated. Mutating the pod selector can lead to the\nunintentional orphaning of Pods, and it was found to be confusing to users.", "zh": "### Pod 选择算符     {#pod-selector}\n\n`.spec.selector` 字段表示 Pod 选择算符，它与\n[Job](/zh-cn/docs/concepts/workloads/controllers/job/) 的 `.spec.selector` 的作用是相同的。\n\n你必须指定与 `.spec.template` 的标签匹配的 Pod 选择算符。\n此外，一旦创建了 DaemonSet，它的 `.spec.selector` 就不能修改。\n修改 Pod 选择算符可能导致 Pod 意外悬浮，并且这对用户来说是费解的。"}
{"en": "The `.spec.selector` is an object consisting of two fields:", "zh": "`spec.selector` 是一个对象，如下两个字段组成："}
{"en": "* `matchLabels` - works the same as the `.spec.selector` of a\n  [ReplicationController](/docs/concepts/workloads/controllers/replicationcontroller/).\n* `matchExpressions` - allows to build more sophisticated selectors by specifying key,\n  list of values and an operator that relates the key and values.", "zh": "* `matchLabels` - 与 [ReplicationController](/zh-cn/docs/concepts/workloads/controllers/replicationcontroller/)\n  的 `.spec.selector` 的作用相同。\n* `matchExpressions` - 允许构建更加复杂的选择器，可以通过指定 key、value\n  列表以及将 key 和 value 列表关联起来的 Operator。"}
{"en": "When the two are specified the result is ANDed.", "zh": "当上述两个字段都指定时，结果会按逻辑与（AND）操作处理。"}
{"en": "The `.spec.selector` must match the `.spec.template.metadata.labels`.\nConfig with these two not matching will be rejected by the API.", "zh": "`.spec.selector` 必须与 `.spec.template.metadata.labels` 相匹配。\n如果配置中这两个字段不匹配，则会被 API 拒绝。"}
{"en": "### Running Pods on select Nodes\n\nIf you specify a `.spec.template.spec.nodeSelector`, then the DaemonSet controller will\ncreate Pods on nodes which match that [node selector](/docs/concepts/scheduling-eviction/assign-pod-node/).\nLikewise if you specify a `.spec.template.spec.affinity`,\nthen DaemonSet controller will create Pods on nodes which match that\n[node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/).\nIf you do not specify either, then the DaemonSet controller will create Pods on all nodes.", "zh": "### 在选定的节点上运行 Pod   {#running-pods-on-select-nodes}\n\n如果指定了 `.spec.template.spec.nodeSelector`，DaemonSet 控制器将在能够与\n[Node 选择算符](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/)匹配的节点上创建 Pod。\n类似这种情况，可以指定 `.spec.template.spec.affinity`，之后 DaemonSet\n控制器将在能够与[节点亲和性](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/)匹配的节点上创建 Pod。\n如果根本就没有指定，则 DaemonSet Controller 将在所有节点上创建 Pod。"}
{"en": "## How Daemon Pods are scheduled", "zh": "## Daemon Pods 是如何被调度的   {#how-daemon-pods-are-scheduled}"}
{"en": "A DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod.\nThe DaemonSet controller creates a Pod for each eligible node and adds the\n`spec.affinity.nodeAffinity` field of the Pod to match the target host. After\nthe Pod is created, the default scheduler typically takes over and then binds\nthe Pod to the target host by setting the `.spec.nodeName` field.  If the new\nPod cannot fit on the node, the default scheduler may preempt (evict) some of\nthe existing Pods based on the\n[priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority)\nof the new Pod.", "zh": "DaemonSet 可用于确保所有符合条件的节点都运行该 Pod 的一个副本。\nDaemonSet 控制器为每个符合条件的节点创建一个 Pod，并添加 Pod 的 `spec.affinity.nodeAffinity`\n字段以匹配目标主机。Pod 被创建之后，默认的调度程序通常通过设置 `.spec.nodeName` 字段来接管 Pod 并将\nPod 绑定到目标主机。如果新的 Pod 无法放在节点上，则默认的调度程序可能会根据新 Pod\n的[优先级](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority)抢占\n（驱逐）某些现存的 Pod。\n\n{{< note >}}"}
{"en": "If it's important that the DaemonSet pod run on each node, it's often desirable\nto set the `.spec.template.spec.priorityClassName` of the DaemonSet to a\n[PriorityClass](/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass)\nwith a higher priority to ensure that this eviction occurs.", "zh": "当 DaemonSet 中的 Pod 必须运行在每个节点上时，通常需要将 DaemonSet\n的 `.spec.template.spec.priorityClassName` 设置为具有更高优先级的\n[PriorityClass](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass)，\n以确保可以完成驱逐。\n{{< /note >}}"}
{"en": "The user can specify a different scheduler for the Pods of the DaemonSet, by\nsetting the `.spec.template.spec.schedulerName` field of the DaemonSet.\n\nThe original node affinity specified at the\n`.spec.template.spec.affinity.nodeAffinity` field (if specified) is taken into\nconsideration by the DaemonSet controller when evaluating the eligible nodes,\nbut is replaced on the created Pod with the node affinity that matches the name\nof the eligible node.", "zh": "用户通过设置 DaemonSet 的 `.spec.template.spec.schedulerName` 字段，可以为 DaemonSet\n的 Pod 指定不同的调度程序。\n\n当评估符合条件的节点时，原本在 `.spec.template.spec.affinity.nodeAffinity` 字段上指定的节点亲和性将由\nDaemonSet 控制器进行考量，但在创建的 Pod 上会被替换为与符合条件的节点名称匹配的节点亲和性。"}
{"en": "`ScheduleDaemonSetPods` allows you to schedule DaemonSets using the default\nscheduler instead of the DaemonSet controller, by adding the `NodeAffinity` term\nto the DaemonSet pods, instead of the `.spec.nodeName` term. The default\nscheduler is then used to bind the pod to the target host. If node affinity of\nthe DaemonSet pod already exists, it is replaced (the original node affinity was\ntaken into account before selecting the target host). The DaemonSet controller only\nperforms these operations when creating or modifying DaemonSet pods, and no\nchanges are made to the `spec.template` of the DaemonSet.", "zh": "`ScheduleDaemonSetPods` 允许你使用默认调度器而不是 DaemonSet 控制器来调度这些 DaemonSet，\n方法是将 `NodeAffinity` 条件而不是 `.spec.nodeName` 条件添加到这些 DaemonSet Pod。\n默认调度器接下来将 Pod 绑定到目标主机。\n如果 DaemonSet Pod 的节点亲和性配置已存在，则被替换\n（原始的节点亲和性配置在选择目标主机之前被考虑）。\nDaemonSet 控制器仅在创建或修改 DaemonSet Pod 时执行这些操作，\n并且不会更改 DaemonSet 的 `spec.template`。\n\n```yaml\nnodeAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n    nodeSelectorTerms:\n    - matchFields:\n      - key: metadata.name\n        operator: In\n        values:\n        - target-host-name\n```"}
{"en": "### Taints and tolerations\n\nThe DaemonSet controller automatically adds a set of {{< glossary_tooltip\ntext=\"tolerations\" term_id=\"toleration\" >}} to DaemonSet Pods:", "zh": "### 污点和容忍度   {#taint-and-toleration}\n\nDaemonSet 控制器会自动将一组容忍度添加到 DaemonSet Pod："}
{"en": "Tolerations for DaemonSet pods", "zh": "{{< table caption=\"DaemonSet Pod 适用的容忍度\" >}}"}
{"en": "| Toleration key                                                                                                        | Effect       | Details                                                                                                                                       |\n| --------------------------------------------------------------------------------------------------------------------- | ------------ | --------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`node.kubernetes.io/not-ready`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-not-ready)             | `NoExecute`  | DaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted. |\n| [`node.kubernetes.io/unreachable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-unreachable)         | `NoExecute`  | DaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted. |\n| [`node.kubernetes.io/disk-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-disk-pressure)     | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with disk pressure issues.                                                                         |\n| [`node.kubernetes.io/memory-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-memory-pressure) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with memory pressure issues.                                                                        |\n| [`node.kubernetes.io/pid-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-pid-pressure) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with process pressure issues.                                                                        |\n| [`node.kubernetes.io/unschedulable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-unschedulable)   | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes that are unschedulable.                                                                            |\n| [`node.kubernetes.io/network-unavailable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-network-unavailable) | `NoSchedule` | **Only added for DaemonSet Pods that request host networking**, i.e., Pods having `spec.hostNetwork: true`. Such DaemonSet Pods can be scheduled onto nodes with unavailable network.|", "zh": "| 容忍度键名                                                | 效果       | 描述                    |\n| -------------------------------------------------------- | ---------- | ----------------------- |\n| [`node.kubernetes.io/not-ready`](/zh-cn/docs/reference/labels-annotations-taints/#node-kubernetes-io-not-ready) | `NoExecute`  | DaemonSet Pod 可以被调度到不健康或还不准备接受 Pod 的节点上。在这些节点上运行的所有 DaemonSet Pod 将不会被驱逐。 |\n| [`node.kubernetes.io/unreachable`](/zh-cn/docs/reference/labels-annotations-taints/#node-kubernetes-io-unreachable)  | `NoExecute`  | DaemonSet Pod 可以被调度到从节点控制器不可达的节点上。在这些节点上运行的所有 DaemonSet Pod 将不会被驱逐。 |\n| [`node.kubernetes.io/disk-pressure`](/zh-cn/docs/reference/labels-annotations-taints/#node-kubernetes-io-disk-pressure) | `NoSchedule` | DaemonSet Pod 可以被调度到具有磁盘压力问题的节点上。   |\n| [`node.kubernetes.io/memory-pressure`](/zh-cn/docs/reference/labels-annotations-taints/#node-kubernetes-io-memory-pressure) | `NoSchedule` | DaemonSet Pod 可以被调度到具有内存压力问题的节点上。 |\n| [`node.kubernetes.io/pid-pressure`](/zh-cn/docs/reference/labels-annotations-taints/#node-kubernetes-io-pid-pressure) | `NoSchedule` | DaemonSet Pod 可以被调度到具有进程压力问题的节点上。 |\n| [`node.kubernetes.io/unschedulable`](/zh-cn/docs/reference/labels-annotations-taints/#node-kubernetes-io-unschedulable) | `NoSchedule` | DaemonSet Pod 可以被调度到不可调度的节点上。 |\n| [`node.kubernetes.io/network-unavailable`](/zh-cn/docs/reference/labels-annotations-taints/#node-kubernetes-io-network-unavailable) | `NoSchedule` | **仅针对请求主机联网的 DaemonSet Pod 添加此容忍度**，即 Pod 具有 `spec.hostNetwork: true`。这些 DaemonSet Pod 可以被调度到网络不可用的节点上。|\n\n{{< /table >}}"}
{"en": "You can add your own tolerations to the Pods of a DaemonSet as well, by\ndefining these in the Pod template of the DaemonSet.\n\nBecause the DaemonSet controller sets the\n`node.kubernetes.io/unschedulable:NoSchedule` toleration automatically,\nKubernetes can run DaemonSet Pods on nodes that are marked as _unschedulable_.", "zh": "你也可以在 DaemonSet 的 Pod 模板中定义自己的容忍度并将其添加到 DaemonSet Pod。\n\n因为 DaemonSet 控制器自动设置 `node.kubernetes.io/unschedulable:NoSchedule` 容忍度，\n所以 Kubernetes 可以在标记为**不可调度**的节点上运行 DaemonSet Pod。"}
{"en": "If you use a DaemonSet to provide an important node-level function, such as\n[cluster networking](/docs/concepts/cluster-administration/networking/), it is\nhelpful that Kubernetes places DaemonSet Pods on nodes before they are ready.\nFor example, without that special toleration, you could end up in a deadlock\nsituation where the node is not marked as ready because the network plugin is\nnot running there, and at the same time the network plugin is not running on\nthat node because the node is not yet ready.", "zh": "如果你使用 DaemonSet 提供重要的节点级别功能，\n例如[集群联网](/zh-cn/docs/concepts/cluster-administration/networking/)，\nKubernetes 在节点就绪之前将 DaemonSet Pod 放到节点上会很有帮助。\n例如，如果没有这种特殊的容忍度，因为网络插件未在节点上运行，所以你可能会在未标记为就绪的节点上陷入死锁状态，\n同时因为该节点还未就绪，所以网络插件不会在该节点上运行。"}
{"en": "## Communicating with Daemon Pods", "zh": "## 与 Daemon Pod 通信   {#communicating-with-daemon-pods}"}
{"en": "Some possible patterns for communicating with Pods in a DaemonSet are:\n\n- **Push**: Pods in the DaemonSet are configured to send updates to another service, such\n  as a stats database.  They do not have clients.\n- **NodeIP and Known Port**: Pods in the DaemonSet can use a `hostPort`, so that the pods\n  are reachable via the node IPs.\n  Clients know the list of node IPs somehow, and know the port by convention.\n- **DNS**: Create a [headless service](/docs/concepts/services-networking/service/#headless-services)\n  with the same pod selector, and then discover DaemonSets using the `endpoints`\n  resource or retrieve multiple A records from DNS.\n- **Service**: Create a service with the same Pod selector, and use the service to reach a\n  daemon on a random node. (No way to reach specific node.)", "zh": "与 DaemonSet 中的 Pod 进行通信的几种可能模式如下：\n\n- **推送（Push）**：配置 DaemonSet 中的 Pod，将更新发送到另一个服务，例如统计数据库。\n  这些服务没有客户端。\n\n- **NodeIP 和已知端口**：DaemonSet 中的 Pod 可以使用 `hostPort`，从而可以通过节点 IP\n  访问到 Pod。客户端能通过某种方法获取节点 IP 列表，并且基于此也可以获取到相应的端口。\n\n- **DNS**：创建具有相同 Pod 选择算符的[无头服务](/zh-cn/docs/concepts/services-networking/service/#headless-services)，\n  通过使用 `endpoints` 资源或从 DNS 中检索到多个 A 记录来发现 DaemonSet。\n\n- **Service**：创建具有相同 Pod 选择算符的服务，并使用该服务随机访问到某个节点上的守护进程（没有办法访问到特定节点）。"}
{"en": "## Updating a DaemonSet\n\nIf node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete\nPods from newly not-matching nodes.\n\nYou can modify the Pods that a DaemonSet creates.  However, Pods do not allow all\nfields to be updated.  Also, the DaemonSet controller will use the original template the next\ntime a node (even with the same name) is created.", "zh": "## 更新 DaemonSet   {#updating-a-daemon-set}\n\n如果节点的标签被修改，DaemonSet 将立刻向新匹配上的节点添加 Pod，\n同时删除不匹配的节点上的 Pod。\n\n你可以修改 DaemonSet 创建的 Pod。不过并非 Pod 的所有字段都可更新。\n下次当某节点（即使具有相同的名称）被创建时，DaemonSet 控制器还会使用最初的模板。"}
{"en": "You can delete a DaemonSet.  If you specify `--cascade=orphan` with `kubectl`, then the Pods\nwill be left on the nodes.  If you subsequently create a new DaemonSet with the same selector,\nthe new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces\nthem according to its `updateStrategy`.\n\nYou can [perform a rolling update](/docs/tasks/manage-daemon/update-daemon-set/) on a DaemonSet.", "zh": "你可以删除一个 DaemonSet。如果使用 `kubectl` 并指定 `--cascade=orphan` 选项，\n则 Pod 将被保留在节点上。接下来如果创建使用相同选择算符的新 DaemonSet，\n新的 DaemonSet 会收养已有的 Pod。\n如果有 Pod 需要被替换，DaemonSet 会根据其 `updateStrategy` 来替换。\n\n你可以对 DaemonSet [执行滚动更新](/zh-cn/docs/tasks/manage-daemon/update-daemon-set/)操作。"}
{"en": "## Alternatives to DaemonSet\n\n### Init scripts", "zh": "## DaemonSet 的替代方案   {#alternatives-to-daemon-set}\n\n### init 脚本   {#init-scripts}"}
{"en": "It is certainly possible to run daemon processes by directly starting them on a node (e.g. using\n`init`, `upstartd`, or `systemd`).  This is perfectly fine.  However, there are several advantages to\nrunning such processes via a DaemonSet:\n\n- Ability to monitor and manage logs for daemons in the same way as applications.\n- Same config language and tools (e.g. Pod templates, `kubectl`) for daemons and applications.\n- Running daemons in containers with resource limits increases isolation between daemons from app\n  containers.  However, this can also be accomplished by running the daemons in a container but not in a Pod.", "zh": "直接在节点上启动守护进程（例如使用 `init`、`upstartd` 或 `systemd`）的做法当然是可行的。\n不过，基于 DaemonSet 来运行这些进程有如下一些好处：\n\n- 像所运行的其他应用一样，DaemonSet 具备为守护进程提供监控和日志管理的能力。\n\n- 为守护进程和应用所使用的配置语言和工具（如 Pod 模板、`kubectl`）是相同的。\n\n- 在资源受限的容器中运行守护进程能够增加守护进程和应用容器的隔离性。\n  然而，这一点也可以通过在容器中运行守护进程但却不在 Pod 中运行之来实现。"}
{"en": "### Bare Pods\n\nIt is possible to create Pods directly which specify a particular node to run on.  However,\na DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of\nnode failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should\nuse a DaemonSet rather than creating individual Pods.", "zh": "### 裸 Pod   {#bare-pods}\n\n直接创建 Pod并指定其运行在特定的节点上也是可以的。\n然而，DaemonSet 能够替换由于任何原因（例如节点失败、例行节点维护、内核升级）\n而被删除或终止的 Pod。\n由于这个原因，你应该使用 DaemonSet 而不是单独创建 Pod。"}
{"en": "### Static Pods\n\nIt is possible to create Pods by writing a file to a certain directory watched by Kubelet.  These\nare called [static pods](/docs/tasks/configure-pod-container/static-pod/).\nUnlike DaemonSet, static Pods cannot be managed with kubectl\nor other Kubernetes API clients.  Static Pods do not depend on the apiserver, making them useful\nin cluster bootstrapping cases.  Also, static Pods may be deprecated in the future.", "zh": "### 静态 Pod   {#static-pods}\n\n通过在一个指定的、受 `kubelet` 监视的目录下编写文件来创建 Pod 也是可行的。\n这类 Pod 被称为[静态 Pod](/zh-cn/docs/tasks/configure-pod-container/static-pod/)。\n不像 DaemonSet，静态 Pod 不受 `kubectl` 和其它 Kubernetes API 客户端管理。\n静态 Pod 不依赖于 API 服务器，这使得它们在启动引导新集群的情况下非常有用。\n此外，静态 Pod 在将来可能会被废弃。"}
{"en": "### Deployments\n\nDaemonSets are similar to [Deployments](/docs/concepts/workloads/controllers/deployment/) in that\nthey both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,\nstorage servers).\n\nUse a Deployment for stateless services, like frontends, where scaling up and down the\nnumber of replicas and rolling out updates are more important than controlling exactly which host\nthe Pod runs on.  Use a DaemonSet when it is important that a copy of a Pod always run on\nall or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.\n\nFor example, [network plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)\noften include a component that runs as a DaemonSet. The DaemonSet component makes sure\nthat the node where it's running has working cluster networking.", "zh": "### Deployment\n\nDaemonSet 与 [Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/) 非常类似，\n它们都能创建 Pod，并且 Pod 中的进程都不希望被终止（例如，Web 服务器、存储服务器）。\n\n建议为无状态的服务使用 Deployment，比如前端服务。\n对这些服务而言，对副本的数量进行扩缩容、平滑升级，比精确控制 Pod 运行在某个主机上要重要得多。\n当需要 Pod 副本总是运行在全部或特定主机上，并且当该 DaemonSet 提供了节点级别的功能（允许其他 Pod 在该特定节点上正确运行）时，\n应该使用 DaemonSet。\n\n例如，[网络插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)通常包含一个以 DaemonSet 运行的组件。\n这个 DaemonSet 组件确保它所在的节点的集群网络正常工作。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn about [Pods](/docs/concepts/workloads/pods).\n  * Learn about [static Pods](#static-pods), which are useful for running Kubernetes\n    {{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} components.\n* Find out how to use DaemonSets\n  * [Perform a rolling update on a DaemonSet](/docs/tasks/manage-daemon/update-daemon-set/)\n  * [Perform a rollback on a DaemonSet](/docs/tasks/manage-daemon/rollback-daemon-set/)\n    (for example, if a roll out didn't work how you expected).\n* Understand [how Kubernetes assigns Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/).\n* Learn about [device plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) and\n  [add ons](/docs/concepts/cluster-administration/addons/), which often run as DaemonSets.\n* `DaemonSet` is a top-level resource in the Kubernetes REST API.\n  Read the {{< api-reference page=\"workload-resources/daemon-set-v1\" >}}\n  object definition to understand the API for daemon sets.", "zh": "* 了解 [Pod](/zh-cn/docs/concepts/workloads/pods)。\n  * 了解[静态 Pod](#static-pods)，这对运行 Kubernetes {{< glossary_tooltip text=\"控制面\" term_id=\"control-plane\" >}}组件有帮助。\n* 了解如何使用 DaemonSet\n  * [对 DaemonSet 执行滚动更新](/zh-cn/docs/tasks/manage-daemon/update-daemon-set/)\n  * [对 DaemonSet 执行回滚](/zh-cn/docs/tasks/manage-daemon/rollback-daemon-set/)（例如：新的版本没有达到你的预期）\n* 理解[Kubernetes 如何将 Pod 分配给节点](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/)。\n* 了解[设备插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)和\n  [扩展（Addons）](/zh-cn/docs/concepts/cluster-administration/addons/)，它们常以 DaemonSet 运行。\n* `DaemonSet` 是 Kubernetes REST API 中的顶级资源。阅读 {{< api-reference page=\"workload-resources/daemon-set-v1\" >}}\n   对象定义理解关于该资源的 API。"}
{"en": "A _Deployment_ provides declarative updates for {{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} and\n{{< glossary_tooltip term_id=\"replica-set\" text=\"ReplicaSets\" >}}.", "zh": "一个 Deployment 为 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}\n和 {{< glossary_tooltip term_id=\"replica-set\" text=\"ReplicaSet\" >}}\n提供声明式的更新能力。"}
{"en": "You describe a _desired state_ in a Deployment, and the Deployment {{< glossary_tooltip term_id=\"controller\" >}} changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.", "zh": "你负责描述 Deployment 中的**目标状态**，而 Deployment {{< glossary_tooltip term_id=\"controller\" >}}\n以受控速率更改实际状态，\n使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment，\n并通过新的 Deployment 收养其资源。\n\n{{< note >}}"}
{"en": "Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.", "zh": "不要管理 Deployment 所拥有的 ReplicaSet。\n如果存在下面未覆盖的使用场景，请考虑在 Kubernetes 仓库中提出 Issue。\n{{< /note >}}"}
{"en": "## Use Case\n\nThe following are typical use cases for Deployments:", "zh": "## 用例\n\n以下是 Deployments 的典型用例："}
{"en": "* [Create a Deployment to rollout a ReplicaSet](#creating-a-deployment). The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.\n* [Declare the new state of the Pods](#updating-a-deployment) by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.", "zh": "* [创建 Deployment 以将 ReplicaSet 上线](#creating-a-deployment)。ReplicaSet 在后台创建 Pod。\n  检查 ReplicaSet 的上线状态，查看其是否成功。\n* 通过更新 Deployment 的 PodTemplateSpec，[声明 Pod 的新状态](#updating-a-deployment) 。\n  新的 ReplicaSet 会被创建，Deployment 以受控速率将 Pod 从旧 ReplicaSet 迁移到新 ReplicaSet。\n  每个新的 ReplicaSet 都会更新 Deployment 的修订版本。"}
{"en": "* [Rollback to an earlier Deployment revision](#rolling-back-a-deployment) if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.\n* [Scale up the Deployment to facilitate more load](#scaling-a-deployment).\n* [Pause the rollout of a Deployment](#pausing-and-resuming-a-deployment) to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.\n* [Use the status of the Deployment](#deployment-status) as an indicator that a rollout has stuck.\n* [Clean up older ReplicaSets](#clean-up-policy) that you don't need anymore.", "zh": "* 如果 Deployment 的当前状态不稳定，[回滚到较早的 Deployment 版本](#rolling-back-a-deployment)。\n  每次回滚都会更新 Deployment 的修订版本。\n* [扩大 Deployment 规模以承担更多负载](#scaling-a-deployment)。\n* [暂停 Deployment 的上线](#pausing-and-resuming-a-deployment) 以应用对 PodTemplateSpec 所作的多项修改，\n  然后恢复其执行以启动新的上线版本。\n* [使用 Deployment 状态](#deployment-status)来判定上线过程是否出现停滞。\n* [清理较旧的不再需要的 ReplicaSet](#clean-up-policy) 。"}
{"en": "The following is an example of a Deployment. It creates a ReplicaSet to bring up three `nginx` Pods:", "zh": "下面是一个 Deployment 示例。其中创建了一个 ReplicaSet，负责启动三个 `nginx` Pod：\n\n{{% code_sample file=\"controllers/nginx-deployment.yaml\" %}}"}
{"en": "In this example:", "zh": "在该例中："}
{"en": "* A Deployment named `nginx-deployment` is created, indicated by the\n  `.metadata.name` field. This name will become the basis for the ReplicaSets\n  and Pods which are created later. See [Writing a Deployment Spec](#writing-a-deployment-spec)\n  for more details.\n* The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the `.spec.replicas` field.\n* The `.spec.selector` field defines how the created ReplicaSet finds which Pods to manage.\n  In this case, you select a label that is defined in the Pod template (`app: nginx`).\n  However, more sophisticated selection rules are possible,\n  as long as the Pod template itself satisfies the rule.", "zh": "* 创建名为 `nginx-deployment`（由 `.metadata.name` 字段标明）的 Deployment。\n  该名称将成为后续创建 ReplicaSet 和 Pod 的命名基础。\n  参阅[编写 Deployment 规约](#writing-a-deployment-spec)获取更多详细信息。\n* 该 Deployment 创建一个 ReplicaSet，它创建三个（由 `.spec.replicas` 字段标明）Pod 副本。\n* `.spec.selector` 字段定义所创建的 ReplicaSet 如何查找要管理的 Pod。\n  在这里，你选择在 Pod 模板中定义的标签（`app: nginx`）。\n  不过，更复杂的选择规则是也可能的，只要 Pod 模板本身满足所给规则即可。\n\n  {{< note >}}"}
{"en": "The `.spec.selector.matchLabels` field is a map of {key,value} pairs.\n  A single {key,value} in the `matchLabels` map is equivalent to an element of `matchExpressions`,\n  whose `key` field is \"key\", the `operator` is \"In\", and the `values` array contains only \"value\".\n  All of the requirements, from both `matchLabels` and `matchExpressions`, must be satisfied in order to match.", "zh": "`.spec.selector.matchLabels` 字段是 `{key,value}` 键值对映射。\n  在 `matchLabels` 映射中的每个 `{key,value}` 映射等效于 `matchExpressions` 中的一个元素，\n  即其 `key` 字段是 “key”，`operator` 为 “In”，`values` 数组仅包含 “value”。\n  在 `matchLabels` 和 `matchExpressions` 中给出的所有条件都必须满足才能匹配。\n  {{< /note >}}"}
{"en": "* The `template` field contains the following sub-fields:\n  * The Pods are labeled `app: nginx`using the `.metadata.labels` field.\n  * The Pod template's specification, or `.template.spec` field, indicates that\n    the Pods run one container, `nginx`, which runs the `nginx`\n    [Docker Hub](https://hub.docker.com/) image at version 1.14.2.\n  * Create one container and name it `nginx` using the `.spec.template.spec.containers[0].name` field.", "zh": "* `template` 字段包含以下子字段：\n  * Pod 被使用 `.metadata.labels` 字段打上 `app: nginx` 标签。\n  * Pod 模板规约（即 `.template.spec` 字段）指示 Pod 运行一个 `nginx` 容器，\n    该容器运行版本为 1.14.2 的 `nginx` [Docker Hub](https://hub.docker.com/) 镜像。\n  * 创建一个容器并使用 `.spec.template.spec.containers[0].name` 字段将其命名为 `nginx`。"}
{"en": "Before you begin, make sure your Kubernetes cluster is up and running.\nFollow the steps given below to create the above Deployment:", "zh": "开始之前，请确保的 Kubernetes 集群已启动并运行。\n按照以下步骤创建上述 Deployment ："}
{"en": "1. Create the Deployment by running the following command:", "zh": "1. 通过运行以下命令创建 Deployment ：\n\n   ```shell\n   kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml\n   ```"}
{"en": "2. Run `kubectl get deployments` to check if the Deployment was created.\n\n   If the Deployment is still being created, the output is similar to the following:", "zh": "2. 运行 `kubectl get deployments` 检查 Deployment 是否已创建。\n   如果仍在创建 Deployment，则输出类似于：\n\n   ```\n   NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n   nginx-deployment   0/3     0            0           1s\n   ```"}
{"en": "When you inspect the Deployments in your cluster, the following fields are displayed:", "zh": "在检查集群中的 Deployment 时，所显示的字段有："}
{"en": "* `NAME` lists the names of the Deployments in the namespace.\n   * `READY` displays how many replicas of the application are available to your users. It follows the pattern ready/desired.\n   * `UP-TO-DATE` displays the number of replicas that have been updated to achieve the desired state.\n   * `AVAILABLE` displays how many replicas of the application are available to your users.\n   * `AGE` displays the amount of time that the application has been running.", "zh": "* `NAME` 列出了名字空间中 Deployment 的名称。\n   * `READY` 显示应用程序的可用的“副本”数。显示的模式是“就绪个数/期望个数”。\n   * `UP-TO-DATE` 显示为了达到期望状态已经更新的副本数。\n   * `AVAILABLE` 显示应用可供用户使用的副本数。\n   * `AGE` 显示应用程序运行的时间。"}
{"en": "Notice how the number of desired replicas is 3 according to `.spec.replicas` field.", "zh": "请注意期望副本数是根据 `.spec.replicas` 字段设置 3。"}
{"en": "3. To see the Deployment rollout status, run `kubectl rollout status deployment/nginx-deployment`.\n\n   The output is similar to:", "zh": "3. 要查看 Deployment 上线状态，运行 `kubectl rollout status deployment/nginx-deployment`。\n\n   输出类似于：\n\n   ```\n   Waiting for rollout to finish: 2 out of 3 new replicas have been updated...\n   deployment \"nginx-deployment\" successfully rolled out\n   ```"}
{"en": "4. Run the `kubectl get deployments` again a few seconds later.\n   The output is similar to this:", "zh": "4. 几秒钟后再次运行 `kubectl get deployments`。输出类似于：\n\n   ```\n   NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n   nginx-deployment   3/3     3            3           18s\n   ```"}
{"en": "Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.", "zh": "注意 Deployment 已创建全部三个副本，并且所有副本都是最新的（它们包含最新的 Pod 模板）\n   并且可用。"}
{"en": "5. To see the ReplicaSet (`rs`) created by the Deployment, run `kubectl get rs`. The output is similar to this:", "zh": "5. 要查看 Deployment 创建的 ReplicaSet（`rs`），运行 `kubectl get rs`。\n   输出类似于：\n\n   ```\n   NAME                          DESIRED   CURRENT   READY   AGE\n   nginx-deployment-75675f5897   3         3         3       18s\n   ```"}
{"en": "ReplicaSet output shows the following fields:\n\n   * `NAME` lists the names of the ReplicaSets in the namespace.\n   * `DESIRED` displays the desired number of _replicas_ of the application, which you define when you create the Deployment. This is the _desired state_.\n   * `CURRENT` displays how many replicas are currently running.\n   * `READY` displays how many replicas of the application are available to your users.\n   * `AGE` displays the amount of time that the application has been running.", "zh": "ReplicaSet 输出中包含以下字段：\n\n   * `NAME` 列出名字空间中 ReplicaSet 的名称；\n   * `DESIRED` 显示应用的期望副本个数，即在创建 Deployment 时所定义的值。\n     此为期望状态；\n   * `CURRENT` 显示当前运行状态中的副本个数；\n   * `READY` 显示应用中有多少副本可以为用户提供服务；\n   * `AGE` 显示应用已经运行的时间长度。"}
{"en": "Notice that the name of the ReplicaSet is always formatted as\n   `[DEPLOYMENT-NAME]-[HASH]`. This name will become the basis for the Pods\n   which are created.\n   The `HASH` string is the same as the `pod-template-hash` label on the ReplicaSet.", "zh": "注意 ReplicaSet 的名称格式始终为 `[Deployment 名称]-[哈希]`。\n   该名称将成为所创建的 Pod 的命名基础。\n   其中的`哈希`字符串与 ReplicaSet 上的 `pod-template-hash` 标签一致。"}
{"en": "6. To see the labels automatically generated for each Pod, run `kubectl get pods --show-labels`.\n   The output is similar to:", "zh": "6. 要查看每个 Pod 自动生成的标签，运行 `kubectl get pods --show-labels`。\n   输出类似于：\n\n   ```\n   NAME                                READY     STATUS    RESTARTS   AGE       LABELS\n   nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\n   nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\n   nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\n   ```"}
{"en": "The created ReplicaSet ensures that there are three `nginx` Pods.", "zh": "所创建的 ReplicaSet 确保总是存在三个 `nginx` Pod。\n\n{{< note >}}"}
{"en": "You must specify an appropriate selector and Pod template labels in a Deployment\n(in this case, `app: nginx`).\n\nDo not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.", "zh": "你必须在 Deployment 中指定适当的选择算符和 Pod 模板标签（在本例中为 `app: nginx`）。\n标签或者选择算符不要与其他控制器（包括其他 Deployment 和 StatefulSet）重叠。\nKubernetes 不会阻止你这样做，但是如果多个控制器具有重叠的选择算符，\n它们可能会发生冲突执行难以预料的操作。\n{{< /note >}}"}
{"en": "### Pod-template-hash label", "zh": "### Pod-template-hash 标签\n\n{{< caution >}}"}
{"en": "Do not change this label.", "zh": "不要更改此标签。\n{{< /caution >}}"}
{"en": "The `pod-template-hash` label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.", "zh": "Deployment 控制器将 `pod-template-hash` 标签添加到 Deployment\n所创建或收留的每个 ReplicaSet 。"}
{"en": "This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the `PodTemplate` of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,\nand in any existing Pods that the ReplicaSet might have.", "zh": "此标签可确保 Deployment 的子 ReplicaSet 不重叠。\n标签是通过对 ReplicaSet 的 `PodTemplate` 进行哈希处理。\n所生成的哈希值被添加到 ReplicaSet 选择算符、Pod 模板标签，并存在于在 ReplicaSet\n可能拥有的任何现有 Pod 中。"}
{"en": "## Updating a Deployment", "zh": "## 更新 Deployment   {#updating-a-deployment}\n\n{{< note >}}"}
{"en": "A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, `.spec.template`)\nis changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.", "zh": "仅当 Deployment Pod 模板（即 `.spec.template`）发生改变时，例如模板的标签或容器镜像被更新，\n才会触发 Deployment 上线。其他更新（如对 Deployment 执行扩缩容的操作）不会触发上线动作。\n{{< /note >}}"}
{"en": "Follow the steps given below to update your Deployment:", "zh": "按照以下步骤更新 Deployment："}
{"en": "1. Let's update the nginx Pods to use the `nginx:1.16.1` image instead of the `nginx:1.14.2` image.", "zh": "1. 先来更新 nginx Pod 以使用 `nginx:1.16.1` 镜像，而不是 `nginx:1.14.2` 镜像。\n\n   ```shell\n   kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1\n   ```"}
{"en": "or use the following command:", "zh": "或者使用下面的命令：\n\n   ```shell\n   kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n   ```"}
{"en": "where `deployment/nginx-deployment` indicates the Deployment,\n   `nginx` indicates the Container the update will take place and\n   `nginx:1.16.1` indicates the new image and its tag.", "zh": "在这里，`deployment/nginx-deployment` 表明 Deployment 的名称，`nginx` 表明需要进行更新的容器，\n   而 `nginx:1.16.1` 则表示镜像的新版本以及它的标签。"}
{"en": "The output is similar to:", "zh": "输出类似于：\n\n   ```\n   deployment.apps/nginx-deployment image updated\n   ```"}
{"en": "Alternatively, you can `edit` the Deployment and change `.spec.template.spec.containers[0].image` from `nginx:1.14.2` to `nginx:1.16.1`:", "zh": "或者，可以对 Deployment 执行 `edit` 操作并将 `.spec.template.spec.containers[0].image` 从\n   `nginx:1.14.2` 更改至 `nginx:1.16.1`。\n\n   ```shell\n   kubectl edit deployment/nginx-deployment\n   ```"}
{"en": "The output is similar to:", "zh": "输出类似于：\n\n   ```\n   deployment.apps/nginx-deployment edited\n   ```"}
{"en": "2. To see the rollout status, run:", "zh": "2. 要查看上线状态，运行：\n\n   ```shell\n   kubectl rollout status deployment/nginx-deployment\n   ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n   ```\n   Waiting for rollout to finish: 2 out of 3 new replicas have been updated...\n   ```"}
{"en": "or", "zh": "或者\n\n   ```\n   deployment \"nginx-deployment\" successfully rolled out\n   ```"}
{"en": "Get more details on your updated Deployment:", "zh": "获取关于已更新的 Deployment 的更多信息："}
{"en": "* After the rollout succeeds, you can view the Deployment by running `kubectl get deployments`.\n  The output is similar to this:", "zh": "* 在上线成功后，可以通过运行 `kubectl get deployments` 来查看 Deployment：\n  输出类似于：\n\n  ```ini\n  NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n  nginx-deployment   3/3     3            3           36s\n  ```"}
{"en": "* Run `kubectl get rs` to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it\nup to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.", "zh": "* 运行 `kubectl get rs` 以查看 Deployment 通过创建新的 ReplicaSet 并将其扩容到\n  3 个副本并将旧 ReplicaSet 缩容到 0 个副本完成了 Pod 的更新操作：\n\n  ```shell\n  kubectl get rs\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  NAME                          DESIRED   CURRENT   READY   AGE\n  nginx-deployment-1564180365   3         3         3       6s\n  nginx-deployment-2035384211   0         0         0       36s\n  ```"}
{"en": "* Running `get pods` should now show only the new Pods:", "zh": "* 现在运行 `get pods` 应仅显示新的 Pod：\n\n  ```shell\n  kubectl get pods\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  NAME                                READY     STATUS    RESTARTS   AGE\n  nginx-deployment-1564180365-khku8   1/1       Running   0          14s\n  nginx-deployment-1564180365-nacti   1/1       Running   0          14s\n  nginx-deployment-1564180365-z9gth   1/1       Running   0          14s\n  ```"}
{"en": "Next time you want to update these Pods, you only need to update the Deployment's Pod template again.\n\n  Deployment ensures that only a certain number of Pods are down while they are being updated. By default,\n  it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).", "zh": "下次要更新这些 Pod 时，只需再次更新 Deployment Pod 模板即可。\n\n  Deployment 可确保在更新时仅关闭一定数量的 Pod。默认情况下，它确保至少所需 Pod 的 75% 处于运行状态（最大不可用比例为 25%）。"}
{"en": "Deployment also ensures that only a certain number of Pods are created above the desired number of Pods.\n  By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).", "zh": "Deployment 还确保仅所创建 Pod 数量只可能比期望 Pod 数高一点点。\n  默认情况下，它可确保启动的 Pod 个数比期望个数最多多出 125%（最大峰值 25%）。"}
{"en": "For example, if you look at the above Deployment closely, you will see that it first creates a new Pod,\n  then deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of\n  new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.\n  It makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of\n  a Deployment with 4 replicas, the number of Pods would be between 3 and 5.", "zh": "例如，如果仔细查看上述 Deployment ，将看到它首先创建了一个新的 Pod，然后删除旧的 Pod，\n  并创建了新的 Pod。它不会杀死旧 Pod，直到有足够数量的新 Pod 已经出现。\n  在足够数量的旧 Pod 被杀死前并没有创建新 Pod。它确保至少 3 个 Pod 可用，\n  同时最多总共 4 个 Pod 可用。\n  当 Deployment 设置为 4 个副本时，Pod 的个数会介于 3 和 5 之间。"}
{"en": "* Get details of your Deployment:", "zh": "* 获取 Deployment 的更多信息\n\n  ```shell\n  kubectl describe deployments\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  Name:                   nginx-deployment\n  Namespace:              default\n  CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000\n  Labels:                 app=nginx\n  Annotations:            deployment.kubernetes.io/revision=2\n  Selector:               app=nginx\n  Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\n  StrategyType:           RollingUpdate\n  MinReadySeconds:        0\n  RollingUpdateStrategy:  25% max unavailable, 25% max surge\n  Pod Template:\n    Labels:  app=nginx\n     Containers:\n      nginx:\n        Image:        nginx:1.16.1\n        Port:         80/TCP\n        Environment:  <none>\n        Mounts:       <none>\n      Volumes:        <none>\n    Conditions:\n      Type           Status  Reason\n      ----           ------  ------\n      Available      True    MinimumReplicasAvailable\n      Progressing    True    NewReplicaSetAvailable\n    OldReplicaSets:  <none>\n    NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)\n    Events:\n      Type    Reason             Age   From                   Message\n      ----    ------             ----  ----                   -------\n      Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3\n      Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1\n      Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2\n      Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2\n      Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1\n      Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3\n      Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0\n  ```"}
{"en": "Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)\n  and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet\n  (nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet\n  to 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.\n  It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.\n  Finally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.", "zh": "可以看到，当第一次创建 Deployment 时，它创建了一个 ReplicaSet（`nginx-deployment-2035384211`）\n  并将其直接扩容至 3 个副本。更新 Deployment 时，它创建了一个新的 ReplicaSet\n  （nginx-deployment-1564180365），并将其扩容为 1，等待其就绪；然后将旧 ReplicaSet 缩容到 2，\n  将新的 ReplicaSet 扩容到 2 以便至少有 3 个 Pod 可用且最多创建 4 个 Pod。\n  然后，它使用相同的滚动更新策略继续对新的 ReplicaSet 扩容并对旧的 ReplicaSet 缩容。\n  最后，你将有 3 个可用的副本在新的 ReplicaSet 中，旧 ReplicaSet 将缩容到 0。\n\n{{< note >}}"}
{"en": "Kubernetes doesn't count terminating Pods when calculating the number of `availableReplicas`, which must be between\n`replicas - maxUnavailable` and `replicas + maxSurge`. As a result, you might notice that there are more Pods than\nexpected during a rollout, and that the total resources consumed by the Deployment is more than `replicas + maxSurge`\nuntil the `terminationGracePeriodSeconds` of the terminating Pods expires.", "zh": "Kubernetes 在计算 `availableReplicas` 数值时不考虑终止过程中的 Pod，\n`availableReplicas` 的值一定介于 `replicas - maxUnavailable` 和 `replicas + maxSurge` 之间。\n因此，你可能在上线期间看到 Pod 个数比预期的多，Deployment 所消耗的总的资源也大于\n`replicas + maxSurge` 个 Pod 所用的资源，直到被终止的 Pod 所设置的\n`terminationGracePeriodSeconds` 到期为止。\n{{< /note >}}"}
{"en": "### Rollover (aka multiple updates in-flight)\n\nEach time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up\nthe desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels\nmatch `.spec.selector` but whose template does not match `.spec.template` are scaled down. Eventually, the new\nReplicaSet is scaled to `.spec.replicas` and all old ReplicaSets is scaled to 0.", "zh": "### 翻转（多 Deployment 动态更新）\n\nDeployment 控制器每次注意到新的 Deployment 时，都会创建一个 ReplicaSet 以启动所需的 Pod。\n如果更新了 Deployment，则控制标签匹配 `.spec.selector` 但模板不匹配 `.spec.template` 的 Pod 的现有 ReplicaSet 被缩容。\n最终，新的 ReplicaSet 缩放为 `.spec.replicas` 个副本，\n所有旧 ReplicaSet 缩放为 0 个副本。"}
{"en": "If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet\nas per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously\n-- it will add it to its list of old ReplicaSets and start scaling it down.", "zh": "当 Deployment 正在上线时被更新，Deployment 会针对更新创建一个新的 ReplicaSet\n并开始对其扩容，之前正在被扩容的 ReplicaSet 会被翻转，添加到旧 ReplicaSet 列表\n并开始缩容。"}
{"en": "For example, suppose you create a Deployment to create 5 replicas of `nginx:1.14.2`,\nbut then update the Deployment to create 5 replicas of `nginx:1.16.1`, when only 3\nreplicas of `nginx:1.14.2` had been created. In that case, the Deployment immediately starts\nkilling the 3 `nginx:1.14.2` Pods that it had created, and starts creating\n`nginx:1.16.1` Pods. It does not wait for the 5 replicas of `nginx:1.14.2` to be created\nbefore changing course.", "zh": "例如，假定你在创建一个 Deployment 以生成 `nginx:1.14.2` 的 5 个副本，但接下来\n更新 Deployment 以创建 5 个 `nginx:1.16.1` 的副本，而此时只有 3 个 `nginx:1.14.2`\n副本已创建。在这种情况下，Deployment 会立即开始杀死 3 个 `nginx:1.14.2` Pod，\n并开始创建 `nginx:1.16.1` Pod。它不会等待 `nginx:1.14.2` 的 5\n个副本都创建完成后才开始执行变更动作。"}
{"en": "### Label selector updates\n\nIt is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.\nIn any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped\nall of the implications.", "zh": "### 更改标签选择算符   {#label-selector-updates}\n\n通常不鼓励更新标签选择算符。建议你提前规划选择算符。\n在任何情况下，如果需要更新标签选择算符，请格外小心，\n并确保自己了解这背后可能发生的所有事情。\n\n{{< note >}}"}
{"en": "In API version `apps/v1`, a Deployment's label selector is immutable after it gets created.", "zh": "在 API 版本 `apps/v1` 中，Deployment 标签选择算符在创建后是不可变的。\n{{< /note >}}"}
{"en": "* Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,\notherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does\nnot select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and\ncreating a new ReplicaSet.\n* Selector updates changes the existing value in a selector key -- result in the same behavior as additions.\n* Selector removals removes an existing key from the Deployment selector -- do not require any changes in the\nPod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the\nremoved label still exists in any existing Pods and ReplicaSets.", "zh": "* 添加选择算符时要求使用新标签更新 Deployment 规约中的 Pod 模板标签，否则将返回验证错误。\n  此更改是非重叠的，也就是说新的选择算符不会选择使用旧选择算符所创建的 ReplicaSet 和 Pod，\n  这会导致创建新的 ReplicaSet 时所有旧 ReplicaSet 都会被孤立。\n* 选择算符的更新如果更改了某个算符的键名，这会导致与添加算符时相同的行为。\n* 删除选择算符的操作会删除从 Deployment 选择算符中删除现有算符。\n  此操作不需要更改 Pod 模板标签。现有 ReplicaSet 不会被孤立，也不会因此创建新的 ReplicaSet，\n  但请注意已删除的标签仍然存在于现有的 Pod 和 ReplicaSet 中。"}
{"en": "## Rolling Back a Deployment", "zh": "## 回滚 Deployment {#rolling-back-a-deployment}"}
{"en": "Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.\nBy default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want\n(you can change that by modifying revision history limit).", "zh": "有时，你可能想要回滚 Deployment；例如，当 Deployment 不稳定时（例如进入反复崩溃状态）。\n默认情况下，Deployment 的所有上线记录都保留在系统中，以便可以随时回滚\n（你可以通过修改修订历史记录限制来更改这一约束）。\n\n{{< note >}}"}
{"en": "A Deployment's revision is created when a Deployment's rollout is triggered. This means that the\nnew revision is created if and only if the Deployment's Pod template (`.spec.template`) is changed,\nfor example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,\ndo not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.\nThis means that when you roll back to an earlier revision, only the Deployment's Pod template part is\nrolled back.", "zh": "Deployment 被触发上线时，系统就会创建 Deployment 的新的修订版本。\n这意味着仅当 Deployment 的 Pod 模板（`.spec.template`）发生更改时，才会创建新修订版本\n-- 例如，模板的标签或容器镜像发生变化。\n其他更新，如 Deployment 的扩缩容操作不会创建 Deployment 修订版本。\n这是为了方便同时执行手动缩放或自动缩放。\n换言之，当你回滚到较早的修订版本时，只有 Deployment 的 Pod 模板部分会被回滚。\n{{< /note >}}"}
{"en": "* Suppose that you made a typo while updating the Deployment, by putting the image name as `nginx:1.161` instead of `nginx:1.16.1`:", "zh": "* 假设你在更新 Deployment 时犯了一个拼写错误，将镜像名称命名设置为\n  `nginx:1.161` 而不是 `nginx:1.16.1`：\n\n  ```shell\n  kubectl set image deployment/nginx-deployment nginx=nginx:1.161\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  deployment.apps/nginx-deployment image updated\n  ```"}
{"en": "* The rollout gets stuck. You can verify it by checking the rollout status:", "zh": "* 此上线进程会出现停滞。你可以通过检查上线状态来验证：\n\n  ```shell\n  kubectl rollout status deployment/nginx-deployment\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  Waiting for rollout to finish: 1 out of 3 new replicas have been updated...\n  ```"}
{"en": "* Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,\n[read more here](#deployment-status).", "zh": "* 按 Ctrl-C 停止上述上线状态观测。有关上线停滞的详细信息，[参考这里](#deployment-status)。"}
{"en": "* You see that the number of old replicas (adding the replica count from\n  `nginx-deployment-1564180365` and `nginx-deployment-2035384211`) is 3, and the number of\n  new replicas (from `nginx-deployment-3066724191`) is 1.", "zh": "* 你可以看到旧的副本（算上来自 `nginx-deployment-1564180365` 和 `nginx-deployment-2035384211` 的副本）有 3 个，\n  新的副本（来自 `nginx-deployment-3066724191`）有 1 个：\n\n  ```shell\n  kubectl get rs\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  NAME                          DESIRED   CURRENT   READY   AGE\n  nginx-deployment-1564180365   3         3         3       25s\n  nginx-deployment-2035384211   0         0         0       36s\n  nginx-deployment-3066724191   1         1         0       6s\n  ```"}
{"en": "* Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.", "zh": "* 查看所创建的 Pod，你会注意到新 ReplicaSet 所创建的 1 个 Pod 卡顿在镜像拉取循环中。\n\n  ```shell\n  kubectl get pods\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  NAME                                READY     STATUS             RESTARTS   AGE\n  nginx-deployment-1564180365-70iae   1/1       Running            0          25s\n  nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s\n  nginx-deployment-1564180365-hysrc   1/1       Running            0          25s\n  nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s\n  ```\n\n  {{< note >}}"}
{"en": "The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (`maxUnavailable` specifically) that you have specified. Kubernetes by default sets the value to 25%.", "zh": "Deployment 控制器自动停止有问题的上线过程，并停止对新的 ReplicaSet 扩容。\n  这行为取决于所指定的 rollingUpdate 参数（具体为 `maxUnavailable`）。\n  默认情况下，Kubernetes 将此值设置为 25%。\n  {{< /note >}}"}
{"en": "* Get the description of the Deployment:", "zh": "* 获取 Deployment 描述信息：\n\n  ```shell\n  kubectl describe deployment\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  Name:           nginx-deployment\n  Namespace:      default\n  CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700\n  Labels:         app=nginx\n  Selector:       app=nginx\n  Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable\n  StrategyType:       RollingUpdate\n  MinReadySeconds:    0\n  RollingUpdateStrategy:  25% max unavailable, 25% max surge\n  Pod Template:\n    Labels:  app=nginx\n    Containers:\n     nginx:\n      Image:        nginx:1.161\n      Port:         80/TCP\n      Host Port:    0/TCP\n      Environment:  <none>\n      Mounts:       <none>\n    Volumes:        <none>\n  Conditions:\n    Type           Status  Reason\n    ----           ------  ------\n    Available      True    MinimumReplicasAvailable\n    Progressing    True    ReplicaSetUpdated\n  OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)\n  NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)\n  Events:\n    FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message\n    --------- --------    -----   ----                    -------------   --------    ------              -------\n    1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3\n    22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1\n    22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2\n    22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2\n    21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1\n    21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3\n    13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0\n    13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1\n  ```"}
{"en": "To fix this, you need to rollback to a previous revision of Deployment that is stable.", "zh": "要解决此问题，需要回滚到以前稳定的 Deployment 版本。"}
{"en": "### Checking Rollout History of a Deployment\n\nFollow the steps given below to check the rollout history:", "zh": "### 检查 Deployment 上线历史\n\n按照如下步骤检查回滚历史："}
{"en": "1. First, check the revisions of this Deployment:", "zh": "1. 首先，检查 Deployment 修订历史：\n\n   ```shell\n   kubectl rollout history deployment/nginx-deployment\n   ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n   ```\n   deployments \"nginx-deployment\"\n   REVISION    CHANGE-CAUSE\n   1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml\n   2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n   3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161\n   ```"}
{"en": "`CHANGE-CAUSE` is copied from the Deployment annotation `kubernetes.io/change-cause` to its revisions upon creation. You can specify the`CHANGE-CAUSE` message by:", "zh": "`CHANGE-CAUSE` 的内容是从 Deployment 的 `kubernetes.io/change-cause` 注解复制过来的。\n   复制动作发生在修订版本创建时。你可以通过以下方式设置 `CHANGE-CAUSE` 消息："}
{"en": "* Annotating the Deployment with `kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.16.1\"`\n   * Manually editing the manifest of the resource.", "zh": "* 使用 `kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.16.1\"`\n     为 Deployment 添加注解。\n   * 手动编辑资源的清单。"}
{"en": "2. To see the details of each revision, run:", "zh": "2. 要查看修订历史的详细信息，运行：\n\n   ```shell\n   kubectl rollout history deployment/nginx-deployment --revision=2\n   ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n   ```\n   deployments \"nginx-deployment\" revision 2\n     Labels:       app=nginx\n             pod-template-hash=1159050644\n     Annotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n     Containers:\n      nginx:\n       Image:      nginx:1.16.1\n       Port:       80/TCP\n        QoS Tier:\n           cpu:      BestEffort\n           memory:   BestEffort\n       Environment Variables:      <none>\n     No volumes.\n   ```"}
{"en": "### Rolling Back to a Previous Revision\nFollow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.", "zh": "### 回滚到之前的修订版本   {#rolling-back-to-a-previous-revision}\n\n按照下面给出的步骤将 Deployment 从当前版本回滚到以前的版本（即版本 2）。"}
{"en": "1. Now you've decided to undo the current rollout and rollback to the previous revision:", "zh": "1. 假定现在你已决定撤消当前上线并回滚到以前的修订版本：\n\n   ```shell\n   kubectl rollout undo deployment/nginx-deployment\n   ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n   ```\n   deployment.apps/nginx-deployment rolled back\n   ```"}
{"en": "Alternatively, you can rollback to a specific revision by specifying it with `--to-revision`:", "zh": "或者，你也可以通过使用 `--to-revision` 来回滚到特定修订版本：\n\n   ```shell\n   kubectl rollout undo deployment/nginx-deployment --to-revision=2\n   ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n   ```\n   deployment.apps/nginx-deployment rolled back\n   ```"}
{"en": "For more details about rollout related commands, read [`kubectl rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout).", "zh": "与回滚相关的指令的更详细信息，请参考\n   [`kubectl rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout)。"}
{"en": "The Deployment is now rolled back to a previous stable revision. As you can see, a `DeploymentRollback` event\n   for rolling back to revision 2 is generated from Deployment controller.", "zh": "现在，Deployment 正在回滚到以前的稳定版本。正如你所看到的，Deployment\n   控制器生成了回滚到修订版本 2 的 `DeploymentRollback` 事件。"}
{"en": "2. Check if the rollback was successful and the Deployment is running as expected, run:", "zh": "2. 检查回滚是否成功以及 Deployment 是否正在运行，运行：\n\n   ```shell\n   kubectl get deployment nginx-deployment\n   ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n   ```\n   NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n   nginx-deployment   3/3     3            3           30m\n   ```"}
{"en": "3. Get the description of the Deployment:", "zh": "3. 获取 Deployment 描述信息：\n\n   ```shell\n   kubectl describe deployment nginx-deployment\n   ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n   ```\n   Name:                   nginx-deployment\n   Namespace:              default\n   CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500\n   Labels:                 app=nginx\n   Annotations:            deployment.kubernetes.io/revision=4\n                           kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n   Selector:               app=nginx\n   Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\n   StrategyType:           RollingUpdate\n   MinReadySeconds:        0\n   RollingUpdateStrategy:  25% max unavailable, 25% max surge\n   Pod Template:\n     Labels:  app=nginx\n     Containers:\n      nginx:\n       Image:        nginx:1.16.1\n       Port:         80/TCP\n       Host Port:    0/TCP\n       Environment:  <none>\n       Mounts:       <none>\n     Volumes:        <none>\n   Conditions:\n     Type           Status  Reason\n     ----           ------  ------\n     Available      True    MinimumReplicasAvailable\n     Progressing    True    NewReplicaSetAvailable\n   OldReplicaSets:  <none>\n   NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)\n   Events:\n     Type    Reason              Age   From                   Message\n     ----    ------              ----  ----                   -------\n     Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3\n     Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1\n     Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2\n     Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2\n     Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1\n     Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3\n     Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0\n     Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1\n     Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment \"nginx-deployment\" to revision 2\n     Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0\n   ```"}
{"en": "## Scaling a Deployment\n\nYou can scale a Deployment by using the following command:", "zh": "## 缩放 Deployment   {#scaling-a-deployment}\n\n你可以使用如下指令缩放 Deployment：\n\n```shell\nkubectl scale deployment/nginx-deployment --replicas=10\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\ndeployment.apps/nginx-deployment scaled\n```"}
{"en": "Assuming [horizontal Pod autoscaling](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) is enabled\nin your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of\nPods you want to run based on the CPU utilization of your existing Pods.", "zh": "假设集群启用了[Pod 的水平自动缩放](/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)，\n你可以为 Deployment 设置自动缩放器，并基于现有 Pod 的 CPU 利用率选择要运行的\nPod 个数下限和上限。\n\n```shell\nkubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\ndeployment.apps/nginx-deployment scaled\n```"}
{"en": "### Proportional scaling\n\nRollingUpdate Deployments support running multiple versions of an application at the same time. When you\nor an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress\nor paused), the Deployment controller balances the additional replicas in the existing active\nReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called *proportional scaling*.", "zh": "### 比例缩放  {#proportional-scaling}\n\nRollingUpdate 的 Deployment 支持同时运行应用程序的多个版本。\n当自动缩放器缩放处于上线进程（仍在进行中或暂停）中的 RollingUpdate Deployment 时，\nDeployment 控制器会平衡现有的活跃状态的 ReplicaSet（含 Pod 的 ReplicaSet）中的额外副本，\n以降低风险。这称为 *比例缩放（Proportional Scaling）*。"}
{"en": "For example, you are running a Deployment with 10 replicas, [maxSurge](#max-surge)=3, and [maxUnavailable](#max-unavailable)=2.", "zh": "例如，你正在运行一个 10 个副本的 Deployment，其\n[maxSurge](#max-surge)=3，[maxUnavailable](#max-unavailable)=2。"}
{"en": "* Ensure that the 10 replicas in your Deployment are running.", "zh": "* 确保 Deployment 的这 10 个副本都在运行。\n\n  ```shell\n  kubectl get deploy\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n  nginx-deployment     10        10        10           10          50s\n  ```"}
{"en": "* You update to a new image which happens to be unresolvable from inside the cluster.", "zh": "* 更新 Deployment 使用新镜像，碰巧该镜像无法从集群内部解析。\n\n  ```shell\n  kubectl set image deployment/nginx-deployment nginx=nginx:sometag\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  deployment.apps/nginx-deployment image updated\n  ```"}
{"en": "* The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the\n`maxUnavailable` requirement that you mentioned above. Check out the rollout status:", "zh": "* 镜像更新使用 ReplicaSet `nginx-deployment-1989198191` 启动新的上线过程，\n  但由于上面提到的 `maxUnavailable` 要求，该进程被阻塞了。检查上线状态：\n\n  ```shell\n  kubectl get rs\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  NAME                          DESIRED   CURRENT   READY     AGE\n  nginx-deployment-1989198191   5         5         0         9s\n  nginx-deployment-618515232    8         8         8         1m\n  ```"}
{"en": "* Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas\nto 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using\nproportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you\nspread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the\nmost replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the\nReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.", "zh": "* 然后，出现了新的 Deployment 扩缩请求。自动缩放器将 Deployment 副本增加到 15。\n  Deployment 控制器需要决定在何处添加 5 个新副本。如果未使用比例缩放，所有 5 个副本\n  都将添加到新的 ReplicaSet 中。使用比例缩放时，可以将额外的副本分布到所有 ReplicaSet。\n  较大比例的副本会被添加到拥有最多副本的 ReplicaSet，而较低比例的副本会进入到\n  副本较少的 ReplicaSet。所有剩下的副本都会添加到副本最多的 ReplicaSet。\n  具有零副本的 ReplicaSet 不会被扩容。"}
{"en": "In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the\nnew ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming\nthe new replicas become healthy. To confirm this, run:", "zh": "在上面的示例中，3 个副本被添加到旧 ReplicaSet 中，2 个副本被添加到新 ReplicaSet。\n假定新的副本都很健康，上线过程最终应将所有副本迁移到新的 ReplicaSet 中。\n要确认这一点，请运行：\n\n```shell\nkubectl get deploy\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment     15        18        7            8           7m\n```"}
{"en": "The rollout status confirms how the replicas were added to each ReplicaSet.", "zh": "上线状态确认了副本是如何被添加到每个 ReplicaSet 的。\n\n```shell\nkubectl get rs\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nNAME                          DESIRED   CURRENT   READY     AGE\nnginx-deployment-1989198191   7         7         0         7m\nnginx-deployment-618515232    11        11        11        7m\n```"}
{"en": "## Pausing and Resuming a rollout of a Deployment {#pausing-and-resuming-a-deployment}\n\nWhen you update a Deployment, or plan to, you can pause rollouts\nfor that Deployment before you trigger one or more updates. When\nyou're ready to apply those changes, you resume rollouts for the\nDeployment. This approach allows you to\napply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.", "zh": "## 暂停、恢复 Deployment 的上线过程  {#pausing-and-resuming-a-deployment}\n\n在你更新一个 Deployment 的时候，或者计划更新它的时候，\n你可以在触发一个或多个更新之前暂停 Deployment 的上线过程。\n当你准备应用这些变更时，你可以重新恢复 Deployment 上线过程。\n这样做使得你能够在暂停和恢复执行之间应用多个修补程序，而不会触发不必要的上线操作。"}
{"en": "* For example, with a Deployment that was created:\n\n  Get the Deployment details:", "zh": "* 例如，对于一个刚刚创建的 Deployment：\n\n  获取该 Deployment 信息：\n\n  ```shell\n  kubectl get deploy\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n  nginx     3         3         3            3           1m\n  ```"}
{"en": "Get the rollout status:", "zh": "获取上线状态：\n\n  ```shell\n  kubectl get rs\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  NAME               DESIRED   CURRENT   READY     AGE\n  nginx-2142116321   3         3         3         1m\n  ```"}
{"en": "* Pause by running the following command:", "zh": "* 使用如下指令暂停上线：\n\n  ```shell\n  kubectl rollout pause deployment/nginx-deployment\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  deployment.apps/nginx-deployment paused\n  ```"}
{"en": "* Then update the image of the Deployment:", "zh": "* 接下来更新 Deployment 镜像：\n\n  ```shell\n  kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  deployment.apps/nginx-deployment image updated\n  ```"}
{"en": "* Notice that no new rollout started:", "zh": "* 注意没有新的上线被触发：\n\n  ```shell\n  kubectl rollout history deployment/nginx-deployment\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  deployments \"nginx\"\n  REVISION  CHANGE-CAUSE\n  1   <none>\n  ```"}
{"en": "* Get the rollout status to verify that the existing ReplicaSet has not changed:", "zh": "* 获取上线状态验证现有的 ReplicaSet 没有被更改：\n\n  ```shell\n  kubectl get rs\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  NAME               DESIRED   CURRENT   READY     AGE\n  nginx-2142116321   3         3         3         2m\n  ```"}
{"en": "* You can make as many updates as you wish, for example, update the resources that will be used:", "zh": "* 你可以根据需要执行很多更新操作，例如，可以要使用的资源：\n\n  ```shell\n  kubectl set resources deployment/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  deployment.apps/nginx-deployment resource requirements updated\n  ```"}
{"en": "The initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to\n  the Deployment will not have any effect as long as the Deployment rollout is paused.", "zh": "暂停 Deployment 上线之前的初始状态将继续发挥作用，但新的更新在 Deployment\n  上线被暂停期间不会产生任何效果。"}
{"en": "* Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:", "zh": "* 最终，恢复 Deployment 上线并观察新的 ReplicaSet 的创建过程，其中包含了所应用的所有更新：\n\n  ```shell\n  kubectl rollout resume deployment/nginx-deployment\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于这样：\n\n  ```\n  deployment.apps/nginx-deployment resumed\n  ```"}
{"en": "* {{< glossary_tooltip text=\"Watch\" term_id=\"watch\" >}} the status of the rollout until it's done.", "zh": "* {{< glossary_tooltip text=\"监视\" term_id=\"watch\" >}}上线的状态，直到完成。\n\n  ```shell\n  kubectl get rs --watch\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  NAME               DESIRED   CURRENT   READY     AGE\n  nginx-2142116321   2         2         2         2m\n  nginx-3926361531   2         2         0         6s\n  nginx-3926361531   2         2         1         18s\n  nginx-2142116321   1         2         2         2m\n  nginx-2142116321   1         2         2         2m\n  nginx-3926361531   3         2         1         18s\n  nginx-3926361531   3         2         1         18s\n  nginx-2142116321   1         1         1         2m\n  nginx-3926361531   3         3         1         18s\n  nginx-3926361531   3         3         2         19s\n  nginx-2142116321   0         1         1         2m\n  nginx-2142116321   0         1         1         2m\n  nginx-2142116321   0         0         0         2m\n  nginx-3926361531   3         3         3         20s\n  ```"}
{"en": "* Get the status of the latest rollout:", "zh": "* 获取最近上线的状态：\n\n  ```shell\n  kubectl get rs\n  ```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n  ```\n  NAME               DESIRED   CURRENT   READY     AGE\n  nginx-2142116321   0         0         0         2m\n  nginx-3926361531   3         3         3         28s\n  ```\n\n{{< note >}}"}
{"en": "You cannot rollback a paused Deployment until you resume it.", "zh": "你不可以回滚处于暂停状态的 Deployment，除非先恢复其执行状态。\n{{< /note >}}"}
{"en": "## Deployment status\n\nA Deployment enters various states during its lifecycle. It can be [progressing](#progressing-deployment) while\nrolling out a new ReplicaSet, it can be [complete](#complete-deployment), or it can [fail to progress](#failed-deployment).", "zh": "##  Deployment 状态 {#deployment-status}\n\nDeployment 的生命周期中会有许多状态。上线新的 ReplicaSet 期间可能处于\n[Progressing（进行中）](#progressing-deployment)，可能是\n[Complete（已完成）](#complete-deployment)，也可能是\n[Failed（失败）](#failed-deployment)以至于无法继续进行。"}
{"en": "### Progressing Deployment\n\nKubernetes marks a Deployment as _progressing_ when one of the following tasks is performed:", "zh": "### 进行中的 Deployment  {#progressing-deployment}\n\n执行下面的任务期间，Kubernetes 标记 Deployment 为**进行中**（Progressing）_："}
{"en": "* The Deployment creates a new ReplicaSet.\n* The Deployment is scaling up its newest ReplicaSet.\n* The Deployment is scaling down its older ReplicaSet(s).\n* New Pods become ready or available (ready for at least [MinReadySeconds](#min-ready-seconds)).", "zh": "* Deployment 创建新的 ReplicaSet\n* Deployment 正在为其最新的 ReplicaSet 扩容\n* Deployment 正在为其旧有的 ReplicaSet(s) 缩容\n* 新的 Pod 已经就绪或者可用（就绪至少持续了 [MinReadySeconds](#min-ready-seconds) 秒）。"}
{"en": "When the rollout becomes “progressing”, the Deployment controller adds a condition with the following\nattributes to the Deployment's `.status.conditions`:", "zh": "当上线过程进入“Progressing”状态时，Deployment 控制器会向 Deployment 的\n`.status.conditions` 中添加包含下面属性的状况条目：\n\n* `type: Progressing`\n* `status: \"True\"`\n* `reason: NewReplicaSetCreated` | `reason: FoundNewReplicaSet` | `reason: ReplicaSetUpdated`"}
{"en": "You can monitor the progress for a Deployment by using `kubectl rollout status`.", "zh": "你可以使用 `kubectl rollout status` 监视 Deployment 的进度。"}
{"en": "### Complete Deployment\n\nKubernetes marks a Deployment as _complete_ when it has the following characteristics:", "zh": "### 完成的 Deployment    {#complete-deployment}\n\n当 Deployment 具有以下特征时，Kubernetes 将其标记为**完成（Complete）**;"}
{"en": "* All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any\nupdates you've requested have been completed.\n* All of the replicas associated with the Deployment are available.\n* No old replicas for the Deployment are running.", "zh": "* 与 Deployment 关联的所有副本都已更新到指定的最新版本，这意味着之前请求的所有更新都已完成。\n* 与 Deployment 关联的所有副本都可用。\n* 未运行 Deployment 的旧副本。"}
{"en": "When the rollout becomes “complete”, the Deployment controller sets a condition with the following\nattributes to the Deployment's `.status.conditions`:", "zh": "当上线过程进入“Complete”状态时，Deployment 控制器会向 Deployment 的\n`.status.conditions` 中添加包含下面属性的状况条目：\n\n* `type: Progressing`\n* `status: \"True\"`\n* `reason: NewReplicaSetAvailable`"}
{"en": "This `Progressing` condition will retain a status value of `\"True\"` until a new rollout\nis initiated. The condition holds even when availability of replicas changes (which\ndoes instead affect the `Available` condition).", "zh": "这一 `Progressing` 状况的状态值会持续为 `\"True\"`，直至新的上线动作被触发。\n即使副本的可用状态发生变化（进而影响 `Available` 状况），`Progressing` 状况的值也不会变化。"}
{"en": "You can check if a Deployment has completed by using `kubectl rollout status`. If the rollout completed\nsuccessfully, `kubectl rollout status` returns a zero exit code.", "zh": "你可以使用 `kubectl rollout status` 检查 Deployment 是否已完成。\n如果上线成功完成，`kubectl rollout status` 返回退出代码 0。\n\n```shell\nkubectl rollout status deployment/nginx-deployment\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nWaiting for rollout to finish: 2 of 3 updated replicas are available...\ndeployment \"nginx-deployment\" successfully rolled out\n```"}
{"en": "and the exit status from `kubectl rollout` is 0 (success):", "zh": "从 `kubectl rollout` 命令获得的返回状态为 0（成功）：\n\n```shell\necho $?\n```\n```\n0\n```"}
{"en": "### Failed Deployment", "zh": "### 失败的 Deployment   {#failed-deployment}"}
{"en": "Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur\ndue to some of the following factors:", "zh": "你的 Deployment 可能会在尝试部署其最新的 ReplicaSet 受挫，一直处于未完成状态。\n造成此情况一些可能因素如下："}
{"en": "* Insufficient quota\n* Readiness probe failures\n* Image pull errors\n* Insufficient permissions\n* Limit ranges\n* Application runtime misconfiguration", "zh": "* 配额（Quota）不足\n* 就绪探测（Readiness Probe）失败\n* 镜像拉取错误\n* 权限不足\n* 限制范围（Limit Ranges）问题\n* 应用程序运行时的配置错误"}
{"en": "One way you can detect this condition is to specify a deadline parameter in your Deployment spec:\n([`.spec.progressDeadlineSeconds`](#progress-deadline-seconds)). `.spec.progressDeadlineSeconds` denotes the\nnumber of seconds the Deployment controller waits before indicating (in the Deployment status) that the\nDeployment progress has stalled.", "zh": "检测此状况的一种方法是在 Deployment 规约中指定截止时间参数：\n（[`.spec.progressDeadlineSeconds`](#progress-deadline-seconds)）。\n`.spec.progressDeadlineSeconds` 给出的是一个秒数值，Deployment 控制器在（通过 Deployment 状态）\n标示 Deployment 进展停滞之前，需要等待所给的时长。"}
{"en": "The following `kubectl` command sets the spec with `progressDeadlineSeconds` to make the controller report\nlack of progress of a rollout for a Deployment after 10 minutes:", "zh": "以下 `kubectl` 命令设置规约中的 `progressDeadlineSeconds`，从而告知控制器\n在 10 分钟后报告 Deployment 的上线没有进展：\n\n```shell\nkubectl patch deployment/nginx-deployment -p '{\"spec\":{\"progressDeadlineSeconds\":600}}'\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\ndeployment.apps/nginx-deployment patched\n```"}
{"en": "Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following\nattributes to the Deployment's `.status.conditions`:", "zh": "超过截止时间后，Deployment 控制器将添加具有以下属性的 Deployment 状况到\nDeployment 的 `.status.conditions` 中：\n\n* `type: Progressing`\n* `status: \"False\"`\n* `reason: ProgressDeadlineExceeded`"}
{"en": "This condition can also fail early and is then set to status value of `\"False\"` due to reasons as `ReplicaSetCreateError`.\nAlso, the deadline is not taken into account anymore once the Deployment rollout completes.", "zh": "这一状况也可能会比较早地失败，因而其状态值被设置为 `\"False\"`，\n其原因为 `ReplicaSetCreateError`。\n一旦 Deployment 上线完成，就不再考虑其期限。"}
{"en": "See the [Kubernetes API conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties) for more information on status conditions.", "zh": "参考\n[Kubernetes API Conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties)\n获取更多状态状况相关的信息。\n\n{{< note >}}"}
{"en": "Kubernetes takes no action on a stalled Deployment other than to report a status condition with\n`reason: ProgressDeadlineExceeded`. Higher level orchestrators can take advantage of it and act accordingly, for\nexample, rollback the Deployment to its previous version.", "zh": "除了报告 `Reason=ProgressDeadlineExceeded` 状态之外，Kubernetes 对已停止的\nDeployment 不执行任何操作。更高级别的编排器可以利用这一设计并相应地采取行动。\n例如，将 Deployment 回滚到其以前的版本。\n{{< /note >}}\n\n{{< note >}}"}
{"en": "If you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline.\nYou can safely pause a Deployment rollout in the middle of a rollout and resume without triggering\nthe condition for exceeding the deadline.", "zh": "如果你暂停了某个 Deployment 上线，Kubernetes 不再根据指定的截止时间检查 Deployment 上线的进展。\n你可以在上线过程中间安全地暂停 Deployment 再恢复其执行，这样做不会导致超出最后时限的问题。\n{{< /note >}}"}
{"en": "You may experience transient errors with your Deployments, either due to a low timeout that you have set or\ndue to any other kind of error that can be treated as transient. For example, let's suppose you have\ninsufficient quota. If you describe the Deployment you will notice the following section:", "zh": "Deployment 可能会出现瞬时性的错误，可能因为设置的超时时间过短，\n也可能因为其他可认为是临时性的问题。例如，假定所遇到的问题是配额不足。\n如果描述 Deployment，你将会注意到以下部分：\n\n```shell\nkubectl describe deployment nginx-deployment\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\n<...>\nConditions:\n  Type            Status  Reason\n  ----            ------  ------\n  Available       True    MinimumReplicasAvailable\n  Progressing     True    ReplicaSetUpdated\n  ReplicaFailure  True    FailedCreate\n<...>\n```"}
{"en": "If you run `kubectl get deployment nginx-deployment -o yaml`, the Deployment status is similar to this:", "zh": "如果运行 `kubectl get deployment nginx-deployment -o yaml`，Deployment 状态输出\n将类似于这样：\n\n```\nstatus:\n  availableReplicas: 2\n  conditions:\n  - lastTransitionTime: 2016-10-04T12:25:39Z\n    lastUpdateTime: 2016-10-04T12:25:39Z\n    message: Replica set \"nginx-deployment-4262182780\" is progressing.\n    reason: ReplicaSetUpdated\n    status: \"True\"\n    type: Progressing\n  - lastTransitionTime: 2016-10-04T12:25:42Z\n    lastUpdateTime: 2016-10-04T12:25:42Z\n    message: Deployment has minimum availability.\n    reason: MinimumReplicasAvailable\n    status: \"True\"\n    type: Available\n  - lastTransitionTime: 2016-10-04T12:25:39Z\n    lastUpdateTime: 2016-10-04T12:25:39Z\n    message: 'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: exceeded quota:\n      object-counts, requested: pods=1, used: pods=3, limited: pods=2'\n    reason: FailedCreate\n    status: \"True\"\n    type: ReplicaFailure\n  observedGeneration: 3\n  replicas: 2\n  unavailableReplicas: 2\n```"}
{"en": "Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the\nreason for the Progressing condition:", "zh": "最终，一旦超过 Deployment 进度限期，Kubernetes 将更新状态和进度状况的原因：\n\n```\nConditions:\n  Type            Status  Reason\n  ----            ------  ------\n  Available       True    MinimumReplicasAvailable\n  Progressing     False   ProgressDeadlineExceeded\n  ReplicaFailure  True    FailedCreate\n```"}
{"en": "You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other\ncontrollers you may be running, or by increasing quota in your namespace. If you satisfy the quota\nconditions and the Deployment controller then completes the Deployment rollout, you'll see the\nDeployment's status update with a successful condition (`status: \"True\"` and `reason: NewReplicaSetAvailable`).", "zh": "可以通过缩容 Deployment 或者缩容其他运行状态的控制器，或者直接在命名空间中增加配额\n来解决配额不足的问题。如果配额条件满足，Deployment 控制器完成了 Deployment 上线操作，\nDeployment 状态会更新为成功状况（`Status=True` 和 `Reason=NewReplicaSetAvailable`）。\n\n```\nConditions:\n  Type          Status  Reason\n  ----          ------  ------\n  Available     True    MinimumReplicasAvailable\n  Progressing   True    NewReplicaSetAvailable\n```"}
{"en": "`type: Available` with `status: \"True\"` means that your Deployment has minimum availability. Minimum availability is dictated\nby the parameters specified in the deployment strategy. `type: Progressing` with `status: \"True\"` means that your Deployment\nis either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum\nrequired new replicas are available (see the Reason of the condition for the particulars - in our case\n`reason: NewReplicaSetAvailable` means that the Deployment is complete).", "zh": "`type: Available` 加上 `status: True` 意味着 Deployment 具有最低可用性。\n最低可用性由 Deployment 策略中的参数指定。\n`type: Progressing` 加上 `status: True` 表示 Deployment 处于上线过程中，并且正在运行，\n或者已成功完成进度，最小所需新副本处于可用。\n请参阅对应状况的 Reason 了解相关细节。\n在我们的案例中 `reason: NewReplicaSetAvailable` 表示 Deployment 已完成。"}
{"en": "You can check if a Deployment has failed to progress by using `kubectl rollout status`. `kubectl rollout status`\nreturns a non-zero exit code if the Deployment has exceeded the progression deadline.", "zh": "你可以使用 `kubectl rollout status` 检查 Deployment 是否未能取得进展。\n如果 Deployment 已超过进度限期，`kubectl rollout status` 返回非零退出代码。\n\n```shell\nkubectl rollout status deployment/nginx-deployment\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nerror: deployment \"nginx\" exceeded its progress deadline\n```"}
{"en": "and the exit status from `kubectl rollout` is 1 (indicating an error):", "zh": "`kubectl rollout` 命令的退出状态为 1（表明发生了错误）：\n\n```shell\necho $?\n```\n```\n1\n```"}
{"en": "### Operating on a failed deployment\n\nAll actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back\nto a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.", "zh": "### 对失败 Deployment 的操作   {#operating-on-a-failed-deployment}\n\n可应用于已完成的 Deployment 的所有操作也适用于失败的 Deployment。\n你可以对其执行扩缩容、回滚到以前的修订版本等操作，或者在需要对 Deployment 的\nPod 模板应用多项调整时，将 Deployment 暂停。"}
{"en": "## Clean up Policy\n\nYou can set `.spec.revisionHistoryLimit` field in a Deployment to specify how many old ReplicaSets for\nthis Deployment you want to retain. The rest will be garbage-collected in the background. By default,\nit is 10.", "zh": "## 清理策略   {#clean-up-policy}\n\n你可以在 Deployment 中设置 `.spec.revisionHistoryLimit` 字段以指定保留此\nDeployment 的多少个旧有 ReplicaSet。其余的 ReplicaSet 将在后台被垃圾回收。\n默认情况下，此值为 10。\n\n{{< note >}}"}
{"en": "Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment\nthus that Deployment will not be able to roll back.", "zh": "显式将此字段设置为 0 将导致 Deployment 的所有历史记录被清空，因此 Deployment 将无法回滚。\n{{< /note >}}"}
{"en": "## Canary Deployment\n\nIf you want to roll out releases to a subset of users or servers using the Deployment, you\ncan create multiple Deployments, one for each release, following the canary pattern described in\n[managing resources](/docs/concepts/workloads/management/#canary-deployments).", "zh": "## 金丝雀部署 {#canary-deployment}\n\n如果要使用 Deployment 向用户子集或服务器子集上线版本，\n则可以遵循[资源管理](/zh-cn/docs/concepts/workloads/management/#canary-deployments)所描述的金丝雀模式，\n创建多个 Deployment，每个版本一个。"}
{"en": "## Writing a Deployment Spec\n\nAs with all other Kubernetes configs, a Deployment needs `.apiVersion`, `.kind`, and `.metadata` fields.\nFor general information about working with config files, see\n[deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/),\nconfiguring containers, and [using kubectl to manage resources](/docs/concepts/overview/working-with-objects/object-management/) documents.", "zh": "## 编写 Deployment 规约       {#writing-a-deployment-spec}\n\n同其他 Kubernetes 配置一样， Deployment 需要 `.apiVersion`，`.kind` 和 `.metadata` 字段。\n有关配置文件的其他信息，请参考[部署 Deployment](/zh-cn/docs/tasks/run-application/run-stateless-application-deployment/)、\n配置容器和[使用 kubectl 管理资源](/zh-cn/docs/concepts/overview/working-with-objects/object-management/)等相关文档。"}
{"en": "When the control plane creates new Pods for a Deployment, the `.metadata.name` of the\nDeployment is part of the basis for naming those Pods. The name of a Deployment must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\n\nA Deployment also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).", "zh": "当控制面为 Deployment 创建新的 Pod 时，Deployment 的 `.metadata.name` 是命名这些 Pod 的部分基础。\nDeployment 的名称必须是一个合法的\n[DNS 子域](/zh-cn/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names)值，\n但这会对 Pod 的主机名产生意外的结果。为获得最佳兼容性，名称应遵循更严格的\n[DNS 标签](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-label-names)规则。\n\nDeployment 还需要\n[`.spec` 部分](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)。"}
{"en": "### Pod Template\n\nThe `.spec.template` and `.spec.selector` are the only required fields of the `.spec`.", "zh": "### Pod 模板     {#pod-template}\n\n`.spec` 中只有 `.spec.template` 和 `.spec.selector` 是必需的字段。"}
{"en": "The `.spec.template` is a [Pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}, except it is nested and does not have an `apiVersion` or `kind`.", "zh": "`.spec.template` 是一个 [Pod 模板](/zh-cn/docs/concepts/workloads/pods/#pod-templates)。\n它和 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 的语法规则完全相同。\n只是这里它是嵌套的，因此不需要 `apiVersion` 或 `kind`。"}
{"en": "In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate\nlabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [selector](#selector).", "zh": "除了 Pod 的必填字段外，Deployment 中的 Pod 模板必须指定适当的标签和适当的重新启动策略。\n对于标签，请确保不要与其他控制器重叠。请参考[选择算符](#selector)。"}
{"en": "Only a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is\nallowed, which is the default if not specified.", "zh": "只有 [`.spec.template.spec.restartPolicy`](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\n等于 `Always` 才是被允许的，这也是在没有指定时的默认设置。"}
{"en": "### Replicas\n\n`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.", "zh": "### 副本   {#replicas}\n\n`.spec.replicas` 是指定所需 Pod 的可选字段。它的默认值是1。"}
{"en": "Should you manually scale a Deployment, example via `kubectl scale deployment\ndeployment --replicas=X`, and then you update that Deployment based on a manifest\n(for example: by running `kubectl apply -f deployment.yaml`),\nthen applying that manifest overwrites the manual scaling that you previously did.", "zh": "如果你对某个 Deployment 执行了手动扩缩操作（例如，通过 \n`kubectl scale deployment deployment --replicas=X`），\n之后基于清单对 Deployment 执行了更新操作（例如通过运行\n`kubectl apply -f deployment.yaml`），那么通过应用清单而完成的更新会覆盖之前手动扩缩所作的变更。"}
{"en": "If a [HorizontalPodAutoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/) (or any\nsimilar API for horizontal scaling) is managing scaling for a Deployment, don't set `.spec.replicas`.", "zh": "如果一个 [HorizontalPodAutoscaler](/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale/)\n（或者其他执行水平扩缩操作的类似 API）在管理 Deployment 的扩缩，\n则不要设置 `.spec.replicas`。"}
{"en": "Instead, allow the Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} to manage the\n`.spec.replicas` field automatically.", "zh": "恰恰相反，应该允许 Kubernetes\n{{< glossary_tooltip text=\"控制面\" term_id=\"control-plane\" >}}来自动管理\n`.spec.replicas` 字段。"}
{"en": "### Selector\n\n`.spec.selector` is a required field that specifies a [label selector](/docs/concepts/overview/working-with-objects/labels/)\nfor the Pods targeted by this Deployment.\n\n`.spec.selector` must match `.spec.template.metadata.labels`, or it will be rejected by the API.", "zh": "### 选择算符   {#selector}\n\n`.spec.selector` 是指定本 Deployment 的 Pod\n[标签选择算符](/zh-cn/docs/concepts/overview/working-with-objects/labels/)的必需字段。\n\n`.spec.selector` 必须匹配 `.spec.template.metadata.labels`，否则请求会被 API 拒绝。"}
{"en": "In API version `apps/v1`, `.spec.selector` and `.metadata.labels` do not default to `.spec.template.metadata.labels` if not set. So they must be set explicitly. Also note that `.spec.selector` is immutable after creation of the Deployment in `apps/v1`.", "zh": "在 API `apps/v1`版本中，`.spec.selector` 和 `.metadata.labels` 如果没有设置的话，\n不会被默认设置为 `.spec.template.metadata.labels`，所以需要明确进行设置。\n同时在 `apps/v1`版本中，Deployment 创建后 `.spec.selector` 是不可变的。"}
{"en": "A Deployment may terminate Pods whose labels match the selector if their template is different\nfrom `.spec.template` or if the total number of such Pods exceeds `.spec.replicas`. It brings up new\nPods with `.spec.template` if the number of Pods is less than the desired number.", "zh": "当 Pod 的标签和选择算符匹配，但其模板和 `.spec.template` 不同时，或者此类 Pod\n的总数超过 `.spec.replicas` 的设置时，Deployment 会终结之。\n如果 Pod 总数未达到期望值，Deployment 会基于 `.spec.template` 创建新的 Pod。\n\n{{< note >}}"}
{"en": "You should not create other Pods whose labels match this selector, either directly, by creating\nanother Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you\ndo so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.", "zh": "你不应直接创建与此选择算符匹配的 Pod，也不应通过创建另一个 Deployment 或者类似于\nReplicaSet 或 ReplicationController 这类控制器来创建标签与此选择算符匹配的 Pod。\n如果这样做，第一个 Deployment 会认为它创建了这些 Pod。\nKubernetes 不会阻止你这么做。\n{{< /note >}}"}
{"en": "If you have multiple controllers that have overlapping selectors, the controllers will fight with each\nother and won't behave correctly.", "zh": "如果有多个控制器的选择算符发生重叠，则控制器之间会因冲突而无法正常工作。"}
{"en": "### Strategy\n\n`.spec.strategy` specifies the strategy used to replace old Pods by new ones.\n`.spec.strategy.type` can be \"Recreate\" or \"RollingUpdate\". \"RollingUpdate\" is\nthe default value.", "zh": "### 策略   {#strategy}\n\n`.spec.strategy` 策略指定用于用新 Pod 替换旧 Pod 的策略。\n`.spec.strategy.type` 可以是 “Recreate” 或 “RollingUpdate”。“RollingUpdate” 是默认值。"}
{"en": "#### Recreate Deployment\n\nAll existing Pods are killed before new ones are created when `.spec.strategy.type==Recreate`.", "zh": "#### 重新创建 Deployment   {#recreate-deployment}\n\n如果 `.spec.strategy.type==Recreate`，在创建新 Pod 之前，所有现有的 Pod 会被杀死。\n\n{{< note >}}"}
{"en": "This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods\nof the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new\nrevision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the\nreplacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an\n\"at most\" guarantee for your Pods, you should consider using a\n[StatefulSet](/docs/concepts/workloads/controllers/statefulset/).", "zh": "这只会确保为了升级而创建新 Pod 之前其他 Pod 都已终止。如果你升级一个 Deployment，\n所有旧版本的 Pod 都会立即被终止。控制器等待这些 Pod 被成功移除之后，\n才会创建新版本的 Pod。如果你手动删除一个 Pod，其生命周期是由 ReplicaSet 来控制的，\n后者会立即创建一个替换 Pod（即使旧的 Pod 仍然处于 Terminating 状态）。\n如果你需要一种“最多 n 个”的 Pod 个数保证，你需要考虑使用\n[StatefulSet](/zh-cn/docs/concepts/workloads/controllers/statefulset/)。\n{{< /note >}}"}
{"en": "#### Rolling Update Deployment\n\nThe Deployment updates Pods in a rolling update\nfashion when `.spec.strategy.type==RollingUpdate`. You can specify `maxUnavailable` and `maxSurge` to control\nthe rolling update process.", "zh": "#### 滚动更新 Deployment   {#rolling-update-deployment}\n\nDeployment 会在 `.spec.strategy.type==RollingUpdate`时，采取\n滚动更新的方式更新 Pod。你可以指定 `maxUnavailable` 和 `maxSurge`\n来控制滚动更新过程。"}
{"en": "##### Max Unavailable", "zh": "##### 最大不可用   {#max-unavailable}"}
{"en": "`.spec.strategy.rollingUpdate.maxUnavailable` is an optional field that specifies the maximum number\nof Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)\nor a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by\nrounding down. The value cannot be 0 if `.spec.strategy.rollingUpdate.maxSurge` is 0. The default value is 25%.", "zh": "`.spec.strategy.rollingUpdate.maxUnavailable` 是一个可选字段，\n用来指定更新过程中不可用的 Pod 的个数上限。该值可以是绝对数字（例如，5），也可以是所需\nPod 的百分比（例如，10%）。百分比值会转换成绝对数并去除小数部分。\n如果 `.spec.strategy.rollingUpdate.maxSurge` 为 0，则此值不能为 0。\n默认值为 25%。"}
{"en": "For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired\nPods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled\ndown further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available\nat all times during the update is at least 70% of the desired Pods.", "zh": "例如，当此值设置为 30% 时，滚动更新开始时会立即将旧 ReplicaSet 缩容到期望 Pod 个数的70%。\n新 Pod 准备就绪后，可以继续缩容旧有的 ReplicaSet，然后对新的 ReplicaSet 扩容，\n确保在更新期间可用的 Pod 总数在任何时候都至少为所需的 Pod 个数的 70%。"}
{"en": "##### Max Surge\n\n`.spec.strategy.rollingUpdate.maxSurge` is an optional field that specifies the maximum number of Pods\nthat can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a\npercentage of desired Pods (for example, 10%). The value cannot be 0 if `MaxUnavailable` is 0. The absolute number\nis calculated from the percentage by rounding up. The default value is 25%.", "zh": "##### 最大峰值   {#max-surge}\n\n`.spec.strategy.rollingUpdate.maxSurge` 是一个可选字段，用来指定可以创建的超出期望\nPod 个数的 Pod 数量。此值可以是绝对数（例如，5）或所需 Pod 的百分比（例如，10%）。\n如果 `MaxUnavailable` 为 0，则此值不能为 0。百分比值会通过向上取整转换为绝对数。\n此字段的默认值为 25%。"}
{"en": "For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the\nrolling update starts, such that the total number of old and new Pods does not exceed 130% of desired\nPods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the\ntotal number of Pods running at any time during the update is at most 130% of desired Pods.\n\nHere are some Rolling Update Deployment examples that use the `maxUnavailable` and `maxSurge`:", "zh": "例如，当此值为 30% 时，启动滚动更新后，会立即对新的 ReplicaSet 扩容，同时保证新旧 Pod\n的总数不超过所需 Pod 总数的 130%。一旦旧 Pod 被杀死，新的 ReplicaSet 可以进一步扩容，\n同时确保更新期间的任何时候运行中的 Pod 总数最多为所需 Pod 总数的 130%。\n\n以下是一些使用 `maxUnavailable` 和 `maxSurge` 的滚动更新 Deployment 的示例：\n\n{{< tabs name=\"tab_with_md\" >}}\n{{% tab name=\"最大不可用\" %}}\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n```\n\n{{% /tab %}}\n{{% tab name=\"最大峰值\" %}}\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n```\n\n{{% /tab %}}\n{{% tab name=\"两项混合\" %}}\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n```\n\n{{% /tab %}}\n{{< /tabs >}}"}
{"en": "### Progress Deadline Seconds\n\n`.spec.progressDeadlineSeconds` is an optional field that specifies the number of seconds you want\nto wait for your Deployment to progress before the system reports back that the Deployment has\n[failed progressing](#failed-deployment) - surfaced as a condition with `type: Progressing`, `status: \"False\"`.\nand `reason: ProgressDeadlineExceeded` in the status of the resource. The Deployment controller will keep\nretrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment\ncontroller will roll back a Deployment as soon as it observes such a condition.", "zh": "### 进度期限秒数    {#progress-deadline-seconds}\n\n`.spec.progressDeadlineSeconds` 是一个可选字段，用于指定系统在报告 Deployment\n[进展失败](#failed-deployment)之前等待 Deployment 取得进展的秒数。\n这类报告会在资源状态中体现为 `type: Progressing`、`status: False`、\n`reason: ProgressDeadlineExceeded`。Deployment 控制器将在默认 600 毫秒内持续重试 Deployment。\n将来，一旦实现了自动回滚，Deployment 控制器将在探测到这样的条件时立即回滚 Deployment。"}
{"en": "If specified, this field needs to be greater than `.spec.minReadySeconds`.", "zh": "如果指定，则此字段值需要大于 `.spec.minReadySeconds` 取值。"}
{"en": "### Min Ready Seconds\n\n`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly\ncreated Pod should be ready without any of its containers crashing, for it to be considered available.\nThis defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when\na Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).", "zh": "### 最短就绪时间    {#min-ready-seconds}\n\n`.spec.minReadySeconds` 是一个可选字段，用于指定新创建的 Pod\n在没有任意容器崩溃情况下的最小就绪时间，\n只有超出这个时间 Pod 才被视为可用。默认值为 0（Pod 在准备就绪后立即将被视为可用）。\n要了解何时 Pod 被视为就绪，\n可参考[容器探针](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)。"}
{"en": "### Revision History Limit\n\nA Deployment's revision history is stored in the ReplicaSets it controls.", "zh": "### 修订历史限制\n\nDeployment 的修订历史记录存储在它所控制的 ReplicaSet 中。"}
{"en": "`.spec.revisionHistoryLimit` is an optional field that specifies the number of old ReplicaSets to retain\nto allow rollback. These old ReplicaSets consume resources in `etcd` and crowd the output of `kubectl get rs`.\nThe configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted,\nyou lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept,\nhowever its ideal value depends on the frequency and stability of new Deployments.", "zh": "`.spec.revisionHistoryLimit` 是一个可选字段，用来设定出于回滚目的所要保留的旧 ReplicaSet 数量。\n这些旧 ReplicaSet 会消耗 etcd 中的资源，并占用 `kubectl get rs` 的输出。\n每个 Deployment 修订版本的配置都存储在其 ReplicaSet 中；因此，一旦删除了旧的 ReplicaSet，\n将失去回滚到 Deployment 的对应修订版本的能力。\n默认情况下，系统保留 10 个旧 ReplicaSet，但其理想值取决于新 Deployment 的频率和稳定性。"}
{"en": "More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.\nIn this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.", "zh": "更具体地说，将此字段设置为 0 意味着将清理所有具有 0 个副本的旧 ReplicaSet。\n在这种情况下，无法撤消新的 Deployment 上线，因为它的修订历史被清除了。"}
{"en": "### Paused\n\n`.spec.paused` is an optional boolean field for pausing and resuming a Deployment. The only difference between\na paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused\nDeployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when\nit is created.", "zh": "### paused（暂停的）  {#paused}\n\n`.spec.paused` 是用于暂停和恢复 Deployment 的可选布尔字段。\n暂停的 Deployment 和未暂停的 Deployment 的唯一区别是，Deployment 处于暂停状态时，\nPodTemplateSpec 的任何修改都不会触发新的上线。\nDeployment 在创建时是默认不会处于暂停状态。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn more about [Pods](/docs/concepts/workloads/pods).\n* [Run a stateless application using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/).\n* Read the {{< api-reference page=\"workload-resources/deployment-v1\" >}} to understand the Deployment API.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\n  you can use it to manage application availability during disruptions.\n* Use kubectl to [create a Deployment](/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/).", "zh": "* 进一步了解 [Pod](/zh-cn/docs/concepts/workloads/pods)。\n* [使用 Deployment 运行一个无状态应用](/zh-cn/docs/tasks/run-application/run-stateless-application-deployment/)。\n* 阅读 {{< api-reference page=\"workload-resources/deployment-v1\" >}}，\n  以了解 Deployment API 的细节。\n* 阅读 [PodDisruptionBudget](/zh-cn/docs/concepts/workloads/pods/disruptions/)\n  了解如何使用它来在可能出现干扰的情况下管理应用的可用性。\n* 使用 kubectl 来[创建一个 Deployment](/zh-cn/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/)。"}
{"en": "StatefulSet is the workload API object used to manage stateful applications.", "zh": "StatefulSet 是用来管理有状态应用的工作负载 API 对象。\n\n{{< glossary_definition term_id=\"statefulset\" length=\"all\" >}}"}
{"en": "## Using StatefulSets\n\nStatefulSets are valuable for applications that require one or more of the\nfollowing.", "zh": "## 使用 StatefulSet   {#using-statefulsets}\n\nStatefulSet 对于需要满足以下一个或多个需求的应用程序很有价值："}
{"en": "* Stable, unique network identifiers.\n* Stable, persistent storage.\n* Ordered, graceful deployment and scaling.\n* Ordered, automated rolling updates.", "zh": "* 稳定的、唯一的网络标识符。\n* 稳定的、持久的存储。\n* 有序的、优雅的部署和扩缩。\n* 有序的、自动的滚动更新。"}
{"en": "In the above, stable is synonymous with persistence across Pod (re)scheduling.\nIf an application doesn't require any stable identifiers or ordered deployment,\ndeletion, or scaling, you should deploy your application using a workload object\nthat provides a set of stateless replicas.\n[Deployment](/docs/concepts/workloads/controllers/deployment/) or\n[ReplicaSet](/docs/concepts/workloads/controllers/replicaset/) may be better suited to your stateless needs.", "zh": "在上面描述中，“稳定的”意味着 Pod 调度或重调度的整个过程是有持久性的。\n如果应用程序不需要任何稳定的标识符或有序的部署、删除或扩缩，\n则应该使用由一组无状态的副本控制器提供的工作负载来部署应用程序，比如\n[Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/) 或者\n[ReplicaSet](/zh-cn/docs/concepts/workloads/controllers/replicaset/)\n可能更适用于你的无状态应用部署需要。"}
{"en": "## Limitations", "zh": "## 限制  {#limitations}"}
{"en": "* The storage for a given Pod must either be provisioned by a\n  [PersistentVolume Provisioner](/docs/concepts/storage/dynamic-provisioning/) ([examples here](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md))\n  based on the requested _storage class_, or pre-provisioned by an admin.\n* Deleting and/or scaling a StatefulSet down will *not* delete the volumes associated with the\n  StatefulSet. This is done to ensure data safety, which is generally more valuable than an\n  automatic purge of all related StatefulSet resources.\n* StatefulSets currently require a [Headless Service](/docs/concepts/services-networking/service/#headless-services)\n  to be responsible for the network identity of the Pods. You are responsible for creating this\n  Service.\n* StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is\n  deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is\n  possible to scale the StatefulSet down to 0 prior to deletion.\n* When using [Rolling Updates](#rolling-updates) with the default\n  [Pod Management Policy](#pod-management-policies) (`OrderedReady`),\n  it's possible to get into a broken state that requires\n  [manual intervention to repair](#forced-rollback).", "zh": "* 给定 Pod 的存储必须由\n  [PersistentVolume Provisioner](/zh-cn/docs/concepts/storage/dynamic-provisioning/)\n  （[例子在这里](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md)）\n  基于所请求的 **storage class** 来制备，或者由管理员预先制备。\n* 删除或者扩缩 StatefulSet 并**不会**删除它关联的存储卷。\n  这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。\n* StatefulSet 当前需要[无头服务](/zh-cn/docs/concepts/services-networking/service/#headless-services)来负责 Pod\n  的网络标识。你需要负责创建此服务。\n* 当删除一个 StatefulSet 时，该 StatefulSet 不提供任何终止 Pod 的保证。\n  为了实现 StatefulSet 中的 Pod 可以有序且体面地终止，可以在删除之前将 StatefulSet\n  缩容到 0。\n* 在默认 [Pod 管理策略](#pod-management-policies)(`OrderedReady`) 时使用[滚动更新](#rolling-updates)，\n  可能进入需要[人工干预](#forced-rollback)才能修复的损坏状态。"}
{"en": "## Components\nThe example below demonstrates the components of a StatefulSet.", "zh": "## 组件  {#components}\n\n下面的示例演示了 StatefulSet 的组件。\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  ports:\n  - port: 80\n    name: web\n  clusterIP: None\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx # 必须匹配 .spec.template.metadata.labels\n  serviceName: \"nginx\"\n  replicas: 3 # 默认值是 1\n  minReadySeconds: 10 # 默认值是 0\n  template:\n    metadata:\n      labels:\n        app: nginx # 必须匹配 .spec.selector.matchLabels\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: nginx\n        image: registry.k8s.io/nginx-slim:0.24\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      storageClassName: \"my-storage-class\"\n      resources:\n        requests:\n          storage: 1Gi\n```\n\n{{< note >}}"}
{"en": "This example uses the `ReadWriteOnce` access mode, for simplicity. For\nproduction use, the Kubernetes project recommends using the `ReadWriteOncePod`\naccess mode instead.", "zh": "这个示例出于简化考虑使用了 `ReadWriteOnce` 访问模式。但对于生产环境，\nKubernetes 项目建议使用 `ReadWriteOncePod` 访问模式。\n{{< /note >}}"}
{"en": "In the above example:\n\n* A Headless Service, named `nginx`, is used to control the network domain.\n* The StatefulSet, named `web`, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.\n* The `volumeClaimTemplates` will provide stable storage using\n  [PersistentVolumes](/docs/concepts/storage/persistent-volumes/) provisioned by a\n  PersistentVolume Provisioner.\n\nThe name of a StatefulSet object must be a valid\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).", "zh": "上述例子中：\n\n* 名为 `nginx` 的 Headless Service 用来控制网络域名。\n* 名为 `web` 的 StatefulSet 有一个 Spec，它表明将在独立的 3 个 Pod 副本中启动 nginx 容器。\n* `volumeClaimTemplates` 将通过 PersistentVolume 制备程序所准备的\n  [PersistentVolumes](/zh-cn/docs/concepts/storage/persistent-volumes/) 来提供稳定的存储。\n\nStatefulSet 的命名需要遵循\n[DNS 标签](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-label-names)规范。"}
{"en": "### Pod Selector", "zh": "### Pod 选择算符     {#pod-selector}"}
{"en": "You must set the `.spec.selector` field of a StatefulSet to match the labels of its\n`.spec.template.metadata.labels`. Failing to specify a matching Pod Selector will result in a\nvalidation error during StatefulSet creation.", "zh": "你必须设置 StatefulSet 的 `.spec.selector` 字段，使之匹配其在\n`.spec.template.metadata.labels` 中设置的标签。\n未指定匹配的 Pod 选择算符将在创建 StatefulSet 期间导致验证错误。"}
{"en": "### Volume Claim Templates\n\nYou can set the `.spec.volumeClaimTemplates` field to create a\n[PersistentVolumeClaim](/docs/concepts/storage/persistent-volumes/).\nThis will provide stable storage to the StatefulSet if either", "zh": "### 卷申领模板  {#volume-claim-templates}\n\n你可以设置 `.spec.volumeClaimTemplates` 字段来创建\n[PersistentVolumeClaim](/zh-cn/docs/concepts/storage/persistent-volumes/)。\n这将为 StatefulSet 提供稳定的存储，如果："}
{"en": "* The StorageClass specified for the volume claim is set up to use [dynamic\n  provisioning](/docs/concepts/storage/dynamic-provisioning/), or\n* The cluster already contains a PersistentVolume with the correct StorageClass\n  and sufficient available storage space.", "zh": "* 为卷申领指定的 StorageClass 配置使用[动态制备](/zh-cn/docs/concepts/storage/dynamic-provisioning/)，或\n* 集群已包含具有正确 StorageClass 和足够可用存储空间的 PersistentVolume。"}
{"en": "### Minimum ready seconds", "zh": "### 最短就绪秒数 {#minimum-ready-seconds}\n\n{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}"}
{"en": "`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly\ncreated Pod should be running and ready without any of its containers crashing, for it to be considered available.\nThis is used to check progression of a rollout when using a [Rolling Update](#rolling-updates) strategy.\nThis field defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when\na Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).", "zh": "`.spec.minReadySeconds` 是一个可选字段。\n它指定新创建的 Pod 应该在没有任何容器崩溃的情况下运行并准备就绪，才能被认为是可用的。\n这用于在使用[滚动更新](#rolling-updates)策略时检查滚动的进度。\n该字段默认为 0（Pod 准备就绪后将被视为可用）。\n要了解有关何时认为 Pod 准备就绪的更多信息，\n请参阅[容器探针](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)。"}
{"en": "## Pod Identity\n\nStatefulSet Pods have a unique identity that consists of an ordinal, a\nstable network identity, and stable storage. The identity sticks to the Pod,\nregardless of which node it's (re)scheduled on.", "zh": "## Pod 标识   {#pod-identity}\n\nStatefulSet Pod 具有唯一的标识，该标识包括顺序标识、稳定的网络标识和稳定的存储。\n该标识和 Pod 是绑定的，与该 Pod 调度到哪个节点上无关。"}
{"en": "### Ordinal Index\n\nFor a StatefulSet with N [replicas](#replicas), each Pod in the StatefulSet\nwill be assigned an integer ordinal, that is unique over the Set. By default,\npods will be assigned ordinals from 0 up through N-1. The StatefulSet controller\nwill also add a pod label with this index: `apps.kubernetes.io/pod-index`.", "zh": "### 序号索引   {#ordinal-index}\n\n对于具有 N 个[副本](#replicas)的 StatefulSet，该 StatefulSet 中的每个 Pod 将被分配一个整数序号，\n该序号在此 StatefulSet 中是唯一的。默认情况下，这些 Pod 将被赋予从 0 到 N-1 的序号。\nStatefulSet 的控制器也会添加一个包含此索引的 Pod 标签：`apps.kubernetes.io/pod-index`。"}
{"en": "### Start ordinal", "zh": "### 起始序号   {#start-ordinal}\n\n{{< feature-state feature_gate_name=\"StatefulSetStartOrdinal\" >}}"}
{"en": "`.spec.ordinals` is an optional field that allows you to configure the integer\nordinals assigned to each Pod. It defaults to nil. Within the field, you can\nconfigure the following options:", "zh": "`.spec.ordinals` 是一个可选的字段，允许你配置分配给每个 Pod 的整数序号。\n该字段默认为 nil 值。在该字段内，你可以配置以下选项："}
{"en": "* `.spec.ordinals.start`: If the `.spec.ordinals.start` field is set, Pods will\n  be assigned ordinals from `.spec.ordinals.start` up through\n  `.spec.ordinals.start + .spec.replicas - 1`.", "zh": "* `.spec.ordinals.start`：如果 `.spec.ordinals.start` 字段被设置，则 Pod 将被分配从\n  `.spec.ordinals.start` 到 `.spec.ordinals.start + .spec.replicas - 1` 的序号。"}
{"en": "### Stable Network ID\n\nEach Pod in a StatefulSet derives its hostname from the name of the StatefulSet\nand the ordinal of the Pod. The pattern for the constructed hostname\nis `$(statefulset name)-$(ordinal)`. The example above will create three Pods\nnamed `web-0,web-1,web-2`.\nA StatefulSet can use a [Headless Service](/docs/concepts/services-networking/service/#headless-services)\nto control the domain of its Pods. The domain managed by this Service takes the form:\n`$(service name).$(namespace).svc.cluster.local`, where \"cluster.local\" is the\ncluster domain.\nAs each Pod is created, it gets a matching DNS subdomain, taking the form:\n`$(podname).$(governing service domain)`, where the governing service is defined\nby the `serviceName` field on the StatefulSet.", "zh": "### 稳定的网络 ID   {#stable-network-id}\n\nStatefulSet 中的每个 Pod 根据 StatefulSet 的名称和 Pod 的序号派生出它的主机名。\n组合主机名的格式为`$(StatefulSet 名称)-$(序号)`。\n上例将会创建三个名称分别为 `web-0、web-1、web-2` 的 Pod。\nStatefulSet 可以使用[无头服务](/zh-cn/docs/concepts/services-networking/service/#headless-services)控制它的\nPod 的网络域。管理域的这个服务的格式为：\n`$(服务名称).$(名字空间).svc.cluster.local`，其中 `cluster.local` 是集群域。\n一旦每个 Pod 创建成功，就会得到一个匹配的 DNS 子域，格式为：\n`$(pod 名称).$(所属服务的 DNS 域名)`，其中所属服务由 StatefulSet 的 `serviceName` 域来设定。"}
{"en": "Depending on how DNS is configured in your cluster, you may not be able to look up the DNS\nname for a newly-run Pod immediately. This behavior can occur when other clients in the\ncluster have already sent queries for the hostname of the Pod before it was created.\nNegative caching (normal in DNS) means that the results of previous failed lookups are\nremembered and reused, even after the Pod is running, for at least a few seconds.\n\nIf you need to discover Pods promptly after they are created, you have a few options:\n\n- Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.\n- Decrease the time of caching in your Kubernetes DNS provider (typically this means editing the\n  config map for CoreDNS, which currently caches for 30 seconds).\n\nAs mentioned in the [limitations](#limitations) section, you are responsible for\ncreating the [Headless Service](/docs/concepts/services-networking/service/#headless-services)\nresponsible for the network identity of the pods.", "zh": "取决于集群域内部 DNS 的配置，有可能无法查询一个刚刚启动的 Pod 的 DNS 命名。\n当集群内其他客户端在 Pod 创建完成前发出 Pod 主机名查询时，就会发生这种情况。\n负缓存 (在 DNS 中较为常见) 意味着之前失败的查询结果会被记录和重用至少若干秒钟，\n即使 Pod 已经正常运行了也是如此。\n\n如果需要在 Pod 被创建之后及时发现它们，可使用以下选项：\n\n- 直接查询 Kubernetes API（比如，利用 watch 机制）而不是依赖于 DNS 查询\n- 缩短 Kubernetes DNS 驱动的缓存时长（通常这意味着修改 CoreDNS 的 ConfigMap，目前缓存时长为 30 秒）\n\n正如[限制](#limitations)中所述，\n你需要负责创建[无头服务](/zh-cn/docs/concepts/services-networking/service/#headless-services)以便为 Pod 提供网络标识。"}
{"en": "Here are some examples of choices for Cluster Domain, Service name,\nStatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.\n\nCluster Domain | Service (ns/name) | StatefulSet (ns/name)  | StatefulSet Domain  | Pod DNS | Pod Hostname |\n-------------- | ----------------- | ----------------- | -------------- | ------- | ------------ |\n cluster.local | default/nginx     | default/web       | nginx.default.svc.cluster.local | web-{0..N-1}.nginx.default.svc.cluster.local | web-{0..N-1} |\n cluster.local | foo/nginx         | foo/web           | nginx.foo.svc.cluster.local     | web-{0..N-1}.nginx.foo.svc.cluster.local     | web-{0..N-1} |\n kube.local    | foo/nginx         | foo/web           | nginx.foo.svc.kube.local        | web-{0..N-1}.nginx.foo.svc.kube.local        | web-{0..N-1} |", "zh": "下面给出一些选择集群域、服务名、StatefulSet 名、及其怎样影响 StatefulSet 的 Pod 上的 DNS 名称的示例：\n\n集群域名       | 服务（名字空间/名字）| StatefulSet（名字空间/名字） | StatefulSet 域名 | Pod DNS | Pod 主机名   |\n-------------- | -------------------- | ---------------------------- | ---------------- | ------- | ------------ |\n cluster.local | default/nginx     | default/web       | nginx.default.svc.cluster.local | web-{0..N-1}.nginx.default.svc.cluster.local | web-{0..N-1} |\n cluster.local | foo/nginx         | foo/web           | nginx.foo.svc.cluster.local     | web-{0..N-1}.nginx.foo.svc.cluster.local     | web-{0..N-1} |\n kube.local    | foo/nginx         | foo/web           | nginx.foo.svc.kube.local        | web-{0..N-1}.nginx.foo.svc.kube.local        | web-{0..N-1} |\n\n{{< note >}}"}
{"en": "Cluster Domain will be set to `cluster.local` unless\n[otherwise configured](/docs/concepts/services-networking/dns-pod-service/).", "zh": "集群域会被设置为 `cluster.local`，除非有[其他配置](/zh-cn/docs/concepts/services-networking/dns-pod-service/)。\n{{< /note >}}"}
{"en": "### Stable Storage\n\nFor each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one\nPersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume\nwith a StorageClass of `my-storage-class` and 1 GiB of provisioned storage. If no StorageClass\nis specified, then the default StorageClass will be used. When a Pod is (re)scheduled\nonto a node, its `volumeMounts` mount the PersistentVolumes associated with its\nPersistentVolume Claims. Note that, the PersistentVolumes associated with the\nPods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.\nThis must be done manually.", "zh": "### 稳定的存储  {#stable-storage}\n\n对于 StatefulSet 中定义的每个 VolumeClaimTemplate，每个 Pod 接收到一个 PersistentVolumeClaim。\n在上面的 nginx 示例中，每个 Pod 将会得到基于 StorageClass `my-storage-class` 制备的\n1 GiB 的 PersistentVolume。如果没有指定 StorageClass，就会使用默认的 StorageClass。\n当一个 Pod 被调度（重新调度）到节点上时，它的 `volumeMounts` 会挂载与其\nPersistentVolumeClaims 相关联的 PersistentVolume。\n请注意，当 Pod 或者 StatefulSet 被删除时，与 PersistentVolumeClaims 相关联的\nPersistentVolume 并不会被删除。要删除它必须通过手动方式来完成。"}
{"en": "### Pod Name Label\n\nWhen the StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} creates a Pod,\nit adds a label, `statefulset.kubernetes.io/pod-name`, that is set to the name of\nthe Pod. This label allows you to attach a Service to a specific Pod in\nthe StatefulSet.", "zh": "### Pod 名称标签   {#pod-name-label}\n\n当 StatefulSet {{<glossary_tooltip text=\"控制器\" term_id=\"controller\">}}创建 Pod 时，\n它会添加一个标签 `statefulset.kubernetes.io/pod-name`，该标签值设置为 Pod 名称。\n这个标签允许你给 StatefulSet 中的特定 Pod 绑定一个 Service。"}
{"en": "### Pod index label", "zh": "### Pod 索引标签  {#pod-index-label}\n\n{{< feature-state for_k8s_version=\"v1.28\" state=\"beta\" >}}"}
{"en": "When the StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} creates a Pod,\nthe new Pod is labelled with `apps.kubernetes.io/pod-index`. The value of this label is the ordinal index of\nthe Pod. This label allows you to route traffic to a particular pod index, filter logs/metrics\nusing the pod index label, and more. Note the feature gate `PodIndexLabel` must be enabled for this\nfeature, and it is enabled by default.", "zh": "当 StatefulSet {{<glossary_tooltip text=\"控制器\" term_id=\"controller\">}}创建一个 Pod 时，\n新的 Pod 会被打上 `apps.kubernetes.io/pod-index` 标签。标签的取值为 Pod 的序号索引。\n此标签使你能够将流量路由到特定索引值的 Pod、使用 Pod 索引标签来过滤日志或度量值等等。\n注意要使用这一特性需要启用特性门控 `PodIndexLabel`，而该门控默认是被启用的。"}
{"en": "## Deployment and Scaling Guarantees\n\n* For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.\n* When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.\n* Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.\n* Before a Pod is terminated, all of its successors must be completely shutdown.", "zh": "## 部署和扩缩保证   {#deployment-and-scaling-guarantees}\n\n* 对于包含 N 个 副本的 StatefulSet，当部署 Pod 时，它们是依次创建的，顺序为 `0..N-1`。\n* 当删除 Pod 时，它们是逆序终止的，顺序为 `N-1..0`。\n* 在将扩缩操作应用到 Pod 之前，它前面的所有 Pod 必须是 Running 和 Ready 状态。\n* 在一个 Pod 终止之前，所有的继任者必须完全关闭。"}
{"en": "The StatefulSet should not specify a `pod.Spec.TerminationGracePeriodSeconds` of 0. This practice\nis unsafe and strongly discouraged. For further explanation, please refer to\n[force deleting StatefulSet Pods](/docs/tasks/run-application/force-delete-stateful-set-pod/).", "zh": "StatefulSet 不应将 `pod.Spec.TerminationGracePeriodSeconds` 设置为 0。\n这种做法是不安全的，要强烈阻止。\n更多的解释请参考[强制删除 StatefulSet Pod](/zh-cn/docs/tasks/run-application/force-delete-stateful-set-pod/)。"}
{"en": "When the nginx example above is created, three Pods will be deployed in the order\nweb-0, web-1, web-2. web-1 will not be deployed before web-0 is\n[Running and Ready](/docs/concepts/workloads/pods/pod-lifecycle/), and web-2 will not be deployed until\nweb-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before\nweb-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and\nbecomes Running and Ready.", "zh": "在上面的 nginx 示例被创建后，会按照 web-0、web-1、web-2 的顺序部署三个 Pod。\n在 web-0 进入 [Running 和 Ready](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/)\n状态前不会部署 web-1。在 web-1 进入 Running 和 Ready 状态前不会部署 web-2。\n如果 web-1 已经处于 Running 和 Ready 状态，而 web-2 尚未部署，在此期间发生了\nweb-0 运行失败，那么 web-2 将不会被部署，要等到 web-0 部署完成并进入 Running 和\nReady 状态后，才会部署 web-2。"}
{"en": "If a user were to scale the deployed example by patching the StatefulSet such that\n`replicas=1`, web-2 would be terminated first. web-1 would not be terminated until web-2\nis fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and\nis completely shutdown, but prior to web-1's termination, web-1 would not be terminated\nuntil web-0 is Running and Ready.", "zh": "如果用户想将示例中的 StatefulSet 扩缩为 `replicas=1`，首先被终止的是 web-2。\n在 web-2 没有被完全停止和删除前，web-1 不会被终止。\n当 web-2 已被终止和删除、web-1 尚未被终止，如果在此期间发生 web-0 运行失败，\n那么就不会终止 web-1，必须等到 web-0 进入 Running 和 Ready 状态后才会终止 web-1。"}
{"en": "### Pod Management Policies\n\nStatefulSet allows you to relax its ordering guarantees while\npreserving its uniqueness and identity guarantees via its `.spec.podManagementPolicy` field.", "zh": "### Pod 管理策略 {#pod-management-policies}\n\nStatefulSet 允许你放宽其排序保证，\n同时通过它的 `.spec.podManagementPolicy` 域保持其唯一性和身份保证。"}
{"en": "#### OrderedReady Pod Management\n\n`OrderedReady` pod management is the default for StatefulSets. It implements the behavior\ndescribed [above](#deployment-and-scaling-guarantees).", "zh": "#### OrderedReady Pod 管理   {#orderedready-pod-management}\n\n`OrderedReady` Pod 管理是 StatefulSet 的默认设置。\n它实现了[上面](#deployment-and-scaling-guarantees)描述的功能。"}
{"en": "#### Parallel Pod Management\n\n`Parallel` pod management tells the StatefulSet controller to launch or\nterminate all Pods in parallel, and to not wait for Pods to become Running\nand Ready or completely terminated prior to launching or terminating another\nPod. This option only affects the behavior for scaling operations. Updates are not\naffected.", "zh": "#### 并行 Pod 管理   {#parallel-pod-management}\n\n`Parallel` Pod 管理让 StatefulSet 控制器并行的启动或终止所有的 Pod，\n启动或者终止其他 Pod 前，无需等待 Pod 进入 Running 和 Ready 或者完全停止状态。\n这个选项只会影响扩缩操作的行为，更新则不会被影响。"}
{"en": "## Update strategies\n\nA StatefulSet's `.spec.updateStrategy` field allows you to configure\nand disable automated rolling updates for containers, labels, resource request/limits, and\nannotations for the Pods in a StatefulSet. There are two possible values:", "zh": "## 更新策略  {#update-strategies}\n\nStatefulSet 的 `.spec.updateStrategy` 字段让你可以配置和禁用掉自动滚动更新 Pod\n的容器、标签、资源请求或限制、以及注解。有两个允许的值："}
{"en": "`OnDelete`\n: When a StatefulSet's `.spec.updateStrategy.type` is set to `OnDelete`,\n  the StatefulSet controller will not automatically update the Pods in a\n  StatefulSet. Users must manually delete Pods to cause the controller to\n  create new Pods that reflect modifications made to a StatefulSet's `.spec.template`.\n\n`RollingUpdate`\n: The `RollingUpdate` update strategy implements automated, rolling updates for the Pods in a\n  StatefulSet. This is the default update strategy.", "zh": "`OnDelete`\n: 当 StatefulSet 的 `.spec.updateStrategy.type` 设置为 `OnDelete` 时，\n  它的控制器将不会自动更新 StatefulSet 中的 Pod。\n  用户必须手动删除 Pod 以便让控制器创建新的 Pod，以此来对 StatefulSet 的\n  `.spec.template` 的变动作出反应。\n\n`RollingUpdate`\n: `RollingUpdate` 更新策略对 StatefulSet 中的 Pod 执行自动的滚动更新。这是默认的更新策略。"}
{"en": "## Rolling Updates\n\nWhen a StatefulSet's `.spec.updateStrategy.type` is set to `RollingUpdate`, the\nStatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed\nin the same order as Pod termination (from the largest ordinal to the smallest), updating\neach Pod one at a time.\n\nThe Kubernetes control plane waits until an updated Pod is Running and Ready prior\nto updating its predecessor. If you have set `.spec.minReadySeconds` (see\n[Minimum Ready Seconds](#minimum-ready-seconds)), the control plane additionally waits that\namount of time after the Pod turns ready, before moving on.", "zh": "## 滚动更新 {#rolling-updates}\n\n当 StatefulSet 的 `.spec.updateStrategy.type` 被设置为 `RollingUpdate` 时，\nStatefulSet 控制器会删除和重建 StatefulSet 中的每个 Pod。\n它将按照与 Pod 终止相同的顺序（从最大序号到最小序号）进行，每次更新一个 Pod。\n\nKubernetes 控制平面会等到被更新的 Pod 进入 Running 和 Ready 状态，然后再更新其前身。\n如果你设置了 `.spec.minReadySeconds`（查看[最短就绪秒数](#minimum-ready-seconds)），\n控制平面在 Pod 就绪后会额外等待一定的时间再执行下一步。"}
{"en": "### Partitioned rolling updates {#partitions}\n\nThe `RollingUpdate` update strategy can be partitioned, by specifying a\n`.spec.updateStrategy.rollingUpdate.partition`. If a partition is specified, all Pods with an\nordinal that is greater than or equal to the partition will be updated when the StatefulSet's\n`.spec.template` is updated. All Pods with an ordinal that is less than the partition will not\nbe updated, and, even if they are deleted, they will be recreated at the previous version. If a\nStatefulSet's `.spec.updateStrategy.rollingUpdate.partition` is greater than its `.spec.replicas`,\nupdates to its `.spec.template` will not be propagated to its Pods.\nIn most cases you will not need to use a partition, but they are useful if you want to stage an\nupdate, roll out a canary, or perform a phased roll out.", "zh": "### 分区滚动更新   {#partitions}\n\n通过声明 `.spec.updateStrategy.rollingUpdate.partition` 的方式，`RollingUpdate`\n更新策略可以实现分区。\n如果声明了一个分区，当 StatefulSet 的 `.spec.template` 被更新时，\n所有序号大于等于该分区序号的 Pod 都会被更新。\n所有序号小于该分区序号的 Pod 都不会被更新，并且，即使它们被删除也会依据之前的版本进行重建。\n如果 StatefulSet 的 `.spec.updateStrategy.rollingUpdate.partition` 大于它的\n`.spec.replicas`，则对它的 `.spec.template` 的更新将不会传递到它的 Pod。\n在大多数情况下，你不需要使用分区，但如果你希望进行阶段更新、执行金丝雀或执行分阶段上线，则这些分区会非常有用。"}
{"en": "### Maximum unavailable Pods", "zh": "### 最大不可用 Pod   {#maximum-unavailable-pods}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"alpha\" >}}"}
{"en": "You can control the maximum number of Pods that can be unavailable during an update\nby specifying the `.spec.updateStrategy.rollingUpdate.maxUnavailable` field.\nThe value can be an absolute number (for example, `5`) or a percentage of desired\nPods (for example, `10%`). Absolute number is calculated from the percentage value\nby rounding it up. This field cannot be 0. The default setting is 1.", "zh": "你可以通过指定 `.spec.updateStrategy.rollingUpdate.maxUnavailable`\n字段来控制更新期间不可用的 Pod 的最大数量。\n该值可以是绝对值（例如，“5”）或者是期望 Pod 个数的百分比（例如，`10%`）。\n绝对值是根据百分比值四舍五入计算的。\n该字段不能为 0。默认设置为 1。"}
{"en": "This field applies to all Pods in the range `0` to `replicas - 1`. If there is any\nunavailable Pod in the range `0` to `replicas - 1`, it will be counted towards\n`maxUnavailable`.", "zh": "该字段适用于 `0` 到 `replicas - 1` 范围内的所有 Pod。\n如果在 `0` 到 `replicas - 1` 范围内存在不可用 Pod，这类 Pod 将被计入 `maxUnavailable` 值。\n\n{{< note >}}"}
{"en": "The `maxUnavailable` field is in Alpha stage and it is honored only by API servers\nthat are running with the `MaxUnavailableStatefulSet`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled.", "zh": "`maxUnavailable` 字段处于 Alpha 阶段，仅当 API 服务器启用了 `MaxUnavailableStatefulSet`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)时才起作用。\n{{< /note >}}"}
{"en": "### Forced rollback\n\nWhen using [Rolling Updates](#rolling-updates) with the default\n[Pod Management Policy](#pod-management-policies) (`OrderedReady`),\nit's possible to get into a broken state that requires manual intervention to repair.\n\nIf you update the Pod template to a configuration that never becomes Running and\nReady (for example, due to a bad binary or application-level configuration error),\nStatefulSet will stop the rollout and wait.", "zh": "### 强制回滚 {#forced-rollback}\n\n在默认 [Pod 管理策略](#pod-management-policies)(`OrderedReady`) 下使用[滚动更新](#rolling-updates)，\n可能进入需要人工干预才能修复的损坏状态。\n\n如果更新后 Pod 模板配置进入无法运行或就绪的状态（例如，\n由于错误的二进制文件或应用程序级配置错误），StatefulSet 将停止回滚并等待。"}
{"en": "In this state, it's not enough to revert the Pod template to a good configuration.\nDue to a [known issue](https://github.com/kubernetes/kubernetes/issues/67250),\nStatefulSet will continue to wait for the broken Pod to become Ready\n(which never happens) before it will attempt to revert it back to the working\nconfiguration.\n\nAfter reverting the template, you must also delete any Pods that StatefulSet had\nalready attempted to run with the bad configuration.\nStatefulSet will then begin to recreate the Pods using the reverted template.", "zh": "在这种状态下，仅将 Pod 模板还原为正确的配置是不够的。\n由于[已知问题](https://github.com/kubernetes/kubernetes/issues/67250)，StatefulSet\n将继续等待损坏状态的 Pod 准备就绪（永远不会发生），然后再尝试将其恢复为正常工作配置。\n\n恢复模板后，还必须删除 StatefulSet 尝试使用错误的配置来运行的 Pod。这样，\nStatefulSet 才会开始使用被还原的模板来重新创建 Pod。"}
{"en": "## PersistentVolumeClaim retention", "zh": "## PersistentVolumeClaim 保留  {#persistentvolumeclaim-retention}\n\n{{< feature-state for_k8s_version=\"v1.27\" state=\"beta\" >}}"}
{"en": "The optional `.spec.persistentVolumeClaimRetentionPolicy` field controls if\nand how PVCs are deleted during the lifecycle of a StatefulSet. You must enable the\n`StatefulSetAutoDeletePVC` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\non the API server and the controller manager to use this field.\nOnce enabled, there are two policies you can configure for each StatefulSet:", "zh": "在 StatefulSet 的生命周期中，可选字段\n`.spec.persistentVolumeClaimRetentionPolicy` 控制是否删除以及如何删除 PVC。\n使用该字段，你必须在 API 服务器和控制器管理器启用 `StatefulSetAutoDeletePVC`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)。\n启用后，你可以为每个 StatefulSet 配置两个策略："}
{"en": "`whenDeleted`\n: configures the volume retention behavior that applies when the StatefulSet is deleted\n\n`whenScaled`\n: configures the volume retention behavior that applies when the replica count of\n  the StatefulSet   is reduced; for example, when scaling down the set.\n\nFor each policy that you can configure, you can set the value to either `Delete` or `Retain`.", "zh": "`whenDeleted`\n: 配置删除 StatefulSet 时应用的卷保留行为。\n\n`whenScaled`\n: 配置当 StatefulSet 的副本数减少时应用的卷保留行为；例如，缩小集合时。\n\n对于你可以配置的每个策略，你可以将值设置为 `Delete` 或 `Retain`。"}
{"en": "`Delete`\n: The PVCs created from the StatefulSet `volumeClaimTemplate` are deleted for each Pod\n  affected by the policy. With the `whenDeleted` policy all PVCs from the\n  `volumeClaimTemplate` are deleted after their Pods have been deleted. With the\n  `whenScaled` policy, only PVCs corresponding to Pod replicas being scaled down are\n  deleted, after their Pods have been deleted.", "zh": "`Delete`\n: 对于受策略影响的每个 Pod，基于 StatefulSet 的 `volumeClaimTemplate` 字段创建的 PVC 都会被删除。\n  使用 `whenDeleted` 策略，所有来自 `volumeClaimTemplate` 的 PVC 在其 Pod 被删除后都会被删除。\n  使用 `whenScaled` 策略，只有与被缩减的 Pod 副本对应的 PVC 在其 Pod 被删除后才会被删除。"}
{"en": "`Retain` (default)\n: PVCs from the `volumeClaimTemplate` are not affected when their Pod is\n  deleted. This is the behavior before this new feature.", "zh": "`Retain`（默认）\n: 来自 `volumeClaimTemplate` 的 PVC 在 Pod 被删除时不受影响。这是此新功能之前的行为。"}
{"en": "Bear in mind that these policies **only** apply when Pods are being removed due to the\nStatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet\nfails due to node failure, and the control plane creates a replacement Pod, the StatefulSet\nretains the existing PVC.  The existing volume is unaffected, and the cluster will attach it to\nthe node where the new Pod is about to launch.\n\nThe default for policies is `Retain`, matching the StatefulSet behavior before this new feature.\n\nHere is an example policy.", "zh": "请记住，这些策略**仅**适用于由于 StatefulSet 被删除或被缩小而被删除的 Pod。\n例如，如果与 StatefulSet 关联的 Pod 由于节点故障而失败，\n并且控制平面创建了替换 Pod，则 StatefulSet 保留现有的 PVC。\n现有卷不受影响，集群会将其附加到新 Pod 即将启动的节点上。\n\n策略的默认值为 `Retain`，与此新功能之前的 StatefulSet 行为相匹配。\n\n这是一个示例策略。\n\n```yaml\napiVersion: apps/v1\nkind: StatefulSet\n...\nspec:\n  persistentVolumeClaimRetentionPolicy:\n    whenDeleted: Retain\n    whenScaled: Delete\n...\n```"}
{"en": "The StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} adds\n[owner references](/docs/concepts/overview/working-with-objects/owners-dependents/#owner-references-in-object-specifications)\nto its PVCs, which are then deleted by the {{<glossary_tooltip text=\"garbage collector\"\nterm_id=\"garbage-collection\">}} after the Pod is terminated. This enables the Pod to\ncleanly unmount all volumes before the PVCs are deleted (and before the backing PV and\nvolume are deleted, depending on the retain policy).  When you set the `whenDeleted`\npolicy to `Delete`, an owner reference to the StatefulSet instance is placed on all PVCs\nassociated with that StatefulSet.", "zh": "StatefulSet {{<glossary_tooltip text=\"控制器\" term_id=\"controller\">}}为其 PVC\n添加了[属主引用](/zh-cn/docs/concepts/overview/working-with-objects/owners-dependents/#owner-references-in-object-specifications)，\n这些 PVC 在 Pod 终止后被{{<glossary_tooltip text=\"垃圾回收器\" term_id=\"garbage-collection\">}}删除。\n这使 Pod 能够在删除 PVC 之前（以及在删除后备 PV 和卷之前，取决于保留策略）干净地卸载所有卷。\n当你设置 `whenDeleted` 删除策略，对 StatefulSet 实例的属主引用放置在与该 StatefulSet 关联的所有 PVC 上。"}
{"en": "The `whenScaled` policy must delete PVCs only when a Pod is scaled down, and not when a\nPod is deleted for another reason. When reconciling, the StatefulSet controller compares\nits desired replica count to the actual Pods present on the cluster. Any StatefulSet Pod\nwhose id greater than the replica count is condemned and marked for deletion. If the\n`whenScaled` policy is `Delete`, the condemned Pods are first set as owners to the\nassociated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCs\nto be garbage collected after only the condemned Pods have terminated.", "zh": "`whenScaled` 策略必须仅在 Pod 缩减时删除 PVC，而不是在 Pod 因其他原因被删除时删除。\n执行协调操作时，StatefulSet 控制器将其所需的副本数与集群上实际存在的 Pod 进行比较。\n对于 StatefulSet 中的所有 Pod 而言，如果其 ID 大于副本数，则将被废弃并标记为需要删除。\n如果 `whenScaled` 策略是 `Delete`，则在删除 Pod 之前，\n首先将已销毁的 Pod 设置为与 StatefulSet 模板对应的 PVC 的属主。\n这会导致 PVC 仅在已废弃的 Pod 终止后被垃圾收集。"}
{"en": "This means that if the controller crashes and restarts, no Pod will be deleted before its\nowner reference has been updated appropriate to the policy. If a condemned Pod is\nforce-deleted while the controller is down, the owner reference may or may not have been\nset up, depending on when the controller crashed. It may take several reconcile loops to\nupdate the owner references, so some condemned Pods may have set up owner references and\nothers may not. For this reason we recommend waiting for the controller to come back up,\nwhich will verify owner references before terminating Pods. If that is not possible, the\noperator should verify the owner references on PVCs to ensure the expected objects are\ndeleted when Pods are force-deleted.", "zh": "这意味着如果控制器崩溃并重新启动，在其属主引用更新到适合策略的 Pod 之前，不会删除任何 Pod。\n如果在控制器关闭时强制删除了已废弃的 Pod，则属主引用可能已被设置，也可能未被设置，具体取决于控制器何时崩溃。\n更新属主引用可能需要几个协调循环，因此一些已废弃的 Pod 可能已经被设置了属主引用，而其他可能没有。\n出于这个原因，我们建议等待控制器恢复，控制器将在终止 Pod 之前验证属主引用。\n如果这不可行，则操作员应验证 PVC 上的属主引用，以确保在强制删除 Pod 时删除预期的对象。"}
{"en": "### Replicas\n\n`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.\n\nShould you manually scale a deployment, example via `kubectl scale\nstatefulset statefulset --replicas=X`, and then you update that StatefulSet\nbased on a manifest (for example: by running `kubectl apply -f\nstatefulset.yaml`), then applying that manifest overwrites the manual scaling\nthat you previously did.", "zh": "### 副本数 {#replicas}\n\n`.spec.replicas` 是一个可选字段，用于指定所需 Pod 的数量。它的默认值为 1。\n\n如果你手动扩缩已部署的负载，例如通过 `kubectl scale statefulset statefulset --replicas=X`，\n然后根据清单更新 StatefulSet（例如：通过运行 `kubectl apply -f statefulset.yaml`），\n那么应用该清单的操作会覆盖你之前所做的手动扩缩。"}
{"en": "If a [HorizontalPodAutoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/)\n(or any similar API for horizontal scaling) is managing scaling for a\nStatefulset, don't set `.spec.replicas`. Instead, allow the Kubernetes\n{{<glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} to manage\nthe `.spec.replicas` field automatically.", "zh": "如果 [HorizontalPodAutoscaler](/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale/)\n（或任何类似的水平扩缩 API）正在管理 StatefulSet 的扩缩，\n请不要设置 `.spec.replicas`。\n相反，允许 Kubernetes 控制平面自动管理 `.spec.replicas` 字段。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn about [Pods](/docs/concepts/workloads/pods).\n* Find out how to use StatefulSets\n  * Follow an example of [deploying a stateful application](/docs/tutorials/stateful-application/basic-stateful-set/).\n  * Follow an example of [deploying Cassandra with Stateful Sets](/docs/tutorials/stateful-application/cassandra/).\n  * Follow an example of [running a replicated stateful application](/docs/tasks/run-application/run-replicated-stateful-application/).\n  * Learn how to [scale a StatefulSet](/docs/tasks/run-application/scale-stateful-set/).\n  * Learn what's involved when you [delete a StatefulSet](/docs/tasks/run-application/delete-stateful-set/).\n  * Learn how to [configure a Pod to use a volume for storage](/docs/tasks/configure-pod-container/configure-volume-storage/).\n  * Learn how to [configure a Pod to use a PersistentVolume for storage](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).\n* `StatefulSet` is a top-level resource in the Kubernetes REST API.\n  Read the {{< api-reference page=\"workload-resources/stateful-set-v1\" >}}\n  object definition to understand the API for stateful sets.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\n  you can use it to manage application availability during disruptions.", "zh": "* 了解 [Pod](/zh-cn/docs/concepts/workloads/pods)。\n* 了解如何使用 StatefulSet\n  * 跟随示例[部署有状态应用](/zh-cn/docs/tutorials/stateful-application/basic-stateful-set/)。\n  * 跟随示例[使用 StatefulSet 部署 Cassandra](/zh-cn/docs/tutorials/stateful-application/cassandra/)。\n  * 跟随示例[运行多副本的有状态应用程序](/zh-cn/docs/tasks/run-application/run-replicated-stateful-application/)。\n  * 了解如何[扩缩 StatefulSet](/zh-cn/docs/tasks/run-application/scale-stateful-set/)。\n  * 了解[删除 StatefulSet](/zh-cn/docs/tasks/run-application/delete-stateful-set/)涉及到的操作。\n  * 了解如何[配置 Pod 以使用卷进行存储](/zh-cn/docs/tasks/configure-pod-container/configure-volume-storage/)。\n  * 了解如何[配置 Pod 以使用 PersistentVolume 作为存储](/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/)。\n* `StatefulSet` 是 Kubernetes REST API 中的顶级资源。阅读 {{< api-reference page=\"workload-resources/stateful-set-v1\" >}}\n   对象定义理解关于该资源的 API。\n* 阅读 [Pod 干扰预算（Disruption Budget）](/zh-cn/docs/concepts/workloads/pods/disruptions/)，了解如何在干扰下运行高度可用的应用。"}
{"en": "A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often\nused to guarantee the availability of a specified number of identical Pods.", "zh": "ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。\n因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。"}
{"en": "## How a ReplicaSet works\n\nA ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number\nof replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods\nit should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating\nand deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod\ntemplate.", "zh": "## ReplicaSet 的工作原理 {#how-a-replicaset-works}\n\nReplicaSet 是通过一组字段来定义的，包括一个用来识别可获得的 Pod\n的集合的选择算符、一个用来标明应该维护的副本个数的数值、一个用来指定应该创建新 Pod\n以满足副本个数条件时要使用的 Pod 模板等等。\n每个 ReplicaSet 都通过根据需要创建和删除 Pod 以使得副本个数达到期望值，\n进而实现其存在价值。当 ReplicaSet 需要创建新的 Pod 时，会使用所提供的 Pod 模板。"}
{"en": "A ReplicaSet is linked to its Pods via the Pods' [metadata.ownerReferences](/docs/concepts/architecture/garbage-collection/#owners-dependents)\nfield, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning\nReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSet\nknows of the state of the Pods it is maintaining and plans accordingly.", "zh": "ReplicaSet 通过 Pod 上的\n[metadata.ownerReferences](/zh-cn/docs/concepts/architecture/garbage-collection/#owners-dependents)\n字段连接到附属 Pod，该字段给出当前对象的属主资源。\nReplicaSet 所获得的 Pod 都在其 ownerReferences 字段中包含了属主 ReplicaSet\n的标识信息。正是通过这一连接，ReplicaSet 知道它所维护的 Pod 集合的状态，\n并据此计划其操作行为。"}
{"en": "A ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no\nOwnerReference or the OwnerReference is not a {{< glossary_tooltip term_id=\"controller\" >}} and it\nmatches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.", "zh": "ReplicaSet 使用其选择算符来辨识要获得的 Pod 集合。如果某个 Pod 没有\nOwnerReference 或者其 OwnerReference 不是一个{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}，\n且其匹配到某 ReplicaSet 的选择算符，则该 Pod 立即被此 ReplicaSet 获得。"}
{"en": "## When to use a ReplicaSet\n\nA ReplicaSet ensures that a specified number of pod replicas are running at any given\ntime. However, a Deployment is a higher-level concept that manages ReplicaSets and\nprovides declarative updates to Pods along with a lot of other useful features.\nTherefore, we recommend using Deployments instead of directly using ReplicaSets, unless\nyou require custom update orchestration or don't require updates at all.\n\nThis actually means that you may never need to manipulate ReplicaSet objects:\nuse a Deployment instead, and define your application in the spec section.", "zh": "## 何时使用 ReplicaSet    {#when-to-use-a-replicaset}\n\nReplicaSet 确保任何时间都有指定数量的 Pod 副本在运行。\n然而，Deployment 是一个更高级的概念，它管理 ReplicaSet，并向 Pod\n提供声明式的更新以及许多其他有用的功能。\n因此，我们建议使用 Deployment 而不是直接使用 ReplicaSet，\n除非你需要自定义更新业务流程或根本不需要更新。\n\n这实际上意味着，你可能永远不需要操作 ReplicaSet 对象：而是使用\nDeployment，并在 spec 部分定义你的应用。"}
{"en": "## Example", "zh": "## 示例    {#example}\n\n{{% code_sample file=\"controllers/frontend.yaml\" %}}"}
{"en": "Saving this manifest into `frontend.yaml` and submitting it to a Kubernetes cluster will\ncreate the defined ReplicaSet and the Pods that it manages.", "zh": "将此清单保存到 `frontend.yaml` 中，并将其提交到 Kubernetes 集群，\n就能创建 yaml 文件所定义的 ReplicaSet 及其管理的 Pod。\n\n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\n```"}
{"en": "You can then get the current ReplicaSets deployed:", "zh": "你可以看到当前被部署的 ReplicaSet：\n\n```shell\nkubectl get rs\n```"}
{"en": "And see the frontend one you created:", "zh": "并看到你所创建的前端：\n\n```\nNAME       DESIRED   CURRENT   READY   AGE\nfrontend   3         3         3       6s\n```"}
{"en": "You can also check on the state of the ReplicaSet:", "zh": "你也可以查看 ReplicaSet 的状态：\n\n```shell\nkubectl describe rs/frontend\n```"}
{"en": "And you will see output similar to:", "zh": "你会看到类似如下的输出：\n\n```\nName:         frontend\nNamespace:    default\nSelector:     tier=frontend\nLabels:       app=guestbook\n              tier=frontend\nAnnotations:  <none>\nReplicas:     3 current / 3 desired\nPods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  tier=frontend\n  Containers:\n   php-redis:\n    Image:        us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5\n    Port:         <none>\n    Host Port:    <none>\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                   Message\n  ----    ------            ----  ----                   -------\n  Normal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-gbgfx\n  Normal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-rwz57\n  Normal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-wkl7w\n```"}
{"en": "And lastly you can check for the Pods brought up:", "zh": "最后可以查看启动了的 Pod 集合：\n\n```shell\nkubectl get pods\n```"}
{"en": "You should see Pod information similar to:", "zh": "你会看到类似如下的 Pod 信息：\n\n```\nNAME             READY   STATUS    RESTARTS   AGE\nfrontend-gbgfx   1/1     Running   0          10m\nfrontend-rwz57   1/1     Running   0          10m\nfrontend-wkl7w   1/1     Running   0          10m\n```"}
{"en": "You can also verify that the owner reference of these pods is set to the frontend ReplicaSet.\nTo do this, get the yaml of one of the Pods running:", "zh": "你也可以查看 Pod 的属主引用被设置为前端的 ReplicaSet。\n要实现这点，可取回运行中的某个 Pod 的 YAML：\n\n```shell\nkubectl get pods frontend-gbgfx -o yaml\n```"}
{"en": "The output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:", "zh": "输出将类似这样，frontend ReplicaSet 的信息被设置在 metadata 的\n`ownerReferences` 字段中：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: \"2024-02-28T22:30:44Z\"\n  generateName: frontend-\n  labels:\n    tier: frontend\n  name: frontend-gbgfx\n  namespace: default\n  ownerReferences:\n  - apiVersion: apps/v1\n    blockOwnerDeletion: true\n    controller: true\n    kind: ReplicaSet\n    name: frontend\n    uid: e129deca-f864-481b-bb16-b27abfd92292\n...\n```"}
{"en": "## Non-Template Pod acquisitions", "zh": "## 非模板 Pod 的获得    {#non-template-pod-acquisitions}"}
{"en": "While you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have\nlabels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited\nto owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections.\n\nTake the previous frontend ReplicaSet example, and the Pods specified in the following manifest:", "zh": "尽管你完全可以直接创建裸的 Pod，强烈建议你确保这些裸的 Pod 并不包含可能与你的某个\nReplicaSet 的选择算符相匹配的标签。原因在于 ReplicaSet 并不仅限于拥有在其模板中设置的\nPod，它还可以像前面小节中所描述的那样获得其他 Pod。\n\n以前面的 frontend ReplicaSet 为例，并在以下清单中指定这些 Pod：\n\n{{% code_sample file=\"pods/pod-rs.yaml\" %}}"}
{"en": "As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend\nReplicaSet, they will immediately be acquired by it.\n\nSuppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to\nfulfill its replica count requirement:", "zh": "由于这些 Pod 没有控制器（Controller，或其他对象）作为其属主引用，\n并且其标签与 frontend ReplicaSet 的选择算符匹配，它们会立即被该 ReplicaSet 获取。\n\n假定你在 frontend ReplicaSet 已经被部署之后创建 Pod，并且你已经在 ReplicaSet\n中设置了其初始的 Pod 副本数以满足其副本计数需要：\n\n```shell\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\n```"}
{"en": "The new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over\nits desired count.\n\nFetching the Pods:", "zh": "新的 Pod 会被该 ReplicaSet 获取，并立即被 ReplicaSet 终止，\n因为它们的存在会使得 ReplicaSet 中 Pod 个数超出其期望值。\n\n取回 Pod：\n\n\n```shell\nkubectl get pods\n```"}
{"en": "The output shows that the new Pods are either already terminated, or in the process of being terminated:", "zh": "输出显示新的 Pod 或者已经被终止，或者处于终止过程中：\n\n```\nNAME             READY   STATUS        RESTARTS   AGE\nfrontend-b2zdv   1/1     Running       0          10m\nfrontend-vcmts   1/1     Running       0          10m\nfrontend-wtsmm   1/1     Running       0          10m\npod1             0/1     Terminating   0          1s\npod2             0/1     Terminating   0          1s\n```"}
{"en": "If you create the Pods first:", "zh": "如果你先行创建 Pod：\n\n```shell\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\n```"}
{"en": "And then create the ReplicaSet however:", "zh": "之后再创建 ReplicaSet：\n\n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\n```"}
{"en": "You shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the\nnumber of its new Pods and the original matches its desired count. As fetching the Pods:", "zh": "你会看到 ReplicaSet 已经获得了该 Pod，并仅根据其规约创建新的 Pod，\n直到新的 Pod 和原来的 Pod 的总数达到其预期个数。\n这时取回 Pod 列表：\n\n```shell\nkubectl get pods\n```"}
{"en": "Will reveal in its output:", "zh": "将会生成下面的输出：\n\n```\nNAME             READY   STATUS    RESTARTS   AGE\nfrontend-hmmj2   1/1     Running   0          9s\npod1             1/1     Running   0          36s\npod2             1/1     Running   0          36s\n```"}
{"en": "In this manner, a ReplicaSet can own a non-homogeneous set of Pods", "zh": "采用这种方式，一个 ReplicaSet 中可以包含异质的 Pod 集合。"}
{"en": "## Writing a ReplicaSet manifest\n\nAs with all other Kubernetes API objects, a ReplicaSet needs the `apiVersion`, `kind`, and `metadata` fields.\nFor ReplicaSets, the `kind` is always a ReplicaSet.\n\nWhen the control plane creates new Pods for a ReplicaSet, the `.metadata.name` of the\nReplicaSet is part of the basis for naming those Pods.  The name of a ReplicaSet must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\n\nA ReplicaSet also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).", "zh": "## 编写 ReplicaSet 的清单    {#writing-a-replicaset-manifest}\n\n与所有其他 Kubernetes API 对象一样，ReplicaSet 也需要 `apiVersion`、`kind`、和 `metadata` 字段。\n对于 ReplicaSet 而言，其 `kind` 始终是 ReplicaSet。\n\n当控制平面为 ReplicaSet 创建新的 Pod 时，ReplicaSet\n的 `.metadata.name` 是命名这些 Pod 的部分基础。ReplicaSet 的名称必须是一个合法的\n[DNS 子域](/zh-cn/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names)值，\n但这可能对 Pod 的主机名产生意外的结果。为获得最佳兼容性，名称应遵循更严格的\n[DNS 标签](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-label-names)规则。\n\nReplicaSet 也需要\n[`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)\n部分。"}
{"en": "### Pod Template\n\nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates) which is also\nrequired to have labels in place. In our `frontend.yaml` example we had one label: `tier: frontend`.\nBe careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.\n\nFor the template's [restart policy](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) field,\n`.spec.template.spec.restartPolicy`, the only allowed value is `Always`, which is the default.", "zh": "### Pod 模板    {#pod-template}\n\n`.spec.template` 是一个 [Pod 模板](/zh-cn/docs/concepts/workloads/pods/#pod-templates)，\n要求设置标签。在 `frontend.yaml` 示例中，我们指定了标签 `tier: frontend`。\n注意不要将标签与其他控制器的选择算符重叠，否则那些控制器会尝试收养此 Pod。\n\n对于模板的[重启策略](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\n字段，`.spec.template.spec.restartPolicy`，唯一允许的取值是 `Always`，这也是默认值."}
{"en": "### Pod Selector\n\nThe `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/). As discussed\n[earlier](#how-a-replicaset-works) these are the labels used to identify potential Pods to acquire. In our\n`frontend.yaml` example, the selector was:\n\n```yaml\nmatchLabels:\n  tier: frontend\n```\n\nIn the ReplicaSet, `.spec.template.metadata.labels` must match `spec.selector`, or it will\nbe rejected by the API.", "zh": "### Pod 选择算符   {#pod-selector}\n\n`.spec.selector` 字段是一个[标签选择算符](/zh-cn/docs/concepts/overview/working-with-objects/labels/)。\n如前文中[所讨论的](#how-a-replicaset-works)，这些是用来标识要被获取的 Pod\n的标签。在签名的 `frontend.yaml` 示例中，选择算符为：\n\n```yaml\nmatchLabels:\n  tier: frontend\n```\n\n在 ReplicaSet 中，`.spec.template.metadata.labels` 的值必须与 `spec.selector`\n值相匹配，否则该配置会被 API 拒绝。\n\n{{< note >}}"}
{"en": "For 2 ReplicaSets specifying the same `.spec.selector` but different\n`.spec.template.metadata.labels` and `.spec.template.spec` fields, each ReplicaSet ignores the\nPods created by the other ReplicaSet.", "zh": "对于设置了相同的 `.spec.selector`，但\n`.spec.template.metadata.labels` 和 `.spec.template.spec` 字段不同的两个\nReplicaSet 而言，每个 ReplicaSet 都会忽略被另一个 ReplicaSet 所创建的 Pod。\n{{< /note >}}"}
{"en": "### Replicas\n\nYou can specify how many Pods should run concurrently by setting `.spec.replicas`. The ReplicaSet will create/delete\nits Pods to match this number.\n\nIf you do not specify `.spec.replicas`, then it defaults to 1.", "zh": "### Replicas\n\n你可以通过设置 `.spec.replicas` 来指定要同时运行的 Pod 个数。\nReplicaSet 创建、删除 Pod 以与此值匹配。\n\n如果你没有指定 `.spec.replicas`，那么默认值为 1。"}
{"en": "## Working with ReplicaSets\n\n### Deleting a ReplicaSet and its Pods\n\nTo delete a ReplicaSet and all of its Pods, use\n[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete). The\n[Garbage collector](/docs/concepts/architecture/garbage-collection/) automatically deletes all of\nthe dependent Pods by default.\n\nWhen using the REST API or the `client-go` library, you must set `propagationPolicy` to\n`Background` or `Foreground` in the `-d` option. For example:", "zh": "## 使用 ReplicaSet    {#working-with-replicasets}\n\n### 删除 ReplicaSet 和它的 Pod    {#deleting-a-replicaset-and-its-pods}\n\n要删除 ReplicaSet 和它的所有 Pod，使用\n[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete) 命令。\n默认情况下，[垃圾收集器](/zh-cn/docs/concepts/architecture/garbage-collection/)\n自动删除所有依赖的 Pod。\n\n当使用 REST API 或 `client-go` 库时，你必须在 `-d` 选项中将 `propagationPolicy`\n设置为 `Background` 或 `Foreground`。例如：\n\n```shell\nkubectl proxy --port=8080\ncurl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \\\n  -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\\n  -H \"Content-Type: application/json\"\n```"}
{"en": "### Deleting just a ReplicaSet\n\nYou can delete a ReplicaSet without affecting any of its Pods using\n[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete)\nwith the `--cascade=orphan` option.\nWhen using the REST API or the `client-go` library, you must set `propagationPolicy` to `Orphan`.\nFor example:", "zh": "### 只删除 ReplicaSet    {#deleting-just-a-replicaset}\n\n你可以只删除 ReplicaSet 而不影响它的各个 Pod，方法是使用\n[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete)\n命令并设置 `--cascade=orphan` 选项。\n\n当使用 REST API 或 `client-go` 库时，你必须将 `propagationPolicy` 设置为 `Orphan`。\n例如：\n\n```shell\nkubectl proxy --port=8080\ncurl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \\\n  -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Orphan\"}' \\\n  -H \"Content-Type: application/json\"\n```"}
{"en": "Once the original is deleted, you can create a new ReplicaSet to replace it.  As long\nas the old and new `.spec.selector` are the same, then the new one will adopt the old Pods.\nHowever, it will not make any effort to make existing Pods match a new, different pod template.\nTo update Pods to a new spec in a controlled way, use a\n[Deployment](/docs/concepts/workloads/controllers/deployment/#creating-a-deployment), as\nReplicaSets do not support a rolling update directly.", "zh": "一旦删除了原来的 ReplicaSet，就可以创建一个新的来替换它。\n由于新旧 ReplicaSet 的 `.spec.selector` 是相同的，新的 ReplicaSet 将接管老的 Pod。\n但是，它不会努力使现有的 Pod 与新的、不同的 Pod 模板匹配。\n若想要以可控的方式更新 Pod 的规约，可以使用\n[Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/#creating-a-deployment)\n资源，因为 ReplicaSet 并不直接支持滚动更新。"}
{"en": "### Isolating Pods from a ReplicaSet\n\nYou can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods\nfrom service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (\nassuming that the number of replicas is not also changed).", "zh": "### 将 Pod 从 ReplicaSet 中隔离    {#isolating-pods-from-a-replicaset}\n\n可以通过改变标签来从 ReplicaSet 中移除 Pod。\n这种技术可以用来从服务中去除 Pod，以便进行排错、数据恢复等。\n以这种方式移除的 Pod 将被自动替换（假设副本的数量没有改变）。"}
{"en": "### Scaling a ReplicaSet\n\nA ReplicaSet can be easily scaled up or down by simply updating the `.spec.replicas` field. The ReplicaSet controller\nensures that a desired number of Pods with a matching label selector are available and operational.", "zh": "### 扩缩 ReplicaSet    {#scaling-a-replicaset}\n\n通过更新 `.spec.replicas` 字段，ReplicaSet 可以被轻松地进行扩缩。ReplicaSet\n控制器能确保匹配标签选择器的数量的 Pod 是可用的和可操作的。"}
{"en": "When scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to\nprioritize scaling down pods based on the following general algorithm:", "zh": "在降低集合规模时，ReplicaSet 控制器通过对可用的所有 Pod 进行排序来优先选择要被删除的那些 Pod。\n其一般性算法如下："}
{"en": "1. Pending (and unschedulable) pods are scaled down first\n1. If `controller.kubernetes.io/pod-deletion-cost` annotation is set, then\n   the pod with the lower value will come first.\n1. Pods on nodes with more replicas come before pods on nodes with fewer replicas.\n1. If the pods' creation times differ, the pod that was created more recently\n   comes before the older pod (the creation times are bucketed on an integer log scale).", "zh": "1. 首先选择剔除悬决（Pending，且不可调度）的各个 Pod\n2. 如果设置了 `controller.kubernetes.io/pod-deletion-cost` 注解，则注解值较小的优先被裁减掉\n3. 所处节点上副本个数较多的 Pod 优先于所处节点上副本较少者\n4. 如果 Pod 的创建时间不同，最近创建的 Pod 优先于早前创建的 Pod 被裁减（创建时间是按整数幂级来分组的）。"}
{"en": "If all of the above match, then selection is random.", "zh": "如果以上比较结果都相同，则随机选择。"}
{"en": "### Pod deletion cost", "zh": "### Pod 删除开销   {#pod-deletion-cost}\n\n{{< feature-state for_k8s_version=\"v1.22\" state=\"beta\" >}}"}
{"en": "Using the [`controller.kubernetes.io/pod-deletion-cost`](/docs/reference/labels-annotations-taints/#pod-deletion-cost) \nannotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.", "zh": "通过使用 [`controller.kubernetes.io/pod-deletion-cost`](/zh-cn/docs/reference/labels-annotations-taints/#pod-deletion-cost)\n注解，用户可以对 ReplicaSet 缩容时要先删除哪些 Pod 设置偏好。"}
{"en": "The annotation should be set on the pod, the range is [-2147483648, 2147483647]. It represents the cost of\ndeleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion\ncost are preferred to be deleted before pods with higher deletion cost.", "zh": "此注解要设置到 Pod 上，取值范围为 [-2147483648, 2147483647]。\n所代表的是删除同一 ReplicaSet 中其他 Pod 相比较而言的开销。\n删除开销较小的 Pod 比删除开销较高的 Pod 更容易被删除。"}
{"en": "The implicit value for this annotation for pods that don't set it is 0; negative values are permitted.\nInvalid values will be rejected by the API server.", "zh": "Pod 如果未设置此注解，则隐含的设置值为 0。负值也是可接受的。\n如果注解值非法，API 服务器会拒绝对应的 Pod。"}
{"en": "This feature is beta and enabled by default. You can disable it using the\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n`PodDeletionCost` in both kube-apiserver and kube-controller-manager.", "zh": "此功能特性处于 Beta 阶段，默认被启用。你可以通过为 kube-apiserver 和\nkube-controller-manager 设置[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)\n`PodDeletionCost` 来禁用此功能。\n\n{{< note >}}"}
{"en": "- This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.\n- Users should avoid updating the annotation frequently, such as updating it based on a metric value,\n  because doing so will generate a significant number of pod updates on the apiserver.", "zh": "- 此机制实施时仅是尽力而为，并不能对 Pod 的删除顺序作出任何保证；\n- 用户应避免频繁更新注解值，例如根据某观测度量值来更新此注解值是应该避免的。\n  这样做会在 API 服务器上产生大量的 Pod 更新操作。\n{{< /note >}}"}
{"en": "#### Example Use Case\n\nThe different pods of an application could have different utilization levels. On scale down, the application \nmay prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application\nshould update `controller.kubernetes.io/pod-deletion-cost` once before issuing a scale down (setting the \nannotation to a value proportional to pod utilization level). This works if the application itself controls\nthe down scaling; for example, the driver pod of a Spark deployment.", "zh": "#### 使用场景示例    {#example-use-case}\n\n同一应用的不同 Pod 可能其利用率是不同的。在对应用执行缩容操作时，\n可能希望移除利用率较低的 Pod。为了避免频繁更新 Pod，应用应该在执行缩容操作之前更新一次\n`controller.kubernetes.io/pod-deletion-cost` 注解值\n（将注解值设置为一个与其 Pod 利用率对应的值）。\n如果应用自身控制器缩容操作时（例如 Spark 部署的驱动 Pod），这种机制是可以起作用的。"}
{"en": "### ReplicaSet as a Horizontal Pod Autoscaler Target\n\nA ReplicaSet can also be a target for\n[Horizontal Pod Autoscalers (HPA)](/docs/tasks/run-application/horizontal-pod-autoscale/). That is,\na ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting\nthe ReplicaSet we created in the previous example.", "zh": "### ReplicaSet 作为水平的 Pod 自动扩缩器目标    {#replicaset-as-a-horizontal-pod-autoscaler-target}\n\nReplicaSet 也可以作为[水平的 Pod 扩缩器 (HPA)](/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale/)\n的目标。也就是说，ReplicaSet 可以被 HPA 自动扩缩。\n以下是 HPA 以我们在前一个示例中创建的副本集为目标的示例。\n\n{{% code_sample file=\"controllers/hpa-rs.yaml\" %}}"}
{"en": "Saving this manifest into `hpa-rs.yaml` and submitting it to a Kubernetes cluster should\ncreate the defined HPA that autoscales the target ReplicaSet depending on the CPU usage\nof the replicated Pods.", "zh": "将这个列表保存到 `hpa-rs.yaml` 并提交到 Kubernetes 集群，就能创建它所定义的\nHPA，进而就能根据复制的 Pod 的 CPU 利用率对目标 ReplicaSet 进行自动扩缩。\n\n```shell\nkubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml\n```"}
{"en": "Alternatively, you can use the `kubectl autoscale` command to accomplish the same\n(and it's easier!)", "zh": "或者，可以使用 `kubectl autoscale` 命令完成相同的操作（而且它更简单！）\n\n```shell\nkubectl autoscale rs frontend --max=10 --min=3 --cpu-percent=50\n```"}
{"en": "## Alternatives to ReplicaSet\n\n### Deployment (recommended)\n\n[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is an object which can own ReplicaSets and update\nthem and their Pods via declarative, server-side rolling updates.\nWhile ReplicaSets can be used independently, today they're mainly used by Deployments as a mechanism to orchestrate Pod\ncreation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that\nthey create. Deployments own and manage their ReplicaSets.\nAs such, it is recommended to use Deployments when you want ReplicaSets.", "zh": "## ReplicaSet 的替代方案    {#alternatives-to-replicaset}\n\n### Deployment（推荐）    {#deployment-recommended}\n\n[`Deployment`](/zh-cn/docs/concepts/workloads/controllers/deployment/) 是一个可以拥有\nReplicaSet 并使用声明式方式在服务器端完成对 Pod 滚动更新的对象。\n尽管 ReplicaSet 可以独立使用，目前它们的主要用途是提供给 Deployment 作为编排\nPod 创建、删除和更新的一种机制。当使用 Deployment 时，你不必关心如何管理它所创建的\nReplicaSet，Deployment 拥有并管理其 ReplicaSet。\n因此，建议你在需要 ReplicaSet 时使用 Deployment。"}
{"en": "### Bare Pods\n\nUnlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or\nterminated for any reason, such as in the case of node failure or disruptive node maintenance,\nsuch as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your\napplication requires only a single Pod. Think of it similarly to a process supervisor, only it\nsupervises multiple Pods across multiple nodes instead of individual processes on a single node. A\nReplicaSet delegates local container restarts to some agent on the node such as Kubelet.", "zh": "### 裸 Pod    {#bare-pods}\n\n与用户直接创建 Pod 的情况不同，ReplicaSet 会替换那些由于某些原因被删除或被终止的\nPod，例如在节点故障或破坏性的节点维护（如内核升级）的情况下。\n因为这个原因，我们建议你使用 ReplicaSet，即使应用程序只需要一个 Pod。\n想像一下，ReplicaSet 类似于进程监视器，只不过它在多个节点上监视多个 Pod，\n而不是在单个节点上监视单个进程。\nReplicaSet 将本地容器重启的任务委托给了节点上的某个代理（例如，Kubelet）去完成。"}
{"en": "### Job\n\nUse a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicaSet for Pods that are\nexpected to terminate on their own (that is, batch jobs).", "zh": "### Job\n\n使用[`Job`](/zh-cn/docs/concepts/workloads/controllers/job/) 代替 ReplicaSet，\n可以用于那些期望自行终止的 Pod。"}
{"en": "### DaemonSet\n\nUse a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicaSet for Pods that provide a\nmachine-level function, such as machine monitoring or machine logging.  These Pods have a lifetime that is tied\nto a machine lifetime: the Pod needs to be running on the machine before other Pods start, and are\nsafe to terminate when the machine is otherwise ready to be rebooted/shutdown.", "zh": "### DaemonSet\n\n对于管理那些提供主机级别功能（如主机监控和主机日志）的容器，\n就要用 [`DaemonSet`](/zh-cn/docs/concepts/workloads/controllers/daemonset/)\n而不用 ReplicaSet。\n这些 Pod 的寿命与主机寿命有关：这些 Pod 需要先于主机上的其他 Pod 运行，\n并且在机器准备重新启动/关闭时安全地终止。\n\n### ReplicationController"}
{"en": "ReplicaSets are the successors to [ReplicationControllers](/docs/concepts/workloads/controllers/replicationcontroller/).\nThe two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based\nselector requirements as described in the [labels user guide](/docs/concepts/overview/working-with-objects/labels/#label-selectors).\nAs such, ReplicaSets are preferred over ReplicationControllers", "zh": "ReplicaSet 是 [ReplicationController](/zh-cn/docs/concepts/workloads/controllers/replicationcontroller/)\n的后继者。二者目的相同且行为类似，只是 ReplicationController 不支持\n[标签用户指南](/zh-cn/docs/concepts/overview/working-with-objects/labels/#label-selectors)\n中讨论的基于集合的选择算符需求。\n因此，相比于 ReplicationController，应优先考虑 ReplicaSet。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn about [Pods](/docs/concepts/workloads/pods).\n* Learn about [Deployments](/docs/concepts/workloads/controllers/deployment/).\n* [Run a Stateless Application Using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/),\n  which relies on ReplicaSets to work.\n* `ReplicaSet` is a top-level resource in the Kubernetes REST API.\n  Read the {{< api-reference page=\"workload-resources/replica-set-v1\" >}}\n  object definition to understand the API for replica sets.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\n  you can use it to manage application availability during disruptions.", "zh": "* 了解 [Pod](/zh-cn/docs/concepts/workloads/pods)。\n* 了解 [Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/)。\n* [使用 Deployment 运行一个无状态应用](/zh-cn/docs/tasks/run-application/run-stateless-application-deployment/)，\n  它依赖于 ReplicaSet。\n* `ReplicaSet` 是 Kubernetes REST API 中的顶级资源。阅读\n  {{< api-reference page=\"workload-resources/replica-set-v1\" >}}\n  对象定义理解关于该资源的 API。\n* 阅读 [Pod 干扰预算（Disruption Budget）](/zh-cn/docs/concepts/workloads/pods/disruptions/)，\n  了解如何在干扰下运行高度可用的应用。"}
{"en": "A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.\nAs pods successfully complete, the Job tracks the successful completions. When a specified number\nof successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up\nthe Pods it created. Suspending a Job will delete its active Pods until the Job\nis resumed again.\n\nA simple case is to create one Job object in order to reliably run one Pod to completion.\nThe Job object will start a new Pod if the first Pod fails or is deleted (for example\ndue to a node hardware failure or a node reboot).\n\nYou can also use a Job to run multiple Pods in parallel.\n\nIf you want to run a Job (either a single task, or several in parallel) on a schedule,\nsee [CronJob](/docs/concepts/workloads/controllers/cron-jobs/).", "zh": "Job 会创建一个或者多个 Pod，并将继续重试 Pod 的执行，直到指定数量的 Pod 成功终止。\n随着 Pod 成功结束，Job 跟踪记录成功完成的 Pod 个数。\n当数量达到指定的成功个数阈值时，任务（即 Job）结束。\n删除 Job 的操作会清除所创建的全部 Pod。\n挂起 Job 的操作会删除 Job 的所有活跃 Pod，直到 Job 被再次恢复执行。\n\n一种简单的使用场景下，你会创建一个 Job 对象以便以一种可靠的方式运行某 Pod 直到完成。\n当第一个 Pod 失败或者被删除（比如因为节点硬件失效或者重启）时，Job\n对象会启动一个新的 Pod。\n\n你也可以使用 Job 以并行的方式运行多个 Pod。\n\n如果你想按某种排期表（Schedule）运行 Job（单个任务或多个并行任务），请参阅\n[CronJob](/zh-cn/docs/concepts/workloads/controllers/cron-jobs/)。"}
{"en": "## Running an example Job\n\nHere is an example Job config. It computes π to 2000 places and prints it out.\nIt takes around 10s to complete.", "zh": "## 运行示例 Job     {#running-an-example-job}\n\n下面是一个 Job 配置示例。它负责计算 π 到小数点后 2000 位，并将结果打印出来。\n此计算大约需要 10 秒钟完成。\n\n{{% code_sample file=\"controllers/job.yaml\" %}}"}
{"en": "You can run the example with this command:", "zh": "你可以使用下面的命令来运行此示例：\n\n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/job.yaml\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\njob.batch/pi created\n```"}
{"en": "Check on the status of the Job with `kubectl`:", "zh": "使用 `kubectl` 来检查 Job 的状态：\n\n{{< tabs name=\"Check status of Job\" >}}\n{{< tab name=\"kubectl describe job pi\" codelang=\"bash\" >}}\nName:           pi\nNamespace:      default\nSelector:       batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nLabels:         batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\n                batch.kubernetes.io/job-name=pi\n                ...\nAnnotations:    batch.kubernetes.io/job-tracking: \"\"\nParallelism:    1\nCompletions:    1\nStart Time:     Mon, 02 Dec 2019 15:20:11 +0200\nCompleted At:   Mon, 02 Dec 2019 15:21:16 +0200\nDuration:       65s\nPods Statuses:  0 Running / 1 Succeeded / 0 Failed\nPod Template:\n  Labels:  batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\n           batch.kubernetes.io/job-name=pi\n  Containers:\n   pi:\n    Image:      perl:5.34.0\n    Port:       <none>\n    Host Port:  <none>\n    Command:\n      perl\n      -Mbignum=bpi\n      -wle\n      print bpi(2000)\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From            Message\n  ----    ------            ----  ----            -------\n  Normal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4\n  Normal  Completed         18s   job-controller  Job completed\n{{< /tab >}}\n{{< tab name=\"kubectl get job pi -o yaml\" codelang=\"bash\" >}}\napiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations: batch.kubernetes.io/job-tracking: \"\"\n             ...  \n  creationTimestamp: \"2022-11-10T17:53:53Z\"\n  generation: 1\n  labels:\n    batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\n    batch.kubernetes.io/job-name: pi\n  name: pi\n  namespace: default\n  resourceVersion: \"4751\"\n  uid: 204fb678-040b-497f-9266-35ffa8716d14\nspec:\n  backoffLimit: 4\n  completionMode: NonIndexed\n  completions: 1\n  parallelism: 1\n  selector:\n    matchLabels:\n      batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\n  suspend: false\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\n        batch.kubernetes.io/job-name: pi\n    spec:\n      containers:\n      - command:\n        - perl\n        - -Mbignum=bpi\n        - -wle\n        - print bpi(2000)\n        image: perl:5.34.0\n        imagePullPolicy: IfNotPresent\n        name: pi\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\nstatus:\n  active: 1\n  ready: 0\n  startTime: \"2022-11-10T17:53:57Z\"\n  uncountedTerminatedPods: {}\n{{< /tab >}}\n{{< /tabs >}}"}
{"en": "To view completed Pods of a Job, use `kubectl get pods`.\n\nTo list all the Pods that belong to a Job in a machine readable form, you can use a command like this:", "zh": "要查看 Job 对应的已完成的 Pod，可以执行 `kubectl get pods`。\n\n要以机器可读的方式列举隶属于某 Job 的全部 Pod，你可以使用类似下面这条命令：\n\n```shell\npods=$(kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}')\necho $pods\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\npi-5rwd7\n```"}
{"en": "Here, the selector is the same as the selector for the Job. The `--output=jsonpath` option specifies an expression\nwith the name from each Pod in the returned list.\n\nView the standard output of one of the pods:", "zh": "这里，选择算符与 Job 的选择算符相同。`--output=jsonpath` 选项给出了一个表达式，\n用来从返回的列表中提取每个 Pod 的 name 字段。\n\n查看其中一个 Pod 的标准输出：\n\n```shell\nkubectl logs $pods\n```"}
{"en": "Another way to view the logs of a Job:", "zh": "另外一种查看 Job 日志的方法：\n\n```shell\nkubectl logs jobs/pi\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\n3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901\n```"}
{"en": "## Writing a Job spec\n\nAs with all other Kubernetes config, a Job needs `apiVersion`, `kind`, and `metadata` fields.\n\nWhen the control plane creates new Pods for a Job, the `.metadata.name` of the\nJob is part of the basis for naming those Pods. The name of a Job must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\nEven when the name is a DNS subdomain, the name must be no longer than 63\ncharacters.\n\nA Job also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).", "zh": "## 编写 Job 规约    {#writing-a-job-spec}\n\n与 Kubernetes 中其他资源的配置类似，Job 也需要 `apiVersion`、`kind` 和 `metadata` 字段。\n\n当控制面为 Job 创建新的 Pod 时，Job 的 `.metadata.name` 是命名这些 Pod 的基础组成部分。\nJob 的名字必须是合法的 [DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)值，\n但这可能对 Pod 主机名产生意料之外的结果。为了获得最佳兼容性，此名字应遵循更严格的\n[DNS 标签](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-label-names)规则。\n即使该名字被要求遵循 DNS 子域名规则，也不得超过 63 个字符。\n\nJob 配置还需要一个 [`.spec` 节](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)。"}
{"en": "### Job Labels", "zh": "### Job 标签"}
{"en": "Job labels will have `batch.kubernetes.io/` prefix for `job-name` and `controller-uid`.", "zh": "Job 标签将为 `job-name` 和 `controller-uid` 加上 `batch.kubernetes.io/` 前缀。"}
{"en": "### Pod Template\n\nThe `.spec.template` is the only required field of the `.spec`.\n\nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates).\nIt has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}},\nexcept it is nested and does not have an `apiVersion` or `kind`.\n\nIn addition to required fields for a Pod, a pod template in a Job must specify appropriate\nlabels (see [pod selector](#pod-selector)) and an appropriate restart policy.\n\nOnly a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\nequal to `Never` or `OnFailure` is allowed.", "zh": "### Pod 模板    {#pod-template}\n\nJob 的 `.spec` 中只有 `.spec.template` 是必需的字段。\n\n字段 `.spec.template` 的值是一个 [Pod 模板](/zh-cn/docs/concepts/workloads/pods/#pod-templates)。\n其定义规范与 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}\n完全相同，只是其中不再需要 `apiVersion` 或 `kind` 字段。\n\n除了作为 Pod 所必需的字段之外，Job 中的 Pod 模板必须设置合适的标签\n（参见 [Pod 选择算符](#pod-selector)）和合适的重启策略。\n\nJob 中 Pod 的 [`RestartPolicy`](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\n只能设置为 `Never` 或 `OnFailure` 之一。"}
{"en": "### Pod selector\n\nThe `.spec.selector` field is optional. In almost all cases you should not specify it.\nSee section [specifying your own pod selector](#specifying-your-own-pod-selector).", "zh": "### Pod 选择算符   {#pod-selector}\n\n字段 `.spec.selector` 是可选的。在绝大多数场合，你都不需要为其赋值。\n参阅[设置自己的 Pod 选择算符](#specifying-your-own-pod-selector)。"}
{"en": "### Parallel execution for Jobs {#parallel-jobs}\n\nThere are three main types of task suitable to run as a Job:", "zh": "### Job 的并行执行 {#parallel-jobs}\n\n适合以 Job 形式来运行的任务主要有三种："}
{"en": "1. Non-parallel Jobs\n   - normally, only one Pod is started, unless the Pod fails.\n   - the Job is complete as soon as its Pod terminates successfully.\n1. Parallel Jobs with a *fixed completion count*:\n   - specify a non-zero positive value for `.spec.completions`.\n   - the Job represents the overall task, and is complete when there are `.spec.completions` successful Pods.\n   - when using `.spec.completionMode=\"Indexed\"`, each Pod gets a different index in the range 0 to `.spec.completions-1`.\n1. Parallel Jobs with a *work queue*:\n   - do not specify `.spec.completions`, default to `.spec.parallelism`.\n   - the Pods must coordinate amongst themselves or an external service to determine\n     what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.\n   - each Pod is independently capable of determining whether or not all its peers are done,\n     and thus that the entire Job is done.\n   - when _any_ Pod from the Job terminates with success, no new Pods are created.\n   - once at least one Pod has terminated with success and all Pods are terminated,\n     then the Job is completed with success.\n   - once any Pod has exited with success, no other Pod should still be doing any work\n     for this task or writing any output. They should all be in the process of exiting.", "zh": "1. 非并行 Job：\n   - 通常只启动一个 Pod，除非该 Pod 失败。\n   - 当 Pod 成功终止时，立即视 Job 为完成状态。\n1. 具有**确定完成计数**的并行 Job：\n   - `.spec.completions` 字段设置为非 0 的正数值。\n   - Job 用来代表整个任务，当成功的 Pod 个数达到 `.spec.completions` 时，Job 被视为完成。\n   - 当使用 `.spec.completionMode=\"Indexed\"` 时，每个 Pod 都会获得一个不同的\n     索引值，介于 0 和 `.spec.completions-1` 之间。\n1. 带**工作队列**的并行 Job：\n   - 不设置 `spec.completions`，默认值为 `.spec.parallelism`。\n   - 多个 Pod 之间必须相互协调，或者借助外部服务确定每个 Pod 要处理哪个工作条目。\n     例如，任一 Pod 都可以从工作队列中取走最多 N 个工作条目。\n   - 每个 Pod 都可以独立确定是否其它 Pod 都已完成，进而确定 Job 是否完成。\n   - 当 Job 中**任何** Pod 成功终止，不再创建新 Pod。\n   - 一旦至少 1 个 Pod 成功完成，并且所有 Pod 都已终止，即可宣告 Job 成功完成。\n   - 一旦任何 Pod 成功退出，任何其它 Pod 都不应再对此任务执行任何操作或生成任何输出。\n     所有 Pod 都应启动退出过程。"}
{"en": "For a _non-parallel_ Job, you can leave both `.spec.completions` and `.spec.parallelism` unset.\nWhen both are unset, both are defaulted to 1.\n\nFor a _fixed completion count_ Job, you should set `.spec.completions` to the number of completions needed.\nYou can set `.spec.parallelism`, or leave it unset and it will default to 1.\n\nFor a _work queue_ Job, you must leave `.spec.completions` unset, and set `.spec.parallelism` to\na non-negative integer.\n\nFor more information about how to make use of the different types of job,\nsee the [job patterns](#job-patterns) section.", "zh": "对于**非并行**的 Job，你可以不设置 `spec.completions` 和 `spec.parallelism`。\n这两个属性都不设置时，均取默认值 1。\n\n对于**确定完成计数**类型的 Job，你应该设置 `.spec.completions` 为所需要的完成个数。\n你可以设置 `.spec.parallelism`，也可以不设置。其默认值为 1。\n\n对于一个**工作队列** Job，你不可以设置 `.spec.completions`，但要将`.spec.parallelism`\n设置为一个非负整数。\n\n关于如何利用不同类型的 Job 的更多信息，请参见 [Job 模式](#job-patterns)一节。"}
{"en": "#### Controlling parallelism\n\nThe requested parallelism (`.spec.parallelism`) can be set to any non-negative value.\nIf it is unspecified, it defaults to 1.\nIf it is specified as 0, then the Job is effectively paused until it is increased.\n\nActual parallelism (number of pods running at any instant) may be more or less than requested\nparallelism, for a variety of reasons:", "zh": "#### 控制并行性   {#controlling-parallelism}\n\n并行性请求（`.spec.parallelism`）可以设置为任何非负整数。\n如果未设置，则默认为 1。\n如果设置为 0，则 Job 相当于启动之后便被暂停，直到此值被增加。\n\n实际并行性（在任意时刻运行状态的 Pod 个数）可能比并行性请求略大或略小，\n原因如下："}
{"en": "- For _fixed completion count_ Jobs, the actual number of pods running in parallel will not exceed the number of\n  remaining completions. Higher values of `.spec.parallelism` are effectively ignored.\n- For _work queue_ Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.\n- If the Job {{< glossary_tooltip term_id=\"controller\" >}} has not had time to react.\n- If the Job controller failed to create Pods for any reason (lack of `ResourceQuota`, lack of permission, etc.),\n  then there may be fewer pods than requested.\n- The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.\n- When a Pod is gracefully shut down, it takes time to stop.", "zh": "- 对于**确定完成计数** Job，实际上并行执行的 Pod 个数不会超出剩余的完成数。\n  如果 `.spec.parallelism` 值较高，会被忽略。\n- 对于**工作队列** Job，有任何 Job 成功结束之后，不会有新的 Pod 启动。\n  不过，剩下的 Pod 允许执行完毕。\n- 如果 Job {{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}} 没有来得及作出响应，或者\n- 如果 Job 控制器因为任何原因（例如，缺少 `ResourceQuota` 或者没有权限）无法创建 Pod。\n  Pod 个数可能比请求的数目小。\n- Job 控制器可能会因为之前同一 Job 中 Pod 失效次数过多而压制新 Pod 的创建。\n- 当 Pod 处于体面终止进程中，需要一定时间才能停止。"}
{"en": "### Completion mode", "zh": "### 完成模式   {#completion-mode}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "Jobs with _fixed completion count_ - that is, jobs that have non null\n`.spec.completions` - can have a completion mode that is specified in `.spec.completionMode`:", "zh": "带有**确定完成计数**的 Job，即 `.spec.completions` 不为 null 的 Job，\n都可以在其 `.spec.completionMode` 中设置完成模式："}
{"en": "- `NonIndexed` (default): the Job is considered complete when there have been\n  `.spec.completions` successfully completed Pods. In other words, each Pod\n  completion is homologous to each other. Note that Jobs that have null\n  `.spec.completions` are implicitly `NonIndexed`.\n- `Indexed`: the Pods of a Job get an associated completion index from 0 to\n  `.spec.completions-1`. The index is available through four mechanisms:\n  - The Pod annotation `batch.kubernetes.io/job-completion-index`.\n  - The Pod label `batch.kubernetes.io/job-completion-index` (for v1.28 and later). Note\n    the feature gate `PodIndexLabel` must be enabled to use this label, and it is enabled\n    by default.\n  - As part of the Pod hostname, following the pattern `$(job-name)-$(index)`.\n    When you use an Indexed Job in combination with a\n    {{< glossary_tooltip term_id=\"Service\" >}}, Pods within the Job can use\n    the deterministic hostnames to address each other via DNS. For more information about\n    how to configure this, see [Job with Pod-to-Pod Communication](/docs/tasks/job/job-with-pod-to-pod-communication/).\n  - From the containerized task, in the environment variable `JOB_COMPLETION_INDEX`.", "zh": "- `NonIndexed`（默认值）：当成功完成的 Pod 个数达到 `.spec.completions` 所\n  设值时认为 Job 已经完成。换言之，每个 Job 完成事件都是独立无关且同质的。\n  要注意的是，当 `.spec.completions` 取值为 null 时，Job 被隐式处理为 `NonIndexed`。\n- `Indexed`：Job 的 Pod 会获得对应的完成索引，取值为 0 到 `.spec.completions-1`。\n  该索引可以通过四种方式获取：\n  - Pod 注解 `batch.kubernetes.io/job-completion-index`。\n  - Pod 标签 `batch.kubernetes.io/job-completion-index`（适用于 v1.28 及更高版本）。\n    请注意，必须启用 `PodIndexLabel` 特性门控才能使用此标签，默认被启用。\n  - 作为 Pod 主机名的一部分，遵循模式 `$(job-name)-$(index)`。\n    当你同时使用带索引的 Job（Indexed Job）与 {{< glossary_tooltip term_id=\"Service\" >}}，\n    Job 中的 Pod 可以通过 DNS 使用确切的主机名互相寻址。\n    有关如何配置的更多信息，请参阅[带 Pod 间通信的 Job](/zh-cn/docs/tasks/job/job-with-pod-to-pod-communication/)。\n  - 对于容器化的任务，在环境变量 `JOB_COMPLETION_INDEX` 中。"}
{"en": "The Job is considered complete when there is one successfully completed Pod\n  for each index. For more information about how to use this mode, see\n  [Indexed Job for Parallel Processing with Static Work Assignment](/docs/tasks/job/indexed-parallel-processing-static/).", "zh": "当每个索引都对应一个成功完成的 Pod 时，Job 被认为是已完成的。\n  关于如何使用这种模式的更多信息，可参阅\n  [用带索引的 Job 执行基于静态任务分配的并行处理](/zh-cn/docs/tasks/job/indexed-parallel-processing-static/)。\n\n{{< note >}}"}
{"en": "Although rare, more than one Pod could be started for the same index (due to various reasons such as node failures,\nkubelet restarts, or Pod evictions). In this case, only the first Pod that completes successfully will\ncount towards the completion count and update the status of the Job. The other Pods that are running\nor completed for the same index will be deleted by the Job controller once they are detected.", "zh": "带同一索引值启动的 Pod 可能不止一个（由于节点故障、kubelet\n重启或 Pod 驱逐等各种原因），尽管这种情况很少发生。\n在这种情况下，只有第一个成功完成的 Pod 才会被记入完成计数中并更新作业的状态。\n其他为同一索引值运行或完成的 Pod 一旦被检测到，将被 Job 控制器删除。\n{{< /note >}}"}
{"en": "## Handling Pod and container failures\n\nA container in a Pod may fail for a number of reasons, such as because the process in it exited with\na non-zero exit code, or the container was killed for exceeding a memory limit, etc. If this\nhappens, and the `.spec.template.spec.restartPolicy = \"OnFailure\"`, then the Pod stays\non the node, but the container is re-run. Therefore, your program needs to handle the case when it is\nrestarted locally, or else specify `.spec.template.spec.restartPolicy = \"Never\"`.\nSee [pod lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/#example-states) for more information on `restartPolicy`.", "zh": "## 处理 Pod 和容器失效    {#handling-pod-and-container-failures}\n\nPod 中的容器可能因为多种不同原因失效，例如因为其中的进程退出时返回值非零，\n或者容器因为超出内存约束而被杀死等等。\n如果发生这类事件，并且 `.spec.template.spec.restartPolicy = \"OnFailure\"`，\nPod 则继续留在当前节点，但容器会被重新运行。\n因此，你的程序需要能够处理在本地被重启的情况，或者要设置\n`.spec.template.spec.restartPolicy = \"Never\"`。\n关于 `restartPolicy` 的更多信息，可参阅\n[Pod 生命周期](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#example-states)。"}
{"en": "An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node\n(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the\n`.spec.template.spec.restartPolicy = \"Never\"`. When a Pod fails, then the Job controller\nstarts a new Pod. This means that your application needs to handle the case when it is restarted in a new\npod. In particular, it needs to handle temporary files, locks, incomplete output and the like\ncaused by previous runs.", "zh": "整个 Pod 也可能会失败，且原因各不相同。\n例如，当 Pod 启动时，节点失效（被升级、被重启、被删除等）或者其中的容器失败而\n`.spec.template.spec.restartPolicy = \"Never\"`。\n当 Pod 失败时，Job 控制器会启动一个新的 Pod。\n这意味着，你的应用需要处理在一个新 Pod 中被重启的情况。\n尤其是应用需要处理之前运行所产生的临时文件、锁、不完整的输出等问题。"}
{"en": "By default, each pod failure is counted towards the `.spec.backoffLimit` limit,\nsee [pod backoff failure policy](#pod-backoff-failure-policy). However, you can\ncustomize handling of pod failures by setting the Job's [pod failure policy](#pod-failure-policy).", "zh": "默认情况下，每个 Pod 失效都被计入 `.spec.backoffLimit` 限制，\n请参阅 [Pod 回退失效策略](#pod-backoff-failure-policy)。\n但你可以通过设置 Job 的 [Pod 失效策略](#pod-failure-policy)自定义对 Pod 失效的处理方式。"}
{"en": "Additionally, you can choose to count the pod failures independently for each\nindex of an [Indexed](#completion-mode) Job by setting the `.spec.backoffLimitPerIndex` field\n(for more information, see [backoff limit per index](#backoff-limit-per-index)).", "zh": "此外，你可以通过设置 `.spec.backoffLimitPerIndex` 字段，\n选择为 [Indexed](#completion-mode) Job 的每个索引独立计算 Pod 失败次数\n（细节参阅[逐索引的回退限制](#backoff-limit-per-index)）。"}
{"en": "Note that even if you specify `.spec.parallelism = 1` and `.spec.completions = 1` and\n`.spec.template.spec.restartPolicy = \"Never\"`, the same program may\nsometimes be started twice.\n\nIf you do specify `.spec.parallelism` and `.spec.completions` both greater than 1, then there may be\nmultiple pods running at once. Therefore, your pods must also be tolerant of concurrency.", "zh": "注意，即使你将 `.spec.parallelism` 设置为 1，且将 `.spec.completions` 设置为\n1，并且 `.spec.template.spec.restartPolicy` 设置为 \"Never\"，同一程序仍然有可能被启动两次。\n\n如果你确实将 `.spec.parallelism` 和 `.spec.completions` 都设置为比 1 大的值，\n那就有可能同时出现多个 Pod 运行的情况。\n为此，你的 Pod 也必须能够处理并发性问题。"}
{"en": "If you specify the `.spec.podFailurePolicy` field, the Job controller does not consider a terminating\nPod (a pod that has a `.metadata.deletionTimestamp` field set) as a failure until that Pod is\nterminal (its `.status.phase` is `Failed` or `Succeeded`). However, the Job controller\ncreates a replacement Pod as soon as the termination becomes apparent. Once the\npod terminates, the Job controller evaluates `.backoffLimit` and `.podFailurePolicy`\nfor the relevant Job, taking this now-terminated Pod into consideration.\n\nIf either of these requirements is not satisfied, the Job controller counts\na terminating Pod as an immediate failure, even if that Pod later terminates\nwith `phase: \"Succeeded\"`.", "zh": "当你指定了 `.spec.podFailurePolicy` 字段，\nJob 控制器不会将终止过程中的 Pod（已设置 `.metadata.deletionTimestamp` 字段的 Pod）视为失效 Pod，\n直到该 Pod 完全终止（其 `.status.phase` 为 `Failed` 或 `Succeeded`）。\n但只要终止变得显而易见，Job 控制器就会创建一个替代的 Pod。一旦 Pod 终止，Job 控制器将把这个刚终止的\nPod 考虑在内，评估相关 Job 的 `.backoffLimit` 和 `.podFailurePolicy`。\n\n如果不满足任一要求，即使 Pod 稍后以 `phase: \"Succeeded\"` 终止，Job 控制器也会将此即将终止的 Pod 计为立即失效。"}
{"en": "### Pod backoff failure policy\n\nThere are situations where you want to fail a Job after some amount of retries\ndue to a logical error in configuration etc.\nTo do so, set `.spec.backoffLimit` to specify the number of retries before\nconsidering a Job as failed. The back-off limit is set by default to 6. Failed\nPods associated with the Job are recreated by the Job controller with an\nexponential back-off delay (10s, 20s, 40s ...) capped at six minutes.\n\nThe number of retries is calculated in two ways:\n\n- The number of Pods with `.status.phase = \"Failed\"`.\n- When using `restartPolicy = \"OnFailure\"`, the number of retries in all the\n  containers of Pods with `.status.phase` equal to `Pending` or `Running`.\n\nIf either of the calculations reaches the `.spec.backoffLimit`, the Job is\nconsidered failed.", "zh": "### Pod 回退失效策略    {#pod-backoff-failure-policy}\n\n在有些情形下，你可能希望 Job 在经历若干次重试之后直接进入失败状态，\n因为这很可能意味着遇到了配置错误。\n为了实现这点，可以将 `.spec.backoffLimit` 设置为视 Job 为失败之前的重试次数。\n失效回退的限制值默认为 6。\n与 Job 相关的失效的 Pod 会被 Job 控制器重建，回退重试时间将会按指数增长\n（从 10 秒、20 秒到 40 秒）最多至 6 分钟。\n\n计算重试次数有以下两种方法：\n- 计算 `.status.phase = \"Failed\"` 的 Pod 数量。\n- 当 Pod 的 `restartPolicy = \"OnFailure\"` 时，针对 `.status.phase` 等于 `Pending` 或\n  `Running` 的 Pod，计算其中所有容器的重试次数。\n\n如果两种方式其中一个的值达到 `.spec.backoffLimit`，则 Job 被判定为失败。\n\n{{< note >}}"}
{"en": "If your job has `restartPolicy = \"OnFailure\"`, keep in mind that your Pod running the Job\nwill be terminated once the job backoff limit has been reached. This can make debugging\nthe Job's executable more difficult. We suggest setting\n`restartPolicy = \"Never\"` when debugging the Job or using a logging system to ensure output\nfrom failed Jobs is not lost inadvertently.", "zh": "如果你的 Job 的 `restartPolicy` 被设置为 \"OnFailure\"，就要注意运行该 Job 的 Pod\n会在 Job 到达失效回退次数上限时自动被终止。\n这会使得调试 Job 中可执行文件的工作变得非常棘手。\n我们建议在调试 Job 时将 `restartPolicy` 设置为 \"Never\"，\n或者使用日志系统来确保失效 Job 的输出不会意外遗失。\n{{< /note >}}"}
{"en": "### Backoff limit per index {#backoff-limit-per-index}", "zh": "### 逐索引的回退限制    {#backoff-limit-per-index}\n\n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}\n\n{{< note >}}"}
{"en": "You can only configure the backoff limit per index for an [Indexed](#completion-mode) Job, if you\nhave the `JobBackoffLimitPerIndex` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled in your cluster.", "zh": "只有在集群中启用了 `JobBackoffLimitPerIndex`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)，\n才能为 [Indexed](#completion-mode) Job 配置逐索引的回退限制。\n{{< /note >}}"}
{"en": "When you run an [indexed](#completion-mode) Job, you can choose to handle retries\nfor pod failures independently for each index. To do so, set the\n`.spec.backoffLimitPerIndex` to specify the maximal number of pod failures\nper index.", "zh": "运行 [Indexed](#completion-mode) Job 时，你可以选择对每个索引独立处理 Pod 失败的重试。\n为此，可以设置 `.spec.backoffLimitPerIndex` 来指定每个索引的最大 Pod 失败次数。"}
{"en": "When the per-index backoff limit is exceeded for an index, Kubernetes considers the index as failed and adds it to the\n`.status.failedIndexes` field. The succeeded indexes, those with a successfully\nexecuted pods, are recorded in the `.status.completedIndexes` field, regardless of whether you set\nthe `backoffLimitPerIndex` field.", "zh": "当某个索引超过逐索引的回退限制后，Kubernetes 将视该索引为已失败，并将其添加到 `.status.failedIndexes` 字段中。\n无论你是否设置了 `backoffLimitPerIndex` 字段，已成功执行的索引（具有成功执行的 Pod）将被记录在\n`.status.completedIndexes` 字段中。"}
{"en": "Note that a failing index does not interrupt execution of other indexes.\nOnce all indexes finish for a Job where you specified a backoff limit per index,\nif at least one of those indexes did fail, the Job controller marks the overall\nJob as failed, by setting the Failed condition in the status. The Job gets\nmarked as failed even if some, potentially nearly all, of the indexes were\nprocessed successfully.", "zh": "请注意，失败的索引不会中断其他索引的执行。一旦在指定了逐索引回退限制的 Job 中的所有索引完成，\n如果其中至少有一个索引失败，Job 控制器会通过在状态中设置 Failed 状况将整个 Job 标记为失败。\n即使其中一些（可能几乎全部）索引已被成功处理，该 Job 也会被标记为失败。"}
{"en": "You can additionally limit the maximal number of indexes marked failed by\nsetting the `.spec.maxFailedIndexes` field.\nWhen the number of failed indexes exceeds the `maxFailedIndexes` field, the\nJob controller triggers termination of all remaining running Pods for that Job.\nOnce all pods are terminated, the entire Job is marked failed by the Job\ncontroller, by setting the Failed condition in the Job status.", "zh": "你还可以通过设置 `.spec.maxFailedIndexes` 字段来限制标记为失败的最大索引数。\n当失败的索引数量超过 `maxFailedIndexes` 字段时，Job 控制器会对该 Job\n的运行中的所有余下 Pod 触发终止操作。一旦所有 Pod 被终止，Job 控制器将通过设置 Job\n状态中的 Failed 状况将整个 Job 标记为失败。"}
{"en": "Here is an example manifest for a Job that defines a `backoffLimitPerIndex`:", "zh": "以下是定义 `backoffLimitPerIndex` 的 Job 示例清单：\n\n{{% code_sample file=\"/controllers/job-backoff-limit-per-index-example.yaml\" %}}"}
{"en": "In the example above, the Job controller allows for one restart for each\nof the indexes. When the total number of failed indexes exceeds 5, then\nthe entire Job is terminated.\n\nOnce the job is finished, the Job status looks as follows:", "zh": "在上面的示例中，Job 控制器允许每个索引重新启动一次。\n当失败的索引总数超过 5 个时，整个 Job 将被终止。\n\nJob 完成后，该 Job 的状态如下所示：\n\n```sh\nkubectl get -o yaml job job-backoff-limit-per-index-example\n```"}
{"en": "# 1 succeeded pod for each of 5 succeeded indexes\n# 2 failed pods (1 retry) for each of 5 failed indexes", "zh": "```yaml\n  status:\n    completedIndexes: 1,3,5,7,9\n    failedIndexes: 0,2,4,6,8\n    succeeded: 5          # 每 5 个成功的索引有 1 个成功的 Pod\n    failed: 10            # 每 5 个失败的索引有 2 个失败的 Pod（1 次重试）\n    conditions:\n    - message: Job has failed indexes\n      reason: FailedIndexes\n      status: \"True\"\n      type: FailureTarget\n    - message: Job has failed indexes\n      reason: FailedIndexes\n      status: \"True\"\n      type: Failed\n```"}
{"en": "The Job controller adds the `FailureTarget` Job condition to trigger\n[Job termination and cleanup](#job-termination-and-cleanup). When all of the\nJob Pods are terminated, the Job controller adds the `Failed` condition\nwith the same values for `reason` and `message` as the `FailureTarget` Job\ncondition. For details, see [Termination of Job Pods](#termination-of-job-pods).", "zh": "Job 控制器添加 `FailureTarget` Job 状况来触发 [Job 终止和清理](#job-termination-and-cleanup)。\n当所有 Job Pod 都终止时，Job 控制器会添加 `Failed` 状况，\n其 `reason` 和 `message` 的值与 `FailureTarget` Job 状况相同。\n有关详细信息，请参阅 [Job Pod 的终止](#termination-of-job-pods)。"}
{"en": "Additionally, you may want to use the per-index backoff along with a\n[pod failure policy](#pod-failure-policy). When using\nper-index backoff, there is a new `FailIndex` action available which allows you to\navoid unnecessary retries within an index.", "zh": "此外，你可能想要结合使用逐索引回退与 [Pod 失效策略](#pod-failure-policy)。\n在使用逐索引回退时，有一个新的 `FailIndex` 操作可用，它让你避免就某个索引进行不必要的重试。"}
{"en": "### Pod failure policy {#pod-failure-policy}", "zh": "### Pod 失效策略 {#pod-failure-policy}\n\n{{< feature-state feature_gate_name=\"JobPodFailurePolicy\" >}}"}
{"en": "A Pod failure policy, defined with the `.spec.podFailurePolicy` field, enables\nyour cluster to handle Pod failures based on the container exit codes and the\nPod conditions.", "zh": "Pod 失效策略使用 `.spec.podFailurePolicy` 字段来定义，\n它能让你的集群根据容器的退出码和 Pod 状况来处理 Pod 失效事件。"}
{"en": "In some situations, you  may want to have a better control when handling Pod\nfailures than the control provided by the [Pod backoff failure policy](#pod-backoff-failure-policy),\nwhich is based on the Job's `.spec.backoffLimit`. These are some examples of use cases:", "zh": "在某些情况下，你可能希望更好地控制 Pod 失效的处理方式，\n而不是仅限于 [Pod 回退失效策略](#pod-backoff-failure-policy)所提供的控制能力，\n后者是基于 Job 的 `.spec.backoffLimit` 实现的。以下是一些使用场景："}
{"en": "* To optimize costs of running workloads by avoiding unnecessary Pod restarts,\n  you can terminate a Job as soon as one of its Pods fails with an exit code\n  indicating a software bug.\n* To guarantee that your Job finishes even if there are disruptions, you can\n  ignore Pod failures caused by disruptions (such as {{< glossary_tooltip text=\"preemption\" term_id=\"preemption\" >}},\n  {{< glossary_tooltip text=\"API-initiated eviction\" term_id=\"api-eviction\" >}}\n  or {{< glossary_tooltip text=\"taint\" term_id=\"taint\" >}}-based eviction) so\n  that they don't count towards the `.spec.backoffLimit` limit of retries.", "zh": "* 通过避免不必要的 Pod 重启来优化工作负载的运行成本，\n  你可以在某 Job 中一个 Pod 失效且其退出码表明存在软件错误时立即终止该 Job。\n* 为了保证即使有干扰也能完成 Job，你可以忽略由干扰导致的 Pod 失效\n  （例如{{< glossary_tooltip text=\"抢占\" term_id=\"preemption\" >}}、\n  {{< glossary_tooltip text=\"通过 API 发起的驱逐\" term_id=\"api-eviction\" >}}\n  或基于{{< glossary_tooltip text=\"污点\" term_id=\"taint\" >}}的驱逐），\n  这样这些失效就不会被计入 `.spec.backoffLimit` 的重试限制中。"}
{"en": "You can configure a Pod failure policy, in the `.spec.podFailurePolicy` field,\nto meet the above use cases. This policy can handle Pod failures based on the\ncontainer exit codes and the Pod conditions.", "zh": "你可以在 `.spec.podFailurePolicy` 字段中配置 Pod 失效策略，以满足上述使用场景。\n该策略可以根据容器退出码和 Pod 状况来处理 Pod 失效。"}
{"en": "Here is a manifest for a Job that defines a `podFailurePolicy`:", "zh": "下面是一个定义了 `podFailurePolicy` 的 Job 的清单：\n\n{{% code_sample file=\"/controllers/job-pod-failure-policy-example.yaml\" %}}"}
{"en": "In the example above, the first rule of the Pod failure policy specifies that\nthe Job should be marked failed if the `main` container fails with the 42 exit\ncode. The following are the rules for the `main` container specifically:", "zh": "在上面的示例中，Pod 失效策略的第一条规则规定如果 `main` 容器失败并且退出码为 42，\nJob 将被标记为失败。以下是 `main` 容器的具体规则："}
{"en": "- an exit code of 0 means that the container succeeded\n- an exit code of 42 means that the **entire Job** failed\n- any other exit code represents that the container failed, and hence the entire\n  Pod. The Pod will be re-created if the total number of restarts is\n  below `backoffLimit`. If the `backoffLimit` is reached the **entire Job** failed.", "zh": "- 退出码 0 代表容器成功\n- 退出码 42 代表**整个 Job** 失败\n- 所有其他退出码都代表容器失败，同时也代表着整个 Pod 失效。\n  如果重启总次数低于 `backoffLimit` 定义的次数，则会重新启动 Pod，\n  如果等于 `backoffLimit` 所设置的次数，则代表**整个 Job** 失效。\n\n{{< note >}}"}
{"en": "Because the Pod template specifies a `restartPolicy: Never`,\nthe kubelet does not restart the `main` container in that particular Pod.", "zh": "因为 Pod 模板中指定了 `restartPolicy: Never`，\n所以 kubelet 将不会重启 Pod 中的 `main` 容器。\n{{< /note >}}"}
{"en": "The second rule of the Pod failure policy, specifying the `Ignore` action for\nfailed Pods with condition `DisruptionTarget` excludes Pod disruptions from\nbeing counted towards the `.spec.backoffLimit` limit of retries.", "zh": "Pod 失效策略的第二条规则，\n指定对于状况为 `DisruptionTarget` 的失效 Pod 采取 `Ignore` 操作，\n统计 `.spec.backoffLimit` 重试次数限制时不考虑 Pod 因干扰而发生的异常。\n\n{{< note >}}"}
{"en": "If the Job failed, either by the Pod failure policy or Pod backoff\nfailure policy, and the Job is running multiple Pods, Kubernetes terminates all\nthe Pods in that Job that are still Pending or Running.", "zh": "如果根据 Pod 失效策略或 Pod 回退失效策略判定 Pod 已经失效，\n并且 Job 正在运行多个 Pod，Kubernetes 将终止该 Job 中仍处于 Pending 或 Running 的所有 Pod。\n{{< /note >}}"}
{"en": "These are some requirements and semantics of the API:\n\n- if you want to use a `.spec.podFailurePolicy` field for a Job, you must\n  also define that Job's pod template with `.spec.restartPolicy` set to `Never`.\n- the Pod failure policy rules you specify under `spec.podFailurePolicy.rules`\n  are evaluated in order. Once a rule matches a Pod failure, the remaining rules\n  are ignored. When no rule matches the Pod failure, the default\n  handling applies.\n- you may want to restrict a rule to a specific container by specifying its name\n  in`spec.podFailurePolicy.rules[*].onExitCodes.containerName`. When not specified the rule\n  applies to all containers. When specified, it should match one the container\n  or `initContainer` names in the Pod template.\n- you may specify the action taken when a Pod failure policy is matched by\n  `spec.podFailurePolicy.rules[*].action`. Possible values are:\n  - `FailJob`: use to indicate that the Pod's job should be marked as Failed and\n     all running Pods should be terminated.\n  - `Ignore`: use to indicate that the counter towards the `.spec.backoffLimit`\n     should not be incremented and a replacement Pod should be created.\n  - `Count`: use to indicate that the Pod should be handled in the default way.\n     The counter towards the `.spec.backoffLimit` should be incremented.\n  - `FailIndex`: use this action along with [backoff limit per index](#backoff-limit-per-index)\n     to avoid unnecessary retries within the index of a failed pod.", "zh": "下面是此 API 的一些要求和语义：\n- 如果你想在 Job 中使用 `.spec.podFailurePolicy` 字段，\n  你必须将 Job 的 Pod 模板中的 `.spec.restartPolicy` 设置为 `Never`。\n- 在 `spec.podFailurePolicy.rules` 中设定的 Pod 失效策略规则将按序评估。\n  一旦某个规则与 Pod 失效策略匹配，其余规则将被忽略。\n  当没有规则匹配 Pod 失效策略时，将会采用默认的处理方式。\n- 你可能希望在 `spec.podFailurePolicy.rules[*].onExitCodes.containerName`\n  中通过指定的名称限制只能针对特定容器应用对应的规则。\n  如果不设置此属性，规则将适用于所有容器。\n  如果指定了容器名称，它应该匹配 Pod 模板中的一个普通容器或一个初始容器（Init Container）。\n- 你可以在 `spec.podFailurePolicy.rules[*].action` 指定当 Pod 失效策略发生匹配时要采取的操作。\n  可能的值为：\n  - `FailJob`：表示 Pod 的任务应标记为 Failed，并且所有正在运行的 Pod 应被终止。\n  - `Ignore`：表示 `.spec.backoffLimit` 的计数器不应该增加，应该创建一个替换的 Pod。\n  - `Count`：表示 Pod 应该以默认方式处理。`.spec.backoffLimit` 的计数器应该增加。\n  - `FailIndex`：表示使用此操作以及[逐索引回退限制](#backoff-limit-per-index)来避免就失败的 Pod\n    的索引进行不必要的重试。\n\n{{< note >}}"}
{"en": "When you use a `podFailurePolicy`, the job controller only matches Pods in the\n`Failed` phase. Pods with a deletion timestamp that are not in a terminal phase\n(`Failed` or `Succeeded`) are considered still terminating. This implies that\nterminating pods retain a [tracking finalizer](#job-tracking-with-finalizers)\nuntil they reach a terminal phase.\nSince Kubernetes 1.27, Kubelet transitions deleted pods to a terminal phase\n(see: [Pod Phase](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase)). This\nensures that deleted pods have their finalizers removed by the Job controller.", "zh": "当你使用 `podFailurePolicy` 时，Job 控制器只匹配处于 `Failed` 阶段的 Pod。\n具有删除时间戳但不处于终止阶段（`Failed` 或 `Succeeded`）的 Pod 被视为仍在终止中。\n这意味着终止中的 Pod 会保留一个[跟踪 Finalizer](#job-tracking-with-finalizers)，\n直到到达终止阶段。\n从 Kubernetes 1.27 开始，kubelet 将删除的 Pod 转换到终止阶段\n（参阅 [Pod 阶段](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase)）。\n这确保已删除的 Pod 的 Finalizer 被 Job 控制器移除。\n{{< /note >}}\n\n{{< note >}}"}
{"en": "Starting with Kubernetes v1.28, when Pod failure policy is used, the Job controller recreates\nterminating Pods only once these Pods reach the terminal `Failed` phase. This behavior is similar\nto `podReplacementPolicy: Failed`. For more information, see [Pod replacement policy](#pod-replacement-policy).", "zh": "自 Kubernetes v1.28 开始，当使用 Pod 失效策略时，Job 控制器仅在这些 Pod 达到终止的\n`Failed` 阶段时才会重新创建终止中的 Pod。这种行为类似于 `podReplacementPolicy: Failed`。\n细节参阅 [Pod 替换策略](#pod-replacement-policy)。\n{{< /note >}}"}
{"en": "When you use the `podFailurePolicy`, and the Job fails due to the pod\nmatching the rule with the `FailJob` action, then the Job controller triggers\nthe Job termination process by adding the `FailureTarget` condition.\nFor more details, see [Job termination and cleanup](#job-termination-and-cleanup).", "zh": "当你使用了 `podFailurePolicy`，并且 Pod 因为与 `FailJob`\n操作的规则匹配而失败时，Job 控制器会通过添加\n`FailureTarget` 状况来触发 Job 终止流程。\n更多详情，请参阅 [Job 的终止和清理](#job-termination-and-cleanup)。"}
{"en": "## Success policy {#success-policy}", "zh": "## 成功策略   {#success-policy}\n\n{{< feature-state feature_gate_name=\"JobSuccessPolicy\" >}}\n\n{{< note >}}"}
{"en": "You can only configure a success policy for an Indexed Job if you have the\n`JobSuccessPolicy` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled in your cluster.", "zh": "只有你在集群中启用了 `JobSuccessPolicy`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)时，\n才可以为带索引的 Job 配置成功策略。\n{{< /note >}}"}
{"en": "When creating an Indexed Job, you can define when a Job can be declared as succeeded using a `.spec.successPolicy`,\nbased on the pods that succeeded.\n\nBy default, a Job succeeds when the number of succeeded Pods equals `.spec.completions`.\nThese are some situations where you might want additional control for declaring a Job succeeded:", "zh": "你在创建带索引的 Job 时，可以基于成功的 Pod 个数使用 `.spec.successPolicy` 来定义 Job 何时可以被声明为成功。\n\n默认情况下，当成功的 Pod 数等于 `.spec.completions` 时，则 Job 成功。\n在以下一些情况下，你可能需要对何时声明 Job 成功作额外的控制："}
{"en": "* When running simulations with different parameters, \n  you might not need all the simulations to succeed for the overall Job to be successful.\n* When following a leader-worker pattern, only the success of the leader determines the success or\n  failure of a Job. Examples of this are frameworks like MPI and PyTorch etc.", "zh": "* 在使用不同的参数运行模拟任务时，你可能不需要所有模拟都成功就可以认为整个 Job 是成功的。\n* 在遵循领导者与工作者模式时，只有领导者的成功才能决定 Job 成功或失败。\n  这类框架的例子包括 MPI 和 PyTorch 等。"}
{"en": "You can configure a success policy, in the `.spec.successPolicy` field,\nto meet the above use cases. This policy can handle Job success based on the\nsucceeded pods. After the Job meets the success policy, the job controller terminates the lingering Pods.\nA success policy is defined by rules. Each rule can take one of the following forms:", "zh": "你可以在 `.spec.successPolicy` 字段中配置成功策略，以满足上述使用场景。\n此策略可以基于 Pod 的成功状况处理 Job 的成功状态。当 Job 满足成功策略后，Job 控制器会终止剩余的 Pod。\n成功策略由规则进行定义。每条规则可以采用以下形式中的一种："}
{"en": "* When you specify the `succeededIndexes` only,\n  once all indexes specified in the `succeededIndexes` succeed, the job controller marks the Job as succeeded.\n  The `succeededIndexes` must be a list of intervals between 0 and `.spec.completions-1`.\n* When you specify the `succeededCount` only,\n  once the number of succeeded indexes reaches the `succeededCount`, the job controller marks the Job as succeeded.\n* When you specify both `succeededIndexes` and `succeededCount`,\n  once the number of succeeded indexes from the subset of indexes specified in the `succeededIndexes` reaches the `succeededCount`,\n  the job controller marks the Job as succeeded.", "zh": "* 当你仅指定 `succeededIndexes` 时，一旦 `succeededIndexes` 中指定的所有索引成功，Job 控制器就会将 Job 标记为成功。\n  `succeededIndexes` 必须是一个介于 0 和 `.spec.completions-1` 之间的间隔列表。\n* 当你仅指定 `succeededCount` 时，一旦成功的索引数量达到 `succeededCount`，Job 控制器就会将 Job 标记为成功。\n* 当你同时指定 `succeededIndexes` 和 `succeededCount` 时，一旦 `succeededIndexes`\n  中指定的索引子集中的成功索引数达到 `succeededCount`，Job 控制器就会将 Job 标记为成功。"}
{"en": "Note that when you specify multiple rules in the `.spec.successPolicy.rules`,\nthe job controller evaluates the rules in order. Once the Job meets a rule, the job controller ignores remaining rules.\n\nHere is a manifest for a Job with `successPolicy`:", "zh": "请注意，当你在 `.spec.successPolicy.rules` 中指定多个规则时，Job 控制器会按顺序评估这些规则。\n一旦 Job 符合某个规则，Job 控制器将忽略剩余的规则。\n\n以下是一个带有 `successPolicy` 的 Job 的清单：\n\n{{% code_sample file=\"/controllers/job-success-policy.yaml\" %}}"}
{"en": "In the example above, both `succeededIndexes` and `succeededCount` have been specified.\nTherefore, the job controller will mark the Job as succeeded and terminate the lingering Pods \nwhen either of the specified indexes, 0, 2, or 3, succeed.\nThe Job that meets the success policy gets the `SuccessCriteriaMet` condition with a `SuccessPolicy` reason.\nAfter the removal of the lingering Pods is issued, the Job gets the `Complete` condition.\n\nNote that the `succeededIndexes` is represented as intervals separated by a hyphen.\nThe number are listed in represented by the first and last element of the series, separated by a hyphen.", "zh": "在上面的例子中，`succeededIndexes` 和 `succeededCount` 都已被指定。\n因此，当指定的索引 0、2 或 3 中的任意一个成功时，Job 控制器将 Job 标记为成功并终止剩余的 Pod。\n符合成功策略的 Job 会被标记 `SuccessCriteriaMet` 状况，且状况的原因为 `SuccessPolicy`。\n在剩余的 Pod 被移除后，Job 会被标记 `Complete` 状况。\n\n请注意，`succeededIndexes` 表示为以连字符分隔的数字序列。\n所表达的数值为一个序列，连字符所连接的为列表中第一个元素和最后一个元素。\n\n{{< note >}}"}
{"en": "When you specify both a success policy and some terminating policies such as `.spec.backoffLimit` and `.spec.podFailurePolicy`,\nonce the Job meets either policy, the job controller respects the terminating policy and ignores the success policy.", "zh": "当你同时设置了成功策略和 `.spec.backoffLimit` 和 `.spec.podFailurePolicy` 这类终止策略时，\n一旦 Job 符合任一策略，Job 控制器将按终止策略处理，忽略成功策略。\n{{< /note >}}"}
{"en": "## Job termination and cleanup\n\nWhen a Job completes, no more Pods are created, but the Pods are [usually](#pod-backoff-failure-policy) not deleted either.\nKeeping them around allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.\nThe job object also remains after it is completed so that you can view its status. It is up to the user to delete\nold jobs after noting their status. Delete the job with `kubectl` (e.g. `kubectl delete jobs/pi` or `kubectl delete -f ./job.yaml`).\nWhen you delete the job using `kubectl`, all the pods it created are deleted too.", "zh": "## Job 终止与清理    {#job-termination-and-cleanup}\n\nJob 完成时不会再创建新的 Pod，不过已有的 Pod [通常](#pod-backoff-failure-policy)也不会被删除。\n保留这些 Pod 使得你可以查看已完成的 Pod 的日志输出，以便检查错误、警告或者其它诊断性输出。\nJob 完成时 Job 对象也一样被保留下来，这样你就可以查看它的状态。\n在查看了 Job 状态之后删除老的 Job 的操作留给了用户自己。\n你可以使用 `kubectl` 来删除 Job（例如，`kubectl delete jobs/pi`\n或者 `kubectl delete -f ./job.yaml`）。\n当使用 `kubectl` 来删除 Job 时，该 Job 所创建的 Pod 也会被删除。"}
{"en": "By default, a Job will run uninterrupted unless a Pod fails (`restartPolicy=Never`)\nor a Container exits in error (`restartPolicy=OnFailure`), at which point the Job defers to the\n`.spec.backoffLimit` described above. Once `.spec.backoffLimit` has been reached the Job will\nbe marked as failed and any running Pods will be terminated.\n\nAnother way to terminate a Job is by setting an active deadline.\nDo this by setting the `.spec.activeDeadlineSeconds` field of the Job to a number of seconds.\nThe `activeDeadlineSeconds` applies to the duration of the job, no matter how many Pods are created.\nOnce a Job reaches `activeDeadlineSeconds`, all of its running Pods are terminated and the Job status\nwill become `type: Failed` with `reason: DeadlineExceeded`.", "zh": "默认情况下，Job 会持续运行，除非某个 Pod 失败（`restartPolicy=Never`）\n或者某个容器出错退出（`restartPolicy=OnFailure`）。\n这时，Job 基于前述的 `spec.backoffLimit` 来决定是否以及如何重试。\n一旦重试次数到达 `.spec.backoffLimit` 所设的上限，Job 会被标记为失败，\n其中运行的 Pod 都会被终止。\n\n终止 Job 的另一种方式是设置一个活跃期限。\n你可以为 Job 的 `.spec.activeDeadlineSeconds` 设置一个秒数值。\n该值适用于 Job 的整个生命期，无论 Job 创建了多少个 Pod。\n一旦 Job 运行时间达到 `activeDeadlineSeconds` 秒，其所有运行中的 Pod 都会被终止，\n并且 Job 的状态更新为 `type: Failed` 及 `reason: DeadlineExceeded`。"}
{"en": "Note that a Job's `.spec.activeDeadlineSeconds` takes precedence over its `.spec.backoffLimit`.\nTherefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once\nit reaches the time limit specified by `activeDeadlineSeconds`, even if the `backoffLimit` is not yet reached.\n\nExample:", "zh": "注意 Job 的 `.spec.activeDeadlineSeconds` 优先级高于其 `.spec.backoffLimit` 设置。\n因此，如果一个 Job 正在重试一个或多个失效的 Pod，该 Job 一旦到达\n`activeDeadlineSeconds` 所设的时限即不再部署额外的 Pod，\n即使其重试次数还未达到 `backoffLimit` 所设的限制。\n\n例如：\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi-with-timeout\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 100\n  template:\n    spec:\n      containers:\n      - name: pi\n        image: perl:5.34.0\n        command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n```"}
{"en": "Note that both the Job spec and the [Pod template spec](/docs/concepts/workloads/pods/init-containers/#detailed-behavior)\nwithin the Job have an `activeDeadlineSeconds` field. Ensure that you set this field at the proper level.\n\nKeep in mind that the `restartPolicy` applies to the Pod, and not to the Job itself:\nthere is no automatic Job restart once the Job status is `type: Failed`.\nThat is, the Job termination mechanisms activated with `.spec.activeDeadlineSeconds`\nand `.spec.backoffLimit` result in a permanent Job failure that requires manual intervention to resolve.", "zh": "注意 Job 规约和 Job 中的\n[Pod 模板规约](/zh-cn/docs/concepts/workloads/pods/init-containers/#detailed-behavior)\n都有 `activeDeadlineSeconds` 字段。\n请确保你在合适的层次设置正确的字段。\n\n还要注意的是，`restartPolicy` 对应的是 Pod，而不是 Job 本身：\n一旦 Job 状态变为 `type: Failed`，就不会再发生 Job 重启的动作。\n换言之，由 `.spec.activeDeadlineSeconds` 和 `.spec.backoffLimit` 所触发的 Job\n终结机制都会导致 Job 永久性的失败，而这类状态都需要手工干预才能解决。"}
{"en": "### Terminal Job conditions\n\nA Job has two possible terminal states, each of which has a corresponding Job\ncondition:\n* Succeeded:  Job condition `Complete`\n* Failed: Job condition `Failed`", "zh": "### Job 终止状况   {#terminal-job-conditions}\n\n一个 Job 有两种可能的终止状况，每种状况都有相应的 Job 状况：\n\n* Succeeded：Job `Complete` 状况\n* Failed：Job `Failed` 状况"}
{"en": "Jobs fail for the following reasons:\n- The number of Pod failures exceeded the specified `.spec.backoffLimit` in the Job\n  specification. For details, see [Pod backoff failure policy](#pod-backoff-failure-policy).\n- The Job runtime exceeded the specified `.spec.activeDeadlineSeconds`\n- An indexed Job that used `.spec.backoffLimitPerIndex` has failed indexes.\n  For details, see [Backoff limit per index](#backoff-limit-per-index).\n- The number of failed indexes in the Job exceeded the specified\n  `spec.maxFailedIndexes`. For details, see [Backoff limit per index](#backoff-limit-per-index)\n- A failed Pod matches a rule in `.spec.podFailurePolicy` that has the `FailJob`\n   action. For details about how Pod failure policy rules might affect failure\n   evaluation, see [Pod failure policy](#pod-failure-policy).", "zh": "Job 失败的原因如下：\n\n- Pod 失败数量超出了 Job 规约中指定的 `.spec.backoffLimit`，\n  详情请参见 [Pod 回退失效策略](#pod-backoff-failure-policy)。\n- Job 运行时间超过了指定的 `.spec.activeDeadlineSeconds`。\n- 使用 `.spec.backoffLimitPerIndex` 的索引 Job 出现索引失败。\n  有关详细信息，请参阅[逐索引的回退限制](#backoff-limit-per-index)。\n- Job 中失败的索引数量超出了指定的 `spec.maxFailedIndexes` 值，\n  详情见[逐索引的回退限制](#backoff-limit-per-index)。\n- 失败的 Pod 匹配了 `.spec.podFailurePolicy` 中定义的一条规则，该规则的动作为 FailJob。\n  有关 Pod 失效策略规则如何影响故障评估的详细信息，请参阅 [Pod 失效策略](#pod-failure-policy)。"}
{"en": "Jobs succeed for the following reasons:\n- The number of succeeded Pods reached the specified `.spec.completions`\n- The criteria specified in `.spec.successPolicy` are met. For details, see\n  [Success policy](#success-policy).", "zh": "Pod 成功的原因如下：\n\n- 成功的 Pod 的数量达到了指定的 `.spec.completions` 数量。\n- `.spec.successPolicy` 中指定的标准已满足。详情请参见[成功策略](#success-policy)。"}
{"en": "In Kubernetes v1.31 and later the Job controller delays the addition of the\nterminal conditions,`Failed` or `Complete`, until all of the Job Pods are terminated.\n\nIn Kubernetes v1.30 and earlier, the Job controller added the `Complete` or the\n`Failed` Job terminal conditions as soon as the Job termination process was\ntriggered and all Pod finalizers were removed. However, some Pods would still\nbe running or terminating at the moment that the terminal condition was added.", "zh": "在 Kubernetes v1.31 及更高版本中，Job 控制器会延迟添加终止状况 `Failed` 或\n`Complete`，直到所有 Job Pod 都终止。\n\n在 Kubernetes v1.30 及更早版本中，一旦触发 Job 终止过程并删除所有\nPod 终结器，Job 控制器就会给 Job 添加 `Complete` 或 `Failed` 终止状况。\n然而，在添加终止状况时，一些 Pod 仍会运行或处于终止过程中。"}
{"en": "In Kubernetes v1.31 and later, the controller only adds the Job terminal conditions\n_after_ all of the Pods are terminated. You can enable this behavior by using the\n`JobManagedBy` or the `JobPodReplacementPolicy` (enabled by default)\n[feature gates](/docs/reference/command-line-tools-reference/feature-gates/).", "zh": "在 Kubernetes v1.31 及更高版本中，控制器仅在所有 Pod 终止后添加 Job 终止状况。\n你可以使用 `JobManagedBy` 或 `JobPodReplacementPolicy`（默认启用）\n启用此行为的[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)。"}
{"en": "### Termination of Job pods\n\nThe Job controller adds the `FailureTarget` condition or the `SuccessCriteriaMet`\ncondition to the Job to trigger Pod termination after a Job meets either the\nsuccess or failure criteria.", "zh": "### Job Pod 的终止\n\nJob 控制器将 `FailureTarget` 状况或 `SuccessCriteriaMet` 状况添加到\nJob，以便在 Job 满足成功或失败标准后触发 Pod 终止。"}
{"en": "Factors like `terminationGracePeriodSeconds` might increase the amount of time\nfrom the moment that the Job controller adds the `FailureTarget` condition or the\n`SuccessCriteriaMet` condition to the moment that all of the Job Pods terminate\nand the Job controller adds a [terminal condition](#terminal-job-conditions)\n(`Failed` or `Complete`).\n\nYou can use the `FailureTarget` or the `SuccessCriteriaMet` condition to evaluate\nwhether the Job has failed or succeeded without having to wait for the controller\nto add a terminal condition.", "zh": "诸如 `terminationGracePeriodSeconds` 之类的因素可能会增加从\nJob 控制器添加 `FailureTarget` 状况或 `SuccessCriteriaMet` 状况到所有\nJob Pod 终止并且 Job 控制器添加[终止状况](#terminal-job-conditions)（`Failed` 或 `Complete`）的这段时间量。\n\n你可以使用 `FailureTarget` 或 `SuccessCriteriaMet`\n状况来评估 Job 是否失败或成功，而无需等待控制器添加终止状况。"}
{"en": "For example, you might want to decide when to create a replacement Job\nthat replaces a failed Job. If you replace the failed Job when the `FailureTarget`\ncondition appears, your replacement Job runs sooner, but could result in Pods\nfrom the failed and the replacement Job running at the same time, using\nextra compute resources.\n\nAlternatively, if your cluster has limited resource capacity, you could choose to\nwait until the `Failed` condition appears on the Job, which would delay your\nreplacement Job but would ensure that you conserve resources by waiting\nuntil all of the failed Pods are removed.", "zh": "例如，你可能想要决定何时创建 Job 来替代某个已失败 Job。\n如果在出现 `FailureTarget` 状况时替换失败的 Job，则替换 Job 启动得会更早，\n但可能会导致失败的 Job 和替换 Job 的 Pod 同时处于运行状态，进而额外耗用计算资源。\n\n或者，如果你的集群资源容量有限，你可以选择等到 Job 上出现 `Failed` 状况后再执行替换操作。\n这样做会延迟替换 Job 的启动，不过通过等待所有失败的 Pod 都被删除，可以节省资源。"}
{"en": "## Clean up finished jobs automatically\n\nFinished Jobs are usually no longer needed in the system. Keeping them around in\nthe system will put pressure on the API server. If the Jobs are managed directly\nby a higher level controller, such as\n[CronJobs](/docs/concepts/workloads/controllers/cron-jobs/), the Jobs can be\ncleaned up by CronJobs based on the specified capacity-based cleanup policy.\n\n### TTL mechanism for finished Jobs", "zh": "## 自动清理完成的 Job   {#clean-up-finished-jobs-automatically}\n\n完成的 Job 通常不需要留存在系统中。在系统中一直保留它们会给 API 服务器带来额外的压力。\n如果 Job 由某种更高级别的控制器来管理，例如\n[CronJob](/zh-cn/docs/concepts/workloads/controllers/cron-jobs/)，\n则 Job 可以被 CronJob 基于特定的根据容量裁定的清理策略清理掉。\n\n### 已完成 Job 的 TTL 机制  {#ttl-mechanisms-for-finished-jobs}\n\n{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}"}
{"en": "Another way to clean up finished Jobs (either `Complete` or `Failed`)\nautomatically is to use a TTL mechanism provided by a\n[TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) for\nfinished resources, by specifying the `.spec.ttlSecondsAfterFinished` field of\nthe Job.\n\nWhen the TTL controller cleans up the Job, it will delete the Job cascadingly,\ni.e. delete its dependent objects, such as Pods, together with the Job. Note\nthat when the Job is deleted, its lifecycle guarantees, such as finalizers, will\nbe honored.\n\nFor example:", "zh": "自动清理已完成 Job （状态为 `Complete` 或 `Failed`）的另一种方式是使用由\n[TTL 控制器](/zh-cn/docs/concepts/workloads/controllers/ttlafterfinished/)所提供的 TTL 机制。\n通过设置 Job 的 `.spec.ttlSecondsAfterFinished` 字段，可以让该控制器清理掉已结束的资源。\n\nTTL 控制器清理 Job 时，会级联式地删除 Job 对象。\n换言之，它会删除所有依赖的对象，包括 Pod 及 Job 本身。\n注意，当 Job 被删除时，系统会考虑其生命周期保障，例如其 Finalizers。\n\n例如：\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pi-with-ttl\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: pi\n        image: perl:5.34.0\n        command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      restartPolicy: Never\n```"}
{"en": "The Job `pi-with-ttl` will be eligible to be automatically deleted, `100`\nseconds after it finishes.\n\nIf the field is set to `0`, the Job will be eligible to be automatically deleted\nimmediately after it finishes. If the field is unset, this Job won't be cleaned\nup by the TTL controller after it finishes.", "zh": "Job `pi-with-ttl` 在结束 100 秒之后，可以成为被自动删除的对象。\n\n如果该字段设置为 `0`，Job 在结束之后立即成为可被自动删除的对象。\n如果该字段没有设置，Job 不会在结束之后被 TTL 控制器自动清除。\n\n{{< note >}}"}
{"en": "It is recommended to set `ttlSecondsAfterFinished` field because unmanaged jobs\n(Jobs that you created directly, and not indirectly through other workload APIs\nsuch as CronJob) have a default deletion\npolicy of `orphanDependents` causing Pods created by an unmanaged Job to be left around\nafter that Job is fully deleted.\nEven though the {{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} eventually\n[garbage collects](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)\nthe Pods from a deleted Job after they either fail or complete, sometimes those\nlingering pods may cause cluster performance degradation or in worst case cause the\ncluster to go offline due to this degradation.", "zh": "建议设置 `ttlSecondsAfterFinished` 字段，因为非托管任务\n（是你直接创建的 Job，而不是通过其他工作负载 API（如 CronJob）间接创建的 Job）\n的默认删除策略是 `orphanDependents`，这会导致非托管 Job 创建的 Pod 在该 Job 被完全删除后被保留。\n即使{{< glossary_tooltip text=\"控制面\" term_id=\"control-plane\" >}}最终在 Pod 失效或完成后\n对已删除 Job 中的这些 Pod 执行[垃圾收集](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)操作，\n这些残留的 Pod 有时可能会导致集群性能下降，或者在最坏的情况下会导致集群因这种性能下降而离线。"}
{"en": "You can use [LimitRanges](/docs/concepts/policy/limit-range/) and\n[ResourceQuotas](/docs/concepts/policy/resource-quotas/) to place a\ncap on the amount of resources that a particular namespace can\nconsume.", "zh": "你可以使用 [LimitRange](/zh-cn/docs/concepts/policy/limit-range/) 和\n[ResourceQuota](/zh-cn/docs/concepts/policy/resource-quotas/)，\n设定一个特定名字空间可以消耗的资源上限。\n{{< /note >}}"}
{"en": "## Job patterns\n\nThe Job object can be used to process a set of independent but related *work items*.\nThese might be emails to be sent, frames to be rendered, files to be transcoded,\nranges of keys in a NoSQL database to scan, and so on.", "zh": "## Job 模式  {#job-patterns}\n\nJob 对象可以用来处理一组相互独立而又彼此关联的“工作条目”。\n这类工作条目可能是要发送的电子邮件、要渲染的视频帧、要编解码的文件、NoSQL\n数据库中要扫描的主键范围等等。"}
{"en": "In a complex system, there may be multiple different sets of work items. Here we are just\nconsidering one set of work items that the user wants to manage together &mdash; a *batch job*.\n\nThere are several different patterns for parallel computation, each with strengths and weaknesses.\nThe tradeoffs are:", "zh": "在一个复杂系统中，可能存在多个不同的工作条目集合。\n这里我们仅考虑用户希望一起管理的工作条目集合之一：**批处理作业**。\n\n并行计算的模式有好多种，每种都有自己的强项和弱点。这里要权衡的因素有："}
{"en": "- One Job object for each work item, vs. a single Job object for all work items.\n  One Job per work item creates some overhead for the user and for the system to manage\n  large numbers of Job objects.\n  A single Job for all work items is better for large numbers of items. \n- Number of Pods created equals number of work items, versus each Pod can process multiple work items.\n  When the number of Pods equals the number of work items, the Pods typically\n  requires less modification to existing code and containers. Having each Pod\n  process multiple work items is better for large numbers of items.", "zh": "- 每个工作条目对应一个 Job 或者所有工作条目对应同一 Job 对象。\n  为每个工作条目创建一个 Job 的做法会给用户带来一些额外的负担，系统需要管理大量的 Job 对象。\n  用一个 Job 对象来完成所有工作条目的做法更适合处理大量工作条目的场景。\n- 创建数目与工作条目相等的 Pod 或者令每个 Pod 可以处理多个工作条目。\n  当 Pod 个数与工作条目数目相等时，通常不需要在 Pod 中对现有代码和容器做较大改动；\n  让每个 Pod 能够处理多个工作条目的做法更适合于工作条目数量较大的场合。"}
{"en": "- Several approaches use a work queue. This requires running a queue service,\n  and modifications to the existing program or container to make it use the work queue.\n  Other approaches are easier to adapt to an existing containerised application.\n- When the Job is associated with a\n  [headless Service](/docs/concepts/services-networking/service/#headless-services),\n  you can enable the Pods within a Job to communicate with each other to\n  collaborate in a computation.", "zh": "- 有几种技术都会用到工作队列。这意味着需要运行一个队列服务，\n  并修改现有程序或容器使之能够利用该工作队列。\n  与之比较，其他方案在修改现有容器化应用以适应需求方面可能更容易一些。\n- 当 Job 与某个[无头 Service](/zh-cn/docs/concepts/services-networking/service/#headless-services)\n  之间存在关联时，你可以让 Job 中的 Pod 之间能够相互通信，从而协作完成计算。"}
{"en": "The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.\nThe pattern names are also links to examples and more detailed description.\n\n|                  Pattern                        | Single Job object | Fewer pods than work items? | Use app unmodified? |\n| ----------------------------------------------- |:-----------------:|:---------------------------:|:-------------------:|\n| [Queue with Pod Per Work Item]                  |         ✓         |                             |      sometimes      |\n| [Queue with Variable Pod Count]                 |         ✓         |             ✓               |                     |\n| [Indexed Job with Static Work Assignment]       |         ✓         |                             |          ✓          |\n| [Job with Pod-to-Pod Communication]             |         ✓         |         sometimes           |      sometimes      |\n| [Job Template Expansion]                        |                   |                             |          ✓          |", "zh": "下面是对这些权衡的汇总，第 2 到 4 列对应上面的权衡比较。\n模式的名称对应了相关示例和更详细描述的链接。\n\n| 模式  | 单个 Job 对象 | Pod 数少于工作条目数？ | 直接使用应用无需修改? |\n| ----- |:-------------:|:-----------------------:|:---------------------:|\n| [每工作条目一 Pod 的队列](/zh-cn/docs/tasks/job/coarse-parallel-processing-work-queue/) | ✓ | | 有时 |\n| [Pod 数量可变的队列](/zh-cn/docs/tasks/job/fine-parallel-processing-work-queue/) | ✓ | ✓ |  |\n| [静态任务分派的带索引的 Job](/zh-cn/docs/tasks/job/indexed-parallel-processing-static) | ✓ |  | ✓ |\n| [带 Pod 间通信的 Job](/zh-cn/docs/tasks/job/job-with-pod-to-pod-communication/)  | ✓ | 有时 | 有时 |\n| [Job 模板扩展](/zh-cn/docs/tasks/job/parallel-processing-expansion/)  |  |  | ✓ |"}
{"en": "When you specify completions with `.spec.completions`, each Pod created by the Job controller\nhas an identical [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\nThis means that all pods for a task will have the same command line and the same\nimage, the same volumes, and (almost) the same environment variables. These patterns\nare different ways to arrange for pods to work on different things.\n\nThis table shows the required settings for `.spec.parallelism` and `.spec.completions` for each of the patterns.\nHere, `W` is the number of work items.", "zh": "当你使用 `.spec.completions` 来设置完成数时，Job 控制器所创建的每个 Pod\n使用完全相同的 [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)。\n这意味着任务的所有 Pod 都有相同的命令行，都使用相同的镜像和数据卷，\n甚至连环境变量都（几乎）相同。\n这些模式是让每个 Pod 执行不同工作的几种不同形式。\n\n下表显示的是每种模式下 `.spec.parallelism` 和 `.spec.completions` 所需要的设置。\n其中，`W` 表示的是工作条目的个数。"}
{"en": "|             Pattern                             | `.spec.completions` |  `.spec.parallelism` |\n| ----------------------------------------------- |:-------------------:|:--------------------:|\n| [Queue with Pod Per Work Item]                  |          W          |        any           |\n| [Queue with Variable Pod Count]                 |         null        |        any           |\n| [Indexed Job with Static Work Assignment]       |          W          |        any           |\n| [Job with Pod-to-Pod Communication]             |          W          |         W            |\n| [Job Template Expansion]                        |          1          |     should be 1      |", "zh": "| 模式  | `.spec.completions` |  `.spec.parallelism` |\n| ----- |:-------------------:|:--------------------:|\n| [每工作条目一 Pod 的队列](/zh-cn/docs/tasks/job/coarse-parallel-processing-work-queue/) | W | 任意值 |\n| [Pod 数量可变的队列](/zh-cn/docs/tasks/job/fine-parallel-processing-work-queue/) | 1 | 任意值 |\n| [静态任务分派的带索引的 Job](/zh-cn/docs/tasks/job/indexed-parallel-processing-static) | W |  | 任意值 |\n| [带 Pod 间通信的 Job](/zh-cn/docs/tasks/job/job-with-pod-to-pod-communication/) | W | W |\n| [Job 模板扩展](/zh-cn/docs/tasks/job/parallel-processing-expansion/) | 1 | 应该为 1 |"}
{"en": "## Advanced usage\n\n### Suspending a Job", "zh": "## 高级用法   {#advanced-usage}\n\n### 挂起 Job   {#suspending-a-job}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "When a Job is created, the Job controller will immediately begin creating Pods\nto satisfy the Job's requirements and will continue to do so until the Job is\ncomplete. However, you may want to temporarily suspend a Job's execution and\nresume it later, or start Jobs in suspended state and have a custom controller\ndecide later when to start them.", "zh": "Job 被创建时，Job 控制器会马上开始执行 Pod 创建操作以满足 Job 的需求，\n并持续执行此操作直到 Job 完成为止。\n不过你可能想要暂时挂起 Job 执行，或启动处于挂起状态的 Job，\n并拥有一个自定义控制器以后再决定什么时候开始。"}
{"en": "To suspend a Job, you can update the `.spec.suspend` field of\nthe Job to true; later, when you want to resume it again, update it to false.\nCreating a Job with `.spec.suspend` set to true will create it in the suspended\nstate.", "zh": "要挂起一个 Job，你可以更新 `.spec.suspend` 字段为 true，\n之后，当你希望恢复其执行时，将其更新为 false。\n创建一个 `.spec.suspend` 被设置为 true 的 Job 本质上会将其创建为被挂起状态。"}
{"en": "When a Job is resumed from suspension, its `.status.startTime` field will be\nreset to the current time. This means that the `.spec.activeDeadlineSeconds`\ntimer will be stopped and reset when a Job is suspended and resumed.", "zh": "当 Job 被从挂起状态恢复执行时，其 `.status.startTime` 字段会被重置为当前的时间。\n这意味着 `.spec.activeDeadlineSeconds` 计时器会在 Job 挂起时被停止，\n并在 Job 恢复执行时复位。"}
{"en": "When you suspend a Job, any running Pods that don't have a status of `Completed`\nwill be [terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)\nwith a SIGTERM signal. The Pod's graceful termination period will be honored and\nyour Pod must handle this signal in this period. This may involve saving\nprogress for later or undoing changes. Pods terminated this way will not count\ntowards the Job's `completions` count.", "zh": "当你挂起一个 Job 时，所有正在运行且状态不是 `Completed` 的 Pod\n将被[终止](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)并附带\nSIGTERM 信号。Pod 的体面终止期限会被考虑，不过 Pod 自身也必须在此期限之内处理完信号。\n处理逻辑可能包括保存进度以便将来恢复，或者取消已经做出的变更等等。\nPod 以这种形式终止时，不会被记入 Job 的 `completions` 计数。"}
{"en": "An example Job definition in the suspended state can be like so:", "zh": "处于被挂起状态的 Job 的定义示例可能是这样子：\n\n```shell\nkubectl get job myjob -o yaml\n```\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: myjob\nspec:\n  suspend: true\n  parallelism: 1\n  completions: 5\n  template:\n    spec:\n      ...\n```"}
{"en": "You can also toggle Job suspension by patching the Job using the command line.\n\nSuspend an active Job:", "zh": "你也可以使用命令行为 Job 打补丁来切换 Job 的挂起状态。\n\n挂起一个活跃的 Job：\n\n```shell\nkubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":true}}'\n```"}
{"en": "Resume a suspended Job:", "zh": "恢复一个挂起的 Job：\n\n```shell\nkubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":false}}'\n```"}
{"en": "The Job's status can be used to determine if a Job is suspended or has been\nsuspended in the past:", "zh": "Job 的 `status` 可以用来确定 Job 是否被挂起，或者曾经被挂起。\n\n```shell\nkubectl get jobs/myjob -o yaml\n```"}
{"en": "# .metadata and .spec omitted", "zh": "```yaml\napiVersion: batch/v1\nkind: Job\n# .metadata 和 .spec 已省略\nstatus:\n  conditions:\n  - lastProbeTime: \"2021-02-05T13:14:33Z\"\n    lastTransitionTime: \"2021-02-05T13:14:33Z\"\n    status: \"True\"\n    type: Suspended\n  startTime: \"2021-02-05T13:13:48Z\"\n```"}
{"en": "The Job condition of type \"Suspended\" with status \"True\" means the Job is\nsuspended; the `lastTransitionTime` field can be used to determine how long the\nJob has been suspended for. If the status of that condition is \"False\", then the\nJob was previously suspended and is now running. If such a condition does not\nexist in the Job's status, the Job has never been stopped.\n\nEvents are also created when the Job is suspended and resumed:", "zh": "Job 的 \"Suspended\" 类型的状况在状态值为 \"True\" 时意味着 Job 正被挂起；\n`lastTransitionTime` 字段可被用来确定 Job 被挂起的时长。\n如果此状况字段的取值为 \"False\"，则 Job 之前被挂起且现在在运行。\n如果 \"Suspended\" 状况在 `status` 字段中不存在，则意味着 Job 从未被停止执行。\n\n当 Job 被挂起和恢复执行时，也会生成事件：\n\n```shell\nkubectl describe jobs/myjob\n```\n\n```\nName:           myjob\n...\nEvents:\n  Type    Reason            Age   From            Message\n  ----    ------            ----  ----            -------\n  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl\n  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl\n  Normal  Suspended         11m   job-controller  Job suspended\n  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44\n  Normal  Resumed           3s    job-controller  Job resumed\n```"}
{"en": "The last four events, particularly the \"Suspended\" and \"Resumed\" events, are\ndirectly a result of toggling the `.spec.suspend` field. In the time between\nthese two events, we see that no Pods were created, but Pod creation restarted\nas soon as the Job was resumed.", "zh": "最后四个事件，特别是 \"Suspended\" 和 \"Resumed\" 事件，都是因为 `.spec.suspend`\n字段值被改来改去造成的。在这两个事件之间，我们看到没有 Pod 被创建，不过当\nJob 被恢复执行时，Pod 创建操作立即被重启执行。"}
{"en": "### Mutable Scheduling Directives", "zh": "### 可变调度指令 {#mutable-scheduling-directives}\n\n{{< feature-state for_k8s_version=\"v1.27\" state=\"stable\" >}}"}
{"en": "In most cases, a parallel job will want the pods to run with constraints,\nlike all in the same zone, or all either on GPU model x or y but not a mix of both.", "zh": "在大多数情况下，并行作业会希望 Pod 在一定约束条件下运行，\n比如所有的 Pod 都在同一个区域，或者所有的 Pod 都在 GPU 型号 x 或 y 上，而不是两者的混合。"}
{"en": "The [suspend](#suspending-a-job) field is the first step towards achieving those semantics. Suspend allows a\ncustom queue controller to decide when a job should start; However, once a job is unsuspended,\na custom queue controller has no influence on where the pods of a job will actually land.", "zh": "[suspend](#suspending-a-job) 字段是实现这些语义的第一步。\nsuspend 允许自定义队列控制器，以决定工作何时开始；然而，一旦工作被取消暂停，\n自定义队列控制器对 Job 中 Pod 的实际放置位置没有影响。"}
{"en": "This feature allows updating a Job's scheduling directives before it starts, which gives custom queue\ncontrollers the ability to influence pod placement while at the same time offloading actual\npod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have never\nbeen unsuspended before.", "zh": "此特性允许在 Job 开始之前更新调度指令，从而为定制队列提供影响 Pod\n放置的能力，同时将 Pod 与节点间的分配关系留给 kube-scheduler 决定。\n这一特性仅适用于之前从未被暂停过的、已暂停的 Job。\n控制器能够影响 Pod 放置，同时参考实际 pod-to-node 分配给 kube-scheduler。\n这仅适用于从未暂停的 Job。"}
{"en": "The fields in a Job's pod template that can be updated are node affinity, node selector,\ntolerations, labels, annotations and [scheduling gates](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/).", "zh": "Job 的 Pod 模板中可以更新的字段是节点亲和性、节点选择器、容忍、标签、注解和\n[调度门控](/zh-cn/docs/concepts/scheduling-eviction/pod-scheduling-readiness/)。"}
{"en": "### Specifying your own Pod selector\n\nNormally, when you create a Job object, you do not specify `.spec.selector`.\nThe system defaulting logic adds this field when the Job is created.\nIt picks a selector value that will not overlap with any other jobs.\n\nHowever, in some cases, you might need to override this automatically set selector.\nTo do this, you can specify the `.spec.selector` of the Job.", "zh": "### 指定你自己的 Pod 选择算符 {#specifying-your-own-pod-selector}\n\n通常，当你创建一个 Job 对象时，你不会设置 `.spec.selector`。\n系统的默认值填充逻辑会在创建 Job 时添加此字段。\n它会选择一个不会与任何其他 Job 重叠的选择算符设置。\n\n不过，有些场合下，你可能需要重载这个自动设置的选择算符。\n为了实现这点，你可以手动设置 Job 的 `spec.selector` 字段。"}
{"en": "Be very careful when doing this. If you specify a label selector which is not\nunique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated\njob may be deleted, or this Job may count other Pods as completing it, or one or both\nJobs may refuse to create Pods or run to completion. If a non-unique selector is\nchosen, then other controllers (e.g. ReplicationController) and their Pods may behave\nin unpredictable ways too. Kubernetes will not stop you from making a mistake when\nspecifying `.spec.selector`.", "zh": "做这个操作时请务必小心。\n如果你所设定的标签选择算符并不唯一针对 Job 对应的 Pod 集合，\n甚或该算符还能匹配其他无关的 Pod，这些无关的 Job 的 Pod 可能会被删除。\n或者当前 Job 会将另外一些 Pod 当作是完成自身工作的 Pod，\n又或者两个 Job 之一或者二者同时都拒绝创建 Pod，无法运行至完成状态。\n如果所设置的算符不具有唯一性，其他控制器（如 RC 副本控制器）及其所管理的 Pod\n集合可能会变得行为不可预测。\nKubernetes 不会在你设置 `.spec.selector` 时尝试阻止你犯这类错误。"}
{"en": "Here is an example of a case when you might want to use this feature.\n\nSay Job `old` is already running. You want existing Pods\nto keep running, but you want the rest of the Pods it creates\nto use a different pod template and for the Job to have a new name.\nYou cannot update the Job because these fields are not updatable.\nTherefore, you delete Job `old` but _leave its pods\nrunning_, using `kubectl delete jobs/old --cascade=orphan`.\nBefore deleting it, you make a note of what selector it uses:", "zh": "下面是一个示例场景，在这种场景下你可能会使用刚刚讲述的特性。\n\n假定名为 `old` 的 Job 已经处于运行状态。\n你希望已有的 Pod 继续运行，但你希望 Job 接下来要创建的其他 Pod\n使用一个不同的 Pod 模板，甚至希望 Job 的名字也发生变化。\n你无法更新现有的 Job，因为这些字段都是不可更新的。\n因此，你会删除 `old` Job，但**允许该 Job 的 Pod 集合继续运行**。\n这是通过 `kubectl delete jobs/old --cascade=orphan` 实现的。\n在删除之前，我们先记下该 Job 所使用的选择算符。\n\n```shell\nkubectl get job old -o yaml\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```yaml\nkind: Job\nmetadata:\n  name: old\n  ...\nspec:\n  selector:\n    matchLabels:\n      batch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002\n  ...\n```"}
{"en": "Then you create a new Job with name `new` and you explicitly specify the same selector.\nSince the existing Pods have label `batch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`,\nthey are controlled by Job `new` as well.\n\nYou need to specify `manualSelector: true` in the new Job since you are not using\nthe selector that the system normally generates for you automatically.", "zh": "接下来你会创建名为 `new` 的新 Job，并显式地为其设置相同的选择算符。\n由于现有 Pod 都具有标签\n`batch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`，\n它们也会被名为 `new` 的 Job 所控制。\n\n你需要在新 Job 中设置 `manualSelector: true`，\n因为你并未使用系统通常自动为你生成的选择算符。\n\n```yaml\nkind: Job\nmetadata:\n  name: new\n  ...\nspec:\n  manualSelector: true\n  selector:\n    matchLabels:\n      batch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002\n  ...\n```"}
{"en": "The new Job itself will have a different uid from `a8f3d00d-c6d2-11e5-9f87-42010af00002`. Setting\n`manualSelector: true` tells the system that you know what you are doing and to allow this\nmismatch.", "zh": "新的 Job 自身会有一个不同于 `a8f3d00d-c6d2-11e5-9f87-42010af00002` 的唯一 ID。\n设置 `manualSelector: true`\n是在告诉系统你知道自己在干什么并要求系统允许这种不匹配的存在。"}
{"en": "### Job tracking with finalizers", "zh": "### 使用 Finalizer 追踪 Job   {#job-tracking-with-finalizers}\n\n{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}"}
{"en": "The control plane keeps track of the Pods that belong to any Job and notices if\nany such Pod is removed from the API server. To do that, the Job controller\ncreates Pods with the finalizer `batch.kubernetes.io/job-tracking`. The\ncontroller removes the finalizer only after the Pod has been accounted for in\nthe Job status, allowing the Pod to be removed by other controllers or users.", "zh": "控制面会跟踪属于任何 Job 的 Pod，并通知是否有任何这样的 Pod 被从 API 服务器中移除。\n为了实现这一点，Job 控制器创建的 Pod 带有 Finalizer `batch.kubernetes.io/job-tracking`。\n控制器只有在 Pod 被记入 Job 状态后才会移除 Finalizer，允许 Pod 可以被其他控制器或用户移除。\n\n{{< note >}}"}
{"en": "See [My pod stays terminating](/docs/tasks/debug/debug-application/debug-pods/) if you\nobserve that pods from a Job are stuck with the tracking finalizer.", "zh": "如果你发现来自 Job 的某些 Pod 因存在负责跟踪的 Finalizer 而无法正常终止，\n请参阅[我的 Pod 一直处于终止状态](/zh-cn/docs/tasks/debug/debug-application/debug-pods/)。\n{{< /note >}}"}
{"en": "### Elastic Indexed Jobs", "zh": "### 弹性索引 Job  {#elastic-indexed-jobs}\n\n{{< feature-state feature_gate_name=\"ElasticIndexedJob\" >}}"}
{"en": "You can scale Indexed Jobs up or down by mutating both `.spec.parallelism` \nand `.spec.completions` together such that `.spec.parallelism == .spec.completions`. \nWhen scaling down, Kubernetes removes the Pods with higher indexes.\n\nUse cases for elastic Indexed Jobs include batch workloads which require \nscaling an indexed Job, such as MPI, Horovod, Ray, and PyTorch training jobs.", "zh": "你可以通过同时改变 `.spec.parallelism` 和 `.spec.completions` 来扩大或缩小带索引 Job，\n从而满足 `.spec.parallelism == .spec.completions`。\n缩减规模时，Kubernetes 会删除具有更高索引的 Pod。\n\n弹性索引 Job 的使用场景包括需要扩展索引 Job 的批处理工作负载，例如 MPI、Horovod、Ray\n和 PyTorch 训练作业。"}
{"en": "### Delayed creation of replacement pods {#pod-replacement-policy}", "zh": "### 延迟创建替换 Pod   {#pod-replacement-policy}\n\n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}\n\n{{< note >}}"}
{"en": "You can only set `podReplacementPolicy` on Jobs if you enable the `JobPodReplacementPolicy`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n(enabled by default).", "zh": "你只有在启用了 `JobPodReplacementPolicy`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)后（默认启用），\n才能为 Job 设置 `podReplacementPolicy`。\n{{< /note >}}"}
{"en": "By default, the Job controller recreates Pods as soon they either fail or are terminating (have a deletion timestamp).\nThis means that, at a given time, when some of the Pods are terminating, the number of running Pods for a Job\ncan be greater than `parallelism` or greater than one Pod per index (if you are using an Indexed Job).", "zh": "默认情况下，当 Pod 失败或正在终止（具有删除时间戳）时，Job 控制器会立即重新创建 Pod。\n这意味着，在某个时间点上，当一些 Pod 正在终止时，为 Job 正运行中的 Pod 数量可以大于 `parallelism`\n或超出每个索引一个 Pod（如果使用 Indexed Job）。"}
{"en": "You may choose to create replacement Pods only when the terminating Pod is fully terminal (has `status.phase: Failed`).\nTo do this, set the `.spec.podReplacementPolicy: Failed`.\nThe default replacement policy depends on whether the Job has a `podFailurePolicy` set.\nWith no Pod failure policy defined for a Job, omitting the `podReplacementPolicy` field selects the\n`TerminatingOrFailed` replacement policy:\nthe control plane creates replacement Pods immediately upon Pod deletion\n(as soon as the control plane sees that a Pod for this Job has `deletionTimestamp` set).\nFor Jobs with a Pod failure policy set, the default  `podReplacementPolicy` is `Failed`, and no other\nvalue is permitted.\nSee [Pod failure policy](#pod-failure-policy) to learn more about Pod failure policies for Jobs.", "zh": "你可以选择仅在终止过程中的 Pod 完全终止（具有 `status.phase: Failed`）时才创建替换 Pod。\n为此，可以设置 `.spec.podReplacementPolicy: Failed`。\n默认的替换策略取决于 Job 是否设置了 `podFailurePolicy`。对于没有定义 Pod 失效策略的 Job，\n省略 `podReplacementPolicy` 字段相当于选择 `TerminatingOrFailed` 替换策略：\n控制平面在 Pod 删除时立即创建替换 Pod（只要控制平面发现该 Job 的某个 Pod 被设置了 `deletionTimestamp`）。\n对于设置了 Pod 失效策略的 Job，默认的 `podReplacementPolicy` 是 `Failed`，不允许其他值。\n请参阅 [Pod 失效策略](#pod-failure-policy)以了解更多关于 Job 的 Pod 失效策略的信息。\n\n```yaml\nkind: Job\nmetadata:\n  name: new\n  ...\nspec:\n  podReplacementPolicy: Failed\n  ...\n```"}
{"en": "Provided your cluster has the feature gate enabled, you can inspect the `.status.terminating` field of a Job.\nThe value of the field is the number of Pods owned by the Job that are currently terminating.", "zh": "如果你的集群启用了此特性门控，你可以检查 Job 的 `.status.terminating` 字段。\n该字段值是当前处于终止过程中的、由该 Job 拥有的 Pod 的数量。\n\n```shell\nkubectl get jobs/myjob -o yaml\n```"}
{"en": "# .metadata and .spec omitted\n# three Pods are terminating and have not yet reached the Failed phase", "zh": "```yaml\napiVersion: batch/v1\nkind: Job\n# .metadata 和 .spec 被省略\nstatus:\n  terminating: 3 # 三个 Pod 正在终止且还未达到 Failed 阶段\n```"}
{"en": "### Delegation of managing a Job object to external controller", "zh": "### 将管理 Job 对象的任务委托给外部控制器   {#delegation-of-managing-a-job-object-to-external-controller}\n\n{{< feature-state feature_gate_name=\"JobManagedBy\" >}}\n\n{{< note >}}"}
{"en": "You can only set the `managedBy` field on Jobs if you enable the `JobManagedBy`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n(disabled by default).", "zh": "你只有在启用了 `JobManagedBy`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)（默认禁用）时，\n才可以在 Job 上设置 `managedBy` 字段。\n{{< /note >}}"}
{"en": "This feature allows you to disable the built-in Job controller, for a specific\nJob, and delegate reconciliation of the Job to an external controller.\n\nYou indicate the controller that reconciles the Job by setting a custom value\nfor the `spec.managedBy` field - any value\nother than `kubernetes.io/job-controller`. The value of the field is immutable.", "zh": "此特性允许你为特定 Job 禁用内置的 Job 控制器，并将 Job 的协调任务委托给外部控制器。\n\n你可以通过为 `spec.managedBy` 字段设置一个自定义值来指示用来协调 Job 的控制器，\n这个自定义值可以是除了 `kubernetes.io/job-controller` 之外的任意值。此字段的值是不可变的。\n\n{{< note >}}"}
{"en": "When using this feature, make sure the controller indicated by the field is\ninstalled, otherwise the Job may not be reconciled at all.", "zh": "在使用此特性时，请确保此字段指示的控制器已被安装，否则 Job 可能根本不会被协调。\n{{< /note >}}\n\n{{< note >}}"}
{"en": "When developing an external Job controller be aware that your controller needs\nto operate in a fashion conformant with the definitions of the API spec and\nstatus fields of the Job object.\n\nPlease review these in detail in the [Job API](/docs/reference/kubernetes-api/workload-resources/job-v1/).\nWe also recommend that you run the e2e conformance tests for the Job object to\nverify your implementation.\n\nFinally, when developing an external Job controller make sure it does not use the\n`batch.kubernetes.io/job-tracking` finalizer, reserved for the built-in controller.", "zh": "在开发外部 Job 控制器时，请注意你的控制器需要以符合 Job 对象的 API 规范和状态字段定义的方式运行。\n\n有关细节请参阅 [Job API](/zh-cn/docs/reference/kubernetes-api/workload-resources/job-v1/)。\n我们也建议你运行 Job 对象的 e2e 合规性测试以检验你的实现。\n\n最后，在开发外部 Job 控制器时，请确保它不使用为内置控制器预留的\n`batch.kubernetes.io/job-tracking` Finalizer。\n{{< /note >}}\n\n{{< warning >}}"}
{"en": "If you are considering to disable the `JobManagedBy` feature gate, or to\ndowngrade the cluster to a version without the feature gate enabled, check if\nthere are jobs with a custom value of the `spec.managedBy` field. If there\nare such jobs, there is a risk that they might be reconciled by two controllers\nafter the operation: the built-in Job controller and the external controller\nindicated by the field value.", "zh": "如果你考虑禁用 `JobManagedBy` 特性门控，或者将集群降级到未启用此特性门控的版本，\n请检查是否有 Job 的 `spec.managedBy` 字段值带有一个自定义值。如果存在这样的 Job，就会有一个风险，\n即禁用或降级操作后这些 Job 可能会被两个控制器（内置的 Job 控制器和字段值指示的外部控制器）进行协调。\n{{< /warning >}}"}
{"en": "## Alternatives\n\n### Bare Pods\n\nWhen the node that a Pod is running on reboots or fails, the pod is terminated\nand will not be restarted. However, a Job will create new Pods to replace terminated ones.\nFor this reason, we recommend that you use a Job rather than a bare Pod, even if your application\nrequires only a single Pod.", "zh": "## 替代方案  {#alternatives}\n\n### 裸 Pod  {#bare-pods}\n\n当 Pod 运行所在的节点重启或者失败，Pod 会被终止并且不会被重启。\nJob 会重新创建新的 Pod 来替代已终止的 Pod。\n因为这个原因，我们建议你使用 Job 而不是独立的裸 Pod，\n即使你的应用仅需要一个 Pod。"}
{"en": "### Replication Controller\n\nJobs are complementary to [Replication Controllers](/docs/concepts/workloads/controllers/replicationcontroller/).\nA Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job\nmanages Pods that are expected to terminate (e.g. batch tasks).\n\nAs discussed in [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/), `Job` is *only* appropriate\nfor pods with `RestartPolicy` equal to `OnFailure` or `Never`.\n(Note: If `RestartPolicy` is not set, the default value is `Always`.)", "zh": "### 副本控制器    {#replication-controller}\n\nJob 与[副本控制器](/zh-cn/docs/concepts/workloads/controllers/replicationcontroller/)是彼此互补的。\n副本控制器管理的是那些不希望被终止的 Pod （例如，Web 服务器），\nJob 管理的是那些希望被终止的 Pod（例如，批处理作业）。\n\n正如在 [Pod 生命期](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/) 中讨论的，\n`Job` 仅适合于 `restartPolicy` 设置为 `OnFailure` 或 `Never` 的 Pod。\n注意：如果 `restartPolicy` 未设置，其默认值是 `Always`。"}
{"en": "### Single Job starts controller Pod\n\nAnother pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort\nof custom controller for those Pods. This allows the most flexibility, but may be somewhat\ncomplicated to get started with and offers less integration with Kubernetes.", "zh": "### 单个 Job 启动控制器 Pod    {#single-job-starts-controller-pod}\n\n另一种模式是用唯一的 Job 来创建 Pod，而该 Pod 负责启动其他 Pod，\n因此扮演了一种后启动 Pod 的控制器的角色。\n这种模式的灵活性更高，但是有时候可能会把事情搞得很复杂，很难入门，\n并且与 Kubernetes 的集成度很低。"}
{"en": "One example of this pattern would be a Job which starts a Pod which runs a script that in turn\nstarts a Spark master controller (see [spark example](https://github.com/kubernetes/examples/tree/master/staging/spark/README.md)),\nruns a spark driver, and then cleans up.\n\nAn advantage of this approach is that the overall process gets the completion guarantee of a Job\nobject, but maintains complete control over what Pods are created and how work is assigned to them.", "zh": "这种模式的实例之一是用 Job 来启动一个运行脚本的 Pod，脚本负责启动 Spark\n主控制器（参见 [Spark 示例](https://github.com/kubernetes/examples/tree/master/staging/spark/README.md)），\n运行 Spark 驱动，之后完成清理工作。\n\n这种方法的优点之一是整个过程得到了 Job 对象的完成保障，\n同时维持了对创建哪些 Pod、如何向其分派工作的完全控制能力，\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn about [Pods](/docs/concepts/workloads/pods).\n* Read about different ways of running Jobs:\n  * [Coarse Parallel Processing Using a Work Queue](/docs/tasks/job/coarse-parallel-processing-work-queue/)\n  * [Fine Parallel Processing Using a Work Queue](/docs/tasks/job/fine-parallel-processing-work-queue/)\n  * Use an [indexed Job for parallel processing with static work assignment](/docs/tasks/job/indexed-parallel-processing-static/)\n  * Create multiple Jobs based on a template: [Parallel Processing using Expansions](/docs/tasks/job/parallel-processing-expansion/)\n* Follow the links within [Clean up finished jobs automatically](#clean-up-finished-jobs-automatically)\n  to learn more about how your cluster can clean up completed and / or failed tasks.\n* `Job` is part of the Kubernetes REST API.\n  Read the {{< api-reference page=\"workload-resources/job-v1\" >}}\n  object definition to understand the API for jobs.\n* Read about [`CronJob`](/docs/concepts/workloads/controllers/cron-jobs/), which you\n  can use to define a series of Jobs that will run based on a schedule, similar to\n  the UNIX tool `cron`.\n* Practice how to configure handling of retriable and non-retriable pod failures\n  using `podFailurePolicy`, based on the step-by-step [examples](/docs/tasks/job/pod-failure-policy/).", "zh": "* 了解 [Pod](/zh-cn/docs/concepts/workloads/pods)。\n* 了解运行 Job 的不同的方式：\n  * [使用工作队列进行粗粒度并行处理](/zh-cn/docs/tasks/job/coarse-parallel-processing-work-queue/)\n  * [使用工作队列进行精细的并行处理](/zh-cn/docs/tasks/job/fine-parallel-processing-work-queue/)\n  * [使用索引作业完成静态工作分配下的并行处理](/zh-cn/docs/tasks/job/indexed-parallel-processing-static/)\n  * 基于一个模板运行多个 Job：[使用展开的方式进行并行处理](/zh-cn/docs/tasks/job/parallel-processing-expansion/)\n* 跟随[自动清理完成的 Job](#clean-up-finished-jobs-automatically) 文中的链接，了解你的集群如何清理完成和失败的任务。\n* `Job` 是 Kubernetes REST API 的一部分。阅读 {{< api-reference page=\"workload-resources/job-v1\" >}}\n  对象定义理解关于该资源的 API。\n* 阅读 [`CronJob`](/zh-cn/docs/concepts/workloads/controllers/cron-jobs/)，\n  它允许你定义一系列定期运行的 Job，类似于 UNIX 工具 `cron`。\n* 根据循序渐进的[示例](/zh-cn/docs/tasks/job/pod-failure-policy/)，\n  练习如何使用 `podFailurePolicy` 配置处理可重试和不可重试的 Pod 失效。"}
{"en": "Kubernetes provides several built-in APIs for declarative management of your\n{{< glossary_tooltip text=\"workloads\" term_id=\"workload\" >}}\nand the components of those workloads.", "zh": "Kubernetes 提供了几个内置的 API\n来声明式管理{{< glossary_tooltip text=\"工作负载\" term_id=\"workload\" >}}及其组件。"}
{"en": "Ultimately, your applications run as containers inside\n{{< glossary_tooltip term_id=\"Pod\" text=\"Pods\" >}}; however, managing individual\nPods would be a lot of effort. For example, if a Pod fails, you probably want to\nrun a new Pod to replace it. Kubernetes can do that for you.", "zh": "最终，你的应用以容器的形式在 {{< glossary_tooltip term_id=\"Pod\" text=\"Pods\" >}} 中运行；\n但是，直接管理单个 Pod 的工作量将会非常繁琐。例如，如果一个 Pod 失败了，你可能希望运行一个新的\nPod 来替换它。Kubernetes 可以为你完成这些操作。"}
{"en": "You use the Kubernetes API to create workload\n{{< glossary_tooltip text=\"object\" term_id=\"object\" >}} that represent a higher level\nabstraction than a Pod, and then the Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} automatically manages\nPod objects on your behalf, based on the specification for the workload object you defined.", "zh": "你可以使用 Kubernetes API 创建工作负载{{< glossary_tooltip text=\"对象\" term_id=\"object\" >}}，\n这些对象所表达的是比 Pod 更高级别的抽象概念，Kubernetes\n{{< glossary_tooltip text=\"控制平面\" term_id=\"control-plane\" >}}根据你定义的工作负载对象规约自动管理 Pod 对象。"}
{"en": "The built-in APIs for managing workloads are:", "zh": "用于管理工作负载的内置 API 包括："}
{"en": "[Deployment](/docs/concepts/workloads/controllers/deployment/) (and, indirectly, [ReplicaSet](/docs/concepts/workloads/controllers/replicaset/)),\nthe most common way to run an application on your cluster.\nDeployment is a good fit for managing a stateless application workload on your cluster, where\nany Pod in the Deployment is interchangeable and can be replaced if needed.\n(Deployments are a replacement for the legacy\n{{< glossary_tooltip text=\"ReplicationController\" term_id=\"replication-controller\" >}} API).", "zh": "[Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/)\n（也间接包括 [ReplicaSet](/zh-cn/docs/concepts/workloads/controllers/replicaset/)）\n是在集群上运行应用的最常见方式。Deployment 适合在集群上管理无状态应用工作负载，\n其中 Deployment 中的任何 Pod 都是可互换的，可以在需要时进行替换。\n（Deployment 替代原来的 {{< glossary_tooltip text=\"ReplicationController\" term_id=\"replication-controller\" >}} API）。"}
{"en": "A [StatefulSet](/docs/concepts/workloads/controllers/statefulset/) lets you\nmanage one or more Pods – all running the same application code – where the Pods rely\non having a distinct identity. This is different from a Deployment where the Pods are\nexpected to be interchangeable.\nThe most common use for a StatefulSet is to be able to make a link between its Pods and\ntheir persistent storage. For example, you can run a StatefulSet that associates each Pod\nwith a [PersistentVolume](/docs/concepts/storage/persistent-volumes/). If one of the Pods\nin the StatefulSet fails, Kubernetes makes a replacement Pod that is connected to the\nsame PersistentVolume.", "zh": "[StatefulSet](/zh-cn/docs/concepts/workloads/controllers/statefulset/)\n允许你管理一个或多个运行相同应用代码、但具有不同身份标识的 Pod。\nStatefulSet 与 Deployment 不同。Deployment 中的 Pod 预期是可互换的。\nStatefulSet 最常见的用途是能够建立其 Pod 与其持久化存储之间的关联。\n例如，你可以运行一个将每个 Pod 关联到 [PersistentVolume](/zh-cn/docs/concepts/storage/persistent-volumes/)\n的 StatefulSet。如果该 StatefulSet 中的一个 Pod 失败了，Kubernetes 将创建一个新的 Pod，\n并连接到相同的 PersistentVolume。"}
{"en": "A [DaemonSet](/docs/concepts/workloads/controllers/daemonset/) defines Pods that provide\nfacilities that are local to a specific {{< glossary_tooltip text=\"node\" term_id=\"node\" >}};\nfor example, a driver that lets containers on that node access a storage system. You use a DaemonSet\nwhen the driver, or other node-level service, has to run on the node where it's useful.\nEach Pod in a DaemonSet performs a role similar to a system daemon on a classic Unix / POSIX\nserver.\nA DaemonSet might be fundamental to the operation of your cluster,\nsuch as a plugin to let that node access\n[cluster networking](/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model),\nit might help you to manage the node,\nor it could provide less essential facilities that enhance the container platform you are running.\nYou can run DaemonSets (and their pods) across every node in your cluster, or across just a subset (for example,\nonly install the GPU accelerator driver on nodes that have a GPU installed).", "zh": "[DaemonSet](/zh-cn/docs/concepts/workloads/controllers/daemonset/)\n定义了在特定{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}上提供本地设施的 Pod，\n例如允许该节点上的容器访问存储系统的驱动。当必须在合适的节点上运行某种驱动或其他节点级别的服务时，\n你可以使用 DaemonSet。DaemonSet 中的每个 Pod 执行类似于经典 Unix / POSIX\n服务器上的系统守护进程的角色。DaemonSet 可能对集群的操作至关重要，\n例如作为插件让该节点访问[集群网络](/zh-cn/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model)，\n也可能帮助你管理节点，或者提供增强正在运行的容器平台所需的、不太重要的设施。\n你可以在集群的每个节点上运行 DaemonSets（及其 Pod），或者仅在某个子集上运行\n（例如，只在安装了 GPU 的节点上安装 GPU 加速驱动）。"}
{"en": "You can use a [Job](/docs/concepts/workloads/controllers/job/) and / or\na [CronJob](/docs/concepts/workloads/controllers/cron-jobs/) to\ndefine tasks that run to completion and then stop. A Job represents a one-off task,\nwhereas each CronJob repeats according to a schedule.", "zh": "你可以使用 [Job](/zh-cn/docs/concepts/workloads/controllers/job/) 和/或\n[CronJob](/zh-cn/docs/concepts/workloads/controllers/cron-jobs/) 定义一次性任务和定时任务。\nJob 表示一次性任务，而每个 CronJob 可以根据排期表重复执行。"}
{"en": "Other topics in this section:", "zh": "本节中的其他主题："}
{"en": "overview", "zh": "{{< note >}}"}
{"en": "A [`Deployment`](/docs/concepts/workloads/controllers/deployment/) that configures a [`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is now the recommended way to set up replication.", "zh": "现在推荐使用配置 [`ReplicaSet`](/zh-cn/docs/concepts/workloads/controllers/replicaset/) 的\n[`Deployment`](/zh-cn/docs/concepts/workloads/controllers/deployment/) 来建立副本管理机制。\n{{< /note >}}"}
{"en": "A _ReplicationController_ ensures that a specified number of pod replicas are running at any one\ntime. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is\nalways up and available.", "zh": "**ReplicationController** 确保在任何时候都有特定数量的 Pod 副本处于运行状态。\n换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的。"}
{"en": "## How a ReplicationController works\n\nIf there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the\nReplicationController starts more pods. Unlike manually created pods, the pods maintained by a\nReplicationController are automatically replaced if they fail, are deleted, or are terminated.\nFor example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.\nFor this reason, you should use a ReplicationController even if your application requires\nonly a single pod. A ReplicationController is similar to a process supervisor,\nbut instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods\nacross multiple nodes.", "zh": "## ReplicationController 如何工作   {#how-a-replicationcontroller-works}\n\n当 Pod 数量过多时，ReplicationController 会终止多余的 Pod。当 Pod 数量太少时，ReplicationController 将会启动新的 Pod。\n与手动创建的 Pod 不同，由 ReplicationController 创建的 Pod 在失败、被删除或被终止时会被自动替换。\n例如，在中断性维护（如内核升级）之后，你的 Pod 会在节点上重新创建。\n因此，即使你的应用程序只需要一个 Pod，你也应该使用 ReplicationController 创建 Pod。\nReplicationController 类似于进程管理器，但是 ReplicationController 不是监控单个节点上的单个进程，而是监控跨多个节点的多个 Pod。"}
{"en": "ReplicationController is often abbreviated to \"rc\" in discussion, and as a shortcut in\nkubectl commands.\n\nA simple case is to create one ReplicationController object to reliably run one instance of\na Pod indefinitely.  A more complex use case is to run several identical replicas of a replicated\nservice, such as web servers.", "zh": "在讨论中，ReplicationController 通常缩写为 \"rc\"，并作为 kubectl 命令的快捷方式。\n\n一个简单的示例是创建一个 ReplicationController 对象来可靠地无限期地运行 Pod 的一个实例。\n更复杂的用例是运行一个多副本服务（如 web 服务器）的若干相同副本。"}
{"en": "## Running an example ReplicationController\n\nThis example ReplicationController config runs three copies of the nginx web server.", "zh": "## 运行一个示例 ReplicationController   {#running-an-example-replicationcontroller}\n\n这个示例 ReplicationController 配置运行 nginx Web 服务器的三个副本。\n\n{{% code_sample file=\"controllers/replication.yaml\" %}}"}
{"en": "Run the example job by downloading the example file and then running this command:", "zh": "通过下载示例文件并运行以下命令来运行示例任务:\n\n```shell\nkubectl apply -f https://k8s.io/examples/controllers/replication.yaml\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nreplicationcontroller/nginx created\n```"}
{"en": "Check on the status of the ReplicationController using this command:", "zh": "使用以下命令检查 ReplicationController 的状态:\n\n```shell\nkubectl describe replicationcontrollers/nginx\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nName:        nginx\nNamespace:   default\nSelector:    app=nginx\nLabels:      app=nginx\nAnnotations:    <none>\nReplicas:    3 current / 3 desired\nPods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:       app=nginx\n  Containers:\n   nginx:\n    Image:              nginx\n    Port:               80/TCP\n    Environment:        <none>\n    Mounts:             <none>\n  Volumes:              <none>\nEvents:\n  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message\n  ---------       --------     -----    ----                        -------------    ----      ------              -------\n  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m\n  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0\n  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v\n```"}
{"en": "Here, three pods are created, but none is running yet, perhaps because the image is being pulled.\nA little later, the same command may show:", "zh": "在这里，创建了三个 Pod，但没有一个 Pod 正在运行，这可能是因为正在拉取镜像。\n稍后，相同的命令可能会显示：\n\n```\nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed\n```"}
{"en": "To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:", "zh": "要以机器可读的形式列出属于 ReplicationController 的所有 Pod，可以使用如下命令：\n\n```shell\npods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name})\necho $pods\n```"}
{"en": "The output is similar to this:", "zh": "输出类似于：\n\n```\nnginx-3ntk0 nginx-4ok8v nginx-qrm3m\n```"}
{"en": "Here, the selector is the same as the selector for the ReplicationController (seen in the\n`kubectl describe` output), and in a different form in `replication.yaml`.  The `--output=jsonpath` option\nspecifies an expression with the name from each pod in the returned list.", "zh": "这里，选择算符与 ReplicationController 的选择算符相同（参见 `kubectl describe` 输出），并以不同的形式出现在 `replication.yaml` 中。\n`--output=jsonpath` 选项指定了一个表达式，仅从返回列表中的每个 Pod 中获取名称。"}
{"en": "## Writing a ReplicationController Manifest\n\nAs with all other Kubernetes config, a ReplicationController needs `apiVersion`, `kind`, and `metadata` fields.\n\nWhen the control plane creates new Pods for a ReplicationController, the `.metadata.name` of the\nReplicationController is part of the basis for naming those Pods.  The name of a ReplicationController must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\n\nFor general information about working with configuration files, see [object management](/docs/concepts/overview/working-with-objects/object-management/).\n\nA ReplicationController also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).", "zh": "## 编写一个 ReplicationController 清单   {#writing-a-replicationcontroller-manifest}\n\n与所有其它 Kubernetes 配置一样，ReplicationController 需要 `apiVersion`、`kind` 和 `metadata` 字段。\n\n当控制平面为 ReplicationController 创建新的 Pod 时，ReplicationController\n的 `.metadata.name` 是命名这些 Pod 的部分基础。ReplicationController 的名称必须是一个合法的\n[DNS 子域](/zh-cn/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names)值，\n但这可能对 Pod 的主机名产生意外的结果。为获得最佳兼容性，名称应遵循更严格的\n[DNS 标签](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-label-names)规则。\n\n有关使用配置文件的常规信息，\n参考[对象管理](/zh-cn/docs/concepts/overview/working-with-objects/object-management/)。\n\nReplicationController 也需要一个 [`.spec` 部分](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)。"}
{"en": "### Pod Template\n\nThe `.spec.template` is the only required field of the `.spec`.\n\nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}, except it is nested and does not have an `apiVersion` or `kind`.", "zh": "### Pod 模板  {#pod-template}\n\n`.spec.template` 是 `.spec` 的唯一必需字段。\n\n`.spec.template` 是一个 [Pod 模板](/zh-cn/docs/concepts/workloads/pods/#pod-templates)。\n它的模式与 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 完全相同，只是它是嵌套的，没有 `apiVersion` 或 `kind` 属性。"}
{"en": "In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate\nlabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [pod selector](#pod-selector).\n\nOnly a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is allowed, which is the default if not specified.\n\nFor local container restarts, ReplicationControllers delegate to an agent on the node,\nfor example the [Kubelet](/docs/reference/command-line-tools-reference/kubelet/).", "zh": "除了 Pod 所需的字段外，ReplicationController 中的 Pod 模板必须指定适当的标签和适当的重新启动策略。\n对于标签，请确保不与其他控制器重叠。参考 [Pod 选择算符](#pod-selector)。\n\n只允许 [`.spec.template.spec.restartPolicy`](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\n等于 `Always`，如果没有指定，这是默认值。\n\n对于本地容器重启，ReplicationController 委托给节点上的代理，\n例如 [Kubelet](/zh-cn/docs/reference/command-line-tools-reference/kubelet/)。"}
{"en": "### Labels on the ReplicationController\n\nThe ReplicationController can itself have labels (`.metadata.labels`).  Typically, you\nwould set these the same as the `.spec.template.metadata.labels`; if `.metadata.labels` is not specified\nthen it defaults to  `.spec.template.metadata.labels`. However, they are allowed to be\ndifferent, and the `.metadata.labels` do not affect the behavior of the ReplicationController.", "zh": "### ReplicationController 上的标签   {#labels-on-the-replicacontroller}\n\nReplicationController 本身可以有标签 （`.metadata.labels`）。\n通常，你可以将这些设置为 `.spec.template.metadata.labels`；\n如果没有指定 `.metadata.labels` 那么它默认为 `.spec.template.metadata.labels`。\n但是，Kubernetes 允许它们是不同的，`.metadata.labels` 不会影响 ReplicationController 的行为。"}
{"en": "### Pod Selector\n\nThe `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors). A ReplicationController\nmanages all the pods with labels that match the selector. It does not distinguish\nbetween pods that it created or deleted and pods that another person or process created or\ndeleted. This allows the ReplicationController to be replaced without affecting the running pods.", "zh": "### Pod 选择算符 {#pod-selector}\n\n`.spec.selector` 字段是一个[标签选择算符](/zh-cn/docs/concepts/overview/working-with-objects/labels/#label-selectors)。\nReplicationController 管理标签与选择算符匹配的所有 Pod。\n它不区分它创建或删除的 Pod 和其他人或进程创建或删除的 Pod。\n这允许在不影响正在运行的 Pod 的情况下替换 ReplicationController。"}
{"en": "If specified, the `.spec.template.metadata.labels` must be equal to the `.spec.selector`, or it will\nbe rejected by the API.  If `.spec.selector` is unspecified, it will be defaulted to\n`.spec.template.metadata.labels`.", "zh": "如果指定了 `.spec.template.metadata.labels`，它必须和 `.spec.selector` 相同，否则它将被 API 拒绝。\n如果没有指定 `.spec.selector`，它将默认为 `.spec.template.metadata.labels`。"}
{"en": "Also you should not normally create any pods whose labels match this selector, either directly, with\nanother ReplicationController, or with another controller such as Job. If you do so, the\nReplicationController thinks that it created the other pods.  Kubernetes does not stop you\nfrom doing this.\n\nIf you do end up with multiple controllers that have overlapping selectors, you\nwill have to manage the deletion yourself (see [below](#working-with-replicationcontrollers)).", "zh": "另外，通常不应直接使用另一个 ReplicationController 或另一个控制器（例如 Job）\n来创建其标签与该选择算符匹配的任何 Pod。如果这样做，ReplicationController 会认为它创建了这些 Pod。\nKubernetes 并没有阻止你这样做。\n\n如果你的确创建了多个控制器并且其选择算符之间存在重叠，那么你将不得不自己管理删除操作（参考[后文](#working-with-replicationcontrollers)）。"}
{"en": "### Multiple Replicas\n\nYou can specify how many pods should run concurrently by setting `.spec.replicas` to the number\nof pods you would like to have running concurrently.  The number running at any time may be higher\nor lower, such as if the replicas were just increased or decreased, or if a pod is gracefully\nshutdown, and a replacement starts early.\n\nIf you do not specify `.spec.replicas`, then it defaults to 1.", "zh": "### 多个副本   {#multiple-replicas}\n\n你可以通过设置 `.spec.replicas` 来指定应该同时运行多少个 Pod。\n在任何时候，处于运行状态的 Pod 个数都可能高于或者低于设定值。例如，副本个数刚刚被增加或减少时，\n或者一个 Pod 处于优雅终止过程中而其替代副本已经提前开始创建时。\n\n如果你没有指定 `.spec.replicas`，那么它默认是 1。"}
{"en": "## Working with ReplicationControllers\n\n### Deleting a ReplicationController and its Pods\n\nTo delete a ReplicationController and all its pods, use [`kubectl\ndelete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  Kubectl will scale the ReplicationController to zero and wait\nfor it to delete each pod before deleting the ReplicationController itself.  If this kubectl\ncommand is interrupted, it can be restarted.\n\nWhen using the REST API or [client library](/docs/reference/using-api/client-libraries), you need to do the steps explicitly (scale replicas to\n0, wait for pod deletions, then delete the ReplicationController).", "zh": "## 使用 ReplicationController {#working-with-replicationcontrollers}\n\n### 删除一个 ReplicationController 以及它的 Pod   {#deleteing-a-replicationcontroller-and-its-pods}\n\n要删除一个 ReplicationController 以及它的 Pod，使用\n[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete)。\nkubectl 将 ReplicationController 缩容为 0 并等待以便在删除 ReplicationController 本身之前删除每个 Pod。\n如果这个 kubectl 命令被中断，可以重新启动它。\n\n当使用 REST API 或[客户端库](/zh-cn/docs/reference/using-api/client-libraries)时，你需要明确地执行这些步骤（缩容副本为 0、\n等待 Pod 删除，之后删除 ReplicationController 资源）。"}
{"en": "### Deleting only a ReplicationController\n\nYou can delete a ReplicationController without affecting any of its pods.\n\nUsing kubectl, specify the `--cascade=orphan` option to [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).\n\nWhen using the REST API or [client library](/docs/reference/using-api/client-libraries), you can delete the ReplicationController object.", "zh": "### 只删除 ReplicationController   {#deleting-only-a-replicationcontroller}\n\n你可以删除一个 ReplicationController 而不影响它的任何 Pod。\n\n使用 kubectl，为 [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete) 指定 `--cascade=orphan` 选项。\n\n当使用 REST API 或[客户端库](/zh-cn/docs/reference/using-api/client-libraries)时，只需删除 ReplicationController 对象。"}
{"en": "Once the original is deleted, you can create a new ReplicationController to replace it.  As long\nas the old and new `.spec.selector` are the same, then the new one will adopt the old pods.\nHowever, it will not make any effort to make existing pods match a new, different pod template.\nTo update pods to a new spec in a controlled way, use a [rolling update](#rolling-updates).", "zh": "一旦原始对象被删除，你可以创建一个新的 ReplicationController 来替换它。\n只要新的和旧的 `.spec.selector` 相同，那么新的控制器将领养旧的 Pod。\n但是，它不会做出任何努力使现有的 Pod 匹配新的、不同的 Pod 模板。\n如果希望以受控方式更新 Pod 以使用新的 spec，请执行[滚动更新](#rolling-updates)操作。"}
{"en": "### Isolating pods from a ReplicationController\n\nPods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).", "zh": "### 从 ReplicationController 中隔离 Pod   {#isolating-pods-from-a-replicationcontroller}\n\n通过更改 Pod 的标签，可以从 ReplicationController 的目标中删除 Pod。\n此技术可用于从服务中删除 Pod 以进行调试、数据恢复等。以这种方式删除的 Pod\n将被自动替换（假设复制副本的数量也没有更改）。"}
{"en": "## Common usage patterns", "zh": "## 常见的使用模式   {#common-usage-patterns}"}
{"en": "### Rescheduling\n\nAs mentioned above, whether you have 1 pod you want to keep running, or 1000, a\nReplicationController will ensure that the specified number of pods exists, even\nin the event of node failure or pod termination (for example, due to an action\nby another control agent).", "zh": "### 重新调度   {#rescheduling}\n\n如上所述，无论你想要继续运行 1 个 Pod 还是 1000 个 Pod，一个\nReplicationController 都将确保存在指定数量的 Pod，即使在节点故障或\nPod 终止（例如，由于另一个控制代理的操作）的情况下也是如此。"}
{"en": "### Scaling\n\nThe ReplicationController enables scaling the number of replicas up or down,\neither manually or by an auto-scaling control agent, by updating the `replicas` field.", "zh": "### 扩缩容   {#scaling}\n\n通过设置 `replicas` 字段，ReplicationController 可以允许扩容或缩容副本的数量。\n你可以手动或通过自动扩缩控制代理来控制 ReplicationController 执行此操作。"}
{"en": "### Rolling updates\n\nThe ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.\n\nAs explained in [#1353](https://issue.k8s.io/1353), the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.", "zh": "### 滚动更新 {#rolling-updates}\n\nReplicationController 的设计目的是通过逐个替换 Pod 以方便滚动更新服务。\n\n如 [#1353](https://issue.k8s.io/1353) PR 中所述，建议的方法是使用 1 个副本创建一个新的 ReplicationController，\n逐个扩容新的（+1）和缩容旧的（-1）控制器，然后在旧的控制器达到 0 个副本后将其删除。\n这一方法能够实现可控的 Pod 集合更新，即使存在意外失效的状况。"}
{"en": "Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.\n\nThe two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.", "zh": "理想情况下，滚动更新控制器将考虑应用程序的就绪情况，并确保在任何给定时间都有足够数量的\nPod 有效地提供服务。\n\n这两个 ReplicationController 将需要创建至少具有一个不同标签的 Pod，\n比如 Pod 主要容器的镜像标签，因为通常是镜像更新触发滚动更新。"}
{"en": "### Multiple release tracks\n\nIn addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.\n\nFor instance, a service might target all pods with `tier in (frontend), environment in (prod)`.  Now say you have 10 replicated pods that make up this tier.  But you want to be able to 'canary' a new version of this component.  You could set up a ReplicationController with `replicas` set to 9 for the bulk of the replicas, with labels `tier=frontend, environment=prod, track=stable`, and another ReplicationController with `replicas` set to 1 for the canary, with labels `tier=frontend, environment=prod, track=canary`.  Now the service is covering both the canary and non-canary pods.  But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.", "zh": "### 多个版本跟踪   {#multiple-release-tracks}\n\n除了在滚动更新过程中运行应用程序的多个版本之外，通常还会使用多个版本跟踪很长时间，\n甚至持续运行多个版本。这些跟踪将根据标签加以区分。\n\n例如，一个服务可能把具有 `tier in (frontend), environment in (prod)` 的所有 Pod 作为目标。\n现在假设你有 10 个副本的 Pod 组成了这个层。但是你希望能够 `canary` （`金丝雀`）发布这个组件的新版本。\n你可以为大部分副本设置一个 ReplicationController，其中 `replicas` 设置为 9，\n标签为 `tier=frontend, environment=prod, track=stable` 而为 `canary`\n设置另一个 ReplicationController，其中 `replicas` 设置为 1，\n标签为 `tier=frontend, environment=prod, track=canary`。\n现在这个服务覆盖了 `canary` 和非 `canary` Pod。但你可以单独处理\nReplicationController，以测试、监控结果等。"}
{"en": "### Using ReplicationControllers with Services\n\nMultiple ReplicationControllers can sit behind a single service, so that, for example, some traffic\ngoes to the old version, and some goes to the new version.\n\nA ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.", "zh": "### 和服务一起使用 ReplicationController   {#using-replicationcontrollers-with-services}\n\n多个 ReplicationController 可以位于一个服务的后面，例如，一部分流量流向旧版本，\n一部分流量流向新版本。\n\n一个 ReplicationController 永远不会自行终止，但它不会像服务那样长时间存活。\n服务可以由多个 ReplicationController 控制的 Pod 组成，并且在服务的生命周期内\n（例如，为了执行 Pod 更新而运行服务），可以创建和销毁许多 ReplicationController。\n服务本身和它们的客户端都应该忽略负责维护服务 Pod 的 ReplicationController 的存在。"}
{"en": "## Writing programs for Replication\n\nPods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the [RabbitMQ work queues](https://www.rabbitmq.com/tutorials/tutorial-two-python.html), as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.", "zh": "## 编写多副本的应用   {#writing-programs-for-replication}\n\n由 ReplicationController 创建的 Pod 是可替换的，语义上是相同的，\n尽管随着时间的推移，它们的配置可能会变得异构。\n这显然适合于多副本的无状态服务器，但是 ReplicationController 也可以用于维护主选、\n分片和工作池应用程序的可用性。\n这样的应用程序应该使用动态的工作分配机制，例如\n[RabbitMQ 工作队列](https://www.rabbitmq.com/tutorials/tutorial-two-python.html)，\n而不是静态的或者一次性定制每个 Pod 的配置，这被认为是一种反模式。\n执行的任何 Pod 定制，例如资源的垂直自动调整大小（例如，CPU 或内存），\n都应该由另一个在线控制器进程执行，这与 ReplicationController 本身没什么不同。"}
{"en": "## Responsibilities of the ReplicationController\n\nThe ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, [readiness](https://issue.k8s.io/620) and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.", "zh": "## ReplicationController 的职责   {#responsibilities-of-the-replicationcontroller}\n\nReplicationController 仅确保所需的 Pod 数量与其标签选择算符匹配，并且是可操作的。\n目前，它的计数中只排除终止的 Pod。\n未来，可能会考虑系统提供的[就绪状态](https://issue.k8s.io/620)和其他信息，\n我们可能会对替换策略添加更多控制，\n我们计划发出事件，这些事件可以被外部客户端用来实现任意复杂的替换和/或缩减策略。"}
{"en": "The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in [#492](https://issue.k8s.io/492)), which would change its `replicas` field. We will not add scheduling policies (for example, [spreading](https://issue.k8s.io/367#issuecomment-48428019)) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation ([#170](https://issue.k8s.io/170)).", "zh": "ReplicationController 永远被限制在这个狭隘的职责范围内。\n它本身既不执行就绪态探测，也不执行活跃性探测。\n它不负责执行自动扩缩，而是由外部自动扩缩器控制（如\n[#492](https://issue.k8s.io/492) 中所述），后者负责更改其 `replicas` 字段值。\n我们不会向 ReplicationController 添加调度策略（例如，\n[spreading](https://issue.k8s.io/367#issuecomment-48428019)）。\n它也不应该验证所控制的 Pod 是否与当前指定的模板匹配，因为这会阻碍自动调整大小和其他自动化过程。\n类似地，完成期限、整理依赖关系、配置扩展和其他特性也属于其他地方。\n我们甚至计划考虑批量创建 Pod 的机制（查阅 [#170](https://issue.k8s.io/170)）。"}
{"en": "The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The \"macro\" operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like [Asgard](https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1) managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.", "zh": "ReplicationController 旨在成为可组合的构建基元。\n我们希望在它和其他补充原语的基础上构建更高级别的 API 或者工具，以便于将来的用户使用。\nkubectl 目前支持的 \"macro\" 操作（运行、扩缩、滚动更新）就是这方面的概念示例。\n例如，我们可以想象类似于 [Asgard](https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1)\n的东西管理 ReplicationController、自动定标器、服务、调度策略、金丝雀发布等。"}
{"en": "## API Object\n\nReplication controller is a top-level resource in the Kubernetes REST API. More details about the\nAPI object can be found at:\n[ReplicationController API object](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replicationcontroller-v1-core).", "zh": "## API 对象   {#api-object}\n\n在 Kubernetes REST API 中 Replication controller 是顶级资源。\n更多关于 API 对象的详细信息可以在\n[ReplicationController API 对象](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replicationcontroller-v1-core)找到。"}
{"en": "## Alternatives to ReplicationController\n\n### ReplicaSet\n\n[`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is the next-generation ReplicationController that supports the new [set-based label selector](/docs/concepts/overview/working-with-objects/labels/#set-based-requirement).\nIt's mainly used by [Deployment](/docs/concepts/workloads/controllers/deployment/) as a mechanism to orchestrate pod creation, deletion and updates.\nNote that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.", "zh": "## ReplicationController 的替代方案   {#alternatives-to-replicationcontroller}\n\n### ReplicaSet\n\n[`ReplicaSet`](/zh-cn/docs/concepts/workloads/controllers/replicaset/) 是下一代 ReplicationController，\n支持新的[基于集合的标签选择算符](/zh-cn/docs/concepts/overview/working-with-objects/labels/#set-based-requirement)。\n它主要被 [`Deployment`](/zh-cn/docs/concepts/workloads/controllers/deployment/)\n用来作为一种编排 Pod 创建、删除及更新的机制。\n请注意，我们推荐使用 Deployment 而不是直接使用 ReplicaSet，除非你需要自定义更新编排或根本不需要更新。"}
{"en": "### Deployment (Recommended)\n\n[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is a higher-level API object that updates its underlying Replica Sets and their Pods.\nDeployments are recommended if you want the rolling update functionality,\nbecause they are declarative, server-side, and have additional features.", "zh": "### Deployment （推荐）\n\n[`Deployment`](/zh-cn/docs/concepts/workloads/controllers/deployment/)\n是一种更高级别的 API 对象，用于更新其底层 ReplicaSet 及其 Pod。\n如果你想要这种滚动更新功能，那么推荐使用 Deployment，因为它们是声明式的、服务端的，并且具有其它特性。"}
{"en": "### Bare Pods\n\nUnlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node.  A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.", "zh": "### 裸 Pod\n\n与用户直接创建 Pod 的情况不同，ReplicationController 能够替换因某些原因被删除或被终止的 Pod，\n例如在节点故障或中断节点维护的情况下，例如内核升级。\n因此，我们建议你使用 ReplicationController，即使你的应用程序只需要一个 Pod。\n可以将其看作类似于进程管理器，它只管理跨多个节点的多个 Pod，而不是单个节点上的单个进程。\nReplicationController 将本地容器重启委托给节点上的某个代理（例如 Kubelet)。"}
{"en": "### Job\n\nUse a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicationController for pods that are expected to terminate on their own\n(that is, batch jobs).", "zh": "### Job\n\n对于预期会自行终止的 Pod (即批处理任务)，使用\n[`Job`](/zh-cn/docs/concepts/workloads/controllers/job/) 而不是 ReplicationController。"}
{"en": "### DaemonSet\n\nUse a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicationController for pods that provide a\nmachine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied\nto a machine lifetime: the pod needs to be running on the machine before other pods start, and are\nsafe to terminate when the machine is otherwise ready to be rebooted/shutdown.", "zh": "### DaemonSet\n\n对于提供机器级功能（例如机器监控或机器日志记录）的 Pod，\n使用 [`DaemonSet`](/zh-cn/docs/concepts/workloads/controllers/daemonset/) 而不是\nReplicationController。\n这些 Pod 的生命期与机器的生命期绑定：它们需要在其他 Pod 启动之前在机器上运行，\n并且在机器准备重新启动或者关闭时安全地终止。"}
{"en": "## {{% heading \"whatsnext\" %}}\n\n* Learn about [Pods](/docs/concepts/workloads/pods).\n* Learn about [Deployment](/docs/concepts/workloads/controllers/deployment/), the replacement\n  for ReplicationController.\n* `ReplicationController` is part of the Kubernetes REST API.\n  Read the {{< api-reference page=\"workload-resources/replication-controller-v1\" >}}\n  object definition to understand the API for replication controllers.", "zh": "## {{% heading \"whatsnext\" %}}\n\n- 了解 [Pod](/zh-cn/docs/concepts/workloads/pods)。\n- 了解 [Depolyment](/zh-cn/docs/concepts/workloads/controllers/deployment/)，ReplicationController 的替代品。\n- `ReplicationController` 是 Kubernetes REST API 的一部分，阅读 {{< api-reference page=\"workload-resources/replication-controller-v1\" >}}\n  对象定义以了解 replication controllers 的 API。"}
{"en": "overview", "zh": "{{< feature-state state=\"beta\" for_k8s_version=\"v1.11\" >}}"}
{"en": "Cloud infrastructure technologies let you run Kubernetes on public, private, and hybrid clouds.\nKubernetes believes in automated, API-driven infrastructure without tight coupling between\ncomponents.", "zh": "使用云基础设施技术，你可以在公有云、私有云或者混合云环境中运行 Kubernetes。\nKubernetes 的信条是基于自动化的、API 驱动的基础设施，同时避免组件间紧密耦合。\n\n{{< glossary_definition term_id=\"cloud-controller-manager\" length=\"all\" prepend=\"组件 cloud-controller-manager 是指云控制器管理器，\">}}"}
{"en": "The cloud-controller-manager is structured using a plugin\nmechanism that allows different cloud providers to integrate their platforms with Kubernetes.", "zh": "`cloud-controller-manager` 组件是基于一种插件机制来构造的，\n这种机制使得不同的云厂商都能将其平台与 Kubernetes 集成。"}
{"en": "## Design\n\n![Kubernetes components](/images/docs/components-of-kubernetes.svg)\n\nThe cloud controller manager runs in the control plane as a replicated set of processes\n(usually, these are containers in Pods). Each cloud-controller-manager implements\nmultiple {{< glossary_tooltip text=\"controllers\" term_id=\"controller\" >}} in a single\nprocess.", "zh": "## 设计  {#design}\n\n{{< figure src=\"/zh-cn/docs/images/components-of-kubernetes.svg\" alt=\"此图展示了 Kubernetes 集群的组件\" class=\"diagram-medium\" >}}\n\n云控制器管理器以一组多副本的进程集合的形式运行在控制面中，通常表现为 Pod\n中的容器。每个 `cloud-controller-manager`\n在同一进程中实现多个{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}。\n\n{{< note >}}"}
{"en": "You can also run the cloud controller manager as a Kubernetes\n{{< glossary_tooltip text=\"addon\" term_id=\"addons\" >}} rather than as part\nof the control plane.", "zh": "你也可以用 Kubernetes {{< glossary_tooltip text=\"插件\" term_id=\"addons\" >}}\n的形式而不是控制面中的一部分来运行云控制器管理器。\n{{< /note >}}"}
{"en": "## Cloud controller manager functions {#functions-of-the-ccm}\n\nThe controllers inside the cloud controller manager include:", "zh": "## 云控制器管理器的功能 {#functions-of-the-ccm}\n\n云控制器管理器中的控制器包括："}
{"en": "### Node controller\n\nThe node controller is responsible for updating {{< glossary_tooltip text=\"Node\" term_id=\"node\" >}} objects\nwhen new servers are created in your cloud infrastructure. The node controller obtains information about the\nhosts running inside your tenancy with the cloud provider. The node controller performs the following functions:", "zh": "### 节点控制器   {#node-controller}\n\n节点控制器负责在云基础设施中创建了新服务器时为之更新{{< glossary_tooltip text=\"节点（Node）\" term_id=\"node\" >}}对象。\n节点控制器从云提供商获取当前租户中主机的信息。节点控制器执行以下功能："}
{"en": "1. Update a Node object with the corresponding server's unique identifier obtained from the cloud provider API.\n1. Annotating and labelling the Node object with cloud-specific information, such as the region the node\n   is deployed into and the resources (CPU, memory, etc) that it has available.\n1. Obtain the node's hostname and network addresses.\n1. Verifying the node's health. In case a node becomes unresponsive, this controller checks with\n   your cloud provider's API to see if the server has been deactivated / deleted / terminated.\n   If the node has been deleted from the cloud, the controller deletes the Node object from your Kubernetes\n   cluster.", "zh": "1. 使用从云平台 API 获取的对应服务器的唯一标识符更新 Node 对象；\n2. 利用特定云平台的信息为 Node 对象添加注解和标签，例如节点所在的区域\n   （Region）和所具有的资源（CPU、内存等等）；\n3. 获取节点的网络地址和主机名；\n4. 检查节点的健康状况。如果节点无响应，控制器通过云平台 API\n   查看该节点是否已从云中禁用、删除或终止。如果节点已从云中删除，\n   则控制器从 Kubernetes 集群中删除 Node 对象。"}
{"en": "Some cloud provider implementations split this into a node controller and a separate node\nlifecycle controller.", "zh": "某些云驱动实现中，这些任务被划分到一个节点控制器和一个节点生命周期控制器中。"}
{"en": "### Route controller\n\nThe route controller is responsible for configuring routes in the cloud\nappropriately so that containers on different nodes in your Kubernetes\ncluster can communicate with each other.\n\nDepending on the cloud provider, the route controller might also allocate blocks\nof IP addresses for the Pod network.", "zh": "### 路由控制器   {#route-controller}\n\nRoute 控制器负责适当地配置云平台中的路由，以便 Kubernetes 集群中不同节点上的容器之间可以相互通信。\n\n取决于云驱动本身，路由控制器可能也会为 Pod 网络分配 IP 地址块。"}
{"en": "### Service controller\n\n{{< glossary_tooltip text=\"Services\" term_id=\"service\" >}} integrate with cloud\ninfrastructure components such as managed load balancers, IP addresses, network\npacket filtering, and target health checking. The service controller interacts with your\ncloud provider's APIs to set up load balancers and other infrastructure components\nwhen you declare a Service resource that requires them.", "zh": "### 服务控制器   {#service-controller}\n\n{{< glossary_tooltip text=\"服务（Service）\" term_id=\"service\" >}}与受控的负载均衡器、\nIP 地址、网络包过滤、目标健康检查等云基础设施组件集成。\n服务控制器与云驱动的 API 交互，以配置负载均衡器和其他基础设施组件。\n你所创建的 Service 资源会需要这些组件服务。"}
{"en": "## Authorization\n\nThis section breaks down the access that the cloud controller manager requires\non various API objects, in order to perform its operations.", "zh": "## 鉴权   {#authorization}\n\n本节分别讲述云控制器管理器为了完成自身工作而产生的对各类 API 对象的访问需求。"}
{"en": "### Node controller {#authorization-node-controller}\n\nThe Node controller only works with Node objects. It requires full access\nto read and modify Node objects.", "zh": "### 节点控制器  {#authorization-node-controller}\n\n节点控制器只操作 Node 对象。它需要读取和修改 Node 对象的完全访问权限。\n\n`v1/Node`：\n\n- get\n- list\n- create\n- update\n- patch\n- watch\n- delete"}
{"en": "### Route controller {#authorization-route-controller}\n\nThe route controller listens to Node object creation and configures\nroutes appropriately. It requires Get access to Node objects.", "zh": "### 路由控制器 {#authorization-route-controller}\n\n路由控制器会监听 Node 对象的创建事件，并据此配置路由设施。\n它需要读取 Node 对象的 Get 权限。\n\n`v1/Node`：\n\n- get"}
{"en": "### Service controller {#authorization-service-controller}\n\nThe service controller watches for Service object **create**, **update** and **delete** events and then\nconfigures Endpoints for those Services appropriately (for EndpointSlices, the\nkube-controller-manager manages these on demand).\n\nTo access Services, it requires **list**, and **watch** access. To update Services, it requires\n**patch** and **update** access.\n\nTo set up Endpoints resources for the Services, it requires access to **create**, **list**,\n**get**, **watch**, and **update**.", "zh": "### 服务控制器 {#authorization-service-controller}\n\n服务控制器监测 Service 对象的 **create**、**update** 和 **delete** 事件，\n并配置对应服务的 Endpoints 对象\n（对于 EndpointSlices，kube-controller-manager 按需对其进行管理）。\n\n为了访问 Service 对象，它需要 **list** 和 **watch** 访问权限。\n为了更新 Service 对象，它需要 **patch** 和 **update** 访问权限。\n\n为了能够配置 Service 对应的 Endpoints 资源，\n它需要 **create**、**list**、**get**、**watch** 和 **update** 等访问权限。\n\n`v1/Service`：\n\n- list\n- get\n- watch\n- patch\n- update"}
{"en": "### Others {#authorization-miscellaneous}\n\nThe implementation of the core of the cloud controller manager requires access to create Event\nobjects, and to ensure secure operation, it requires access to create ServiceAccounts.", "zh": "### 其他  {#authorization-miscellaneous}\n\n在云控制器管理器的实现中，其核心部分需要创建 Event 对象的访问权限，\n并创建 ServiceAccount 资源以保证操作安全性的权限。\n\n`v1/Event`:\n\n- create\n- patch\n- update\n\n`v1/ServiceAccount`:\n\n- create"}
{"en": "The {{< glossary_tooltip term_id=\"rbac\" text=\"RBAC\" >}} ClusterRole for the cloud\ncontroller manager looks like:", "zh": "用于云控制器管理器 {{< glossary_tooltip term_id=\"rbac\" text=\"RBAC\" >}}\n的 ClusterRole 如下例所示：\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cloud-controller-manager\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - events\n  verbs:\n  - create\n  - patch\n  - update\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  verbs:\n  - '*'\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes/status\n  verbs:\n  - patch\n- apiGroups:\n  - \"\"\n  resources:\n  - services\n  verbs:\n  - list\n  - patch\n  - update\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - serviceaccounts\n  verbs:\n  - create\n- apiGroups:\n  - \"\"\n  resources:\n  - persistentvolumes\n  verbs:\n  - get\n  - list\n  - update\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - endpoints\n  verbs:\n  - create\n  - get\n  - list\n  - watch\n  - update\n```\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* [Cloud Controller Manager Administration](/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager)\n  has instructions on running and managing the cloud controller manager.\n\n* To upgrade a HA control plane to use the cloud controller manager, see\n  [Migrate Replicated Control Plane To Use Cloud Controller Manager](/docs/tasks/administer-cluster/controller-manager-leader-migration/).\n\n* Want to know how to implement your own cloud controller manager, or extend an existing project?", "zh": "* [云控制器管理器的管理](/zh-cn/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager)\n给出了运行和管理云控制器管理器的指南。\n\n* 要升级 HA 控制平面以使用云控制器管理器，\n请参见[将复制的控制平面迁移以使用云控制器管理器](/zh-cn/docs/tasks/administer-cluster/controller-manager-leader-migration/)。\n\n* 想要了解如何实现自己的云控制器管理器，或者对现有项目进行扩展么？"}
{"en": "- The cloud controller manager uses Go interfaces, specifically, `CloudProvider` interface defined in\n    [`cloud.go`](https://github.com/kubernetes/cloud-provider/blob/release-1.21/cloud.go#L42-L69)\n    from [kubernetes/cloud-provider](https://github.com/kubernetes/cloud-provider) to allow\n    implementations from any cloud to be plugged in.", "zh": "- 云控制器管理器使用 Go 语言的接口（具体指在\n    [kubernetes/cloud-provider](https://github.com/kubernetes/cloud-provider)\n    项目中 [`cloud.go`](https://github.com/kubernetes/cloud-provider/blob/release-1.26/cloud.go#L43-L69)\n    文件中所定义的 `CloudProvider` 接口），从而使得针对各种云平台的具体实现都可以接入。"}
{"en": "- The implementation of the shared controllers highlighted in this document (Node, Route, and Service),\n    and some scaffolding along with the shared cloudprovider interface, is part of the Kubernetes core.\n    Implementations specific to cloud providers are outside the core of Kubernetes and implement\n    the `CloudProvider` interface.", "zh": "- 本文中列举的共享控制器（节点控制器、路由控制器和服务控制器等）的实现以及其他一些生成具有\n    CloudProvider 接口的框架的代码，都是 Kubernetes 的核心代码。\n    特定于云驱动的实现虽不是 Kubernetes 核心成分，仍要实现 `CloudProvider` 接口。"}
{"en": "- For more information about developing plugins,\n    see [Developing Cloud Controller Manager](/docs/tasks/administer-cluster/developing-cloud-controller-manager/).", "zh": "- 关于如何开发插件的详细信息，\n    可参考[开发云控制器管理器](/zh-cn/docs/tasks/administer-cluster/developing-cloud-controller-manager/)文档。"}
{"en": "This document catalogs the communication paths between the {{< glossary_tooltip term_id=\"kube-apiserver\" text=\"API server\" >}}\nand the Kubernetes {{< glossary_tooltip text=\"cluster\" term_id=\"cluster\" length=\"all\" >}}.\nThe intent is to allow users to customize their installation to harden the network configuration\nsuch that the cluster can be run on an untrusted network (or on fully public IPs on a cloud\nprovider).", "zh": "本文列举控制面节点（确切地说是 {{< glossary_tooltip term_id=\"kube-apiserver\" text=\"API 服务器\" >}}）和\nKubernetes {{< glossary_tooltip text=\"集群\" term_id=\"cluster\" length=\"all\" >}}之间的通信路径。\n目的是为了让用户能够自定义他们的安装，以实现对网络配置的加固，\n使得集群能够在不可信的网络上（或者在一个云服务商完全公开的 IP 上）运行。"}
{"en": "## Node to Control Plane\n\nKubernetes has a \"hub-and-spoke\" API pattern. All API usage from nodes (or the pods they run)\nterminates at the API server. None of the other control plane components are designed to expose\nremote services. The API server is configured to listen for remote connections on a secure HTTPS\nport (typically 443) with one or more forms of client\n[authentication](/docs/reference/access-authn-authz/authentication/) enabled.\nOne or more forms of [authorization](/docs/reference/access-authn-authz/authorization/) should be\nenabled, especially if [anonymous requests](/docs/reference/access-authn-authz/authentication/#anonymous-requests)\nor [service account tokens](/docs/reference/access-authn-authz/authentication/#service-account-tokens)\nare allowed.", "zh": "## 节点到控制面   {#node-to-control-plane}\n\nKubernetes 采用的是中心辐射型（Hub-and-Spoke）API 模式。\n所有从节点（或运行于其上的 Pod）发出的 API 调用都终止于 API 服务器。\n其它控制面组件都没有被设计为可暴露远程服务。\nAPI 服务器被配置为在一个安全的 HTTPS 端口（通常为 443）上监听远程连接请求，\n并启用一种或多种形式的客户端[身份认证](/zh-cn/docs/reference/access-authn-authz/authentication/)机制。\n一种或多种客户端[鉴权机制](/zh-cn/docs/reference/access-authn-authz/authorization/)应该被启用，\n特别是在允许使用[匿名请求](/zh-cn/docs/reference/access-authn-authz/authentication/#anonymous-requests)\n或[服务账户令牌](/zh-cn/docs/reference/access-authn-authz/authentication/#service-account-tokens)的时候。"}
{"en": "Nodes should be provisioned with the public root {{< glossary_tooltip text=\"certificate\" term_id=\"certificate\" >}} for the cluster such that they can\nconnect securely to the API server along with valid client credentials. A good approach is that the\nclient credentials provided to the kubelet are in the form of a client certificate. See\n[kubelet TLS bootstrapping](/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/)\nfor automated provisioning of kubelet client certificates.", "zh": "应该使用集群的公共根{{< glossary_tooltip text=\"证书\" term_id=\"certificate\" >}}开通节点，\n这样它们就能够基于有效的客户端凭据安全地连接 API 服务器。\n一种好的方法是以客户端证书的形式将客户端凭据提供给 kubelet。\n请查看 [kubelet TLS 启动引导](/zh-cn/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/)\n以了解如何自动提供 kubelet 客户端证书。"}
{"en": "{{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} that wish to connect to the API server can do so securely by leveraging a service account so\nthat Kubernetes will automatically inject the public root certificate and a valid bearer token\ninto the pod when it is instantiated.\nThe `kubernetes` service (in `default` namespace) is configured with a virtual IP address that is\nredirected (via `{{< glossary_tooltip text=\"kube-proxy\" term_id=\"kube-proxy\" >}}`) to the HTTPS endpoint on the API server.\n\nThe control plane components also communicate with the API server over the secure port.", "zh": "想要连接到 API 服务器的 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}\n可以使用服务账号安全地进行连接。\n当 Pod 被实例化时，Kubernetes 自动把公共根证书和一个有效的持有者令牌注入到 Pod 里。\n`kubernetes` 服务（位于 `default` 名字空间中）配置了一个虚拟 IP 地址，\n用于（通过 `{{< glossary_tooltip text=\"kube-proxy\" term_id=\"kube-proxy\" >}}`）转发请求到\nAPI 服务器的 HTTPS 末端。\n\n控制面组件也通过安全端口与集群的 API 服务器通信。"}
{"en": "As a result, the default operating mode for connections from the nodes and pod running on the\nnodes to the control plane is secured by default and can run over untrusted and/or public\nnetworks.", "zh": "这样，从集群节点和节点上运行的 Pod 到控制面的连接的缺省操作模式即是安全的，\n能够在不可信的网络或公网上运行。"}
{"en": "## Control plane to node\n\nThere are two primary communication paths from the control plane (the API server) to the nodes.\nThe first is from the API server to the {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} process which runs on each node in the cluster.\nThe second is from the API server to any node, pod, or service through the API server's _proxy_\nfunctionality.", "zh": "## 控制面到节点  {#control-plane-to-node}\n\n从控制面（API 服务器）到节点有两种主要的通信路径。\n第一种是从 API 服务器到集群中每个节点上运行的\n{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} 进程。\n第二种是从 API 服务器通过它的**代理**功能连接到任何节点、Pod 或者服务。"}
{"en": "### API server to kubelet\n\nThe connections from the API server to the kubelet are used for:\n\n* Fetching logs for pods.\n* Attaching (usually through `kubectl`) to running pods.\n* Providing the kubelet's port-forwarding functionality.\n\nThese connections terminate at the kubelet's HTTPS endpoint. By default, the API server does not\nverify the kubelet's serving certificate, which makes the connection subject to man-in-the-middle\nattacks and **unsafe** to run over untrusted and/or public networks.", "zh": "### API 服务器到 kubelet  {#api-server-to-kubelet}\n\n从 API 服务器到 kubelet 的连接用于：\n\n* 获取 Pod 日志。\n* 挂接（通过 kubectl）到运行中的 Pod。\n* 提供 kubelet 的端口转发功能。\n\n这些连接终止于 kubelet 的 HTTPS 末端。\n默认情况下，API 服务器不检查 kubelet 的服务证书。这使得此类连接容易受到中间人攻击，\n在非受信网络或公开网络上运行也是 **不安全的**。"}
{"en": "To verify this connection, use the `--kubelet-certificate-authority` flag to provide the API\nserver with a root certificate bundle to use to verify the kubelet's serving certificate.\n\nIf that is not possible, use [SSH tunneling](#ssh-tunnels) between the API server and kubelet if\nrequired to avoid connecting over an\nuntrusted or public network.\n\nFinally, [Kubelet authentication and/or authorization](/docs/reference/access-authn-authz/kubelet-authn-authz/)\nshould be enabled to secure the kubelet API.", "zh": "为了对这个连接进行认证，使用 `--kubelet-certificate-authority` 标志给\nAPI 服务器提供一个根证书包，用于 kubelet 的服务证书。\n\n如果无法实现这点，又要求避免在非受信网络或公共网络上进行连接，可在 API 服务器和\nkubelet 之间使用 [SSH 隧道](#ssh-tunnels)。\n\n最后，应该启用\n[Kubelet 认证/鉴权](/zh-cn/docs/reference/access-authn-authz/kubelet-authn-authz/)\n来保护 kubelet API。"}
{"en": "### API server to nodes, pods, and services\n\nThe connections from the API server to a node, pod, or service default to plain HTTP connections\nand are therefore neither authenticated nor encrypted. They can be run over a secure HTTPS\nconnection by prefixing `https:` to the node, pod, or service name in the API URL, but they will\nnot validate the certificate provided by the HTTPS endpoint nor provide client credentials. So\nwhile the connection will be encrypted, it will not provide any guarantees of integrity. These\nconnections **are not currently safe** to run over untrusted or public networks.", "zh": "### API 服务器到节点、Pod 和服务  {#api-server-to-nodes-pods-and-services}\n\n从 API 服务器到节点、Pod 或服务的连接默认为纯 HTTP 方式，因此既没有认证，也没有加密。\n这些连接可通过给 API URL 中的节点、Pod 或服务名称添加前缀 `https:` 来运行在安全的 HTTPS 连接上。\n不过这些连接既不会验证 HTTPS 末端提供的证书，也不会提供客户端证书。\n因此，虽然连接是加密的，仍无法提供任何完整性保证。\n这些连接 **目前还不能安全地** 在非受信网络或公共网络上运行。"}
{"en": "### SSH tunnels\n\nKubernetes supports [SSH tunnels](https://www.ssh.com/academy/ssh/tunneling) to protect the control plane to nodes communication paths. In this\nconfiguration, the API server initiates an SSH tunnel to each node in the cluster (connecting to\nthe SSH server listening on port 22) and passes all traffic destined for a kubelet, node, pod, or\nservice through the tunnel.\nThis tunnel ensures that the traffic is not exposed outside of the network in which the nodes are\nrunning.", "zh": "### SSH 隧道 {#ssh-tunnels}\n\nKubernetes 支持使用\n[SSH 隧道](https://www.ssh.com/academy/ssh/tunneling)来保护从控制面到节点的通信路径。\n在这种配置下，API 服务器建立一个到集群中各节点的 SSH 隧道（连接到在 22 端口监听的 SSH 服务器）\n并通过这个隧道传输所有到 kubelet、节点、Pod 或服务的请求。\n这一隧道保证通信不会被暴露到集群节点所运行的网络之外。\n\n{{< note >}}"}
{"en": "SSH tunnels are currently deprecated, so you shouldn't opt to use them unless you know what you\nare doing. The [Konnectivity service](#konnectivity-service) is a replacement for this\ncommunication channel.", "zh": "SSH 隧道目前已被废弃。除非你了解个中细节，否则不应使用。\n[Konnectivity 服务](#konnectivity-service)是 SSH 隧道的替代方案。\n{{< /note >}}"}
{"en": "### Konnectivity service", "zh": "### Konnectivity 服务   {#konnectivity-service}\n\n{{< feature-state for_k8s_version=\"v1.18\" state=\"beta\" >}}"}
{"en": "As a replacement to the SSH tunnels, the Konnectivity service provides TCP level proxy for the\ncontrol plane to cluster communication. The Konnectivity service consists of two parts: the\nKonnectivity server in the control plane network and the Konnectivity agents in the nodes network.\nThe Konnectivity agents initiate connections to the Konnectivity server and maintain the network\nconnections.\nAfter enabling the Konnectivity service, all control plane to nodes traffic goes through these\nconnections.\n\nFollow the [Konnectivity service task](/docs/tasks/extend-kubernetes/setup-konnectivity/) to set\nup the Konnectivity service in your cluster.", "zh": "作为 SSH 隧道的替代方案，Konnectivity 服务提供 TCP 层的代理，以便支持从控制面到集群的通信。\nKonnectivity 服务包含两个部分：Konnectivity 服务器和 Konnectivity 代理，\n分别运行在控制面网络和节点网络中。\nKonnectivity 代理建立并维持到 Konnectivity 服务器的网络连接。\n启用 Konnectivity 服务之后，所有控制面到节点的通信都通过这些连接传输。\n\n请浏览 [Konnectivity 服务任务](/zh-cn/docs/tasks/extend-kubernetes/setup-konnectivity/)\n在你的集群中配置 Konnectivity 服务。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about the [Kubernetes control plane components](/docs/concepts/architecture/#control-plane-components)\n* Learn more about [Hubs and Spoke model](https://book.kubebuilder.io/multiversion-tutorial/conversion-concepts.html#hubs-spokes-and-other-wheel-metaphors)\n* Learn how to [Secure a Cluster](/docs/tasks/administer-cluster/securing-a-cluster/) \n* Learn more about the [Kubernetes API](/docs/concepts/overview/kubernetes-api/)\n* [Set up Konnectivity service](/docs/tasks/extend-kubernetes/setup-konnectivity/)\n* [Use Port Forwarding to Access Applications in a Cluster](/docs/tasks/access-application-cluster/port-forward-access-application-cluster/)\n* Learn how to [Fetch logs for Pods](/docs/tasks/debug/debug-application/debug-running-pod/#examine-pod-logs), [use kubectl port-forward](/docs/tasks/access-application-cluster/port-forward-access-application-cluster/#forward-a-local-port-to-a-port-on-the-pod)", "zh": "* 阅读 [Kubernetes 控制面组件](/zh-cn/docs/concepts/architecture/#control-plane-components)\n* 进一步了解 [Hubs and Spoke model](https://book.kubebuilder.io/multiversion-tutorial/conversion-concepts.html#hubs-spokes-and-other-wheel-metaphors)\n* 进一步了解如何[保护集群](/zh-cn/docs/tasks/administer-cluster/securing-a-cluster/)\n* 进一步了解 [Kubernetes API](/zh-cn/docs/concepts/overview/kubernetes-api/)\n* [设置 Konnectivity 服务](/zh-cn/docs/tasks/extend-kubernetes/setup-konnectivity/)\n* [使用端口转发来访问集群中的应用](/zh-cn/docs/tasks/access-application-cluster/port-forward-access-application-cluster/)\n* 学习如何[检查 Pod 的日志](/zh-cn/docs/tasks/debug/debug-application/debug-running-pod/#examine-pod-logs)\n  以及如何[使用 kubectl 端口转发](/zh-cn/docs/tasks/access-application-cluster/port-forward-access-application-cluster/#forward-a-local-port-to-a-port-on-the-pod)"}
{"en": "overview", "zh": "{{< feature-state feature_gate_name=\"UnknownVersionInteroperabilityProxy\" >}}"}
{"en": "Kubernetes {{< skew currentVersion >}} includes an alpha feature that lets an\n{{< glossary_tooltip text=\"API Server\" term_id=\"kube-apiserver\" >}}\nproxy a resource requests to other _peer_ API servers. This is useful when there are multiple\nAPI servers running different versions of Kubernetes in one cluster\n(for example, during a long-lived rollout to a new release of Kubernetes).", "zh": "Kubernetes {{<skew currentVersion>}} 包含了一个 Alpha 特性，可以让\n{{<glossary_tooltip text=\"API 服务器\" term_id=\"kube-apiserver\">}}代理指向其他**对等**\nAPI 服务器的资源请求。当一个集群中运行着多个 API 服务器，且各服务器的 Kubernetes 版本不同时\n（例如在上线 Kubernetes 新版本的时间跨度较长时），这一特性非常有用。"}
{"en": "This enables cluster administrators to configure highly available clusters that can be upgraded\nmore safely, by directing resource requests (made during the upgrade) to the correct kube-apiserver.\nThat proxying prevents users from seeing unexpected 404 Not Found errors that stem\nfrom the upgrade process.\n\nThis mechanism is called the _Mixed Version Proxy_.", "zh": "此特性通过将（升级过程中所发起的）资源请求引导到正确的 kube-apiserver\n使得集群管理员能够配置高可用的、升级动作更安全的集群。\n该代理机制可以防止用户在升级过程中看到意外的 404 Not Found 错误。\n\n这个机制称为 **Mixed Version Proxy（混合版本代理）**。"}
{"en": "## Enabling the Mixed Version Proxy\n\nEnsure that `UnknownVersionInteroperabilityProxy` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) \nis enabled when you start the {{< glossary_tooltip text=\"API Server\" term_id=\"kube-apiserver\" >}}:", "zh": "## 启用混合版本代理   {#enabling-the-mixed-version-proxy}\n\n当你启动 {{<glossary_tooltip text=\"API 服务器\" term_id=\"kube-apiserver\">}}时，\n确保启用了 `UnknownVersionInteroperabilityProxy`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)："}
{"en": "```shell\nkube-apiserver \\\n--feature-gates=UnknownVersionInteroperabilityProxy=true \\\n# required command line arguments for this feature\n--peer-ca-file=<path to kube-apiserver CA cert>\n--proxy-client-cert-file=<path to aggregator proxy cert>,\n--proxy-client-key-file=<path to aggregator proxy key>,\n--requestheader-client-ca-file=<path to aggregator CA cert>,\n# requestheader-allowed-names can be set to blank to allow any Common Name\n--requestheader-allowed-names=<valid Common Names to verify proxy client cert against>,\n\n# optional flags for this feature\n--peer-advertise-ip=`IP of this kube-apiserver that should be used by peers to proxy requests`\n--peer-advertise-port=`port of this kube-apiserver that should be used by peers to proxy requests`\n\n# …and other flags as usual\n```", "zh": "```shell\nkube-apiserver \\\n--feature-gates=UnknownVersionInteroperabilityProxy=true \\\n# 需要为此特性添加的命令行参数\n--peer-ca-file=<指向 kube-apiserver CA 证书的路径>\n--proxy-client-cert-file=<指向聚合器代理证书的路径>,\n--proxy-client-key-file=<指向聚合器代理密钥的路径>,\n--requestheader-client-ca-file=<指向聚合器 CA 证书的路径>,\n# requestheader-allowed-names 可设置为空以允许所有 Common Name\n--requestheader-allowed-names=<验证代理客户端证书的合法 Common Name>,\n\n# 此特性的可选标志\n--peer-advertise-ip=`应由对等方用于代理请求的 kube-apiserver IP`\n--peer-advertise-port=`应由对等方用于代理请求的 kube-apiserver 端口`\n\n# ... 和其他常规标志\n```"}
{"en": "### Proxy transport and authentication between API servers {#transport-and-authn}\n\n* The source kube-apiserver reuses the\n  [existing APIserver client authentication flags](/docs/tasks/extend-kubernetes/configure-aggregation-layer/#kubernetes-apiserver-client-authentication)\n  `--proxy-client-cert-file` and `--proxy-client-key-file` to present its identity that\n  will be verified by its peer (the destination kube-apiserver). The destination API server\n  verifies that peer connection based on the configuration you specify using the\n  `--requestheader-client-ca-file` command line argument.\n\n* To authenticate the destination server's serving certs, you must configure a certificate\n  authority bundle by specifying the `--peer-ca-file` command line argument to the **source** API server.", "zh": "### API 服务器之间的代理传输和身份验证   {#transport-and-authn}\n\n* 源 kube-apiserver\n  重用[现有的 API 服务器客户端身份验证标志](/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/#kubernetes-apiserver-client-authentication)\n  `--proxy-client-cert-file` 和 `--proxy-client-key-file` 来表明其身份，供对等（目标 kube-apiserver）验证。\n  目标 API 服务器根据你使用 `--requestheader-client-ca-file` 命令行参数指定的配置来验证对等连接。\n\n* 要对目标服务器所用的证书进行身份验证，必须通过指定 `--peer-ca-file` 命令行参数来为**源**\n  API 服务器配置一个证书机构包。"}
{"en": "### Configuration for peer API server connectivity\n\nTo set the network location of a kube-apiserver that peers will use to proxy requests, use the\n`--peer-advertise-ip` and `--peer-advertise-port` command line arguments to kube-apiserver or specify\nthese fields in the API server configuration file.\nIf these flags are unspecified, peers will use the value from either `--advertise-address` or\n`--bind-address` command line argument to the kube-apiserver.\nIf those too, are unset, the host's default interface is used.", "zh": "### 对等 API 服务器连接的配置   {#config-for-peer-apiserver-connectivity}\n\n要设置 kube-apiserver 的网络位置以供对等方来代理请求，\n使用为 kube-apiserver 设置的 `--peer-advertise-ip` 和 `--peer-advertise-port` 命令行参数，\n或在 API 服务器配置文件中指定这些字段。如果未指定这些参数，对等方将使用 `--advertise-address`\n或 `--bind-address` 命令行参数的值。如果这些也未设置，则使用主机的默认接口。"}
{"en": "## Mixed version proxying\n\nWhen you enable mixed version proxying, the [aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)\nloads a special filter that does the following:", "zh": "## 混合版本代理   {#mixed-version-proxying}\n\n启用混合版本代理时，\n[聚合层](/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)会加载一个特殊的过滤器，\n完成以下操作："}
{"en": "* When a resource request reaches an API server that cannot serve that API\n  (either because it is at a version pre-dating the introduction of the API or the API is turned off on the API server)\n  the API server attempts to send the request to a peer API server that can serve the requested API.\n  It does so by identifying API groups / versions / resources that the local server doesn't recognise,\n  and tries to proxy those requests to a peer API server that is capable of handling the request.\n* If the peer API server fails to respond, the _source_ API server responds with 503 (\"Service Unavailable\") error.", "zh": "* 当资源请求到达无法提供该 API 的 API 服务器时\n  （可能的原因是服务器早于该 API 的正式引入日期或该 API 在 API 服务器上被关闭），\n  API 服务器会尝试将请求发送到能够提供所请求 API 的对等 API 服务器。\n  API 服务器通过发现本地服务器无法识别的 API 组/版本/资源来实现这一点，\n  并尝试将这些请求代理到能够处理这些请求的对等 API 服务器。\n* 如果对等 API 服务器无法响应，则**源** API 服务器将以 503（\"Service Unavailable\"）错误进行响应。"}
{"en": "### How it works under the hood\n\nWhen an API Server receives a resource request, it first checks which API servers can\nserve the requested resource. This check happens using the internal\n[`StorageVersion` API](/docs/reference/generated/kubernetes-api/v{{< skew currentVersion >}}/#storageversioncondition-v1alpha1-internal-apiserver-k8s-io).\n\n* If the resource is known to the API server that received the request\n  (for example, `GET /api/v1/pods/some-pod`), the request is handled locally.", "zh": "### 内部工作原理   {#how-it-works-under-the-hood}\n\n当 API 服务器收到一个资源请求时，它首先检查哪些 API 服务器可以提供所请求的资源。\n这个检查是使用内部的\n[`StorageVersion` API](/zh-cn/docs/reference/generated/kubernetes-api/v{{< skew currentVersion >}}/#storageversioncondition-v1alpha1-internal-apiserver-k8s-io)\n进行的。\n\n* 如果资源被收到请求（例如 `GET /api/v1/pods/some-pod`）的 API 服务器所了解，则请求会在本地处理。"}
{"en": "* If there is no internal `StorageVersion` object found for the requested resource\n  (for example, `GET /my-api/v1/my-resource`) and the configured APIService specifies proxying\n  to an extension API server, that proxying happens following the usual\n  [flow](/docs/tasks/extend-kubernetes/configure-aggregation-layer/) for extension APIs.", "zh": "* 如果没有找到适合所请求资源（例如 `GET /my-api/v1/my-resource`）的内部 `StorageVersion` 对象，\n  并且所配置的 APIService 设置了指向扩展 API 服务器的代理，那么代理操作将按照扩展 API\n  的常规[流程](/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/)进行。"}
{"en": "* If a valid internal `StorageVersion` object is found for the requested resource\n  (for example, `GET /batch/v1/jobs`) and the API server trying to handle the request\n  (the _handling API server_) has the `batch` API disabled, then the _handling API server_\n  fetches the peer API servers that do serve the relevant API group / version / resource\n  (`api/v1/batch` in this case) using the information in the fetched `StorageVersion` object.\n  The _handling API server_ then proxies the request to one of the matching peer kube-apiservers\n  that are aware of the requested resource.\n\n  * If there is no peer known for that API group / version / resource, the handling API server\n    passes the request to its own handler chain which should eventually return a 404 (\"Not Found\") response.\n\n  * If the handling API server has identified and selected a peer API server, but that peer fails\n    to respond (for reasons such as network connectivity issues, or a data race between the request\n    being received and a controller registering the peer's info into the control plane), then the handling\n    API server responds with a 503 (\"Service Unavailable\") error.", "zh": "* 如果找到了对应所请求资源（例如 `GET /batch/v1/jobs`）的合法的内部 `StorageVersion` 对象，\n  并且正在处理请求的 API 服务器（**处理中的 API 服务器**）禁用了 `batch` API，\n  则**正处理的 API 服务器**使用已获取的 `StorageVersion` 对象中的信息，\n  获取提供相关 API 组/版本/资源（在此情况下为 `api/v1/batch`）的对等 API 服务器。\n  **处理中的 API 服务器**随后将请求代理到能够理解所请求资源且匹配的对等 kube-apiserver 之一。\n\n  * 如果没有对等方了解所给的 API 组/版本/资源，则处理请求的 API 服务器将请求传递给自己的处理程序链，\n    最终应返回 404（\"Not Found\"）响应。\n\n  * 如果处理请求的 API 服务器已经识别并选择了一个对等 API 服务器，但该对等方无法响应\n    （原因可能是网络连接问题或正接收的请求与向控制平面注册对等信息的控制器之间存在数据竞争等），\n    则处理请求的 API 服务器会以 503（\"Service Unavailable\"）错误进行响应。"}
{"en": "Kubernetes runs your {{< glossary_tooltip text=\"workload\" term_id=\"workload\" >}}\nby placing containers into Pods to run on _Nodes_.\nA node may be a virtual or physical machine, depending on the cluster. Each node\nis managed by the\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}}\nand contains the services necessary to run\n{{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}}.\n\nTypically you have several nodes in a cluster; in a learning or resource-limited\nenvironment, you might have only one node.\n\nThe [components](/docs/concepts/architecture/#node-components) on a node include the\n{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}}, a\n{{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}, and the\n{{< glossary_tooltip text=\"kube-proxy\" term_id=\"kube-proxy\" >}}.", "zh": "Kubernetes 通过将容器放入在节点（Node）上运行的 Pod\n中来执行你的{{< glossary_tooltip text=\"工作负载\" term_id=\"workload\" >}}。\n节点可以是一个虚拟机或者物理机器，取决于所在的集群配置。\n每个节点包含运行 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 所需的服务；\n这些节点由{{< glossary_tooltip text=\"控制面\" term_id=\"control-plane\" >}}负责管理。\n\n通常集群中会有若干个节点；而在一个学习所用或者资源受限的环境中，你的集群中也可能只有一个节点。\n\n节点上的[组件](/zh-cn/docs/concepts/architecture/#node-components)包括\n{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}}、\n{{< glossary_tooltip text=\"容器运行时\" term_id=\"container-runtime\" >}}以及\n{{< glossary_tooltip text=\"kube-proxy\" term_id=\"kube-proxy\" >}}。"}
{"en": "## Management\n\nThere are two main ways to have Nodes added to the\n{{< glossary_tooltip text=\"API server\" term_id=\"kube-apiserver\" >}}:\n\n1. The kubelet on a node self-registers to the control plane\n2. You (or another human user) manually add a Node object\n\nAfter you create a Node {{< glossary_tooltip text=\"object\" term_id=\"object\" >}},\nor the kubelet on a node self-registers, the control plane checks whether the new Node object\nis valid. For example, if you try to create a Node from the following JSON manifest:", "zh": "## 管理  {#management}\n\n向 {{< glossary_tooltip text=\"API 服务器\" term_id=\"kube-apiserver\" >}}添加节点的方式主要有两种：\n\n1. 节点上的 `kubelet` 向控制面执行自注册；\n2. 你（或者别的什么人）手动添加一个 Node 对象。\n\n在你创建了 Node {{< glossary_tooltip text=\"对象\" term_id=\"object\" >}}或者节点上的\n`kubelet` 执行了自注册操作之后，控制面会检查新的 Node 对象是否合法。\n例如，如果你尝试使用下面的 JSON 对象来创建 Node 对象：\n\n```json\n{\n  \"kind\": \"Node\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"10.240.79.157\",\n    \"labels\": {\n      \"name\": \"my-first-k8s-node\"\n    }\n  }\n}\n```"}
{"en": "Kubernetes creates a Node object internally (the representation). Kubernetes checks\nthat a kubelet has registered to the API server that matches the `metadata.name`\nfield of the Node. If the node is healthy (i.e. all necessary services are running),\nthen it is eligible to run a Pod. Otherwise, that node is ignored for any cluster activity\nuntil it becomes healthy.", "zh": "Kubernetes 会在内部创建一个 Node 对象作为节点的表示。Kubernetes 检查 `kubelet`\n向 API 服务器注册节点时使用的 `metadata.name` 字段是否匹配。\n如果节点是健康的（即所有必要的服务都在运行中），则该节点可以用来运行 Pod。\n否则，直到该节点变为健康之前，所有的集群活动都会忽略该节点。\n\n{{< note >}}"}
{"en": "Kubernetes keeps the object for the invalid Node and continues checking to see whether\nit becomes healthy.\n\nYou, or a {{< glossary_tooltip term_id=\"controller\" text=\"controller\">}}, must explicitly\ndelete the Node object to stop that health checking.", "zh": "Kubernetes 会一直保存着非法节点对应的对象，并持续检查该节点是否已经变得健康。\n\n你，或者某个{{< glossary_tooltip term_id=\"controller\" text=\"控制器\">}}必须显式地删除该\nNode 对象以停止健康检查操作。\n{{< /note >}}"}
{"en": "The name of a Node object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).", "zh": "Node 对象的名称必须是合法的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。"}
{"en": "### Node name uniqueness\n\nThe [name](/docs/concepts/overview/working-with-objects/names#names) identifies a Node. Two Nodes\ncannot have the same name at the same time. Kubernetes also assumes that a resource with the same\nname is the same object. In case of a Node, it is implicitly assumed that an instance using the\nsame name will have the same state (e.g. network settings, root disk contents) and attributes like\nnode labels. This may lead to inconsistencies if an instance was modified without changing its name.\nIf the Node needs to be replaced or updated significantly, the existing Node object needs to be\nremoved from API server first and re-added after the update.", "zh": "### 节点名称唯一性     {#node-name-uniqueness}\n\n节点的[名称](/zh-cn/docs/concepts/overview/working-with-objects/names#names)用来标识 Node 对象。\n没有两个 Node 可以同时使用相同的名称。 Kubernetes 还假定名字相同的资源是同一个对象。\n就 Node 而言，隐式假定使用相同名称的实例会具有相同的状态（例如网络配置、根磁盘内容）\n和类似节点标签这类属性。这可能在节点被更改但其名称未变时导致系统状态不一致。\n如果某个 Node 需要被替换或者大量变更，需要从 API 服务器移除现有的 Node 对象，\n之后再在更新之后重新将其加入。"}
{"en": "### Self-registration of Nodes\n\nWhen the kubelet flag `--register-node` is true (the default), the kubelet will attempt to\nregister itself with the API server. This is the preferred pattern, used by most distros.\n\nFor self-registration, the kubelet is started with the following options:", "zh": "### 节点自注册    {#self-registration-of-nodes}\n\n当 kubelet 标志 `--register-node` 为 true（默认）时，它会尝试向 API 服务注册自己。\n这是首选模式，被绝大多数发行版选用。\n\n对于自注册模式，kubelet 使用下列参数启动："}
{"en": "- `--kubeconfig` - Path to credentials to authenticate itself to the API server.\n- `--cloud-provider` - How to talk to a {{< glossary_tooltip text=\"cloud provider\" term_id=\"cloud-provider\" >}}\n  to read metadata about itself.\n- `--register-node` - Automatically register with the API server.\n- `--register-with-taints` - Register the node with the given list of\n  {{< glossary_tooltip text=\"taints\" term_id=\"taint\" >}} (comma separated `<key>=<value>:<effect>`).\n\n  No-op if `register-node` is false.", "zh": "- `--kubeconfig` - 用于向 API 服务器执行身份认证所用的凭据的路径。\n- `--cloud-provider` - 与某{{< glossary_tooltip text=\"云驱动\" term_id=\"cloud-provider\" >}}\n  进行通信以读取与自身相关的元数据的方式。\n- `--register-node` - 自动向 API 服务器注册。\n- `--register-with-taints` - 使用所给的{{< glossary_tooltip text=\"污点\" term_id=\"taint\" >}}列表\n  （逗号分隔的 `<key>=<value>:<effect>`）注册节点。当 `register-node` 为 false 时无效。"}
{"en": "- `--node-ip` - Optional comma-separated list of the IP addresses for the node.\n  You can only specify a single address for each address family.\n  For example, in a single-stack IPv4 cluster, you set this value to be the IPv4 address that the\n  kubelet should use for the node.\n  See [configure IPv4/IPv6 dual stack](/docs/concepts/services-networking/dual-stack/#configure-ipv4-ipv6-dual-stack)\n  for details of running a dual-stack cluster.\n\n  If you don't provide this argument, the kubelet uses the node's default IPv4 address, if any;\n  if the node has no IPv4 addresses then the kubelet uses the node's default IPv6 address.", "zh": "- `--node-ip` - 可选的以英文逗号隔开的节点 IP 地址列表。你只能为每个地址簇指定一个地址。\n  例如在单协议栈 IPv4 集群中，需要将此值设置为 kubelet 应使用的节点 IPv4 地址。\n  参阅[配置 IPv4/IPv6 双协议栈](/zh-cn/docs/concepts/services-networking/dual-stack/#configure-ipv4-ipv6-dual-stack)了解运行双协议栈集群的详情。\n\n  如果你未提供这个参数，kubelet 将使用节点默认的 IPv4 地址（如果有）；\n  如果节点没有 IPv4 地址，则 kubelet 使用节点的默认 IPv6 地址。"}
{"en": "- `--node-labels` - {{< glossary_tooltip text=\"Labels\" term_id=\"label\" >}} to add when registering the node\n  in the cluster (see label restrictions enforced by the\n  [NodeRestriction admission plugin](/docs/reference/access-authn-authz/admission-controllers/#noderestriction)).\n- `--node-status-update-frequency` - Specifies how often kubelet posts its node status to the API server.", "zh": "- `--node-labels` - 在集群中注册节点时要添加的{{< glossary_tooltip text=\"标签\" term_id=\"label\" >}}。\n  （参见 [NodeRestriction 准入控制插件](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#noderestriction)所实施的标签限制）。\n- `--node-status-update-frequency` - 指定 kubelet 向 API 服务器发送其节点状态的频率。"}
{"en": "When the [Node authorization mode](/docs/reference/access-authn-authz/node/) and\n[NodeRestriction admission plugin](/docs/reference/access-authn-authz/admission-controllers/#noderestriction)\nare enabled, kubelets are only authorized to create/modify their own Node resource.", "zh": "当 [Node 鉴权模式](/zh-cn/docs/reference/access-authn-authz/node/)和\n[NodeRestriction 准入插件](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#noderestriction)被启用后，\n仅授权 kubelet 创建/修改自己的 Node 资源。\n\n{{< note >}}"}
{"en": "As mentioned in the [Node name uniqueness](#node-name-uniqueness) section,\nwhen Node configuration needs to be updated, it is a good practice to re-register\nthe node with the API server. For example, if the kubelet is being restarted with\na new set of `--node-labels`, but the same Node name is used, the change will\nnot take effect, as labels are only set (or modified) upon Node registration with the API server.", "zh": "正如[节点名称唯一性](#node-name-uniqueness)一节所述，当 Node 的配置需要被更新时，\n一种好的做法是重新向 API 服务器注册该节点。例如，如果 kubelet 重启时其 `--node-labels`\n是新的值集，但同一个 Node 名称已经被使用，则所作变更不会起作用，\n因为节点标签是在 Node 注册到 API 服务器时完成（或修改）的。"}
{"en": "Pods already scheduled on the Node may misbehave or cause issues if the Node\nconfiguration will be changed on kubelet restart. For example, already running\nPod may be tainted against the new labels assigned to the Node, while other\nPods, that are incompatible with that Pod will be scheduled based on this new\nlabel. Node re-registration ensures all Pods will be drained and properly\nre-scheduled.", "zh": "如果在 kubelet 重启期间 Node 配置发生了变化，已经被调度到某 Node 上的 Pod\n可能会出现行为不正常或者出现其他问题，例如，已经运行的 Pod\n可能通过污点机制设置了与 Node 上新设置的标签相排斥的规则，也有一些其他 Pod，\n本来与此 Pod 之间存在不兼容的问题，也会因为新的标签设置而被调到同一节点。\n节点重新注册操作可以确保节点上所有 Pod 都被排空并被正确地重新调度。\n{{< /note >}}"}
{"en": "### Manual Node administration\n\nYou can create and modify Node objects using\n{{< glossary_tooltip text=\"kubectl\" term_id=\"kubectl\" >}}.\n\nWhen you want to create Node objects manually, set the kubelet flag `--register-node=false`.\n\nYou can modify Node objects regardless of the setting of `--register-node`.\nFor example, you can set labels on an existing Node or mark it unschedulable.", "zh": "### 手动节点管理 {#manual-node-administration}\n\n你可以使用 {{< glossary_tooltip text=\"kubectl\" term_id=\"kubectl\" >}}\n来创建和修改 Node 对象。\n\n如果你希望手动创建节点对象时，请设置 kubelet 标志 `--register-node=false`。\n\n你可以修改 Node 对象（忽略 `--register-node` 设置）。\n例如，你可以修改节点上的标签或并标记其为不可调度。"}
{"en": "You can use labels on Nodes in conjunction with node selectors on Pods to control\nscheduling. For example, you can constrain a Pod to only be eligible to run on\na subset of the available nodes.\n\nMarking a node as unschedulable prevents the scheduler from placing new pods onto\nthat Node but does not affect existing Pods on the Node. This is useful as a\npreparatory step before a node reboot or other maintenance.\n\nTo mark a Node unschedulable, run:", "zh": "你可以结合使用 Node 上的标签和 Pod 上的选择算符来控制调度。\n例如，你可以限制某 Pod 只能在符合要求的节点子集上运行。\n\n如果标记节点为不可调度（unschedulable），将阻止新 Pod 调度到该 Node 之上，\n但不会影响任何已经在其上的 Pod。\n这是重启节点或者执行其他维护操作之前的一个有用的准备步骤。\n\n要标记一个 Node 为不可调度，执行以下命令：\n\n```shell\nkubectl cordon $NODENAME\n```"}
{"en": "See [Safely Drain a Node](/docs/tasks/administer-cluster/safely-drain-node/)\nfor more details.", "zh": "更多细节参考[安全地腾空节点](/zh-cn/docs/tasks/administer-cluster/safely-drain-node/)。\n\n{{< note >}}"}
{"en": "Pods that are part of a {{< glossary_tooltip term_id=\"daemonset\" >}} tolerate\nbeing run on an unschedulable Node. DaemonSets typically provide node-local services\nthat should run on the Node even if it is being drained of workload applications.", "zh": "被 {{< glossary_tooltip term_id=\"daemonset\" text=\"DaemonSet\" >}} 控制器创建的 Pod\n能够容忍节点的不可调度属性。\nDaemonSet 通常提供节点本地的服务，即使节点上的负载应用已经被腾空，\n这些服务也仍需运行在节点之上。\n{{< /note >}}"}
{"en": "## Node status\n\nA Node's status contains the following information:\n\n* [Addresses](/docs/reference/node/node-status/#addresses)\n* [Conditions](/docs/reference/node/node-status/#condition)\n* [Capacity and Allocatable](/docs/reference/node/node-status/#capacity)\n* [Info](/docs/reference/node/node-status/#info)", "zh": "## 节点状态   {#node-status}\n\n一个节点的状态包含以下信息:\n\n* [地址（Addresses）](/zh-cn/docs/reference/node/node-status/#addresses)\n* [状况（Condition）](/zh-cn/docs/reference/node/node-status/#condition)\n* [容量与可分配（Capacity）](/zh-cn/docs/reference/node/node-status/#capacity)\n* [信息（Info）](/zh-cn/docs/reference/node/node-status/#info)"}
{"en": "You can use `kubectl` to view a Node's status and other details:\n\n```shell\nkubectl describe node <insert-node-name-here>\n```", "zh": "你可以使用 `kubectl` 来查看节点状态和其他细节信息：\n\n```shell\nkubectl describe node <节点名称>\n```"}
{"en": "See [Node Status](/docs/reference/node/node-status/) for more details.", "zh": "更多细节参见 [Node Status](/zh-cn/docs/reference/node/node-status)。"}
{"en": "## Node heartbeats\n\nHeartbeats, sent by Kubernetes nodes, help your cluster determine the\navailability of each node, and to take action when failures are detected.\n\nFor nodes there are two forms of heartbeats:", "zh": "## 节点心跳  {#node-heartbeats}\n\nKubernetes 节点发送的心跳帮助你的集群确定每个节点的可用性，并在检测到故障时采取行动。\n\n对于节点，有两种形式的心跳："}
{"en": "* Updates to the [`.status`](/docs/reference/node/node-status/) of a Node.\n* [Lease](/docs/concepts/architecture/leases/) objects\n  within the `kube-node-lease`\n  {{< glossary_tooltip term_id=\"namespace\" text=\"namespace\">}}.\n  Each Node has an associated Lease object.", "zh": "* 更新节点的 [`.status`](/zh-cn/docs/reference/node/node-status/)\n* `kube-node-lease` {{<glossary_tooltip term_id=\"namespace\" text=\"名字空间\">}}中的\n  [Lease（租约）](/zh-cn/docs/concepts/architecture/leases/)对象。\n  每个节点都有一个关联的 Lease 对象。"}
{"en": "## Node controller\n\nThe node {{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}} is a\nKubernetes control plane component that manages various aspects of nodes.\n\nThe node controller has multiple roles in a node's life. The first is assigning a\nCIDR block to the node when it is registered (if CIDR assignment is turned on).", "zh": "## 节点控制器  {#node-controller}\n\n节点{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}是 Kubernetes 控制面组件，\n管理节点的方方面面。\n\n节点控制器在节点的生命周期中扮演多个角色。\n第一个是当节点注册时为它分配一个 CIDR 区段（如果启用了 CIDR 分配）。"}
{"en": "The second is keeping the node controller's internal list of nodes up to date with\nthe cloud provider's list of available machines. When running in a cloud\nenvironment and whenever a node is unhealthy, the node controller asks the cloud\nprovider if the VM for that node is still available. If not, the node\ncontroller deletes the node from its list of nodes.", "zh": "第二个是保持节点控制器内的节点列表与云服务商所提供的可用机器列表同步。\n如果在云环境下运行，只要某节点不健康，节点控制器就会询问云服务是否节点的虚拟机仍可用。\n如果不可用，节点控制器会将该节点从它的节点列表删除。"}
{"en": "The third is monitoring the nodes' health. The node controller is\nresponsible for:\n\n- In the case that a node becomes unreachable, updating the `Ready` condition\n  in the Node's `.status` field. In this case the node controller sets the\n  `Ready` condition to `Unknown`.\n- If a node remains unreachable: triggering\n  [API-initiated eviction](/docs/concepts/scheduling-eviction/api-eviction/)\n  for all of the Pods on the unreachable node. By default, the node controller\n  waits 5 minutes between marking the node as `Unknown` and submitting\n  the first eviction request.\n\nBy default, the node controller checks the state of each node every 5 seconds.\nThis period can be configured using the `--node-monitor-period` flag on the\n`kube-controller-manager` component.", "zh": "第三个是监控节点的健康状况。节点控制器负责：\n\n- 在节点不可达的情况下，在 Node 的 `.status` 中更新 `Ready` 状况。\n  在这种情况下，节点控制器将 NodeReady 状况更新为 `Unknown`。\n- 如果节点仍然无法访问：对于不可达节点上的所有 Pod 触发\n  [API 发起的逐出](/zh-cn/docs/concepts/scheduling-eviction/api-eviction/)操作。\n  默认情况下，节点控制器在将节点标记为 `Unknown` 后等待 5 分钟提交第一个驱逐请求。\n\n默认情况下，节点控制器每 5 秒检查一次节点状态，可以使用 `kube-controller-manager`\n组件上的 `--node-monitor-period` 参数来配置周期。"}
{"en": "### Rate limits on eviction\n\nIn most cases, the node controller limits the eviction rate to\n`--node-eviction-rate` (default 0.1) per second, meaning it won't evict pods\nfrom more than 1 node per 10 seconds.", "zh": "### 逐出速率限制  {#rate-limits-on-eviction}\n\n大部分情况下，节点控制器把逐出速率限制在每秒 `--node-eviction-rate` 个（默认为 0.1）。\n这表示它每 10 秒钟内至多从一个节点驱逐 Pod。"}
{"en": "The node eviction behavior changes when a node in a given availability zone\nbecomes unhealthy. The node controller checks what percentage of nodes in the zone\nare unhealthy (the `Ready` condition is `Unknown` or `False`) at\nthe same time:", "zh": "当一个可用区域（Availability Zone）中的节点变为不健康时，节点的驱逐行为将发生改变。\n节点控制器会同时检查可用区域中不健康（`Ready` 状况为 `Unknown` 或 `False`）\n的节点的百分比："}
{"en": "- If the fraction of unhealthy nodes is at least `--unhealthy-zone-threshold`\n  (default 0.55), then the eviction rate is reduced.\n- If the cluster is small (i.e. has less than or equal to\n  `--large-cluster-size-threshold` nodes - default 50), then evictions are stopped.\n- Otherwise, the eviction rate is reduced to `--secondary-node-eviction-rate`\n  (default 0.01) per second.", "zh": "- 如果不健康节点的比例超过 `--unhealthy-zone-threshold`（默认为 0.55），\n  驱逐速率将会降低。\n- 如果集群较小（意即小于等于 `--large-cluster-size-threshold` 个节点 - 默认为 50），\n  驱逐操作将会停止。\n- 否则驱逐速率将降为每秒 `--secondary-node-eviction-rate` 个（默认为 0.01）。"}
{"en": "The reason these policies are implemented per availability zone is because one\navailability zone might become partitioned from the control plane while the others remain\nconnected. If your cluster does not span multiple cloud provider availability zones,\nthen the eviction mechanism does not take per-zone unavailability into account.", "zh": "在逐个可用区域中实施这些策略的原因是，\n当一个可用区域可能从控制面脱离时其它可用区域可能仍然保持连接。\n如果你的集群没有跨越云服务商的多个可用区域，那（整个集群）就只有一个可用区域。"}
{"en": "A key reason for spreading your nodes across availability zones is so that the\nworkload can be shifted to healthy zones when one entire zone goes down.\nTherefore, if all nodes in a zone are unhealthy, then the node controller evicts at\nthe normal rate of `--node-eviction-rate`. The corner case is when all zones are\ncompletely unhealthy (none of the nodes in the cluster are healthy). In such a\ncase, the node controller assumes that there is some problem with connectivity\nbetween the control plane and the nodes, and doesn't perform any evictions.\n(If there has been an outage and some nodes reappear, the node controller does\nevict pods from the remaining nodes that are unhealthy or unreachable).", "zh": "跨多个可用区域部署你的节点的一个关键原因是当某个可用区域整体出现故障时，\n工作负载可以转移到健康的可用区域。\n因此，如果一个可用区域中的所有节点都不健康时，节点控制器会以正常的速率\n`--node-eviction-rate` 进行驱逐操作。\n在所有的可用区域都不健康（也即集群中没有健康节点）的极端情况下，\n节点控制器将假设控制面与节点间的连接出了某些问题，它将停止所有驱逐动作\n（如果故障后部分节点重新连接，节点控制器会从剩下不健康或者不可达节点中驱逐 Pod）。"}
{"en": "The node controller is also responsible for evicting pods running on nodes with\n`NoExecute` taints, unless those pods tolerate that taint.\nThe node controller also adds {{< glossary_tooltip text=\"taints\" term_id=\"taint\" >}}\ncorresponding to node problems like node unreachable or not ready. This means\nthat the scheduler won't place Pods onto unhealthy nodes.", "zh": "节点控制器还负责驱逐运行在拥有 `NoExecute` 污点的节点上的 Pod，\n除非这些 Pod 能够容忍此污点。\n节点控制器还负责根据节点故障（例如节点不可访问或没有就绪）\n为其添加{{< glossary_tooltip text=\"污点\" term_id=\"taint\" >}}。\n这意味着调度器不会将 Pod 调度到不健康的节点上。"}
{"en": "## Resource capacity tracking {#node-capacity}\n\nNode objects track information about the Node's resource capacity: for example, the amount\nof memory available and the number of CPUs.\nNodes that [self register](#self-registration-of-nodes) report their capacity during\nregistration. If you [manually](#manual-node-administration) add a Node, then\nyou need to set the node's capacity information when you add it.", "zh": "### 资源容量跟踪   {#node-capacity}\n\nNode 对象会跟踪节点上资源的容量（例如可用内存和 CPU 数量）。\n通过[自注册](#self-registration-of-nodes)机制生成的 Node 对象会在注册期间报告自身容量。\n如果你[手动](#manual-node-administration)添加了 Node，\n你就需要在添加节点时手动设置节点容量。"}
{"en": "The Kubernetes {{< glossary_tooltip text=\"scheduler\" term_id=\"kube-scheduler\" >}} ensures that\nthere are enough resources for all the Pods on a Node. The scheduler checks that the sum\nof the requests of containers on the node is no greater than the node's capacity.\nThat sum of requests includes all containers managed by the kubelet, but excludes any\ncontainers started directly by the container runtime, and also excludes any\nprocesses running outside of the kubelet's control.", "zh": "Kubernetes {{< glossary_tooltip text=\"调度器\" term_id=\"kube-scheduler\" >}}\n保证节点上有足够的资源供其上的所有 Pod 使用。\n它会检查节点上所有容器的请求的总和不会超过节点的容量。\n总的请求包括由 kubelet 启动的所有容器，但不包括由容器运行时直接启动的容器，\n也不包括不受 `kubelet` 控制的其他进程。\n\n{{< note >}}"}
{"en": "If you want to explicitly reserve resources for non-Pod processes, see\n[reserve resources for system daemons](/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved).", "zh": "如果要为非 Pod 进程显式保留资源。\n请参考[为系统守护进程预留资源](/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved)。\n{{< /note >}}"}
{"en": "## Node topology", "zh": "## 节点拓扑  {#node-topology}\n\n{{< feature-state feature_gate_name=\"TopologyManager\" >}}"}
{"en": "If you have enabled the `TopologyManager`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/), then\nthe kubelet can use topology hints when making resource assignment decisions.\nSee [Control Topology Management Policies on a Node](/docs/tasks/administer-cluster/topology-manager/)\nfor more information.", "zh": "如果启用了 `TopologyManager` [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)，\n`kubelet` 可以在作出资源分配决策时使用拓扑提示。\n参考[控制节点上拓扑管理策略](/zh-cn/docs/tasks/administer-cluster/topology-manager/)了解详细信息。"}
{"en": "## Swap memory management {#swap-memory}", "zh": "## 交换内存（swap）管理 {#swap-memory}\n\n{{< feature-state feature_gate_name=\"NodeSwap\" >}}"}
{"en": "To enable swap on a node, the `NodeSwap` feature gate must be enabled on\nthe kubelet (default is true), and the `--fail-swap-on` command line flag or `failSwapOn`\n[configuration setting](/docs/reference/config-api/kubelet-config.v1beta1/)\nmust be set to false.\nTo allow Pods to utilize swap, `swapBehavior` should not be set to `NoSwap` (which is the default behavior) in the kubelet config.", "zh": "要在节点上启用交换内存，必须启用 kubelet 的 `NodeSwap` 特性门控（默认启用），\n同时使用 `--fail-swap-on` 命令行参数或者将 `failSwapOn`\n[配置](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/)设置为 false。\n为了允许 Pod 使用交换内存，在 kubelet 配置中不应将 `swapBehavior` 设置为 `NoSwap`（默认行为）。\n\n{{< warning >}}"}
{"en": "When the memory swap feature is turned on, Kubernetes data such as the content\nof Secret objects that were written to tmpfs now could be swapped to disk.", "zh": "当内存交换功能被启用后，Kubernetes 数据（如写入 tmpfs 的 Secret 对象的内容）可以被交换到磁盘。\n{{< /warning >}}"}
{"en": "A user can also optionally configure `memorySwap.swapBehavior` in order to\nspecify how a node will use swap memory. For example,", "zh": "用户还可以选择配置 `memorySwap.swapBehavior` 以指定节点使用交换内存的方式。例如：\n\n```yaml\nmemorySwap:\n  swapBehavior: LimitedSwap\n```"}
{"en": "- `NoSwap` (default): Kubernetes workloads will not use swap.\n- `LimitedSwap`: The utilization of swap memory by Kubernetes workloads is subject to limitations.\n  Only Pods of Burstable QoS are permitted to employ swap.", "zh": "- `NoSwap`（默认）：Kubernetes 工作负载不会使用交换内存。\n- `LimitedSwap`：Kubernetes 工作负载对交换内存的使用受到限制。\n  只有具有 Burstable QoS 的 Pod 可以使用交换内存。"}
{"en": "If configuration for `memorySwap` is not specified and the feature gate is\nenabled, by default the kubelet will apply the same behaviour as the\n`NoSwap` setting.", "zh": "如果启用了特性门控但是未指定 `memorySwap` 的配置，默认情况下 kubelet 将使用与\n`NoSwap` 设置相同的行为。"}
{"en": "With `LimitedSwap`, Pods that do not fall under the Burstable QoS classification (i.e.\n`BestEffort`/`Guaranteed` Qos Pods) are prohibited from utilizing swap memory.\nTo maintain the aforementioned security and node\nhealth guarantees, these Pods are not permitted to use swap memory when `LimitedSwap` is\nin effect.", "zh": "采用 `LimitedSwap` 时，不属于 Burstable QoS 分类的 Pod\n（即 `BestEffort`/`Guaranteed` QoS Pod）\n被禁止使用交换内存。为了保持上述的安全性和节点健康性保证，\n在 `LimitedSwap` 生效时，不允许这些 Pod 使用交换内存。"}
{"en": "Prior to detailing the calculation of the swap limit, it is necessary to define the following terms:\n* `nodeTotalMemory`: The total amount of physical memory available on the node.\n* `totalPodsSwapAvailable`: The total amount of swap memory on the node that is available for use by Pods (some swap memory may be reserved for system use).\n* `containerMemoryRequest`: The container's memory request.", "zh": "在详细介绍交换限制的计算之前，有必要定义以下术语：\n\n* `nodeTotalMemory`：节点上可用的物理内存总量。\n* `totalPodsSwapAvailable`：节点上可供 Pod 使用的交换内存总量\n  （一些交换内存可能被保留由系统使用）。\n* `containerMemoryRequest`：容器的内存请求。"}
{"en": "Swap limitation is configured as:\n`(containerMemoryRequest / nodeTotalMemory) * totalPodsSwapAvailable`.\n\nIt is important to note that, for containers within Burstable QoS Pods, it is possible to\nopt-out of swap usage by specifying memory requests that are equal to memory limits.\nContainers configured in this manner will not have access to swap memory.", "zh": "交换内存限制被配置为 `(containerMemoryRequest / nodeTotalMemory) * totalPodsSwapAvailable` 的值。\n\n需要注意的是，位于 Burstable QoS Pod 中的容器可以通过将内存请求设置为与内存限制相同来选择不使用交换内存。\n以这种方式配置的容器将无法访问交换内存。"}
{"en": "Swap is supported only with **cgroup v2**, cgroup v1 is not supported. \n\nFor more information, and to assist with testing and provide feedback, please\nsee the blog-post about [Kubernetes 1.28: NodeSwap graduates to Beta1](/blog/2023/08/24/swap-linux-beta/),\n[KEP-2400](https://github.com/kubernetes/enhancements/issues/4128) and its\n[design proposal](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md).", "zh": "只有 **Cgroup v2** 支持交换内存，Cgroup v1 不支持。\n\n如需了解更多信息、协助测试和提交反馈，请参阅关于\n[Kubernetes 1.28：NodeSwap 进阶至 Beta1](/zh-cn/blog/2023/08/24/swap-linux-beta/) 的博客文章、\n[KEP-2400](https://github.com/kubernetes/enhancements/issues/4128)\n及其[设计提案](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md)。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "Learn more about the following:\n* [Components](/docs/concepts/architecture/#node-components) that make up a node.\n* [API definition for Node](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#node-v1-core).\n* [Node](https://git.k8s.io/design-proposals-archive/architecture/architecture.md#the-kubernetes-node) section of the architecture design document.\n* [Cluster autoscaling](/docs/concepts/cluster-administration/cluster-autoscaling/) to\n  manage the number and size of nodes in your cluster.\n* [Taints and Tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration/).\n* [Node Resource Managers](/docs/concepts/policy/node-resource-managers/).\n* [Resource Management for Windows nodes](/docs/concepts/configuration/windows-resource-management/).", "zh": "进一步了解以下资料：\n\n* 构成节点的[组件](/zh-cn/docs/concepts/architecture/#node-components) 。\n* [Node 的 API 定义](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#node-v1-core)。\n* 架构设计文档中有关\n  [Node](https://git.k8s.io/design-proposals-archive/architecture/architecture.md#the-kubernetes-node)\n  的章节。\n* [集群自动扩缩](https://git.k8s.io/design-proposals-archive/architecture/architecture.md#the-kubernetes-node)\n  以管理集群中节点的数量和规模。\n* [污点和容忍度](/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/)。\n* [节点资源管理器](/zh-cn/docs/concepts/policy/node-resource-managers/)。\n* [Windows 节点的资源管理](/zh-cn/docs/concepts/configuration/windows-resource-management/)。"}
{"en": "In robotics and automation, a _control loop_ is\na non-terminating loop that regulates the state of a system.\n\nHere is one example of a control loop: a thermostat in a room.\n\nWhen you set the temperature, that's telling the thermostat\nabout your *desired state*. The actual room temperature is the\n*current state*. The thermostat acts to bring the current state\ncloser to the desired state, by turning equipment on or off.", "zh": "在机器人技术和自动化领域，控制回路（Control Loop）是一个非终止回路，用于调节系统状态。\n\n这是一个控制环的例子：房间里的温度自动调节器。\n\n当你设置了温度，告诉了温度自动调节器你的**期望状态（Desired State）**。\n房间的实际温度是**当前状态（Current State）**。\n通过对设备的开关控制，温度自动调节器让其当前状态接近期望状态。\n\n{{< glossary_definition term_id=\"controller\" length=\"short\">}}"}
{"en": "## Controller pattern\n\nA controller tracks at least one Kubernetes resource type.\nThese {{< glossary_tooltip text=\"objects\" term_id=\"object\" >}}\nhave a spec field that represents the desired state. The\ncontroller(s) for that resource are responsible for making the current\nstate come closer to that desired state.\n\nThe controller might carry the action out itself; more commonly, in Kubernetes,\na controller will send messages to the\n{{< glossary_tooltip text=\"API server\" term_id=\"kube-apiserver\" >}} that have\nuseful side effects. You'll see examples of this below.\n\n{{< comment >}}\nSome built-in controllers, such as the namespace controller, act on objects\nthat do not have a spec. For simplicity, this page omits explaining that\ndetail.\n{{< /comment >}}", "zh": "## 控制器模式 {#controller-pattern}\n\n一个控制器至少追踪一种类型的 Kubernetes 资源。这些\n{{< glossary_tooltip text=\"对象\" term_id=\"object\" >}}\n有一个代表期望状态的 `spec` 字段。\n该资源的控制器负责确保其当前状态接近期望状态。\n\n控制器可能会自行执行操作；在 Kubernetes 中更常见的是一个控制器会发送信息给\n{{< glossary_tooltip text=\"API 服务器\" term_id=\"kube-apiserver\" >}}，这会有副作用。\n具体可参看后文的例子。\n\n{{< comment >}}\n一些内置的控制器，比如名字空间控制器，针对没有指定 `spec` 的对象。\n为了简单起见，本文没有详细介绍这些细节。\n{{< /comment >}}"}
{"en": "### Control via API server\n\nThe {{< glossary_tooltip term_id=\"job\" >}} controller is an example of a\nKubernetes built-in controller. Built-in controllers manage state by\ninteracting with the cluster API server.\n\nJob is a Kubernetes resource that runs a\n{{< glossary_tooltip term_id=\"pod\" >}}, or perhaps several Pods, to carry out\na task and then stop.\n\n(Once [scheduled](/docs/concepts/scheduling-eviction/), Pod objects become part of the\ndesired state for a kubelet).\n\nWhen the Job controller sees a new task it makes sure that, somewhere\nin your cluster, the kubelets on a set of Nodes are running the right\nnumber of Pods to get the work done.\nThe Job controller does not run any Pods or containers\nitself. Instead, the Job controller tells the API server to create or remove\nPods.\nOther components in the\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}}\nact on the new information (there are new Pods to schedule and run),\nand eventually the work is done.", "zh": "### 通过 API 服务器来控制 {#control-via-API-server}\n\n{{< glossary_tooltip text=\"Job\" term_id=\"job\" >}} 控制器是一个 Kubernetes 内置控制器的例子。\n内置控制器通过和集群 API 服务器交互来管理状态。\n\nJob 是一种 Kubernetes 资源，它运行一个或者多个 {{< glossary_tooltip term_id=\"pod\" >}}，\n来执行一个任务然后停止。\n（一旦[被调度了](/zh-cn/docs/concepts/scheduling-eviction/)，对 `kubelet` 来说 Pod\n对象就会变成期望状态的一部分）。\n\n在集群中，当 Job 控制器拿到新任务时，它会保证一组 Node 节点上的 `kubelet`\n可以运行正确数量的 Pod 来完成工作。\nJob 控制器不会自己运行任何的 Pod 或者容器。Job 控制器是通知 API 服务器来创建或者移除 Pod。\n{{< glossary_tooltip text=\"控制面\" term_id=\"control-plane\" >}}中的其它组件\n根据新的消息作出反应（调度并运行新 Pod）并且最终完成工作。"}
{"en": "After you create a new Job, the desired state is for that Job to be completed.\nThe Job controller makes the current state for that Job be nearer to your\ndesired state: creating Pods that do the work you wanted for that Job, so that\nthe Job is closer to completion.\n\nControllers also update the objects that configure them.\nFor example: once the work is done for a Job, the Job controller\nupdates that Job object to mark it `Finished`.\n\n(This is a bit like how some thermostats turn a light off to\nindicate that your room is now at the temperature you set).", "zh": "创建新 Job 后，所期望的状态就是完成这个 Job。Job 控制器会让 Job 的当前状态不断接近期望状态：创建为 Job 要完成工作所需要的 Pod，使 Job 的状态接近完成。\n\n控制器也会更新配置对象。例如：一旦 Job 的工作完成了，Job 控制器会更新 Job 对象的状态为 `Finished`。\n\n（这有点像温度自动调节器关闭了一个灯，以此来告诉你房间的温度现在到你设定的值了）。"}
{"en": "### Direct control\n\nIn contrast with Job, some controllers need to make changes to\nthings outside of your cluster.\n\nFor example, if you use a control loop to make sure there\nare enough {{< glossary_tooltip text=\"Nodes\" term_id=\"node\" >}}\nin your cluster, then that controller needs something outside the\ncurrent cluster to set up new Nodes when needed.\n\nControllers that interact with external state find their desired state from\nthe API server, then communicate directly with an external system to bring\nthe current state closer in line.\n\n(There actually is a [controller](https://github.com/kubernetes/autoscaler/)\nthat horizontally scales the nodes in your cluster.)", "zh": "### 直接控制 {#direct-control}\n\n相比 Job 控制器，有些控制器需要对集群外的一些东西进行修改。\n\n例如，如果你使用一个控制回路来保证集群中有足够的\n{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}，那么控制器就需要当前集群外的\n一些服务在需要时创建新节点。\n\n和外部状态交互的控制器从 API 服务器获取到它想要的状态，然后直接和外部系统进行通信\n并使当前状态更接近期望状态。\n\n（实际上有一个[控制器](https://github.com/kubernetes/autoscaler/)\n可以水平地扩展集群中的节点。）"}
{"en": "The important point here is that the controller makes some changes to bring about\nyour desired state, and then reports the current state back to your cluster's API server.\nOther control loops can observe that reported data and take their own actions.", "zh": "这里的重点是，控制器做出了一些变更以使得事物更接近你的期望状态，\n之后将当前状态报告给集群的 API 服务器。\n其他控制回路可以观测到所汇报的数据的这种变化并采取其各自的行动。"}
{"en": "In the thermostat example, if the room is very cold then a different controller\nmight also turn on a frost protection heater. With Kubernetes clusters, the control\nplane indirectly works with IP address management tools, storage services,\ncloud provider APIs, and other services by\n[extending Kubernetes](/docs/concepts/extend-kubernetes/) to implement that.", "zh": "在温度计的例子中，如果房间很冷，那么某个控制器可能还会启动一个防冻加热器。\n就 Kubernetes 集群而言，控制面间接地与 IP 地址管理工具、存储服务、云驱动\nAPIs 以及其他服务协作，通过[扩展 Kubernetes](/zh-cn/docs/concepts/extend-kubernetes/)\n来实现这点。"}
{"en": "## Desired versus current state {#desired-vs-current}\n\nKubernetes takes a cloud-native view of systems, and is able to handle\nconstant change.\n\nYour cluster could be changing at any point as work happens and\ncontrol loops automatically fix failures. This means that,\npotentially, your cluster never reaches a stable state.\n\nAs long as the controllers for your cluster are running and able to make\nuseful changes, it doesn't matter if the overall state is stable or not.", "zh": "## 期望状态与当前状态 {#desired-vs-current}\n\nKubernetes 采用了系统的云原生视图，并且可以处理持续的变化。\n\n在任务执行时，集群随时都可能被修改，并且控制回路会自动修复故障。\n这意味着很可能集群永远不会达到稳定状态。\n\n只要集群中的控制器在运行并且进行有效的修改，整体状态的稳定与否是无关紧要的。"}
{"en": "## Design\n\nAs a tenet of its design, Kubernetes uses lots of controllers that each manage\na particular aspect of cluster state. Most commonly, a particular control loop\n(controller) uses one kind of resource as its desired state, and has a different\nkind of resource that it manages to make that desired state happen. For example,\na controller for Jobs tracks Job objects (to discover new work) and Pod objects\n(to run the Jobs, and then to see when the work is finished). In this case\nsomething else creates the Jobs, whereas the Job controller creates Pods.\n\nIt's useful to have simple controllers rather than one, monolithic set of control\nloops that are interlinked. Controllers can fail, so Kubernetes is designed to\nallow for that.", "zh": "## 设计 {#design}\n\n作为设计原则之一，Kubernetes 使用了很多控制器，每个控制器管理集群状态的一个特定方面。\n最常见的一个特定的控制器使用一种类型的资源作为它的期望状态，\n控制器管理控制另外一种类型的资源向它的期望状态演化。\n例如，Job 的控制器跟踪 Job 对象（以发现新的任务）和 Pod 对象（以运行 Job，然后查看任务何时完成）。\n在这种情况下，新任务会创建 Job，而 Job 控制器会创建 Pod。\n\n使用简单的控制器而不是一组相互连接的单体控制回路是很有用的。\n控制器会失败，所以 Kubernetes 的设计正是考虑到了这一点。"}
{"en": "There can be several controllers that create or update the same kind of object.\nBehind the scenes, Kubernetes controllers make sure that they only pay attention\nto the resources linked to their controlling resource.\n\nFor example, you can have Deployments and Jobs; these both create Pods.\nThe Job controller does not delete the Pods that your Deployment created,\nbecause there is information ({{< glossary_tooltip term_id=\"label\" text=\"labels\" >}})\nthe controllers can use to tell those Pods apart.", "zh": "{{< note >}}\n可以有多个控制器来创建或者更新相同类型的对象。\n在后台，Kubernetes 控制器确保它们只关心与其控制资源相关联的资源。\n\n例如，你可以创建 Deployment 和 Job；它们都可以创建 Pod。\nJob 控制器不会删除 Deployment 所创建的 Pod，因为有信息\n（{{< glossary_tooltip term_id=\"label\" text=\"标签\" >}}）让控制器可以区分这些 Pod。\n{{< /note >}}"}
{"en": "## Ways of running controllers {#running-controllers}\n\nKubernetes comes with a set of built-in controllers that run inside\nthe {{< glossary_tooltip term_id=\"kube-controller-manager\" >}}. These\nbuilt-in controllers provide important core behaviors.\n\nThe Deployment controller and Job controller are examples of controllers that\ncome as part of Kubernetes itself (\"built-in\" controllers).\nKubernetes lets you run a resilient control plane, so that if any of the built-in\ncontrollers were to fail, another part of the control plane will take over the work.\n\nYou can find controllers that run outside the control plane, to extend Kubernetes.\nOr, if you want, you can write a new controller yourself.\nYou can run your own controller as a set of Pods,\nor externally to Kubernetes. What fits best will depend on what that particular\ncontroller does.", "zh": "## 运行控制器的方式 {#running-controllers}\n\nKubernetes 内置一组控制器，运行在 {{< glossary_tooltip term_id=\"kube-controller-manager\" >}} 内。\n这些内置的控制器提供了重要的核心功能。\n\nDeployment 控制器和 Job 控制器是 Kubernetes 内置控制器的典型例子。\nKubernetes 允许你运行一个稳定的控制平面，这样即使某些内置控制器失败了，\n控制平面的其他部分会接替它们的工作。\n\n你会遇到某些控制器运行在控制面之外，用以扩展 Kubernetes。\n或者，如果你愿意，你也可以自己编写新控制器。\n你可以以一组 Pod 来运行你的控制器，或者运行在 Kubernetes 之外。\n最合适的方案取决于控制器所要执行的功能是什么。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about the [Kubernetes control plane](/docs/concepts/architecture/#control-plane-components)\n* Discover some of the basic [Kubernetes objects](/docs/concepts/overview/working-with-objects/)\n* Learn more about the [Kubernetes API](/docs/concepts/overview/kubernetes-api/)\n* If you want to write your own controller, see\n  [Kubernetes extension patterns](/docs/concepts/extend-kubernetes/#extension-patterns)\n  and the [sample-controller](https://github.com/kubernetes/sample-controller) repository.", "zh": "* 阅读 [Kubernetes 控制平面组件](/zh-cn/docs/concepts/architecture/#control-plane-components)\n* 了解 [Kubernetes 对象](/zh-cn/docs/concepts/overview/working-with-objects/)\n  的一些基本知识\n* 进一步学习 [Kubernetes API](/zh-cn/docs/concepts/overview/kubernetes-api/)\n* 如果你想编写自己的控制器，请查看\n  [Kubernetes 扩展模式](/zh-cn/docs/concepts/extend-kubernetes/#extension-patterns)\n  以及[控制器样例](https://github.com/kubernetes/sample-controller)。"}
{"en": "{{<glossary_definition term_id=\"garbage-collection\" length=\"short\">}} This\nallows the clean up of resources like the following:", "zh": "{{<glossary_definition term_id=\"garbage-collection\" length=\"short\">}}\n垃圾收集允许系统清理如下资源："}
{"en": "* [Terminated pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)\n* [Completed Jobs](/docs/concepts/workloads/controllers/ttlafterfinished/)\n* [Objects without owner references](#owners-dependents)\n* [Unused containers and container images](#containers-images)\n* [Dynamically provisioned PersistentVolumes with a StorageClass reclaim policy of Delete](/docs/concepts/storage/persistent-volumes/#delete)\n* [Stale or expired CertificateSigningRequests (CSRs)](/docs/reference/access-authn-authz/certificate-signing-requests/#request-signing-process)\n* {{<glossary_tooltip text=\"Nodes\" term_id=\"node\">}} deleted in the following scenarios:\n  * On a cloud when the cluster uses a [cloud controller manager](/docs/concepts/architecture/cloud-controller/)\n  * On-premises when the cluster uses an addon similar to a cloud controller\n    manager\n* [Node Lease objects](/docs/concepts/architecture/nodes/#heartbeats)", "zh": "* [终止的 Pod](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)\n* [已完成的 Job](/zh-cn/docs/concepts/workloads/controllers/ttlafterfinished/)\n* [不再存在属主引用的对象](#owners-dependents)\n* [未使用的容器和容器镜像](#containers-images)\n* [动态制备的、StorageClass 回收策略为 Delete 的 PV 卷](/zh-cn/docs/concepts/storage/persistent-volumes/#delete)\n* [阻滞或者过期的 CertificateSigningRequest (CSR)](/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests/#request-signing-process)\n* 在以下情形中删除了的{{<glossary_tooltip text=\"节点\" term_id=\"node\">}}对象：\n  * 当集群使用[云控制器管理器](/zh-cn/docs/concepts/architecture/cloud-controller/)运行于云端时；\n  * 当集群使用类似于云控制器管理器的插件运行在本地环境中时。\n* [节点租约对象](/zh-cn/docs/concepts/architecture/nodes/#heartbeats)"}
{"en": "## Owners and dependents {#owners-dependents}\n\nMany objects in Kubernetes link to each other through [*owner references*](/docs/concepts/overview/working-with-objects/owners-dependents/).\nOwner references tell the control plane which objects are dependent on others.\nKubernetes uses owner references to give the control plane, and other API\nclients, the opportunity to clean up related resources before deleting an\nobject. In most cases, Kubernetes manages owner references automatically.", "zh": "## 属主与依赖   {#owners-dependents}\n\nKubernetes 中很多对象通过[**属主引用**](/zh-cn/docs/concepts/overview/working-with-objects/owners-dependents/)\n链接到彼此。属主引用（Owner Reference）可以告诉控制面哪些对象依赖于其他对象。\nKubernetes 使用属主引用来为控制面以及其他 API 客户端在删除某对象时提供一个清理关联资源的机会。\n在大多数场合，Kubernetes 都是自动管理属主引用的。"}
{"en": "Ownership is different from the [labels and selectors](/docs/concepts/overview/working-with-objects/labels/)\nmechanism that some resources also use. For example, consider a\n{{<glossary_tooltip text=\"Service\" term_id=\"service\">}} that creates\n`EndpointSlice` objects. The Service uses *labels* to allow the control plane to\ndetermine which `EndpointSlice` objects are used for that Service. In addition\nto the labels, each `EndpointSlice` that is managed on behalf of a Service has\nan owner reference. Owner references help different parts of Kubernetes avoid\ninterfering with objects they don’t control.", "zh": "属主关系与某些资源所使用的[标签和选择算符](/zh-cn/docs/concepts/overview/working-with-objects/labels/)不同。\n例如，考虑一个创建 `EndpointSlice` 对象的 {{<glossary_tooltip text=\"Service\" term_id=\"service\">}}。\nService 使用**标签**来允许控制面确定哪些 `EndpointSlice` 对象被该 Service 使用。\n除了标签，每个被 Service 托管的 `EndpointSlice` 对象还有一个属主引用属性。\n属主引用可以帮助 Kubernetes 中的不同组件避免干预并非由它们控制的对象。\n\n{{< note >}}"}
{"en": "Cross-namespace owner references are disallowed by design.\nNamespaced dependents can specify cluster-scoped or namespaced owners.\nA namespaced owner **must** exist in the same namespace as the dependent.\nIf it does not, the owner reference is treated as absent, and the dependent\nis subject to deletion once all owners are verified absent.", "zh": "根据设计，系统不允许出现跨名字空间的属主引用。名字空间作用域的依赖对象可以指定集群作用域或者名字空间作用域的属主。\n名字空间作用域的属主**必须**存在于依赖对象所在的同一名字空间。\n如果属主位于不同名字空间，则属主引用被视为不存在，而当检查发现所有属主都已不存在时，依赖对象会被删除。"}
{"en": "Cluster-scoped dependents can only specify cluster-scoped owners.\nIn v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner,\nit is treated as having an unresolvable owner reference, and is not able to be garbage collected.", "zh": "集群作用域的依赖对象只能指定集群作用域的属主。\n在 1.20 及更高版本中，如果一个集群作用域的依赖对象指定了某个名字空间作用域的类别作为其属主，\n则该对象被视为拥有一个无法解析的属主引用，因而无法被垃圾收集处理。"}
{"en": "In v1.20+, if the garbage collector detects an invalid cross-namespace `ownerReference`,\nor a cluster-scoped dependent with an `ownerReference` referencing a namespaced kind, a warning Event\nwith a reason of `OwnerRefInvalidNamespace` and an `involvedObject` of the invalid dependent is reported.\nYou can check for that kind of Event by running\n`kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace`.", "zh": "在 1.20 及更高版本中，如果垃圾收集器检测到非法的跨名字空间 `ownerReference`，\n或者某集群作用域的依赖对象的 `ownerReference` 引用某名字空间作用域的类别，\n系统会生成一个警告事件，其原因为 `OwnerRefInvalidNamespace` 和 `involvedObject`\n设置为非法的依赖对象。你可以通过运行\n`kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace`\n来检查是否存在这类事件。\n{{< /note >}}"}
{"en": "## Cascading deletion {#cascading-deletion}\n\nKubernetes checks for and deletes objects that no longer have owner\nreferences, like the pods left behind when you delete a ReplicaSet. When you\ndelete an object, you can control whether Kubernetes deletes the object's\ndependents automatically, in a process called *cascading deletion*. There are\ntwo types of cascading deletion, as follows:\n\n* Foreground cascading deletion\n* Background cascading deletion", "zh": "## 级联删除    {#cascading-deletion}\n\nKubernetes 会检查并删除那些不再拥有属主引用的对象，例如在你删除了 ReplicaSet\n之后留下来的 Pod。当你删除某个对象时，你可以控制 Kubernetes 是否去自动删除该对象的依赖对象，\n这个过程称为**级联删除（Cascading Deletion）**。\n级联删除有两种类型，分别如下：\n\n* 前台级联删除\n* 后台级联删除"}
{"en": "You can also control how and when garbage collection deletes resources that have\nowner references using Kubernetes {{<glossary_tooltip text=\"finalizers\" term_id=\"finalizer\">}}.", "zh": "你也可以使用 Kubernetes {{<glossary_tooltip text=\"Finalizers\" term_id=\"finalizer\">}}\n来控制垃圾收集机制如何以及何时删除包含属主引用的资源。"}
{"en": "### Foreground cascading deletion {#foreground-deletion}\n\nIn foreground cascading deletion, the owner object you're deleting first enters\na *deletion in progress* state. In this state, the following happens to the\nowner object:", "zh": "### 前台级联删除 {#foreground-deletion}\n\n在前台级联删除中，正在被你删除的属主对象首先进入 **deletion in progress** 状态。\n在这种状态下，针对属主对象会发生以下事情："}
{"en": "* The Kubernetes API server sets the object's `metadata.deletionTimestamp`\n  field to the time the object was marked for deletion.\n* The Kubernetes API server also sets the `metadata.finalizers` field to\n  `foregroundDeletion`. \n* The object remains visible through the Kubernetes API until the deletion\n  process is complete.", "zh": "* Kubernetes API 服务器将某对象的 `metadata.deletionTimestamp`\n  字段设置为该对象被标记为要删除的时间点。\n* Kubernetes API 服务器也会将 `metadata.finalizers` 字段设置为 `foregroundDeletion`。\n* 在删除过程完成之前，通过 Kubernetes API 仍然可以看到该对象。"}
{"en": "After the owner object enters the *deletion in progress* state, the controller\ndeletes dependents it knows about. After deleting all the dependent objects it knows about,\nthe controller deletes the owner object. At this point, the object is no longer visible in the\nKubernetes API.\n\nDuring foreground cascading deletion, the only dependents that block owner\ndeletion are those that have the `ownerReference.blockOwnerDeletion=true` field\nand are in the garbage collection controller cache. The garbage collection controller\ncache may not contain objects whose resource type cannot be listed / watched successfully,\nor objects that are created concurrent with deletion of an owner object.\nSee [Use foreground cascading deletion](/docs/tasks/administer-cluster/use-cascading-deletion/#use-foreground-cascading-deletion)\nto learn more.", "zh": "当属主对象进入**删除进行中**状态后，控制器会删除其已知的依赖对象。\n在删除所有已知的依赖对象后，控制器会删除属主对象。\n这时，通过 Kubernetes API 就无法再看到该对象。\n\n在前台级联删除过程中，唯一会阻止属主对象被删除的是那些带有\n`ownerReference.blockOwnerDeletion=true` 字段并且存在于垃圾收集控制器缓存中的依赖对象。\n垃圾收集控制器缓存中可能不包含那些无法成功被列举/监视的资源类型的对象，\n或在属主对象删除的同时创建的对象。\n参阅[使用前台级联删除](/zh-cn/docs/tasks/administer-cluster/use-cascading-deletion/#use-foreground-cascading-deletion)\n以了解进一步的细节。"}
{"en": "### Background cascading deletion {#background-deletion}\n\nIn background cascading deletion, the Kubernetes API server deletes the owner\nobject immediately and the garbage collector controller (custom or default)\ncleans up the dependent objects in the background.\nIf a finalizer exists, it ensures that objects are not deleted until all necessary clean-up tasks are completed.\nBy default, Kubernetes uses background cascading deletion unless\nyou manually use foreground deletion or choose to orphan the dependent objects.\n\nSee [Use background cascading deletion](/docs/tasks/administer-cluster/use-cascading-deletion/#use-background-cascading-deletion)\nto learn more.", "zh": "### 后台级联删除 {#background-deletion}\n\n在后台级联删除过程中，Kubernetes 服务器立即删除属主对象，\n而垃圾收集控制器（无论是自定义的还是默认的）在后台清理所有依赖对象。\n如果存在 Finalizers，它会确保所有必要的清理任务完成后对象才被删除。\n默认情况下，Kubernetes 使用后台级联删除方案，除非你手动设置了要使用前台删除，\n或者选择遗弃依赖对象。\n\n参阅[使用后台级联删除](/zh-cn/docs/tasks/administer-cluster/use-cascading-deletion/#use-background-cascading-deletion)以了解进一步的细节。"}
{"en": "### Orphaned dependents\n\nWhen Kubernetes deletes an owner object, the dependents left behind are called\n*orphan* objects. By default, Kubernetes deletes dependent objects. To learn how\nto override this behaviour, see [Delete owner objects and orphan dependents](/docs/tasks/administer-cluster/use-cascading-deletion/#set-orphan-deletion-policy).", "zh": "### 被遗弃的依赖对象    {#orphaned-dependents}\n\n当 Kubernetes 删除某个属主对象时，被留下来的依赖对象被称作被遗弃的（Orphaned）对象。\n默认情况下，Kubernetes 会删除依赖对象。要了解如何重载这种默认行为，\n可参阅[删除属主对象和遗弃依赖对象](/zh-cn/docs/tasks/administer-cluster/use-cascading-deletion/#set-orphan-deletion-policy)。"}
{"en": "## Garbage collection of unused containers and images {#containers-images}\n\nThe {{<glossary_tooltip text=\"kubelet\" term_id=\"kubelet\">}} performs garbage\ncollection on unused images every two minutes and on unused containers every\nminute. You should avoid using external garbage collection tools, as these can\nbreak the kubelet behavior and remove containers that should exist.", "zh": "## 未使用容器和镜像的垃圾收集     {#containers-images}\n\n{{<glossary_tooltip text=\"kubelet\" term_id=\"kubelet\">}} 会每两分钟对未使用的镜像执行一次垃圾收集，\n每分钟对未使用的容器执行一次垃圾收集。\n你应该避免使用外部的垃圾收集工具，因为外部工具可能会破坏 kubelet\n的行为，移除应该保留的容器。"}
{"en": "To configure options for unused container and image garbage collection, tune the\nkubelet using a [configuration file](/docs/tasks/administer-cluster/kubelet-config-file/)\nand change the parameters related to garbage collection using the\n[`KubeletConfiguration`](/docs/reference/config-api/kubelet-config.v1beta1/)\nresource type.", "zh": "要配置对未使用容器和镜像的垃圾收集选项，\n可以使用一个[配置文件](/zh-cn/docs/tasks/administer-cluster/kubelet-config-file/)，基于\n[`KubeletConfiguration`](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/)\n资源类型来调整与垃圾收集相关的 kubelet 行为。"}
{"en": "### Container image lifecycle\n\nKubernetes manages the lifecycle of all images through its *image manager*,\nwhich is part of the kubelet, with the cooperation of\n{{< glossary_tooltip text=\"cadvisor\" term_id=\"cadvisor\" >}}. The kubelet\nconsiders the following disk usage limits when making garbage collection\ndecisions:", "zh": "### 容器镜像生命周期     {#container-image-lifecycle}\n\nKubernetes 通过其**镜像管理器（Image Manager）** 来管理所有镜像的生命周期，\n该管理器是 kubelet 的一部分，工作时与\n{{< glossary_tooltip text=\"cadvisor\" term_id=\"cadvisor\" >}} 协同。\nkubelet 在作出垃圾收集决定时会考虑如下磁盘用量约束：\n\n* `HighThresholdPercent`\n* `LowThresholdPercent`"}
{"en": "Disk usage above the configured `HighThresholdPercent` value triggers garbage\ncollection, which deletes images in order based on the last time they were used,\nstarting with the oldest first. The kubelet deletes images\nuntil disk usage reaches the `LowThresholdPercent` value.", "zh": "磁盘用量超出所配置的 `HighThresholdPercent` 值时会触发垃圾收集，\n垃圾收集器会基于镜像上次被使用的时间来按顺序删除它们，首先删除的是最近未使用的镜像。\nkubelet 会持续删除镜像，直到磁盘用量到达 `LowThresholdPercent` 值为止。"}
{"en": "#### Garbage collection for unused container images {#image-maximum-age-gc}", "zh": "#### 未使用容器镜像的垃圾收集     {#image-maximum-age-gc}\n\n{{< feature-state feature_gate_name=\"ImageMaximumGCAge\" >}}"}
{"en": "As an beta feature, you can specify the maximum time a local image can be unused for,\nregardless of disk usage. This is a kubelet setting that you configure for each node.", "zh": "这是一个 Beta 特性，不论磁盘使用情况如何，你都可以指定本地镜像未被使用的最长时间。\n这是一个可以为每个节点配置的 kubelet 设置。"}
{"en": "To configure the setting, you need to set a value for the `imageMaximumGCAge` \nfield in the kubelet configuration file.", "zh": "要配置该设置，你需要在 kubelet 配置文件中为 `imageMaximumGCAge`\n字段设置一个值。"}
{"en": "The value is specified as a Kubernetes _duration_; \nValid time units for the `imageMaximumGCAge` field in the kubelet configuration file are:\n- \"ns\" for nanoseconds\n- \"us\" or \"µs\" for microseconds\n- \"ms\" for milliseconds\n- \"s\" for seconds\n- \"m\" for minutes\n- \"h\" for hours", "zh": "该值应遵循 Kubernetes **持续时间（Duration）** 格式；\n在 kubelet 配置文件中，`imageMaximumGCAge` 字段的有效时间单位如下：\n\n- \"ns\" 表示纳秒\n- \"us\" 或 \"µs\" 表示微秒\n- \"ms\" 表示毫秒\n- \"s\" 表示秒\n- \"m\" 表示分钟\n- \"h\" 表示小时"}
{"en": "For example, you can set the configuration field to `12h45m`,\nwhich means 12 hours and 45 minutes.", "zh": "例如，你可以将配置字段设置为 `12h45m`，代表 12 小时 45 分钟。\n\n{{< note >}}"}
{"en": "This feature does not track image usage across kubelet restarts. If the kubelet\nis restarted, the tracked image age is reset, causing the kubelet to wait the full\n`imageMaximumGCAge` duration before qualifying images for garbage collection\nbased on image age.", "zh": "这个特性不会跟踪 kubelet 重新启动后的镜像使用情况。\n如果 kubelet 被重新启动，所跟踪的镜像年龄会被重置，\n导致 kubelet 在根据镜像年龄进行垃圾收集时需要等待完整的\n`imageMaximumGCAge` 时长。\n{{< /note>}}"}
{"en": "### Container garbage collection {#container-image-garbage-collection}\n\nThe kubelet garbage collects unused containers based on the following variables,\nwhich you can define:", "zh": "### 容器垃圾收集    {#container-image-garbage-collection}\n\nkubelet 会基于如下变量对所有未使用的容器执行垃圾收集操作，这些变量都是你可以定义的："}
{"en": "* `MinAge`: the minimum age at which the kubelet can garbage collect a\n  container. Disable by setting to `0`.\n* `MaxPerPodContainer`: the maximum number of dead containers each Pod\n  can have. Disable by setting to less than `0`.\n* `MaxContainers`: the maximum number of dead containers the cluster can have.\n  Disable by setting to less than `0`.", "zh": "* `MinAge`：kubelet 可以垃圾回收某个容器时该容器的最小年龄。设置为 `0`\n  表示禁止使用此规则。\n* `MaxPerPodContainer`：每个 Pod 可以包含的已死亡的容器个数上限。设置为小于 `0`\n  的值表示禁止使用此规则。\n* `MaxContainers`：集群中可以存在的已死亡的容器个数上限。设置为小于 `0`\n  的值意味着禁止应用此规则。"}
{"en": "In addition to these variables, the kubelet garbage collects unidentified and\ndeleted containers, typically starting with the oldest first.\n\n`MaxPerPodContainer` and `MaxContainers` may potentially conflict with each other\nin situations where retaining the maximum number of containers per Pod\n(`MaxPerPodContainer`) would go outside the allowable total of global dead\ncontainers (`MaxContainers`). In this situation, the kubelet adjusts\n`MaxPerPodContainer` to address the conflict. A worst-case scenario would be to\ndowngrade `MaxPerPodContainer` to `1` and evict the oldest containers.\nAdditionally, containers owned by pods that have been deleted are removed once\nthey are older than `MinAge`.", "zh": "除以上变量之外，kubelet 还会垃圾收集除无标识的以及已删除的容器，通常从最近未使用的容器开始。\n\n当保持每个 Pod 的最大数量的容器（`MaxPerPodContainer`）会使得全局的已死亡容器个数超出上限\n（`MaxContainers`）时，`MaxPerPodContainer` 和 `MaxContainers` 之间可能会出现冲突。\n在这种情况下，kubelet 会调整 `MaxPerPodContainer` 来解决这一冲突。\n最坏的情形是将 `MaxPerPodContainer` 降格为 `1`，并驱逐最近未使用的容器。\n此外，当隶属于某已被删除的 Pod 的容器的年龄超过 `MinAge` 时，它们也会被删除。\n\n{{<note>}}"}
{"en": "The kubelet only garbage collects the containers it manages.", "zh": "kubelet 仅会回收由它所管理的容器。\n{{</note>}}"}
{"en": "## Configuring garbage collection {#configuring-gc}\n\nYou can tune garbage collection of resources by configuring options specific to\nthe controllers managing those resources. The following pages show you how to\nconfigure garbage collection:\n\n* [Configuring cascading deletion of Kubernetes objects](/docs/tasks/administer-cluster/use-cascading-deletion/)\n* [Configuring cleanup of finished Jobs](/docs/concepts/workloads/controllers/ttlafterfinished/)", "zh": "## 配置垃圾收集     {#configuring-gc}\n\n你可以通过配置特定于管理资源的控制器来调整资源的垃圾收集行为。\n下面的页面为你展示如何配置垃圾收集：\n\n* [配置 Kubernetes 对象的级联删除](/zh-cn/docs/tasks/administer-cluster/use-cascading-deletion/)\n* [配置已完成 Job 的清理](/zh-cn/docs/concepts/workloads/controllers/ttlafterfinished/)\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn more about [ownership of Kubernetes objects](/docs/concepts/overview/working-with-objects/owners-dependents/).\n* Learn more about Kubernetes [finalizers](/docs/concepts/overview/working-with-objects/finalizers/).\n* Learn about the [TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) that cleans up finished Jobs.", "zh": "* 进一步了解 [Kubernetes 对象的属主关系](/zh-cn/docs/concepts/overview/working-with-objects/owners-dependents/)。\n* 进一步了解 Kubernetes [finalizers](/zh-cn/docs/concepts/overview/working-with-objects/finalizers/)。\n* 进一步了解 [TTL 控制器](/zh-cn/docs/concepts/workloads/controllers/ttlafterfinished/)，\n  该控制器负责清理已完成的 Job。"}
{"en": "Distributed systems often have a need for _leases_, which provide a mechanism to lock shared resources\nand coordinate activity between members of a set.\nIn Kubernetes, the lease concept is represented by [Lease](/docs/reference/kubernetes-api/cluster-resources/lease-v1/)\nobjects in the `coordination.k8s.io` {{< glossary_tooltip text=\"API Group\" term_id=\"api-group\" >}},\nwhich are used for system-critical capabilities such as node heartbeats and component-level leader election.", "zh": "分布式系统通常需要**租约（Lease）**；租约提供了一种机制来锁定共享资源并协调集合成员之间的活动。\n在 Kubernetes 中，租约概念表示为 `coordination.k8s.io`\n{{< glossary_tooltip text=\"API 组\" term_id=\"api-group\" >}}中的\n[Lease](/zh-cn/docs/reference/kubernetes-api/cluster-resources/lease-v1/) 对象，\n常用于类似节点心跳和组件级领导者选举等系统核心能力。"}
{"en": "## Node heartbeats {#node-heart-beats}\n\nKubernetes uses the Lease API to communicate kubelet node heartbeats to the Kubernetes API server.\nFor every `Node` , there is a `Lease` object with a matching name in the `kube-node-lease`\nnamespace. Under the hood, every kubelet heartbeat is an **update** request to this `Lease` object, updating\nthe `spec.renewTime` field for the Lease. The Kubernetes control plane uses the time stamp of this field\nto determine the availability of this `Node`.\n\nSee [Node Lease objects](/docs/concepts/architecture/nodes/#node-heartbeats) for more details.", "zh": "## 节点心跳  {#node-heart-beats}\n\nKubernetes 使用 Lease API 将 kubelet 节点心跳传递到 Kubernetes API 服务器。\n对于每个 `Node`，在 `kube-node-lease` 名字空间中都有一个具有匹配名称的 `Lease` 对象。\n在此基础上，每个 kubelet 心跳都是对该 `Lease` 对象的 **update** 请求，更新该 Lease 的 `spec.renewTime` 字段。\nKubernetes 控制平面使用此字段的时间戳来确定此 `Node` 的可用性。\n\n更多细节请参阅 [Node Lease 对象](/zh-cn/docs/concepts/architecture/nodes/#node-heartbeats)。"}
{"en": "## Leader election\n\nKubernetes also uses Leases to ensure only one instance of a component is running at any given time.\nThis is used by control plane components like `kube-controller-manager` and `kube-scheduler` in\nHA configurations, where only one instance of the component should be actively running while the other\ninstances are on stand-by.", "zh": "## 领导者选举  {#leader-election}\n\nKubernetes 也使用 Lease 确保在任何给定时间某个组件只有一个实例在运行。\n这在高可用配置中由 `kube-controller-manager` 和 `kube-scheduler` 等控制平面组件进行使用，\n这些组件只应有一个实例激活运行，而其他实例待机。"}
{"en": "Read [coordinated leader election](/docs/concepts/cluster-administration/coordinated-leader-election)\nto learn about how Kubernetes builds on the Lease API to select which component instance\nacts as leader.", "zh": "参阅[协调领导者选举](/zh-cn/docs/concepts/cluster-administration/coordinated-leader-election)以了解\nKubernetes 如何基于 Lease API 来选择哪个组件实例充当领导者。"}
{"en": "## API server identity", "zh": "## API 服务器身份   {#api-server-identity}\n\n{{< feature-state feature_gate_name=\"APIServerIdentity\" >}}"}
{"en": "Starting in Kubernetes v1.26, each `kube-apiserver` uses the Lease API to publish its identity to the\nrest of the system. While not particularly useful on its own, this provides a mechanism for clients to\ndiscover how many instances of `kube-apiserver` are operating the Kubernetes control plane.\nExistence of kube-apiserver leases enables future capabilities that may require coordination between\neach kube-apiserver.\n\nYou can inspect Leases owned by each kube-apiserver by checking for lease objects in the `kube-system` namespace\nwith the name `kube-apiserver-<sha256-hash>`. Alternatively you can use the label selector `apiserver.kubernetes.io/identity=kube-apiserver`:", "zh": "从 Kubernetes v1.26 开始，每个 `kube-apiserver` 都使用 Lease API 将其身份发布到系统中的其他位置。\n虽然它本身并不是特别有用，但为客户端提供了一种机制来发现有多少个 `kube-apiserver` 实例正在操作\nKubernetes 控制平面。kube-apiserver 租约的存在使得未来可以在各个 kube-apiserver 之间协调新的能力。\n\n你可以检查 `kube-system` 名字空间中名为 `kube-apiserver-<sha256-hash>` 的 Lease 对象来查看每个\nkube-apiserver 拥有的租约。你还可以使用标签选择算符 `apiserver.kubernetes.io/identity=kube-apiserver`：\n\n```shell\nkubectl -n kube-system get lease -l apiserver.kubernetes.io/identity=kube-apiserver\n```\n\n```\nNAME                                        HOLDER                                                                           AGE\napiserver-07a5ea9b9b072c4a5f3d1c3702        apiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05        5m33s\napiserver-7be9e061c59d368b3ddaf1376e        apiserver-7be9e061c59d368b3ddaf1376e_84f2a85d-37c1-4b14-b6b9-603e62e4896f        4m23s\napiserver-1dfef752bcb36637d2763d1868        apiserver-1dfef752bcb36637d2763d1868_c5ffa286-8a9a-45d4-91e7-61118ed58d2e        4m43s\n```"}
{"en": "The SHA256 hash used in the lease name is based on the OS hostname as seen by that API server. Each kube-apiserver should be\nconfigured to use a hostname that is unique within the cluster. New instances of kube-apiserver that use the same hostname\nwill take over existing Leases using a new holder identity, as opposed to instantiating new Lease objects. You can check the\nhostname used by kube-apisever by checking the value of the `kubernetes.io/hostname` label:", "zh": "租约名称中使用的 SHA256 哈希基于 API 服务器所看到的操作系统主机名生成。\n每个 kube-apiserver 都应该被配置为使用集群中唯一的主机名。\n使用相同主机名的 kube-apiserver 新实例将使用新的持有者身份接管现有 Lease，而不是实例化新的 Lease 对象。\n你可以通过检查 `kubernetes.io/hostname` 标签的值来查看 kube-apisever 所使用的主机名：\n\n```shell\nkubectl -n kube-system get lease apiserver-07a5ea9b9b072c4a5f3d1c3702 -o yaml\n```\n\n```yaml\napiVersion: coordination.k8s.io/v1\nkind: Lease\nmetadata:\n  creationTimestamp: \"2023-07-02T13:16:48Z\"\n  labels:\n    apiserver.kubernetes.io/identity: kube-apiserver\n    kubernetes.io/hostname: master-1\n  name: apiserver-07a5ea9b9b072c4a5f3d1c3702\n  namespace: kube-system\n  resourceVersion: \"334899\"\n  uid: 90870ab5-1ba9-4523-b215-e4d4e662acb1\nspec:\n  holderIdentity: apiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05\n  leaseDurationSeconds: 3600\n  renewTime: \"2023-07-04T21:58:48.065888Z\"\n```"}
{"en": "Expired leases from kube-apiservers that no longer exist are garbage collected by new kube-apiservers after 1 hour.\n\nYou can disable API server identity leases by disabling the `APIServerIdentity`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/).", "zh": "kube-apiserver 中不再存续的已到期租约将在到期 1 小时后被新的 kube-apiserver 作为垃圾收集。\n\n你可以通过禁用 `APIServerIdentity`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)来禁用 API 服务器身份租约。"}
{"en": "## Workloads {#custom-workload}\n\nYour own workload can define its own use of Leases. For example, you might run a custom\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}} where a primary or leader member\nperforms operations that its peers do not. You define a Lease so that the controller replicas can select\nor elect a leader, using the Kubernetes API for coordination.\nIf you do use a Lease, it's a good practice to define a name for the Lease that is obviously linked to\nthe product or component. For example, if you have a component named Example Foo, use a Lease named\n`example-foo`.", "zh": "## 工作负载    {#custom-workload}\n\n你自己的工作负载可以定义自己使用的 Lease。例如，\n你可以运行自定义的{{< glossary_tooltip term_id=\"controller\" text=\"控制器\" >}}，\n让主要成员或领导者成员在其中执行其对等方未执行的操作。\n你定义一个 Lease，以便控制器副本可以使用 Kubernetes API 进行协调以选择或选举一个领导者。\n如果你使用 Lease，良好的做法是为明显关联到产品或组件的 Lease 定义一个名称。\n例如，如果你有一个名为 Example Foo 的组件，可以使用名为 `example-foo` 的 Lease。"}
{"en": "If a cluster operator or another end user could deploy multiple instances of a component, select a name\nprefix and pick a mechanism (such as hash of the name of the Deployment) to avoid name collisions\nfor the Leases.\n\nYou can use another approach so long as it achieves the same outcome: different software products do\nnot conflict with one another.", "zh": "如果集群操作员或其他终端用户可以部署一个组件的多个实例，\n则选择名称前缀并挑选一种机制（例如 Deployment 名称的哈希）以避免 Lease 的名称冲突。\n\n你可以使用另一种方式来达到相同的效果：不同的软件产品不相互冲突。"}
{"en": "On Linux, {{< glossary_tooltip text=\"control groups\" term_id=\"cgroup\" >}}\nconstrain resources that are allocated to processes.\n\nThe {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} and the\nunderlying container runtime need to interface with cgroups to enforce\n[resource management for pods and containers](/docs/concepts/configuration/manage-resources-containers/) which\nincludes cpu/memory requests and limits for containerized workloads.\n\nThere are two versions of cgroups in Linux: cgroup v1 and cgroup v2. cgroup v2 is\nthe new generation of the `cgroup` API.", "zh": "在 Linux 上，{{< glossary_tooltip text=\"控制组\" term_id=\"cgroup\" >}}约束分配给进程的资源。\n\n{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} 和底层容器运行时都需要对接 cgroup\n来强制执行[为 Pod 和容器管理资源](/zh-cn/docs/concepts/configuration/manage-resources-containers/)，\n这包括为容器化工作负载配置 CPU/内存请求和限制。\n\nLinux 中有两个 cgroup 版本：cgroup v1 和 cgroup v2。cgroup v2 是新一代的 `cgroup` API。"}
{"en": "## What is cgroup v2? {#cgroup-v2}", "zh": "## 什么是 cgroup v2？  {#cgroup-v2}\n\n{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}"}
{"en": "cgroup v2 is the next version of the Linux `cgroup` API. cgroup v2 provides a\nunified control system with enhanced resource management\ncapabilities.", "zh": "cgroup v2 是 Linux `cgroup` API 的下一个版本。cgroup v2 提供了一个具有增强资源管理能力的统一控制系统。"}
{"en": "cgroup v2 offers several improvements over cgroup v1, such as the following:\n\n- Single unified hierarchy design in API\n- Safer sub-tree delegation to containers\n- Newer features like [Pressure Stall Information](https://www.kernel.org/doc/html/latest/accounting/psi.html)\n- Enhanced resource allocation management and isolation across multiple resources\n  - Unified accounting for different types of memory allocations (network memory, kernel memory, etc)\n  - Accounting for non-immediate resource changes such as page cache write backs", "zh": "cgroup v2 对 cgroup v1 进行了多项改进，例如：\n\n- API 中单个统一的层次结构设计\n- 更安全的子树委派给容器\n- 更新的功能特性，\n  例如[压力阻塞信息（Pressure Stall Information，PSI）](https://www.kernel.org/doc/html/latest/accounting/psi.html)\n- 跨多个资源的增强资源分配管理和隔离\n  - 统一核算不同类型的内存分配（网络内存、内核内存等）\n  - 考虑非即时资源变化，例如页面缓存回写"}
{"en": "Some Kubernetes features exclusively use cgroup v2 for enhanced resource\nmanagement and isolation. For example, the\n[MemoryQoS](/docs/concepts/workloads/pods/pod-qos/#memory-qos-with-cgroup-v2) feature improves memory QoS\nand relies on cgroup v2 primitives.", "zh": "一些 Kubernetes 特性专门使用 cgroup v2 来增强资源管理和隔离。\n例如，[MemoryQoS](/zh-cn/docs/concepts/workloads/pods/pod-qos/#memory-qos-with-cgroup-v2) 特性改进了内存 QoS 并依赖于 cgroup v2 原语。"}
{"en": "## Using cgroup v2 {#using-cgroupv2}\n\nThe recommended way to use cgroup v2 is to use a Linux distribution that\nenables and uses cgroup v2 by default.\n\nTo check if your distribution uses cgroup v2, refer to [Identify cgroup version on Linux nodes](#check-cgroup-version).", "zh": "## 使用 cgroup v2  {#using-cgroupv2}\n\n使用 cgroup v2 的推荐方法是使用一个默认启用 cgroup v2 的 Linux 发行版。\n\n要检查你的发行版是否使用 cgroup v2，请参阅[识别 Linux 节点上的 cgroup 版本](#check-cgroup-version)。"}
{"en": "### Requirements\n\ncgroup v2 has the following requirements:\n\n* OS distribution enables cgroup v2\n* Linux Kernel version is 5.8 or later\n* Container runtime supports cgroup v2. For example:\n  * [containerd](https://containerd.io/) v1.4 and later\n  * [cri-o](https://cri-o.io/) v1.20 and later\n* The kubelet and the container runtime are configured to use the [systemd cgroup driver](/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver)", "zh": "### 要求  {#requirements}\n\ncgroup v2 具有以下要求：\n\n* 操作系统发行版启用 cgroup v2\n* Linux 内核为 5.8 或更高版本\n* 容器运行时支持 cgroup v2。例如：\n  * [containerd](https://containerd.io/) v1.4 和更高版本\n  * [cri-o](https://cri-o.io/) v1.20 和更高版本\n* kubelet 和容器运行时被配置为使用\n  [systemd cgroup 驱动](/zh-cn/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver)"}
{"en": "### Linux Distribution cgroup v2 support\n\nFor a list of Linux distributions that use cgroup v2, refer to the [cgroup v2 documentation](https://github.com/opencontainers/runc/blob/main/docs/cgroup-v2.md)", "zh": "### Linux 发行版 cgroup v2 支持  {#linux-distribution-cgroup-v2-support}\n\n有关使用 cgroup v2 的 Linux 发行版的列表，\n请参阅 [cgroup v2 文档](https://github.com/opencontainers/runc/blob/main/docs/cgroup-v2.md)。"}
{"en": "* Container Optimized OS (since M97)\n* Ubuntu (since 21.10, 22.04+ recommended)\n* Debian GNU/Linux (since Debian 11 bullseye)\n* Fedora (since 31)\n* Arch Linux (since April 2021)\n* RHEL and RHEL-like distributions (since 9)", "zh": "* Container-Optimized OS（从 M97 开始）\n* Ubuntu（从 21.10 开始，推荐 22.04+）\n* Debian GNU/Linux（从 Debian 11 Bullseye 开始）\n* Fedora（从 31 开始）\n* Arch Linux（从 2021 年 4 月开始）\n* RHEL 和类似 RHEL 的发行版（从 9 开始）"}
{"en": "To check if your distribution is using cgroup v2, refer to your distribution's\ndocumentation or follow the instructions in [Identify the cgroup version on Linux nodes](#check-cgroup-version).\n\nYou can also enable cgroup v2 manually on your Linux distribution by modifying\nthe kernel cmdline boot arguments. If your distribution uses GRUB,\n`systemd.unified_cgroup_hierarchy=1` should be added in `GRUB_CMDLINE_LINUX`\nunder `/etc/default/grub`, followed by `sudo update-grub`.  However, the\nrecommended approach is to use a distribution that already enables cgroup v2 by\ndefault.", "zh": "要检查你的发行版是否使用 cgroup v2，\n请参阅你的发行版文档或遵循[识别 Linux 节点上的 cgroup 版本](#check-cgroup-version)中的指示说明。\n\n你还可以通过修改内核 cmdline 引导参数在你的 Linux 发行版上手动启用 cgroup v2。\n如果你的发行版使用 GRUB，则应在 `/etc/default/grub` 下的 `GRUB_CMDLINE_LINUX`\n中添加 `systemd.unified_cgroup_hierarchy=1`，\n然后执行 `sudo update-grub`。不过，推荐的方法仍是使用一个默认已启用 cgroup v2 的发行版。"}
{"en": "### Migrating to cgroup v2 {#migrating-cgroupv2}\n\nTo migrate to cgroup v2, ensure that you meet the [requirements](#requirements), then upgrade\nto a kernel version that enables cgroup v2 by default.\n\nThe kubelet automatically detects that the OS is running on cgroup v2 and\nperforms accordingly with no additional configuration required.", "zh": "### 迁移到 cgroup v2   {#migrating-cgroupv2}\n\n要迁移到 cgroup v2，需确保满足[要求](#requirements)，然后升级到一个默认启用 cgroup v2 的内核版本。\n\nkubelet 能够自动检测操作系统是否运行在 cgroup v2 上并相应调整其操作，无需额外配置。"}
{"en": "There should not be any noticeable difference in the user experience when\nswitching to cgroup v2, unless users are accessing the cgroup file system\ndirectly, either on the node or from within the containers.\n\ncgroup v2 uses a different API than cgroup v1, so if there are any\napplications that directly access the cgroup file system, they need to be\nupdated to newer versions that support cgroup v2. For example:", "zh": "切换到 cgroup v2 时，用户体验应没有任何明显差异，除非用户直接在节点上或从容器内访问 cgroup 文件系统。\n\ncgroup v2 使用一个与 cgroup v1 不同的 API，因此如果有任何应用直接访问 cgroup 文件系统，\n则需要将这些应用更新为支持 cgroup v2 的版本。例如："}
{"en": "* Some third-party monitoring and security agents may depend on the cgroup filesystem.\n Update these agents to versions that support cgroup v2.\n* If you run [cAdvisor](https://github.com/google/cadvisor) as a stand-alone\n DaemonSet for monitoring pods and containers, update it to v0.43.0 or later.\n * If you deploy Java applications, prefer to use versions which fully support cgroup v2:\n    * [OpenJDK / HotSpot](https://bugs.openjdk.org/browse/JDK-8230305): jdk8u372, 11.0.16, 15 and later\n    * [IBM Semeru Runtimes](https://www.ibm.com/support/pages/apar/IJ46681): 8.0.382.0, 11.0.20.0, 17.0.8.0, and later\n    * [IBM Java](https://www.ibm.com/support/pages/apar/IJ46681): 8.0.8.6 and later\n* If you are using the [uber-go/automaxprocs](https://github.com/uber-go/automaxprocs) package, make sure\n  the version you use is v1.5.1 or higher.", "zh": "* 一些第三方监控和安全代理可能依赖于 cgroup 文件系统。你要将这些代理更新到支持 cgroup v2 的版本。\n* 如果以独立的 DaemonSet 的形式运行 [cAdvisor](https://github.com/google/cadvisor) 以监控 Pod 和容器，\n  需将其更新到 v0.43.0 或更高版本。\n* 如果你部署 Java 应用程序，最好使用完全支持 cgroup v2 的版本：\n    * [OpenJDK / HotSpot](https://bugs.openjdk.org/browse/JDK-8230305): jdk8u372、11.0.16、15 及更高的版本\n    * [IBM Semeru Runtimes](https://www.ibm.com/support/pages/apar/IJ46681): 8.0.382.0、11.0.20.0、17.0.8.0 及更高的版本\n    * [IBM Java](https://www.ibm.com/support/pages/apar/IJ46681): 8.0.8.6 及更高的版本\n* 如果你正在使用 [uber-go/automaxprocs](https://github.com/uber-go/automaxprocs) 包，\n  确保你使用的版本是 v1.5.1 或者更高。"}
{"en": "## Identify the cgroup version on Linux Nodes  {#check-cgroup-version}\n\nThe cgroup version depends on the Linux distribution being used and the\ndefault cgroup version configured on the OS. To check which cgroup version your\ndistribution uses, run the `stat -fc %T /sys/fs/cgroup/` command on\nthe node:", "zh": "## 识别 Linux 节点上的 cgroup 版本 {#check-cgroup-version}\n\ncgroup 版本取决于正在使用的 Linux 发行版和操作系统上配置的默认 cgroup 版本。\n要检查你的发行版使用的是哪个 cgroup 版本，请在该节点上运行 `stat -fc %T /sys/fs/cgroup/` 命令：\n\n```shell\nstat -fc %T /sys/fs/cgroup/\n```"}
{"en": "For cgroup v2, the output is `cgroup2fs`.\n\nFor cgroup v1, the output is `tmpfs.`", "zh": "对于 cgroup v2，输出为 `cgroup2fs`。\n\n对于 cgroup v1，输出为 `tmpfs`。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Learn more about [cgroups](https://man7.org/linux/man-pages/man7/cgroups.7.html)\n- Learn more about [container runtime](/docs/concepts/architecture/cri)\n- Learn more about [cgroup drivers](/docs/setup/production-environment/container-runtimes#cgroup-drivers)", "zh": "- 进一步了解 [cgroups](https://man7.org/linux/man-pages/man7/cgroups.7.html)\n- 进一步了解[容器运行时](/zh-cn/docs/concepts/architecture/cri)\n- 进一步了解 [cgroup 驱动](/zh-cn/docs/setup/production-environment/container-runtimes#cgroup-drivers)"}
{"en": "A Kubernetes cluster consists of a control plane plus a set of worker machines, called nodes,\nthat run containerized applications. Every cluster needs at least one worker node in order to run Pods.\n\nThe worker node(s) host the Pods that are the components of the application workload.\nThe control plane manages the worker nodes and the Pods in the cluster. In production\nenvironments, the control plane usually runs across multiple computers and a cluster\nusually runs multiple nodes, providing fault-tolerance and high availability.\n\nThis document outlines the various components you need to have for a complete and working Kubernetes cluster.", "zh": "Kubernetes 集群由一个控制平面和一组用于运行容器化应用的工作机器组成，这些工作机器称作节点（Node）。\n每个集群至少需要一个工作节点来运行 Pod。\n\n工作节点托管着组成应用负载的 Pod。控制平面管理集群中的工作节点和 Pod。\n在生产环境中，控制平面通常跨多台计算机运行，而一个集群通常运行多个节点，以提供容错和高可用。\n\n本文概述了构建一个完整且可运行的 Kubernetes 集群所需的各种组件。"}
{"en": "{{< figure src=\"/images/docs/kubernetes-cluster-architecture.svg\" alt=\"The control plane (kube-apiserver, etcd, kube-controller-manager, kube-scheduler) and several nodes. Each node is running a kubelet and kube-proxy.\"\ntitle=\"Kubernetes cluster components\"\ncaption=\"**Note:** This diagram presents an example reference architecture for a Kubernetes cluster. The actual distribution of components can vary based on specific cluster setups and requirements.\" class=\"diagram-large\" >}}", "zh": "{{< figure src=\"/images/docs/kubernetes-cluster-architecture.svg\" alt=\"控制平面（kube-apiserver、etcd、kube-controller-manager、kube-scheduler）和多个节点。每个节点运行 kubelet 和 kube-proxy。\"\ntitle=\"Kubernetes 集群组件\"\ncaption=\"**注意：** 此图展示了 Kubernetes 集群的参考架构示例。这些组件的实际分布可能会基于特定的集群设置和要求而有所不同。\" class=\"diagram-large\" >}}"}
{"en": "## Control plane components\n\nThe control plane's components make global decisions about the cluster (for example, scheduling),\nas well as detecting and responding to cluster events (for example, starting up a new\n{{< glossary_tooltip text=\"pod\" term_id=\"pod\">}} when a Deployment's\n`{{< glossary_tooltip text=\"replicas\" term_id=\"replica\" >}}` field is unsatisfied).", "zh": "## 控制平面组件   {#control-plane-components}\n\n控制平面组件会为集群做出全局决策，比如资源的调度。\n以及检测和响应集群事件，例如当不满足 Deployment 的 `{{< glossary_tooltip text=\"replicas\" term_id=\"replica\" >}}`\n字段时，要启动新的 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\">}}）。"}
{"en": "Control plane components can be run on any machine in the cluster. However, for simplicity, setup scripts\ntypically start all control plane components on the same machine, and do not run user containers on this machine.\nSee [Creating Highly Available clusters with kubeadm](/docs/setup/production-environment/tools/kubeadm/high-availability/)\nfor an example control plane setup that runs across multiple machines.", "zh": "控制平面组件可以在集群中的任何节点上运行。\n然而，为了简单起见，安装脚本通常会在同一个计算机上启动所有控制平面组件，\n并且不会在此计算机上运行用户容器。\n请参阅[使用 kubeadm 构建高可用性集群](/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/)中关于跨多机器安装控制平面的示例。\n\n### kube-apiserver\n\n{{< glossary_definition term_id=\"kube-apiserver\" length=\"all\" >}}\n\n### etcd\n\n{{< glossary_definition term_id=\"etcd\" length=\"all\" >}}\n\n### kube-scheduler\n\n{{< glossary_definition term_id=\"kube-scheduler\" length=\"all\" >}}\n\n### kube-controller-manager\n\n{{< glossary_definition term_id=\"kube-controller-manager\" length=\"all\" >}}"}
{"en": "There are many different types of controllers. Some examples of them are:\n\n- Node controller: Responsible for noticing and responding when nodes go down.\n- Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.\n- EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods).\n- ServiceAccount controller: Create default ServiceAccounts for new namespaces.\n\nThe above is not an exhaustive list.", "zh": "控制器有许多不同类型。以下是一些例子：\n\n* Node 控制器：负责在节点出现故障时进行通知和响应\n* Job 控制器：监测代表一次性任务的 Job 对象，然后创建 Pod 来运行这些任务直至完成\n* EndpointSlice 控制器：填充 EndpointSlice 对象（以提供 Service 和 Pod 之间的链接）。\n* ServiceAccount 控制器：为新的命名空间创建默认的 ServiceAccount。\n\n以上并不是一个详尽的列表。\n\n### cloud-controller-manager\n\n{{< glossary_definition term_id=\"cloud-controller-manager\" length=\"short\" >}}"}
{"en": "The cloud-controller-manager only runs controllers that are specific to your cloud provider.\nIf you are running Kubernetes on your own premises, or in a learning environment inside your\nown PC, the cluster does not have a cloud controller manager.\n\nAs with the kube-controller-manager, the cloud-controller-manager combines several logically\nindependent control loops into a single binary that you run as a single process. You can scale\nhorizontally (run more than one copy) to improve performance or to help tolerate failures.", "zh": "`cloud-controller-manager` 仅运行特定于云平台的控制器。\n因此如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境，\n所部署的集群不包含云控制器管理器。\n\n与 `kube-controller-manager` 类似，`cloud-controller-manager`\n将若干逻辑上独立的控制回路组合到同一个可执行文件中，以同一进程的方式供你运行。\n你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。"}
{"en": "The following controllers can have cloud provider dependencies:\n\n- Node controller: For checking the cloud provider to determine if a node has been\n  deleted in the cloud after it stops responding\n- Route controller: For setting up routes in the underlying cloud infrastructure\n- Service controller: For creating, updating and deleting cloud provider load balancers", "zh": "下面的控制器都包含对云平台驱动的依赖：\n\n- Node 控制器：用于在节点终止响应后检查云平台以确定节点是否已被删除\n- Route 控制器：用于在底层云基础架构中设置路由\n- Service 控制器：用于创建、更新和删除云平台上的负载均衡器"}
{"en": "## Node components\n\nNode components run on every node, maintaining running pods and providing the Kubernetes runtime environment.", "zh": "## 节点组件   {#node-components}\n\n节点组件会在每个节点上运行，负责维护运行的 Pod 并提供 Kubernetes 运行时环境。\n\n### kubelet\n\n{{< glossary_definition term_id=\"kubelet\" length=\"all\" >}}"}
{"en": "### kube-proxy (optional) {#kube-proxy}\n\nIf you use a [network plugin](#network-plugins) that implements packet forwarding for Services\nby itself, and providing equivalent behavior to kube-proxy, then you do not need to run\nkube-proxy on the nodes in your cluster.\n\n### Container runtime", "zh": "### kube-proxy（可选）  {#kube-proxy}\n\n{{< glossary_definition term_id=\"kube-proxy\" length=\"all\" >}}\n如果你使用[网络插件](#network-plugins)为 Service 实现本身的数据包转发，\n并提供与 kube-proxy 等效的行为，那么你不需要在集群中的节点上运行 kube-proxy。\n\n### 容器运行时   {#container-runtime}\n\n{{< glossary_definition term_id=\"container-runtime\" length=\"all\" >}}"}
{"en": "## Addons\n\nAddons use Kubernetes resources ({{< glossary_tooltip term_id=\"daemonset\" >}},\n{{< glossary_tooltip term_id=\"deployment\" >}}, etc) to implement cluster features.\nBecause these are providing cluster-level features, namespaced resources for\naddons belong within the `kube-system` namespace.\n\nSelected addons are described below; for an extended list of available addons,\nplease see [Addons](/docs/concepts/cluster-administration/addons/).", "zh": "## 插件（Addons）    {#addons}\n\n插件使用 Kubernetes 资源（{{< glossary_tooltip text=\"DaemonSet\" term_id=\"daemonset\" >}}、\n{{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}} 等）实现集群功能。\n因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 `kube-system` 命名空间。\n\n下面描述众多插件中的几种。有关可用插件的完整列表，\n请参见[插件（Addons）](/zh-cn/docs/concepts/cluster-administration/addons/)。\n\n### DNS"}
{"en": "While the other addons are not strictly required, all Kubernetes clusters should have\n[cluster DNS](/docs/concepts/services-networking/dns-pod-service/), as many examples rely on it.\n\nCluster DNS is a DNS server, in addition to the other DNS server(s) in your environment,\nwhich serves DNS records for Kubernetes services.\n\nContainers started by Kubernetes automatically include this DNS server in their DNS searches.", "zh": "尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes\n集群都应该有[集群 DNS](/zh-cn/docs/concepts/services-networking/dns-pod-service/)，\n因为很多示例都需要 DNS 服务。\n\n集群 DNS 是一个 DNS 服务器，和环境中的其他 DNS 服务器一起工作，它为 Kubernetes 服务提供 DNS 记录。\n\nKubernetes 启动的容器自动将此 DNS 服务器包含在其 DNS 搜索列表中。"}
{"en": "### Web UI (Dashboard)\n\n[Dashboard](/docs/tasks/access-application-cluster/web-ui-dashboard/) is a general purpose,\nweb-based UI for Kubernetes clusters. It allows users to manage and troubleshoot applications\nrunning in the cluster, as well as the cluster itself.", "zh": "### Web 界面（仪表盘）   {#web-ui-dashboard}\n\n[Dashboard](/zh-cn/docs/tasks/access-application-cluster/web-ui-dashboard/)\n是 Kubernetes 集群的通用的、基于 Web 的用户界面。\n它使用户可以管理集群中运行的应用程序以及集群本身，并进行故障排除。"}
{"en": "### Container resource monitoring\n\n[Container Resource Monitoring](/docs/tasks/debug/debug-cluster/resource-usage-monitoring/)\nrecords generic time-series metrics about containers in a central database, and provides a UI for browsing that data.\n\n### Cluster-level Logging\n\nA [cluster-level logging](/docs/concepts/cluster-administration/logging/) mechanism is responsible\nfor saving container logs to a central log store with a search/browsing interface.", "zh": "### 容器资源监控   {#container-resource-monitoring}\n\n[容器资源监控](/zh-cn/docs/tasks/debug/debug-cluster/resource-usage-monitoring/)\n将关于容器的一些常见的时序度量值保存到一个集中的数据库中，并提供浏览这些数据的界面。\n\n### 集群层面日志   {#cluster-level-logging}\n\n[集群层面日志](/zh-cn/docs/concepts/cluster-administration/logging/)机制负责将容器的日志数据保存到一个集中的日志存储中，\n这种集中日志存储提供搜索和浏览接口。"}
{"en": "### Network plugins\n\n[Network plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins)\nare software components that implement the container network interface (CNI) specification.\nThey are responsible for allocating IP addresses to pods and enabling them to communicate\nwith each other within the cluster.", "zh": "### 网络插件   {#network-plugins}\n\n[网络插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins)\n是实现容器网络接口（CNI）规范的软件组件。它们负责为 Pod 分配 IP 地址，并使这些 Pod 能在集群内部相互通信。"}
{"en": "## Architecture variations\n\nWhile the core components of Kubernetes remain consistent, the way they are deployed and\nmanaged can vary. Understanding these variations is crucial for designing and maintaining\nKubernetes clusters that meet specific operational needs.", "zh": "## 架构变种    {#architecture-variations}\n\n虽然 Kubernetes 的核心组件保持一致，但它们的部署和管理方式可能有所不同。\n了解这些变化对于设计和维护满足特定运营需求的 Kubernetes 集群至关重要。"}
{"en": "### Control plane deployment options\n\nThe control plane components can be deployed in several ways:\n\nTraditional deployment\n: Control plane components run directly on dedicated machines or VMs, often managed as systemd services.\n\nStatic Pods\n: Control plane components are deployed as static Pods, managed by the kubelet on specific nodes.\n  This is a common approach used by tools like kubeadm.", "zh": "### 控制平面部署选项    {#control-plane-deployment-options}\n\n控制平面组件可以通过以下几种方式部署：\n\n传统部署\n: 控制平面组件直接在专用机器或虚拟机上运行，通常作为 systemd 服务进行管理。\n\n静态 Pod\n: 控制平面组件作为静态 Pod 部署，由特定节点上的 kubelet 管理。\n  这是像 kubeadm 这样的工具常用的方法。"}
{"en": "Self-hosted\n: The control plane runs as Pods within the Kubernetes cluster itself, managed by Deployments\n  and StatefulSets or other Kubernetes primitives.\n\nManaged Kubernetes services\n: Cloud providers often abstract away the control plane, managing its components as part of their service offering.", "zh": "自托管\n: 控制平面在 Kubernetes 集群本身内部作为 Pod 运行，\n  由 Deployments、StatefulSets 或其他 Kubernetes 原语管理。\n\n托管 Kubernetes 服务\n: 云平台通常将控制平面抽象出来，将其组件作为其服务的一部分进行管理。"}
{"en": "### Workload placement considerations\n\nThe placement of workloads, including the control plane components, can vary based on cluster size,\nperformance requirements, and operational policies:\n\n- In smaller or development clusters, control plane components and user workloads might run on the same nodes.\n- Larger production clusters often dedicate specific nodes to control plane components,\n  separating them from user workloads.\n- Some organizations run critical add-ons or monitoring tools on control plane nodes.", "zh": "### 工作负载调度说明   {#workload-placement-considerations}\n\n含控制平面组件在内的工作负载的调度可能因集群大小、性能要求和操作策略而有所不同：\n\n- 在较小或开发集群中，控制平面组件和用户工作负载可能在同一节点上运行。\n- 较大的生产集群通常将特定节点专用于控制平面组件，将其与用户工作负载隔离。\n- 一些组织在控制平面节点上运行关键组件或监控工具。"}
{"en": "### Cluster management tools\n\nTools like kubeadm, kops, and Kubespray offer different approaches to deploying and managing clusters,\neach with its own method of component layout and management.\n\nThe flexibility of Kubernetes architecture allows organizations to tailor their clusters to specific needs,\nbalancing factors such as operational complexity, performance, and management overhead.", "zh": "### 集群管理工具   {#cluster-management-tools}\n\n像 kubeadm、kops 和 Kubespray 这样的工具提供了不同的集群部署和管理方法，每种方法都有自己的组件布局和管理方式。\n\nKubernetes 架构的灵活性使各组织能够根据特定需求调整其集群，平衡操作复杂性、性能和管理开销等因素。"}
{"en": "### Customization and extensibility\n\nKubernetes architecture allows for significant customization:\n\n- Custom schedulers can be deployed to work alongside the default Kubernetes scheduler or to replace it entirely.\n- API servers can be extended with CustomResourceDefinitions and API Aggregation.\n- Cloud providers can integrate deeply with Kubernetes using the cloud-controller-manager.\n\nThe flexibility of Kubernetes architecture allows organizations to tailor their clusters to specific needs,\nbalancing factors such as operational complexity, performance, and management overhead.", "zh": "### 定制和可扩展性   {#customization-and-extensibility}\n\nKubernetes 架构允许大幅度的定制：\n\n- 你可以部署自定义的调度器与默认的 Kubernetes 调度器协同工作，也可以完全替换掉默认的调度器。\n- API 服务器可以通过 CustomResourceDefinition 和 API 聚合进行扩展。\n- 云平台可以使用 cloud-controller-manager 与 Kubernetes 深度集成。\n\nKubernetes 架构的灵活性使各组织能够根据特定需求调整其集群，平衡操作复杂性、性能和管理开销等因素。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "Learn more about the following:\n\n- [Nodes](/docs/concepts/architecture/nodes/) and\n  [their communication](/docs/concepts/architecture/control-plane-node-communication/)\n  with the control plane.\n- Kubernetes [controllers](/docs/concepts/architecture/controller/).\n- [kube-scheduler](/docs/concepts/scheduling-eviction/kube-scheduler/) which is the default scheduler for Kubernetes.\n- Etcd's official [documentation](https://etcd.io/docs/).\n- Several [container runtimes](/docs/setup/production-environment/container-runtimes/) in Kubernetes.\n- Integrating with cloud providers using [cloud-controller-manager](/docs/concepts/architecture/cloud-controller/).\n- [kubectl](/docs/reference/generated/kubectl/kubectl-commands) commands.", "zh": "了解更多内容：\n\n- [节点](/zh-cn/docs/concepts/architecture/nodes/)及其与控制平面的[通信](/zh-cn/docs/concepts/architecture/control-plane-node-communication/)。\n- Kubernetes [控制器](/zh-cn/docs/concepts/architecture/controller/)。\n- Kubernetes 的默认调度器 [kube-scheduler](/zh-cn/docs/concepts/scheduling-eviction/kube-scheduler/)。\n- Etcd 的官方[文档](https://etcd.io/docs/)。\n- Kubernetes 中的几个[容器运行时](/zh-cn/docs/setup/production-environment/container-runtimes/)。\n- 使用 [cloud-controller-manager](/zh-cn/docs/concepts/architecture/cloud-controller/) 与云平台集成。\n- [kubectl](/zh-cn/docs/reference/generated/kubectl/kubectl-commands) 命令。"}
{"en": "The CRI is a plugin interface which enables the kubelet to use a wide variety of\ncontainer runtimes, without having a need to recompile the cluster components.\n\nYou need a working\n{{<glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\">}} on\neach Node in your cluster, so that the\n{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} can launch\n{{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} and their containers.", "zh": "CRI 是一个插件接口，它使 kubelet 能够使用各种容器运行时，无需重新编译集群组件。\n\n你需要在集群中的每个节点上都有一个可以正常工作的{{<glossary_tooltip text=\"容器运行时\" term_id=\"container-runtime\">}}，\n这样 {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} 能启动\n{{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 及其容器。\n\n{{< glossary_definition prepend=\"容器运行时接口（CRI）是\" term_id=\"container-runtime-interface\" length=\"all\" >}}"}
{"en": "## The API {#api}", "zh": "## API {#api}\n\n{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}"}
{"en": "The kubelet acts as a client when connecting to the container runtime via gRPC.\nThe runtime and image service endpoints have to be available in the container\nruntime, which can be configured separately within the kubelet by using the\n`--image-service-endpoint` [command line flags](/docs/reference/command-line-tools-reference/kubelet).", "zh": "当通过 gRPC 连接到容器运行时，kubelet 将充当客户端。运行时和镜像服务端点必须在容器运行时中可用，\n可以使用 `--image-service-endpoint` \n[命令行标志](/zh-cn/docs/reference/command-line-tools-reference/kubelet)在 kubelet 中单独配置。"}
{"en": "For Kubernetes v{{< skew currentVersion >}}, the kubelet prefers to use CRI `v1`.\nIf a container runtime does not support `v1` of the CRI, then the kubelet tries to\nnegotiate any older supported version.\nThe v{{< skew currentVersion >}} kubelet can also negotiate CRI `v1alpha2`, but\nthis version is considered as deprecated.\nIf the kubelet cannot negotiate a supported CRI version, the kubelet gives up\nand doesn't register as a node.", "zh": "对 Kubernetes v{{< skew currentVersion >}}，kubelet 偏向于使用 CRI `v1` 版本。\n如果容器运行时不支持 CRI 的 `v1` 版本，那么 kubelet 会尝试协商较老的、仍被支持的所有版本。\nv{{< skew currentVersion >}} 版本的 kubelet 也可协商 CRI `v1alpha2` 版本，但该版本被视为已弃用。\n如果 kubelet 无法协商出可支持的 CRI 版本，则 kubelet 放弃并且不会注册为节点。"}
{"en": "## Upgrading\n\nWhen upgrading Kubernetes, the kubelet tries to automatically select the\nlatest CRI version on restart of the component. If that fails, then the fallback\nwill take place as mentioned above. If a gRPC re-dial was required because the\ncontainer runtime has been upgraded, then the container runtime must also\nsupport the initially selected version or the redial is expected to fail. This\nrequires a restart of the kubelet.", "zh": "## 升级  {#upgrading}\n\n升级 Kubernetes 时，kubelet 会尝试在组件重启时自动选择最新的 CRI 版本。\n如果失败，则将如上所述进行回退。如果由于容器运行时已升级而需要 gRPC 重拨，\n则容器运行时还必须支持最初选择的版本，否则重拨预计会失败。\n这需要重新启动 kubelet。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Learn more about the CRI [protocol definition](https://github.com/kubernetes/cri-api/blob/c75ef5b/pkg/apis/runtime/v1/api.proto)", "zh": "- 了解更多有关 CRI [协议定义](https://github.com/kubernetes/cri-api/blob/c75ef5b/pkg/apis/runtime/v1/api.proto)"}
{"en": "Operators are software extensions to Kubernetes that make use of\n[custom resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\nto manage applications and their components. Operators follow\nKubernetes principles, notably the [control loop](/docs/concepts/architecture/controller).", "zh": "Operator 是 Kubernetes 的扩展软件，\n它利用[定制资源](/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources/)管理应用及其组件。\nOperator 遵循 Kubernetes 的理念，特别是在[控制器](/zh-cn/docs/concepts/architecture/controller)方面。"}
{"en": "## Motivation\n\nThe _operator pattern_ aims to capture the key aim of a human operator who\nis managing a service or set of services. Human operators who look after\nspecific applications and services have deep knowledge of how the system\nought to behave, how to deploy it, and how to react if there are problems.\n\nPeople who run workloads on Kubernetes often like to use automation to take\ncare of repeatable tasks. The operator pattern captures how you can write\ncode to automate a task beyond what Kubernetes itself provides.", "zh": "## 初衷 {#motivation}\n\n**Operator 模式** 旨在记述（正在管理一个或一组服务的）运维人员的关键目标。\n这些运维人员负责一些特定的应用和 Service，他们需要清楚地知道系统应该如何运行、如何部署以及出现问题时如何处理。\n\n在 Kubernetes 上运行工作负载的人们都喜欢通过自动化来处理重复的任务。\nOperator 模式会封装你编写的（Kubernetes 本身提供功能以外的）任务自动化代码。"}
{"en": "## Operators in Kubernetes\n\nKubernetes is designed for automation. Out of the box, you get lots of\nbuilt-in automation from the core of Kubernetes. You can use Kubernetes\nto automate deploying and running workloads, *and* you can automate how\nKubernetes does that.\n\nKubernetes' {{< glossary_tooltip text=\"operator pattern\" term_id=\"operator-pattern\" >}}\nconcept lets you extend the cluster's behaviour without modifying the code of Kubernetes\nitself by linking {{< glossary_tooltip text=\"controllers\" term_id=\"controller\" >}} to\none or more custom resources. Operators are clients of the Kubernetes API that act as\ncontrollers for a [Custom Resource](/docs/concepts/extend-kubernetes/api-extension/custom-resources/).", "zh": "## Kubernetes 上的 Operator {#operators-in-kubernetes}\n\nKubernetes 为自动化而生。无需任何修改，你即可以从 Kubernetes 核心中获得许多内置的自动化功能。\n你可以使用 Kubernetes 自动化部署和运行工作负载，**甚至** 可以自动化 Kubernetes 自身。\n\nKubernetes 的 {{< glossary_tooltip text=\"Operator 模式\" term_id=\"operator-pattern\" >}}概念允许你在不修改\nKubernetes 自身代码的情况下，\n通过为一个或多个自定义资源关联{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}来扩展集群的能力。\nOperator 是 Kubernetes API 的客户端，\n充当[自定义资源](/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources/)的控制器。"}
{"en": "## An example operator {#example}\n\nSome of the things that you can use an operator to automate include:\n\n* deploying an application on demand\n* taking and restoring backups of that application's state\n* handling upgrades of the application code alongside related changes such\n  as database schemas or extra configuration settings\n* publishing a Service to applications that don't support Kubernetes APIs to\n  discover them\n* simulating failure in all or part of your cluster to test its resilience\n* choosing a leader for a distributed application without an internal\n  member election process", "zh": "## Operator 示例 {#example}\n\n使用 Operator 可以自动化的事情包括：\n\n* 按需部署应用\n* 获取/还原应用状态的备份\n* 处理应用代码的升级以及相关改动。例如数据库 Schema 或额外的配置设置\n* 发布一个 Service，要求不支持 Kubernetes API 的应用也能发现它\n* 模拟整个或部分集群中的故障以测试其稳定性\n* 在没有内部成员选举程序的情况下，为分布式应用选择首领角色"}
{"en": "What might an operator look like in more detail? Here's an example:\n\n1. A custom resource named SampleDB, that you can configure into the cluster.\n2. A Deployment that makes sure a Pod is running that contains the\n   controller part of the operator.\n3. A container image of the operator code.\n4. Controller code that queries the control plane to find out what SampleDB\n   resources are configured.\n5. The core of the operator is code to tell the API server how to make\n   reality match the configured resources.\n   * If you add a new SampleDB, the operator sets up PersistentVolumeClaims\n     to provide durable database storage, a StatefulSet to run SampleDB and\n     a Job to handle initial configuration.\n   * If you delete it, the operator takes a snapshot, then makes sure that\n     the StatefulSet and Volumes are also removed.\n6. The operator also manages regular database backups. For each SampleDB\n   resource, the operator determines when to create a Pod that can connect\n   to the database and take backups. These Pods would rely on a ConfigMap\n   and / or a Secret that has database connection details and credentials.\n7. Because the operator aims to provide robust automation for the resource\n   it manages, there would be additional supporting code. For this example,\n   code checks to see if the database is running an old version and, if so,\n   creates Job objects that upgrade it for you.", "zh": "想要更详细的了解 Operator？下面是一个示例：\n\n1. 有一个名为 SampleDB 的自定义资源，你可以将其配置到集群中。\n2. 一个包含 Operator 控制器部分的 Deployment，用来确保 Pod 处于运行状态。\n3. Operator 代码的容器镜像。\n4. 控制器代码，负责查询控制平面以找出已配置的 SampleDB 资源。\n5. Operator 的核心是告诉 API 服务器，如何使现实与代码里配置的资源匹配。\n   * 如果添加新的 SampleDB，Operator 将设置 PersistentVolumeClaims 以提供持久化的数据库存储，\n     设置 StatefulSet 以运行 SampleDB，并设置 Job 来处理初始配置。\n   * 如果你删除它，Operator 将建立快照，然后确保 StatefulSet 和 Volume 已被删除。\n6. Operator 也可以管理常规数据库的备份。对于每个 SampleDB 资源，Operator\n   会确定何时创建（可以连接到数据库并进行备份的）Pod。这些 Pod 将依赖于\n   ConfigMap 和/或具有数据库连接详细信息和凭据的 Secret。\n7. 由于 Operator 旨在为其管理的资源提供强大的自动化功能，因此它还需要一些额外的支持性代码。\n   在这个示例中，代码将检查数据库是否正运行在旧版本上，\n   如果是，则创建 Job 对象为你升级数据库。"}
{"en": "## Deploying operators\n\nThe most common way to deploy an operator is to add the\nCustom Resource Definition and its associated Controller to your cluster.\nThe Controller will normally run outside of the\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}},\nmuch as you would run any containerized application.\nFor example, you can run the controller in your cluster as a Deployment.", "zh": "## 部署 Operator {#deploying-operators}\n\n部署 Operator 最常见的方法是将自定义资源及其关联的控制器添加到你的集群中。\n跟运行容器化应用一样，控制器通常会运行在{{< glossary_tooltip text=\"控制平面\" term_id=\"control-plane\" >}}之外。\n例如，你可以在集群中将控制器作为 Deployment 运行。"}
{"en": "## Using an operator {#using-operators}\n\nOnce you have an operator deployed, you'd use it by adding, modifying or\ndeleting the kind of resource that the operator uses. Following the above\nexample, you would set up a Deployment for the operator itself, and then:\n\n```shell\nkubectl get SampleDB                   # find configured databases\n\nkubectl edit SampleDB/example-database # manually change some settings\n```", "zh": "## 使用 Operator {#using-operators}\n\n部署 Operator 后，你可以对 Operator 所使用的资源执行添加、修改或删除操作。\n按照上面的示例，你将为 Operator 本身建立一个 Deployment，然后：\n\n```shell\nkubectl get SampleDB                   # 查找所配置的数据库\n\nkubectl edit SampleDB/example-database # 手动修改某些配置\n```"}
{"en": "&hellip;and that's it! The operator will take care of applying the changes\nas well as keeping the existing service in good shape.", "zh": "可以了！Operator 会负责应用所作的更改并保持现有服务处于良好的状态。"}
{"en": "## Writing your own operator {#writing-operator}", "zh": "## 编写你自己的 Operator {#writing-operator}"}
{"en": "If there isn't an operator in the ecosystem that implements the behavior you\nwant, you can code your own. \n\nYou also implement an operator (that is, a Controller) using any language / runtime\nthat can act as a [client for the Kubernetes API](/docs/reference/using-api/client-libraries/).", "zh": "如果生态系统中没有可以实现你目标的 Operator，你可以自己编写代码。\n\n你还可以使用任何支持\n[Kubernetes API 客户端](/zh-cn/docs/reference/using-api/client-libraries/)的语言或运行时来实现\nOperator（即控制器）。"}
{"en": "Following are a few libraries and tools you can use to write your own cloud native\noperator.", "zh": "以下是一些库和工具，你可用于编写自己的云原生 Operator。\n\n{{% thirdparty-content %}}"}
{"en": "* [Charmed Operator Framework](https://juju.is/)\n* [Java Operator SDK](https://github.com/operator-framework/java-operator-sdk)\n* [Kopf](https://github.com/nolar/kopf) (Kubernetes Operator Pythonic Framework)\n* [kube-rs](https://kube.rs/) (Rust)\n* [kubebuilder](https://book.kubebuilder.io/)\n* [KubeOps](https://buehler.github.io/dotnet-operator-sdk/) (.NET operator SDK)\n* [Mast](https://docs.ansi.services/mast/user_guide/operator/)\n* [Metacontroller](https://metacontroller.github.io/metacontroller/intro.html) along with WebHooks that\n  you implement yourself\n* [Operator Framework](https://operatorframework.io)\n* [shell-operator](https://github.com/flant/shell-operator)", "zh": "* [Charmed Operator Framework](https://juju.is/)\n* [Java Operator SDK](https://github.com/operator-framework/java-operator-sdk)\n* [Kopf](https://github.com/nolar/kopf) (Kubernetes Operator Pythonic Framework)\n* [kube-rs](https://kube.rs/) (Rust)\n* [kubebuilder](https://book.kubebuilder.io/)\n* [KubeOps](https://buehler.github.io/dotnet-operator-sdk/) (.NET operator SDK)\n* [Mast](https://docs.ansi.services/mast/user_guide/operator/)\n* [Metacontroller](https://metacontroller.github.io/metacontroller/intro.html)，可与 Webhook 结合使用，以实现自己的功能。\n* [Operator Framework](https://operatorframework.io)\n* [shell-operator](https://github.com/flant/shell-operator)\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read the {{< glossary_tooltip text=\"CNCF\" term_id=\"cncf\" >}}\n  [Operator White Paper](https://github.com/cncf/tag-app-delivery/blob/163962c4b1cd70d085107fc579e3e04c2e14d59c/operator-wg/whitepaper/Operator-WhitePaper_v1-0.md).\n* Learn more about [Custom Resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\n* Find ready-made operators on [OperatorHub.io](https://operatorhub.io/) to suit your use case\n* [Publish](https://operatorhub.io/) your operator for other people to use\n* Read [CoreOS' original article](https://web.archive.org/web/20170129131616/https://coreos.com/blog/introducing-operators.html)\n  that introduced the operator pattern (this is an archived version of the original article).\n* Read an [article](https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-building-kubernetes-operators-and-stateful-apps)\n  from Google Cloud about best practices for building operators", "zh": "* 阅读 {{< glossary_tooltip text=\"CNCF\" term_id=\"cncf\" >}} [Operator 白皮书](https://github.com/cncf/tag-app-delivery/blob/163962c4b1cd70d085107fc579e3e04c2e14d59c/operator-wg/whitepaper/Operator-WhitePaper_v1-0.md)。\n* 详细了解[定制资源](/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\n* 在 [OperatorHub.io](https://operatorhub.io/) 上找到现成的、适合你的 Operator\n* [发布](https://operatorhub.io/)你的 Operator，让别人也可以使用\n* 阅读 [CoreOS 原始文章](https://web.archive.org/web/20170129131616/https://coreos.com/blog/introducing-operators.html)，它介绍了 Operator 模式（这是一个存档版本的原始文章）。\n* 阅读这篇来自谷歌云的关于构建 Operator\n  最佳实践的[文章](https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-building-kubernetes-operators-and-stateful-apps)"}
{"en": "Kubernetes is highly configurable and extensible. As a result, there is rarely a need to fork or\nsubmit patches to the Kubernetes project code.\n\nThis guide describes the options for customizing a Kubernetes cluster. It is aimed at\n{{< glossary_tooltip text=\"cluster operators\" term_id=\"cluster-operator\" >}} who want to understand\nhow to adapt their Kubernetes cluster to the needs of their work environment. Developers who are\nprospective {{< glossary_tooltip text=\"Platform Developers\" term_id=\"platform-developer\" >}} or\nKubernetes Project {{< glossary_tooltip text=\"Contributors\" term_id=\"contributor\" >}} will also\nfind it useful as an introduction to what extension points and patterns exist, and their\ntrade-offs and limitations.", "zh": "Kubernetes 是高度可配置且可扩展的。因此，大多数情况下，\n你不需要派生自己的 Kubernetes 副本或者向项目代码提交补丁。\n\n本指南描述定制 Kubernetes 的可选方式。主要针对的读者是希望了解如何针对自身工作环境需要来调整\nKubernetes 的{{< glossary_tooltip text=\"集群管理者\" term_id=\"cluster-operator\" >}}。\n对于那些充当{{< glossary_tooltip text=\"平台开发人员\" term_id=\"platform-developer\" >}}的开发人员或\nKubernetes 项目的{{< glossary_tooltip text=\"贡献者\" term_id=\"contributor\" >}}而言，\n他们也会在本指南中找到有用的介绍信息，了解系统中存在哪些扩展点和扩展模式，\n以及它们所附带的各种权衡和约束等等。"}
{"en": "Customization approaches can be broadly divided into [configuration](#configuration), which only\ninvolves changing command line arguments, local configuration files, or API resources; and [extensions](#extensions),\nwhich involve running additional programs, additional network services, or both.\nThis document is primarily about _extensions_.", "zh": "定制化的方法主要可分为[配置](#configuration)和[扩展](#extensions)两种。\n前者主要涉及更改命令行参数、本地配置文件或者 API 资源；\n后者则需要额外运行一些程序、网络服务或两者。\n本文主要关注**扩展**。"}
{"en": "## Configuration\n\n*Configuration files* and *command arguments* are documented in the [Reference](/docs/reference/) section of the online\ndocumentation, with a page for each binary:\n\n* [`kube-apiserver`](/docs/reference/command-line-tools-reference/kube-apiserver/)\n* [`kube-controller-manager`](/docs/reference/command-line-tools-reference/kube-controller-manager/)\n* [`kube-scheduler`](/docs/reference/command-line-tools-reference/kube-scheduler/)\n* [`kubelet`](/docs/reference/command-line-tools-reference/kubelet/)\n* [`kube-proxy`](/docs/reference/command-line-tools-reference/kube-proxy/)", "zh": "## 配置   {#configuration}\n\n**配置文件**和**命令参数**的说明位于在线文档的[参考](/zh-cn/docs/reference/)一节，\n每个可执行文件一个页面：\n\n* [`kube-apiserver`](/zh-cn/docs/reference/command-line-tools-reference/kube-apiserver/)\n* [`kube-controller-manager`](/zh-cn/docs/reference/command-line-tools-reference/kube-controller-manager/)\n* [`kube-scheduler`](/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/)\n* [`kubelet`](/zh-cn/docs/reference/command-line-tools-reference/kubelet/)\n* [`kube-proxy`](/zh-cn/docs/reference/command-line-tools-reference/kube-proxy/)"}
{"en": "Command arguments and configuration files may not always be changeable in a hosted Kubernetes service or a\ndistribution with managed installation. When they are changeable, they are usually only changeable\nby the cluster operator. Also, they are subject to change in future Kubernetes versions, and\nsetting them may require restarting processes. For those reasons, they should be used only when\nthere are no other options.", "zh": "在托管的 Kubernetes 服务中或者受控安装的发行版本中，命令参数和配置文件不总是可以修改的。\n即使它们是可修改的，通常其修改权限也仅限于集群操作员。\n此外，这些内容在将来的 Kubernetes 版本中很可能发生变化，设置新参数或配置文件可能需要重启进程。\n有鉴于此，应该在没有其他替代方案时才会使用这些命令参数和配置文件。"}
{"en": "Built-in *policy APIs*, such as [ResourceQuota](/docs/concepts/policy/resource-quotas/),\n[NetworkPolicy](/docs/concepts/services-networking/network-policies/) and Role-based Access Control\n([RBAC](/docs/reference/access-authn-authz/rbac/)), are built-in Kubernetes APIs that provide declaratively configured policy settings.\nAPIs are typically usable even with hosted Kubernetes services and with managed Kubernetes installations.\nThe built-in policy APIs follow the same conventions as other Kubernetes resources such as Pods.\nWhen you use a policy APIs that is [stable](/docs/reference/using-api/#api-versioning), you benefit from a\n[defined support policy](/docs/reference/using-api/deprecation-policy/) like other Kubernetes APIs.\nFor these reasons, policy APIs are recommended over *configuration files* and *command arguments* where suitable.", "zh": "诸如 [ResourceQuota](/zh-cn/docs/concepts/policy/resource-quotas/)、\n[NetworkPolicy](/zh-cn/docs/concepts/services-networking/network-policies/)\n和基于角色的访问控制（[RBAC](/zh-cn/docs/reference/access-authn-authz/rbac/)）\n等**内置策略 API** 都是以声明方式配置策略选项的内置 Kubernetes API。\n即使在托管的 Kubernetes 服务和受控的 Kubernetes 安装环境中，API 通常也是可用的。\n内置策略 API 遵循与 Pod 这类其他 Kubernetes 资源相同的约定。\n当你使用[稳定版本](/zh-cn/docs/reference/using-api/#api-versioning)的策略 API，\n它们与其他 Kubernetes API 一样，采纳的是一种[预定义的支持策略](/zh-cn/docs/reference/using-api/deprecation-policy/)。\n出于以上原因，在条件允许的情况下，基于策略 API 的方案应该优先于**配置文件**和**命令参数**。"}
{"en": "## Extensions\n\nExtensions are software components that extend and deeply integrate with Kubernetes.\nThey adapt it to support new types and new kinds of hardware.\n\nMany cluster administrators use a hosted or distribution instance of Kubernetes.\nThese clusters come with extensions pre-installed. As a result, most Kubernetes\nusers will not need to install extensions and even fewer users will need to author new ones.", "zh": "## 扩展    {#extensions}\n\n扩展（Extensions）是一些扩充 Kubernetes 能力并与之深度集成的软件组件。\n它们调整 Kubernetes 的工作方式使之支持新的类型和新的硬件种类。\n\n大多数集群管理员会使用一种托管的 Kubernetes 服务或者其某种发行版本。\n这类集群通常都预先安装了扩展。因此，大多数 Kubernetes 用户不需要安装扩展，\n至于需要自己编写新的扩展的情况就更少了。"}
{"en": "### Extension patterns\n\nKubernetes is designed to be automated by writing client programs. Any\nprogram that reads and/or writes to the Kubernetes API can provide useful\nautomation. *Automation* can run on the cluster or off it. By following\nthe guidance in this doc you can write highly available and robust automation.\nAutomation generally works with any Kubernetes cluster, including hosted\nclusters and managed installations.", "zh": "### 扩展模式   {#extension-patterns}\n\nKubernetes 从设计上即支持通过编写客户端程序来将其操作自动化。\n任何能够对 Kubernetes API 发出读写指令的程序都可以提供有用的自动化能力。\n**自动化组件**可以运行在集群上，也可以运行在集群之外。\n通过遵从本文中的指南，你可以编写高度可用的、运行稳定的自动化组件。\n自动化组件通常可以用于所有 Kubernetes 集群，包括托管的集群和受控的安装环境。"}
{"en": "There is a specific pattern for writing client programs that work well with\nKubernetes called the {{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}}\npattern. Controllers typically read an object's `.spec`, possibly do things, and then\nupdate the object's `.status`.\n\nA controller is a client of the Kubernetes API. When Kubernetes is the client and calls\nout to a remote service, Kubernetes calls this a *webhook*. The remote service is called\na *webhook backend*. As with custom controllers, webhooks do add a point of failure.", "zh": "编写客户端程序有一种特殊的{{< glossary_tooltip term_id=\"controller\" text=\"控制器（Controller）\" >}}模式，\n能够与 Kubernetes 很好地协同工作。控制器通常会读取某个对象的 `.spec`，或许还会执行一些操作，\n之后更新对象的 `.status`。\n\n控制器是 Kubernetes API 的客户端。当 Kubernetes 充当客户端且调用某远程服务时，\nKubernetes 将此称作 **Webhook**。该远程服务称作 **Webhook 后端**。\n与定制的控制器相似，Webhook 也会引入失效点（Point of Failure）。\n\n{{< note >}}"}
{"en": "Outside of Kubernetes, the term “webhook” typically refers to a mechanism for asynchronous\nnotifications, where the webhook call serves as a one-way notification to another system or\ncomponent. In the Kubernetes ecosystem, even synchronous HTTP callouts are often\ndescribed as “webhooks”.", "zh": "在 Kubernetes 之外，“Webhook” 这个词通常是指一种异步通知机制，\n其中 Webhook 调用将用作对另一个系统或组件的单向通知。\n在 Kubernetes 生态系统中，甚至同步的 HTTP 调用也经常被描述为 “Webhook”。\n{{< /note >}}"}
{"en": "In the webhook model, Kubernetes makes a network request to a remote service.\nWith the alternative *binary Plugin* model, Kubernetes executes a binary (program).\nBinary plugins are used by the kubelet (for example, [CSI storage plugins](https://kubernetes-csi.github.io/docs/)\nand [CNI network plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)),\nand by kubectl (see [Extend kubectl with plugins](/docs/tasks/extend-kubectl/kubectl-plugins/)).", "zh": "在 Webhook 模型中，Kubernetes 向远程服务发起网络请求。\n在另一种称作**可执行文件插件（Binary Plugin）** 模型中，Kubernetes 执行某个可执行文件（程序）。\n这些可执行文件插件由 kubelet（例如，[CSI 存储插件](https://kubernetes-csi.github.io/docs/)和\n[CNI 网络插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)）\n和 kubectl 使用。"}
{"en": "### Extension points\n\nThis diagram shows the extension points in a Kubernetes cluster and the\nclients that access it.", "zh": "### 扩展点   {#extension-points}\n\n下图展示了 Kubernetes 集群中的这些扩展点及其访问集群的客户端。"}
{"en": "image source: https://docs.google.com/drawings/d/1k2YdJgNTtNfW7_A8moIIkij-DmVgEhNrn3y2OODwqQQ/view", "zh": "{{< figure src=\"/docs/concepts/extend-kubernetes/extension-points.png\"\n    alt=\"用符号表示的七个编号的 Kubernetes 扩展点\"\n    class=\"diagram-large\" caption=\"Kubernetes 扩展点\" >}}"}
{"en": "#### Key to the figure", "zh": "#### 图示要点   {#key-to-the-figure}"}
{"en": "1. Users often interact with the Kubernetes API using `kubectl`. [Plugins](#client-extensions)\n   customise the behaviour of clients. There are generic extensions that can apply to different clients,\n   as well as specific ways to extend `kubectl`.\n\n1. The API server handles all requests. Several types of extension points in the API server allow\n   authenticating requests, or blocking them based on their content, editing content, and handling\n   deletion. These are described in the [API Access Extensions](#api-access-extensions) section.\n\n1. The API server serves various kinds of *resources*. *Built-in resource kinds*, such as\n   `pods`, are defined by the Kubernetes project and can't be changed.\n   Read [API extensions](#api-extensions) to learn about extending the Kubernetes API.", "zh": "1. 用户通常使用 `kubectl` 与 Kubernetes API 交互。\n   [插件](#client-extensions)定制客户端的行为。\n   有一些通用的扩展可以应用到不同的客户端，还有一些特定的方式可以扩展 `kubectl`。\n\n2. API 服务器处理所有请求。API 服务器中的几种扩展点能够使用户对请求执行身份认证、\n   基于其内容阻止请求、编辑请求内容、处理删除操作等等。\n   这些扩展点在 [API 访问扩展](#api-access-extensions)节详述。\n\n3. API 服务器能提供各种类型的**资源（Resources）** 服务。\n   诸如 `pods` 的**内置资源类型**是由 Kubernetes 项目所定义的，无法改变。\n   请查阅 [API 扩展](#api-extensions)了解如何扩展 Kubernetes API。"}
{"en": "1. The Kubernetes scheduler [decides](/docs/concepts/scheduling-eviction/assign-pod-node/)\n   which nodes to place pods on. There are several ways to extend scheduling, which are\n   described in the [Scheduling extensions](#scheduling-extensions) section.\n\n1. Much of the behavior of Kubernetes is implemented by programs called\n   {{< glossary_tooltip term_id=\"controller\" text=\"controllers\" >}}, that are\n   clients of the API server. Controllers are often used in conjunction with custom resources.\n   Read [combining new APIs with automation](#combining-new-apis-with-automation) and\n   [Changing built-in resources](#changing-built-in-resources) to learn more.", "zh": "4. Kubernetes 调度器负责[决定](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/)\n   Pod 要放置到哪些节点上执行。有几种方式来扩展调度行为，这些方法将在[调度器扩展](#scheduling-extensions)节中展开说明。\n\n5. Kubernetes 中的很多行为都是通过称为{{< glossary_tooltip term_id=\"controller\" text=\"控制器（Controller）\" >}}的程序来实现的，\n   这些程序也都是 API 服务器的客户端。控制器常常与定制资源结合使用。\n   进一步了解请查阅[结合使用新的 API 与自动化组件](#combining-new-apis-with-automation)和[更改内置资源](#changing-built-in-resources)。"}
{"en": "1. The kubelet runs on servers (nodes), and helps pods appear like virtual servers with their own IPs on\n   the cluster network. [Network Plugins](#network-plugins) allow for different implementations of\n   pod networking.\n\n1. You can use [Device Plugins](#device-plugins) to integrate custom hardware or other special\n   node-local facilities, and make these available to Pods running in your cluster. The kubelet\n   includes support for working with device plugins.\n\n   The kubelet also mounts and unmounts\n   {{< glossary_tooltip text=\"volume\" term_id=\"volume\" >}} for pods and their containers.\n   You can use [Storage Plugins](#storage-plugins) to add support for new kinds\n   of storage and other volume types.", "zh": "6. Kubelet 运行在各个服务器（节点）上，帮助 Pod 展现为虚拟的服务器并在集群网络中拥有自己的 IP。\n   [网络插件](#network-plugins)使得 Kubernetes 能够采用不同实现技术来连接 Pod 网络。\n\n7. 你可以使用[设备插件](#device-plugins)集成定制硬件或其他专用的节点本地设施，\n   使得这些设施可用于集群中运行的 Pod。Kubelet 包括了对使用设备插件的支持。\n\n   kubelet 也会为 Pod 及其容器增加或解除{{< glossary_tooltip text=\"卷\" term_id=\"volume\" >}}的挂载。\n   你可以使用[存储插件](#storage-plugins)增加对新存储类别和其他卷类型的支持。"}
{"en": "#### Extension point choice flowchart {#extension-flowchart}\n\nIf you are unsure where to start, this flowchart can help. Note that some solutions may involve\nseveral types of extensions.", "zh": "#### 扩展点选择流程图   {#extension-flowchart}\n\n如果你无法确定从何处入手，下面的流程图可能对你有些帮助。\n注意，某些方案可能需要同时采用几种类型的扩展。"}
{"en": "image source for flowchart: https://docs.google.com/drawings/d/1sdviU6lDz4BpnzJNHfNpQrqI9F19QZ07KnhnxVrp2yg/edit", "zh": "{{< figure src=\"/zh-cn/docs/concepts/extend-kubernetes/flowchart.svg\"\n    alt=\"附带使用场景问题和实现指南的流程图。绿圈表示是；红圈表示否。\"\n    class=\"diagram-large\" caption=\"选择一个扩展方式的流程图指导\" >}}\n\n---"}
{"en": "## Client extensions\n\nPlugins for kubectl are separate binaries that add or replace the behavior of specific subcommands.\nThe `kubectl` tool can also integrate with [credential plugins](/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins)\nThese extensions only affect a individual user's local environment, and so cannot enforce site-wide policies.\n\nIf you want to extend the `kubectl` tool, read [Extend kubectl with plugins](/docs/tasks/extend-kubectl/kubectl-plugins/).", "zh": "## 客户端扩展   {#client-extensions}\n\nkubectl 所用的插件是单独的二进制文件，用于添加或替换特定子命令的行为。\n`kubectl` 工具还可以与[凭据插件](/zh-cn/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins)集成。\n这些扩展只影响单个用户的本地环境，因此不能强制执行站点范围的策略。\n\n如果你要扩展 `kubectl` 工具，请阅读[用插件扩展 kubectl](/zh-cn/docs/tasks/extend-kubectl/kubectl-plugins/)。"}
{"en": "## API extensions\n\n### Custom resource definitions\n\nConsider adding a _Custom Resource_ to Kubernetes if you want to define new controllers, application\nconfiguration objects or other declarative APIs, and to manage them using Kubernetes tools, such\nas `kubectl`.\n\nFor more about Custom Resources, see the\n[Custom Resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/) concept guide.", "zh": "## API 扩展  {#api-extensions}\n\n### 定制资源对象   {#custom-resource-definitions}\n\n如果你想要定义新的控制器、应用配置对象或者其他声明式 API，并且使用 Kubernetes\n工具（如 `kubectl`）来管理它们，可以考虑向 Kubernetes 添加**定制资源**。\n\n关于定制资源的更多信息，可参见[定制资源概念指南](/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources/)。"}
{"en": "### API aggregation layer\n\nYou can use Kubernetes' [API Aggregation Layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)\nto integrate the Kubernetes API with additional services such as for [metrics](/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/).", "zh": "### API 聚合层   {#api-aggregation-layer}\n\n你可以使用 Kubernetes 的\n[API 聚合层](/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)将\nKubernetes API 与其他服务集成，例如[指标](/zh-cn/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/)。"}
{"en": "### Combining new APIs with automation\n\nA combination of a custom resource API and a control loop is called the\n{{< glossary_tooltip term_id=\"controller\" text=\"controllers\" >}} pattern. If your controller takes\nthe place of a human operator deploying infrastructure based on a desired state, then the controller\nmay also be following the {{< glossary_tooltip text=\"operator pattern\" term_id=\"operator-pattern\" >}}.\nThe Operator pattern is used to manage specific applications; usually, these are applications that\nmaintain state and require care in how they are managed.\n\nYou can also make your own custom APIs and control loops that manage other resources, such as storage,\nor to define policies (such as an access control restriction).", "zh": "### 结合使用新 API 与自动化组件 {#combinding-new-apis-with-automation}\n\n定制资源 API 与控制回路的组合称作{{< glossary_tooltip term_id=\"controller\" text=\"控制器\" >}}模式。\n如果你的控制器代替人工操作员根据所需状态部署基础设施，那么控制器也可以遵循\n{{<glossary_tooltip text=\"Operator 模式\" term_id=\"operator-pattern\" >}}。\nOperator 模式用于管理特定的应用；通常，这些应用需要维护状态并需要仔细考虑状态的管理方式。\n\n你还可以创建自己的定制 API 和控制回路来管理其他资源（例如存储）或定义策略（例如访问控制限制）。"}
{"en": "### Changing built-in resources\n\nWhen you extend the Kubernetes API by adding custom resources, the added resources always fall\ninto a new API Groups. You cannot replace or change existing API groups.\nAdding an API does not directly let you affect the behavior of existing APIs (such as Pods), whereas\n_API Access Extensions_ do.", "zh": "### 更改内置资源   {#changing-built-in-resources}\n\n当你通过添加定制资源来扩展 Kubernetes 时，所添加的资源总是会被放在一个新的 API 组中。\n你不可以替换或更改现有的 API 组。添加新的 API 不会直接让你影响现有\nAPI（如 Pod）的行为，不过 **API 访问扩展**能够实现这点。"}
{"en": "## API access extensions\n\nWhen a request reaches the Kubernetes API Server, it is first _authenticated_, then _authorized_,\nand is then subject to various types of _admission control_ (some requests are in fact not\nauthenticated, and get special treatment). See\n[Controlling Access to the Kubernetes API](/docs/concepts/security/controlling-access/)\nfor more on this flow.\n\nEach of the steps in the Kubernetes authentication / authorization flow offers extension points.", "zh": "## API 访问扩展    {#api-access-extensions}\n\n当请求到达 Kubernetes API 服务器时，首先要经过**身份认证**，之后是**鉴权**操作，\n再之后要经过若干类型的**准入控制**（某些请求实际上未通过身份认证，需要特殊处理）。\n参见[控制 Kubernetes API 访问](/zh-cn/docs/concepts/security/controlling-access/)以了解此流程的细节。\n\nKubernetes 身份认证/授权流程中的每个步骤都提供了扩展点。"}
{"en": "### Authentication\n\n[Authentication](/docs/reference/access-authn-authz/authentication/) maps headers or certificates\nin all requests to a username for the client making the request.\n\nKubernetes has several built-in authentication methods that it supports. It can also sit behind an\nauthenticating proxy, and it can send a token from an `Authorization:` header to a remote service for\nverification (an [authentication webhook](/docs/reference/access-authn-authz/authentication/#webhook-token-authentication))\nif those don't meet your needs.", "zh": "### 身份认证    {#authentication}\n\n[身份认证](/zh-cn/docs/reference/access-authn-authz/authentication/)负责将所有请求中的头部或证书映射到发出该请求的客户端的用户名。\n\nKubernetes 提供若干内置的身份认证方法。它也可以运行在某种身份认证代理的后面，\n并且可以将来自 `Authorization:` 头部的令牌发送到某个远程服务\n（[认证 Webhook](/zh-cn/docs/reference/access-authn-authz/authentication/#webhook-token-authentication)\n来执行验证操作，以备内置方法无法满足你的要求。"}
{"en": "### Authorization\n\n[Authorization](/docs/reference/access-authn-authz/authorization/) determines whether specific\nusers can read, write, and do other operations on API resources. It works at the level of whole\nresources -- it doesn't discriminate based on arbitrary object fields.\n\nIf the built-in authorization options don't meet your needs, an\n[authorization webhook](/docs/reference/access-authn-authz/webhook/)\nallows calling out to custom code that makes an authorization decision.", "zh": "### 鉴权    {#authorization}\n\n[鉴权](/zh-cn/docs/reference/access-authn-authz/authorization/)操作负责确定特定的用户是否可以读、写 API\n资源或对其执行其他操作。此操作仅在整个资源集合的层面进行。\n换言之，它不会基于对象的特定字段作出不同的判决。\n\n如果内置的鉴权选项无法满足你的需要，\n你可以使用[鉴权 Webhook](/zh-cn/docs/reference/access-authn-authz/webhook/)\n来调用用户提供的代码，执行定制的鉴权决定。"}
{"en": "### Dynamic admission control\n\nAfter a request is authorized, if it is a write operation, it also goes through\n[Admission Control](/docs/reference/access-authn-authz/admission-controllers/) steps.\nIn addition to the built-in steps, there are several extensions:\n\n* The [Image Policy webhook](/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook)\n  restricts what images can be run in containers.\n* To make arbitrary admission control decisions, a general\n  [Admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)\n  can be used. Admission webhooks can reject creations or updates.\n  Some admission webhooks modify the incoming request data before it is handled further by Kubernetes.", "zh": "### 动态准入控制  {#dynamic-admission-control}\n\n请求的鉴权操作结束之后，如果请求的是写操作，\n还会经过[准入控制](/zh-cn/docs/reference/access-authn-authz/admission-controllers/)处理步骤。\n除了内置的处理步骤，还存在一些扩展点：\n\n* [镜像策略 Webhook](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook)\n  能够限制容器中可以运行哪些镜像。\n* 为了执行任意的准入控制决定，\n  可以使用一种通用的[准入 Webhook](/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)\n  机制。这类准入 Webhook 可以拒绝创建或更新请求。\n  一些准入 Webhook 会先修改传入的请求数据，才会由 Kubernetes 进一步处理这些传入请求数据。"}
{"en": "## Infrastructure extensions\n\n### Device plugins\n\n_Device plugins_ allow a node to discover new Node resources (in addition to the\nbuiltin ones like cpu and memory) via a\n[Device Plugin](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/).", "zh": "## 基础设施扩展    {#infrastructure-extensions}\n\n### 设备插件   {#device-plugins}\n\n**设备插件**允许一个节点通过[设备插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)发现新的\nNode 资源（除了内置的类似 CPU 和内存这类资源之外）。"}
{"en": "### Storage plugins\n\n{{< glossary_tooltip text=\"Container Storage Interface\" term_id=\"csi\" >}} (CSI) plugins provide\na way to extend Kubernetes with supports for new kinds of volumes. The volumes can be backed by\ndurable external storage, or provide ephemeral storage, or they might offer a read-only interface\nto information using a filesystem paradigm.\n\nKubernetes also includes support for [FlexVolume](/docs/concepts/storage/volumes/#flexvolume) plugins,\nwhich are deprecated since Kubernetes v1.23 (in favour of CSI).", "zh": "### 存储插件  {#storage-plugins}\n\n{{< glossary_tooltip text=\"容器存储接口\" term_id=\"csi\" >}} (CSI) 插件提供了一种扩展\nKubernetes 的方式使其支持新类别的卷。\n这些卷可以由持久的外部存储提供支持，可以提供临时存储，还可以使用文件系统范型为信息提供只读接口。\n\nKubernetes 还包括对 [FlexVolume](/zh-cn/docs/concepts/storage/volumes/#flexvolume)\n插件的支持，该插件自 Kubernetes v1.23 起被弃用（被 CSI 替代）。"}
{"en": "FlexVolume plugins allow users to mount volume types that aren't natively supported by Kubernetes. When\nyou run a Pod that relies on FlexVolume storage, the kubelet calls a binary plugin to mount the volume.\nThe archived [FlexVolume](https://git.k8s.io/design-proposals-archive/storage/flexvolume-deployment.md)\ndesign proposal has more detail on this approach.\n\nThe [Kubernetes Volume Plugin FAQ for Storage Vendors](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors)\nincludes general information on storage plugins.", "zh": "FlexVolume 插件允许用户挂载 Kubernetes 本身不支持的卷类型。\n当你运行依赖于 FlexVolume 存储的 Pod 时，kubelet 会调用一个二进制插件来挂载该卷。\n归档的 [FlexVolume](https://git.k8s.io/design-proposals-archive/storage/flexvolume-deployment.md)\n设计提案对此方法有更多详细说明。\n\n[Kubernetes 存储供应商的卷插件 FAQ](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors)\n包含了有关存储插件的通用信息。"}
{"en": "### Network plugins\n\nYour Kubernetes cluster needs a _network plugin_ in order to have a working Pod network\nand to support other aspects of the Kubernetes network model.\n\n[Network Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)\nallow Kubernetes to work with different networking topologies and technologies.", "zh": "### 网络插件   {#network-plugins}\n\n你的 Kubernetes 集群需要一个**网络插件**才能拥有一个正常工作的 Pod 网络，\n才能支持 Kubernetes 网络模型的其他方面。\n\n[网络插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)可以让\nKubernetes 使用不同的网络拓扑和技术。"}
{"en": "### Kubelet image credential provider plugins\n\n{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}\nKubelet image credential providers are plugins for the kubelet to dynamically retrieve image registry\ncredentials. The credentials are then used when pulling images from container image registries that\nmatch the configuration.\n\nThe plugins can communicate with external services or use local files to obtain credentials. This way,\nthe kubelet does not need to have static credentials for each registry, and can support various\nauthentication methods and protocols.\n\nFor plugin configuration details, see\n[Configure a kubelet image credential provider](/docs/tasks/administer-cluster/kubelet-credential-provider/).", "zh": "### Kubelet 镜像凭据提供程序插件   {#kubelet-image-credential-provider-plugins}\n\n{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}\nKubelet 镜像凭据提供程序是 Kubelet 动态检索镜像仓库凭据的插件。\n当你从与配置匹配的容器镜像仓库中拉取镜像时，这些凭据将被使用。\n\n这些插件可以与外部服务通信或使用本地文件来获取凭据。这样，kubelet\n就不需要为每个仓库都设置静态凭据，并且可以支持各种身份验证方法和协议。\n\n有关插件配置的详细信息，请参阅\n[配置 kubelet 镜像凭据提供程序](/zh-cn/docs/tasks/administer-cluster/kubelet-credential-provider/)。"}
{"en": "## Scheduling extensions\n\nThe scheduler is a special type of controller that watches pods, and assigns\npods to nodes. The default scheduler can be replaced entirely, while\ncontinuing to use other Kubernetes components, or\n[multiple schedulers](/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)\ncan run at the same time.\n\nThis is a significant undertaking, and almost all Kubernetes users find they\ndo not need to modify the scheduler.", "zh": "## 调度扩展   {#scheduling-extensions}\n\n调度器是一种特殊的控制器，负责监视 Pod 变化并将 Pod 分派给节点。\n默认的调度器可以被整体替换掉，同时继续使用其他 Kubernetes 组件。\n或者也可以在同一时刻使用[多个调度器](/zh-cn/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)。\n\n这是一项非同小可的任务，几乎绝大多数 Kubernetes\n用户都会发现其实他们不需要修改调度器。"}
{"en": "You can control which [scheduling plugins](/docs/reference/scheduling/config/#scheduling-plugins)\nare active, or associate sets of plugins with different named [scheduler profiles](/docs/reference/scheduling/config/#multiple-profiles).\nYou can also write your own plugin that integrates with one or more of the kube-scheduler's\n[extension points](/docs/concepts/scheduling-eviction/scheduling-framework/#extension-points).\n\nFinally, the built in `kube-scheduler` component supports a\n[webhook](https://git.k8s.io/design-proposals-archive/scheduling/scheduler_extender.md)\nthat permits a remote HTTP backend (scheduler extension) to filter and / or prioritize\nthe nodes that the kube-scheduler chooses for a pod.", "zh": "你可以控制哪些[调度插件](/zh-cn/docs/reference/scheduling/config/#scheduling-plugins)处于激活状态，\n或将插件集关联到名字不同的[调度器配置文件](/zh-cn/docs/reference/scheduling/config/#multiple-profiles)上。\n你还可以编写自己的插件，与一个或多个 kube-scheduler\n的[扩展点](/zh-cn/docs/concepts/scheduling-eviction/scheduling-framework/#extension-points)集成。\n\n最后，内置的 `kube-scheduler` 组件支持\n[Webhook](https://git.k8s.io/design-proposals-archive/scheduling/scheduler_extender.md)，\n从而允许远程 HTTP 后端（调度器扩展）来为 kube-scheduler 选择的 Pod 所在节点执行过滤和优先排序操作。\n\n{{< note >}}"}
{"en": "You can only affect node filtering\nand node prioritization with a scheduler extender webhook; other extension points are\nnot available through the webhook integration.", "zh": "你只能使用调度器扩展程序 Webhook 来影响节点过滤和节点优先排序；\n其他扩展点无法通过集成 Webhook 获得。\n{{< /note >}}\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn more about infrastructure extensions\n  * [Device Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)\n  * [Network Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)\n  * CSI [storage plugins](https://kubernetes-csi.github.io/docs/)\n* Learn about [kubectl plugins](/docs/tasks/extend-kubectl/kubectl-plugins/)\n* Learn more about [Custom Resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\n* Learn more about [Extension API Servers](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)\n* Learn about [Dynamic admission control](/docs/reference/access-authn-authz/extensible-admission-controllers/)\n* Learn about the [Operator pattern](/docs/concepts/extend-kubernetes/operator/)", "zh": "* 进一步了解基础设施扩展\n  * [设备插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)\n  * [网络插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)\n  * CSI [存储插件](https://kubernetes-csi.github.io/docs/)\n* 进一步了解 [kubectl 插件](/zh-cn/docs/tasks/extend-kubectl/kubectl-plugins/)\n* 进一步了解[定制资源](/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\n* 进一步了解[扩展 API 服务器](/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)\n* 进一步了解[动态准入控制](/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/)\n* 进一步了解 [Operator 模式](/zh-cn/docs/concepts/extend-kubernetes/operator/)"}
{"en": "Kubernetes (version 1.3 through to the latest {{< skew latestVersion >}}, and likely onwards) lets you use\n[Container Network Interface](https://github.com/containernetworking/cni)\n(CNI) plugins for cluster networking. You must use a CNI plugin that is compatible with your\ncluster and that suits your needs. Different plugins are available (both open- and closed- source)\nin the wider Kubernetes ecosystem.", "zh": "Kubernetes（1.3 版本至最新 {{< skew latestVersion >}}，并可能包括未来版本）\n允许你使用[容器网络接口](https://github.com/containernetworking/cni)（CNI）\n插件来完成集群联网。\n你必须使用和你的集群相兼容并且满足你的需求的 CNI 插件。\n在更广泛的 Kubernetes 生态系统中你可以使用不同的插件（开源和闭源）。"}
{"en": "A CNI plugin is required to implement the\n[Kubernetes network model](/docs/concepts/services-networking/#the-kubernetes-network-model).", "zh": "要实现 [Kubernetes 网络模型](/zh-cn/docs/concepts/services-networking/#the-kubernetes-network-model)，你需要一个 CNI 插件。"}
{"en": "You must use a CNI plugin that is compatible with the \n[v0.4.0](https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md) or later\nreleases of the CNI specification. The Kubernetes project recommends using a plugin that is\ncompatible with the [v1.0.0](https://github.com/containernetworking/cni/blob/spec-v1.0.0/SPEC.md)\nCNI specification (plugins can be compatible with multiple spec versions).", "zh": "你必须使用与 [v0.4.0](https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md)\n或更高版本的 CNI 规范相符合的 CNI 插件。\nKubernetes 推荐使用一个兼容 [v1.0.0](https://github.com/containernetworking/cni/blob/spec-v1.0.0/SPEC.md)\nCNI 规范的插件（插件可以兼容多个规范版本）。"}
{"en": "## Installation\n\nA Container Runtime, in the networking context, is a daemon on a node configured to provide CRI\nServices for kubelet. In particular, the Container Runtime must be configured to load the CNI\nplugins required to implement the Kubernetes network model.", "zh": "## 安装   {#installation}\n\n在网络语境中，容器运行时（Container Runtime）是在节点上的守护进程，\n被配置用来为 kubelet 提供 CRI 服务。具体而言，容器运行时必须配置为加载所需的\nCNI 插件，从而实现 Kubernetes 网络模型。\n\n{{< note >}}"}
{"en": "Prior to Kubernetes 1.24, the CNI plugins could also be managed by the kubelet using the\n`cni-bin-dir` and `network-plugin` command-line parameters.\nThese command-line parameters were removed in Kubernetes 1.24, with management of the CNI no\nlonger in scope for kubelet.", "zh": "在 Kubernetes 1.24 之前，CNI 插件也可以由 kubelet 使用命令行参数 `cni-bin-dir`\n和 `network-plugin` 管理。Kubernetes 1.24 移除了这些命令行参数，\nCNI 的管理不再是 kubelet 的工作。"}
{"en": "See [Troubleshooting CNI plugin-related errors](/docs/tasks/administer-cluster/migrating-from-dockershim/troubleshooting-cni-plugin-related-errors/)\nif you are facing issues following the removal of dockershim.", "zh": "如果你在移除 dockershim 之后遇到问题，\n请参阅[排查 CNI 插件相关的错误](/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/troubleshooting-cni-plugin-related-errors/)。\n{{< /note >}}"}
{"en": "For specific information about how a Container Runtime manages the CNI plugins, see the\ndocumentation for that Container Runtime, for example:", "zh": "要了解容器运行时如何管理 CNI 插件的具体信息，可参见对应容器运行时的文档，例如：\n\n- [containerd](https://github.com/containerd/containerd/blob/main/script/setup/install-cni)\n- [CRI-O](https://github.com/cri-o/cri-o/blob/main/contrib/cni/README.md)"}
{"en": "For specific information about how to install and manage a CNI plugin, see the documentation for\nthat plugin or [networking provider](/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model).", "zh": "要了解如何安装和管理 CNI 插件的具体信息，可参阅对应的插件或\n[网络驱动（Networking Provider）](/zh-cn/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model)\n的文档。"}
{"en": "## Network Plugin Requirements\n\n### Loopback CNI\n\nIn addition to the CNI plugin installed on the nodes for implementing the Kubernetes network\nmodel, Kubernetes also requires the container runtimes to provide a loopback interface `lo`, which\nis used for each sandbox (pod sandboxes, vm sandboxes, ...).\nImplementing the loopback interface can be accomplished by re-using the\n[CNI loopback plugin.](https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback/loopback.go)\nor by developing your own code to achieve this (see\n[this example from CRI-O](https://github.com/cri-o/ocicni/blob/release-1.24/pkg/ocicni/util_linux.go#L91)).", "zh": "## 网络插件要求   {#network-plugin-requirements}\n\n### 本地回路 CNI   {#loopback-cni}\n\n除了安装到节点上用于实现 Kubernetes 网络模型的 CNI 插件外，Kubernetes\n还需要容器运行时提供一个本地回路接口 `lo`，用于各个沙箱（Pod 沙箱、虚机沙箱……）。\n实现本地回路接口的工作可以通过复用\n[CNI 本地回路插件](https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback/loopback.go)来实现，\n也可以通过开发自己的代码来实现\n（参阅 [CRI-O 中的示例](https://github.com/cri-o/ocicni/blob/release-1.24/pkg/ocicni/util_linux.go#L91)）。"}
{"en": "### Support hostPort\n\nThe CNI networking plugin supports `hostPort`. You can use the official\n[portmap](https://github.com/containernetworking/plugins/tree/master/plugins/meta/portmap)\nplugin offered by the CNI plugin team or use your own plugin with portMapping functionality.\n\nIf you want to enable `hostPort` support, you must specify `portMappings capability` in your\n`cni-conf-dir`. For example:", "zh": "### 支持 hostPort   {#support-hostport}\n\nCNI 网络插件支持 `hostPort`。你可以使用官方\n[portmap](https://github.com/containernetworking/plugins/tree/master/plugins/meta/portmap)\n插件，它由 CNI 插件团队提供，或者使用你自己的带有 portMapping 功能的插件。\n\n如果你想要启动 `hostPort` 支持，则必须在 `cni-conf-dir` 指定 `portMappings capability`。\n例如：\n\n```json\n{\n  \"name\": \"k8s-pod-network\",\n  \"cniVersion\": \"0.4.0\",\n  \"plugins\": [\n    {\n      \"type\": \"calico\",\n      \"log_level\": \"info\",\n      \"datastore_type\": \"kubernetes\",\n      \"nodename\": \"127.0.0.1\",\n      \"ipam\": {\n        \"type\": \"host-local\",\n        \"subnet\": \"usePodCidr\"\n      },\n      \"policy\": {\n        \"type\": \"k8s\"\n      },\n      \"kubernetes\": {\n        \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\"\n      }\n    },\n    {\n      \"type\": \"portmap\",\n      \"capabilities\": {\"portMappings\": true},\n      \"externalSetMarkChain\": \"KUBE-MARK-MASQ\"\n    }\n  ]\n}\n```"}
{"en": "### Support traffic shaping\n\n**Experimental Feature**\n\nThe CNI networking plugin also supports pod ingress and egress traffic shaping. You can use the\nofficial [bandwidth](https://github.com/containernetworking/plugins/tree/master/plugins/meta/bandwidth)\nplugin offered by the CNI plugin team or use your own plugin with bandwidth control functionality.\n\nIf you want to enable traffic shaping support, you must add the `bandwidth` plugin to your CNI\nconfiguration file (default `/etc/cni/net.d`) and ensure that the binary is included in your CNI\nbin dir (default `/opt/cni/bin`).", "zh": "### 支持流量整形   {#support-traffic-shaping}\n\n**实验功能**\n\nCNI 网络插件还支持 Pod 入站和出站流量整形。\n你可以使用 CNI 插件团队提供的\n[bandwidth](https://github.com/containernetworking/plugins/tree/master/plugins/meta/bandwidth)\n插件，也可以使用你自己的具有带宽控制功能的插件。\n\n如果你想要启用流量整形支持，你必须将 `bandwidth` 插件添加到 CNI 配置文件\n（默认是 `/etc/cni/net.d`）并保证该可执行文件包含在你的 CNI 的 bin\n文件夹内（默认为 `/opt/cni/bin`）。\n\n```json\n{\n  \"name\": \"k8s-pod-network\",\n  \"cniVersion\": \"0.4.0\",\n  \"plugins\": [\n    {\n      \"type\": \"calico\",\n      \"log_level\": \"info\",\n      \"datastore_type\": \"kubernetes\",\n      \"nodename\": \"127.0.0.1\",\n      \"ipam\": {\n        \"type\": \"host-local\",\n        \"subnet\": \"usePodCidr\"\n      },\n      \"policy\": {\n        \"type\": \"k8s\"\n      },\n      \"kubernetes\": {\n        \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\"\n      }\n    },\n    {\n      \"type\": \"bandwidth\",\n      \"capabilities\": {\"bandwidth\": true}\n    }\n  ]\n}\n```"}
{"en": "Now you can add the `kubernetes.io/ingress-bandwidth` and `kubernetes.io/egress-bandwidth`\nannotations to your Pod. For example:", "zh": "现在，你可以将 `kubernetes.io/ingress-bandwidth` 和 `kubernetes.io/egress-bandwidth`\n注解添加到 Pod 中。例如：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubernetes.io/ingress-bandwidth: 1M\n    kubernetes.io/egress-bandwidth: 1M\n...\n```\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Learn more about [Cluster Networking](/docs/concepts/cluster-administration/networking/)\n- Learn more about [Network Policies](/docs/concepts/services-networking/network-policies/)\n- Learn about the [Troubleshooting CNI plugin-related errors](/docs/tasks/administer-cluster/migrating-from-dockershim/troubleshooting-cni-plugin-related-errors/)", "zh": "- 进一步了解关于[集群网络](/zh-cn/docs/concepts/cluster-administration/networking/)的信息\n- 进一步了解关于[网络策略](/zh-cn/docs/concepts/services-networking/network-policies/)的信息\n- 进一步了解关于[排查 CNI 插件相关错误](/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/troubleshooting-cni-plugin-related-errors/)的信息"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}"}
{"en": "Kubernetes provides a device plugin framework that you can use to advertise system hardware\nresources to the {{< glossary_tooltip term_id=\"kubelet\" >}}.\n\nInstead of customizing the code for Kubernetes itself, vendors can implement a\ndevice plugin that you deploy either manually or as a {{< glossary_tooltip term_id=\"daemonset\" >}}.\nThe targeted devices include GPUs, high-performance NICs, FPGAs, InfiniBand adapters,\nand other similar computing resources that may require vendor specific initialization\nand setup.", "zh": "Kubernetes 提供了一个设备插件框架，你可以用它来将系统硬件资源发布到\n{{< glossary_tooltip term_id=\"kubelet\" >}}。\n\n供应商可以实现设备插件，由你手动部署或作为 {{< glossary_tooltip term_id=\"daemonset\" >}}\n来部署，而不必定制 Kubernetes 本身的代码。目标设备包括 GPU、高性能 NIC、FPGA、\nInfiniBand 适配器以及其他类似的、可能需要特定于供应商的初始化和设置的计算资源。"}
{"en": "## Device plugin registration", "zh": "## 注册设备插件    {#device-plugin-registration}"}
{"en": "The kubelet exports a `Registration` gRPC service:", "zh": "`kubelet` 提供了一个 `Registration` 的 gRPC 服务：\n\n```gRPC\nservice Registration {\n\trpc Register(RegisterRequest) returns (Empty) {}\n}\n```"}
{"en": "A device plugin can register itself with the kubelet through this gRPC service.\nDuring the registration, the device plugin needs to send:\n\n* The name of its Unix socket.\n* The Device Plugin API version against which it was built.\n* The `ResourceName` it wants to advertise. Here `ResourceName` needs to follow the\n  [extended resource naming scheme](/docs/concepts/configuration/manage-resources-containers/#extended-resources)\n  as `vendor-domain/resourcetype`.\n  (For example, an NVIDIA GPU is advertised as `nvidia.com/gpu`.)\n\nFollowing a successful registration, the device plugin sends the kubelet the\nlist of devices it manages, and the kubelet is then in charge of advertising those\nresources to the API server as part of the kubelet node status update.\nFor example, after a device plugin registers `hardware-vendor.example/foo` with the kubelet\nand reports two healthy devices on a node, the node status is updated\nto advertise that the node has 2 \"Foo\" devices installed and available.", "zh": "设备插件可以通过此 gRPC 服务在 kubelet 进行注册。在注册期间，设备插件需要发送下面几样内容：\n\n* 设备插件的 UNIX 套接字。\n* 设备插件的 API 版本。\n* `ResourceName` 是需要公布的。这里 `ResourceName`\n  需要遵循[扩展资源命名方案](/zh-cn/docs/concepts/configuration/manage-resources-containers/#extended-resources)，\n  类似于 `vendor-domain/resourcetype`。（比如 NVIDIA GPU 就被公布为 `nvidia.com/gpu`。）\n\n成功注册后，设备插件就向 kubelet 发送它所管理的设备列表，然后 kubelet\n负责将这些资源发布到 API 服务器，作为 kubelet 节点状态更新的一部分。\n\n比如，设备插件在 kubelet 中注册了 `hardware-vendor.example/foo`\n并报告了节点上的两个运行状况良好的设备后，节点状态将更新以通告该节点已安装 2 个\n\"Foo\" 设备并且是可用的。"}
{"en": "Then, users can request devices as part of a Pod specification\n(see [`container`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container)).\nRequesting extended resources is similar to how you manage requests and limits for\nother resources, with the following differences:\n* Extended resources are only supported as integer resources and cannot be overcommitted.\n* Devices cannot be shared between containers.", "zh": "然后，用户可以请求设备作为 Pod 规范的一部分，\n参见 [Container](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container)。\n请求扩展资源类似于管理请求和限制的方式，\n其他资源，有以下区别：\n\n* 扩展资源仅可作为整数资源使用，并且不能被过量使用\n* 设备不能在容器之间共享"}
{"en": "### Example {#example-pod}", "zh": "### 示例 {#example-pod}"}
{"en": "Suppose a Kubernetes cluster is running a device plugin that advertises resource `hardware-vendor.example/foo`\non certain nodes. Here is an example of a pod requesting this resource to run a demo workload:", "zh": "假设 Kubernetes 集群正在运行一个设备插件，该插件在一些节点上公布的资源为 `hardware-vendor.example/foo`。\n下面就是一个 Pod 示例，请求此资源以运行一个工作负载的示例："}
{"en": "#\n# This Pod needs 2 of the hardware-vendor.example/foo devices\n# and can only schedule onto a Node that's able to satisfy\n# that need.\n#\n# If the Node has more than 2 of those devices available, the\n# remainder would be available for other Pods to use.", "zh": "```yaml\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-pod\nspec:\n  containers:\n    - name: demo-container-1\n      image: registry.k8s.io/pause:2.0\n      resources:\n        limits:\n          hardware-vendor.example/foo: 2\n#\n# 这个 Pod 需要两个 hardware-vendor.example/foo 设备\n# 而且只能够调度到满足需求的节点上\n#\n# 如果该节点中有 2 个以上的设备可用，其余的可供其他 Pod 使用\n```"}
{"en": "## Device plugin implementation\n\nThe general workflow of a device plugin includes the following steps:\n\n1. Initialization. During this phase, the device plugin performs vendor-specific\n   initialization and setup to make sure the devices are in a ready state.\n\n1. The plugin starts a gRPC service, with a Unix socket under the host path\n   `/var/lib/kubelet/device-plugins/`, that implements the following interfaces:", "zh": "## 设备插件的实现    {#device-plugin-implementation}\n\n设备插件的常规工作流程包括以下几个步骤：\n\n1. 初始化。在这个阶段，设备插件将执行特定于供应商的初始化和设置，以确保设备处于就绪状态。\n\n2. 插件使用主机路径 `/var/lib/kubelet/device-plugins/` 下的 UNIX 套接字启动一个\n   gRPC 服务，该服务实现以下接口："}
{"en": "```gRPC\n   service DevicePlugin {\n         // GetDevicePluginOptions returns options to be communicated with Device Manager.\n         rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}\n\n         // ListAndWatch returns a stream of List of Devices\n         // Whenever a Device state change or a Device disappears, ListAndWatch\n         // returns the new list\n         rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}\n\n         // Allocate is called during container creation so that the Device\n         // Plugin can run device specific operations and instruct Kubelet\n         // of the steps to make the Device available in the container\n         rpc Allocate(AllocateRequest) returns (AllocateResponse) {}\n\n         // GetPreferredAllocation returns a preferred set of devices to allocate\n         // from a list of available ones. The resulting preferred allocation is not\n         // guaranteed to be the allocation ultimately performed by the\n         // devicemanager. It is only designed to help the devicemanager make a more\n         // informed allocation decision when possible.\n         rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}\n\n         // PreStartContainer is called, if indicated by Device Plugin during registeration phase,\n         // before each container start. Device plugin can run device specific operations\n         // such as resetting the device before making devices available to the container.\n         rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}\n   }\n   ```", "zh": "```gRPC\n   service DevicePlugin {\n         // GetDevicePluginOptions 返回与设备管理器沟通的选项。\n         rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}\n\n         // ListAndWatch 返回 Device 列表构成的数据流。\n         // 当 Device 状态发生变化或者 Device 消失时，ListAndWatch\n         // 会返回新的列表。\n         rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}\n\n         // Allocate 在容器创建期间调用，这样设备插件可以运行一些特定于设备的操作，\n         // 并告诉 kubelet 如何令 Device 可在容器中访问的所需执行的具体步骤\n         rpc Allocate(AllocateRequest) returns (AllocateResponse) {}\n\n         // GetPreferredAllocation 从一组可用的设备中返回一些优选的设备用来分配，\n         // 所返回的优选分配结果不一定会是设备管理器的最终分配方案。\n         // 此接口的设计仅是为了让设备管理器能够在可能的情况下做出更有意义的决定。\n         rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}\n\n         // PreStartContainer 在设备插件注册阶段根据需要被调用，调用发生在容器启动之前。\n         // 在将设备提供给容器使用之前，设备插件可以运行一些诸如重置设备之类的特定于\n         // 具体设备的操作，\n         rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}\n   }\n   ```\n\n   {{< note >}}"}
{"en": "Plugins are not required to provide useful implementations for\n   `GetPreferredAllocation()` or `PreStartContainer()`. Flags indicating\n   the availability of these calls, if any, should be set in the `DevicePluginOptions`\n   message sent back by a call to `GetDevicePluginOptions()`. The `kubelet` will\n   always call `GetDevicePluginOptions()` to see which optional functions are\n   available, before calling any of them directly.", "zh": "插件并非必须为 `GetPreferredAllocation()` 或 `PreStartContainer()` 提供有用的实现逻辑，\n   调用 `GetDevicePluginOptions()` 时所返回的 `DevicePluginOptions`\n   消息中应该设置一些标志，表明这些调用（如果有）是否可用。`kubelet` 在直接调用这些函数之前，总会调用\n   `GetDevicePluginOptions()` 来查看哪些可选的函数可用。\n   {{< /note >}}"}
{"en": "1. The plugin registers itself with the kubelet through the Unix socket at host\n   path `/var/lib/kubelet/device-plugins/kubelet.sock`.", "zh": "3. 插件通过位于主机路径 `/var/lib/kubelet/device-plugins/kubelet.sock` 下的 UNIX\n   套接字向 kubelet 注册自身。\n\n   {{< note >}}"}
{"en": "The ordering of the workflow is important. A plugin MUST start serving gRPC\n   service before registering itself with kubelet for successful registration.", "zh": "工作流程的顺序很重要。插件必须在向 kubelet 注册自己之前开始提供 gRPC 服务，才能保证注册成功。\n   {{< /note >}}"}
{"en": "1. After successfully registering itself, the device plugin runs in serving mode, during which it keeps\n   monitoring device health and reports back to the kubelet upon any device state changes.\n   It is also responsible for serving `Allocate` gRPC requests. During `Allocate`, the device plugin may\n   do device-specific preparation; for example, GPU cleanup or QRNG initialization.\n   If the operations succeed, the device plugin returns an `AllocateResponse` that contains container\n   runtime configurations for accessing the allocated devices. The kubelet passes this information\n   to the container runtime.", "zh": "4. 成功注册自身后，设备插件将以提供服务的模式运行，在此期间，它将持续监控设备运行状况，\n   并在设备状态发生任何变化时向 kubelet 报告。它还负责响应 `Allocate` gRPC 请求。\n   在 `Allocate` 期间，设备插件可能还会做一些特定于设备的准备；例如 GPU 清理或 QRNG 初始化。\n   如果操作成功，则设备插件将返回 `AllocateResponse`，其中包含用于访问被分配的设备容器运行时的配置。\n   kubelet 将此信息传递到容器运行时。"}
{"en": "An `AllocateResponse` contains zero or more `ContainerAllocateResponse` objects. In these, the\n   device plugin defines modifications that must be made to a container's definition to provide\n   access to the device. These modifications include:", "zh": "`AllocateResponse` 包含零个或多个 `ContainerAllocateResponse` 对象。\n   设备插件在这些对象中给出为了访问设备而必须对容器定义所进行的修改。\n   这些修改包括："}
{"en": "* [Annotations](/docs/concepts/overview/working-with-objects/annotations/)\n   * device nodes\n   * environment variables\n   * mounts\n   * fully-qualified CDI device names", "zh": "* [注解](/zh-cn/docs/concepts/overview/working-with-objects/annotations/)\n   * 设备节点\n   * 环境变量\n   * 挂载点\n   * 完全限定的 CDI 设备名称\n\n   {{< note >}}"}
{"en": "The processing of the fully-qualified CDI device names by the Device Manager requires\n   that the `DevicePluginCDIDevices` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n   is enabled for both the kubelet and the kube-apiserver. This was added as an alpha feature in Kubernetes\n   v1.28, graduated to beta in v1.29 and to GA in v1.31.", "zh": "设备管理器处理完全限定的 CDI 设备名称时，\n   需要为 kubelet 和 kube-apiserver 启用 `DevicePluginCDIDevices`\n   [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)。\n   在 Kubernetes v1.28 版本中作为 Alpha 特性被加入，在 v1.29 版本中升级为 Beta 特性并在\n   v1.31 版本升级为稳定可用特性。\n   {{< /note >}}"}
{"en": "### Handling kubelet restarts\n\nA device plugin is expected to detect kubelet restarts and re-register itself with the new\nkubelet instance. A new kubelet instance deletes all the existing Unix sockets under\n`/var/lib/kubelet/device-plugins` when it starts. A device plugin can monitor the deletion\nof its Unix socket and re-register itself upon such an event.", "zh": "### 处理 kubelet 重启   {#handling-kubelet-restarts}\n\n设备插件应能监测到 kubelet 重启，并且向新的 kubelet 实例来重新注册自己。\n新的 kubelet 实例启动时会删除 `/var/lib/kubelet/device-plugins` 下所有已经存在的 UNIX 套接字。\n设备插件需要能够监控到它的 UNIX 套接字被删除，并且当发生此类事件时重新注册自己。"}
{"en": "### Device plugin and unhealthy devices\n\nThere are cases when devices fail or are shut down. The responsibility of the Device Plugin\nin this case is to notify the kubelet about the situation using the `ListAndWatchResponse` API.", "zh": "### 设备插件和不健康的设备\n\n有时会发生设备出现故障或者被关闭的情况，这时，设备插件的职责是使用\n`ListAndWatch Response` API 将相关情况通报给 kubelet。"}
{"en": "Once a device is marked as unhealthy, the kubelet will decrease the allocatable count\nfor this resource on the Node to reflect how many devices can be used for scheduling new pods.\nCapacity count for the resource will not change.", "zh": "一旦设备被标记为不健康，kubelet 将减少节点上此资源的可分配数量，\n以反映有多少设备可用于调度新的 Pod，资源的容量数量不会因此发生改变。"}
{"en": "Pods that were assigned to the failed devices will continue be assigned to this device.\nIt is typical that code relying on the device will start failing and Pod may get\ninto Failed phase if `restartPolicy` for the Pod was not `Always` or enter the crash loop\notherwise.", "zh": "分配给故障设备的 Pod 将继续分配给该设备。\n通常情况下，依赖于设备的代码将开始失败，如果 Pod 的 `restartPolicy` 不是\n`Always`，则 Pod 可能会进入 Failed 阶段，否则会进入崩溃循环。"}
{"en": "Before Kubernetes v1.31, the way to know whether or not a Pod is associated with the\nfailed device is to use the [PodResources API](#monitoring-device-plugin-resources).", "zh": "在 Kubernetes v1.31 之前，要知道 Pod 是否与故障设备关联，\n可以使用 [PodResources API](#monitoring-device-plugin-resources)。\n\n{{< feature-state feature_gate_name=\"ResourceHealthStatus\" >}}"}
{"en": "By enabling the feature gate `ResourceHealthStatus`, the field `allocatedResourcesStatus`\nwill be added to each container status, within the `.status` for each Pod. The `allocatedResourcesStatus`\nfield\nreports health information for each device assigned to the container.", "zh": "通过启用特性门控 `ResourceHealthStatus`，系统将在每个 Pod 的\n`.status` 字段中的每个容器状态内添加 `allocatedResourcesStatus` 字段，\n`allocatedResourcesStatus` 字段报告分配给容器的每个设备的健康信息。"}
{"en": "For a failed Pod, or or where you suspect a fault, you can use this status to understand whether\nthe Pod behavior may be associated with device failure. For example, if an accelerator is reporting\nan over-temperature event, the `allocatedResourcesStatus` field may be able to report this.", "zh": "对于发生故障的 Pod，或者你怀疑存在故障的情况，你可以使用此状态来了解\nPod 行为是否可能与设备故障有关。例如，如果加速器报告过热事件，\n则 `allocatedResourcesStatus` 字段可能能够报告此情况。"}
{"en": "## Device plugin deployment\n\nYou can deploy a device plugin as a DaemonSet, as a package for your node's operating system,\nor manually.\n\nThe canonical directory `/var/lib/kubelet/device-plugins` requires privileged access,\nso a device plugin must run in a privileged security context.\nIf you're deploying a device plugin as a DaemonSet, `/var/lib/kubelet/device-plugins`\nmust be mounted as a {{< glossary_tooltip term_id=\"volume\" >}}\nin the plugin's [PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core).\n\nIf you choose the DaemonSet approach you can rely on Kubernetes to: place the device plugin's\nPod onto Nodes, to restart the daemon Pod after failure, and to help automate upgrades.", "zh": "## 设备插件部署   {#device-plugin-deployment}\n\n你可以将你的设备插件作为节点操作系统的软件包来部署、作为 DaemonSet 来部署或者手动部署。\n\n规范目录 `/var/lib/kubelet/device-plugins` 是需要特权访问的，\n所以设备插件必须要在被授权的安全的上下文中运行。\n如果你将设备插件部署为 DaemonSet，`/var/lib/kubelet/device-plugins` 目录必须要在插件的\n[PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core)\n中声明作为 {{< glossary_tooltip term_id=\"volume\" >}} 被挂载到插件中。\n\n如果你选择 DaemonSet 方法，你可以通过 Kubernetes 进行以下操作：\n将设备插件的 Pod 放置在节点上，在出现故障后重新启动守护进程 Pod，来进行自动升级。"}
{"en": "## API compatibility\n\nPreviously, the versioning scheme required the Device Plugin's API version to match\nexactly the Kubelet's version. Since the graduation of this feature to Beta in v1.12\nthis is no longer a hard requirement. The API is versioned and has been stable since\nBeta graduation of this feature. Because of this, kubelet upgrades should be seamless\nbut there still may be changes in the API before stabilization making upgrades not\nguaranteed to be non-breaking.", "zh": "## API 兼容性   {#api-compatibility}\n\n之前版本控制方案要求设备插件的 API 版本与 kubelet 的版本完全匹配。\n自从此特性在 v1.12 中进阶为 Beta 后，这不再是硬性要求。\nAPI 是版本化的，并且自此特性进阶 Beta 后一直表现稳定。\n因此，kubelet 升级应该是无缝的，但在稳定之前 API 仍然可能会有变更，还不能保证升级不会中断。\n\n{{< note >}}"}
{"en": "Although the Device Manager component of Kubernetes is a generally available feature,\nthe _device plugin API_ is not stable. For information on the device plugin API and\nversion compatibility, read [Device Plugin API versions](/docs/reference/node/device-plugin-api-versions/).", "zh": "尽管 Kubernetes 的设备管理器（Device Manager）组件是正式发布的特性，\n但**设备插件 API** 还不稳定。有关设备插件 API 和版本兼容性的信息，\n请参阅[设备插件 API 版本](/zh-cn/docs/reference/node/device-plugin-api-versions/)。\n{{< /note >}}"}
{"en": "As a project, Kubernetes recommends that device plugin developers:\n\n* Watch for Device Plugin API changes in the future releases.\n* Support multiple versions of the device plugin API for backward/forward compatibility.", "zh": "作为一个项目，Kubernetes 建议设备插件开发者：\n\n* 注意未来版本中设备插件 API 的变更。\n* 支持多个版本的设备插件 API，以实现向后/向前兼容性。"}
{"en": "To run device plugins on nodes that need to be upgraded to a Kubernetes release with\na newer device plugin API version, upgrade your device plugins to support both versions\nbefore upgrading these nodes. Taking that approach will ensure the continuous functioning\nof the device allocations during the upgrade.", "zh": "若在需要升级到具有较新设备插件 API 版本的某个 Kubernetes 版本的节点上运行这些设备插件，\n请在升级这些节点之前先升级设备插件以支持这两个版本。\n采用该方法将确保升级期间设备分配的连续运行。"}
{"en": "## Monitoring device plugin resources", "zh": "## 监控设备插件资源   {#monitoring-device-plugin-resources}\n\n{{< feature-state for_k8s_version=\"v1.28\" state=\"stable\" >}}"}
{"en": "In order to monitor resources provided by device plugins, monitoring agents need to be able to\ndiscover the set of devices that are in-use on the node and obtain metadata to describe which\ncontainer the metric should be associated with. [Prometheus](https://prometheus.io/) metrics\nexposed by device monitoring agents should follow the\n[Kubernetes Instrumentation Guidelines](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/instrumentation.md),\nidentifying containers using `pod`, `namespace`, and `container` prometheus labels.", "zh": "为了监控设备插件提供的资源，监控代理程序需要能够发现节点上正在使用的设备，\n并获取元数据来描述哪个指标与容器相关联。\n设备监控代理暴露给 [Prometheus](https://prometheus.io/) 的指标应该遵循\n[Kubernetes Instrumentation Guidelines（英文）](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/instrumentation.md)，\n使用 `pod`、`namespace` 和 `container` 标签来标识容器。"}
{"en": "The kubelet provides a gRPC service to enable discovery of in-use devices, and to provide metadata\nfor these devices:\n\n```gRPC\n// PodResourcesLister is a service provided by the kubelet that provides information about the\n// node resources consumed by pods and containers on the node\nservice PodResourcesLister {\n    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}\n    rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}\n    rpc Get(GetPodResourcesRequest) returns (GetPodResourcesResponse) {}\n}\n```", "zh": "kubelet 提供了 gRPC 服务来使得正在使用中的设备被发现，并且还为这些设备提供了元数据：\n\n```gRPC\n// PodResourcesLister 是一个由 kubelet 提供的服务，用来提供供节点上\n// Pod 和容器使用的节点资源的信息\nservice PodResourcesLister {\n    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}\n    rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}\n    rpc Get(GetPodResourcesRequest) returns (GetPodResourcesResponse) {}\n}\n```"}
{"en": "### `List` gRPC endpoint {#grpc-endpoint-list}", "zh": "### `List` gRPC 端点 {#grpc-endpoint-list}"}
{"en": "The `List` endpoint provides information on resources of running pods, with details such as the\nid of exclusively allocated CPUs, device id as it was reported by device plugins and id of\nthe NUMA node where these devices are allocated. Also, for NUMA-based machines, it contains the\ninformation about memory and hugepages reserved for a container.", "zh": "这一 `List` 端点提供运行中 Pod 的资源信息，包括类似独占式分配的\nCPU ID、设备插件所报告的设备 ID 以及这些设备分配所处的 NUMA 节点 ID。\n此外，对于基于 NUMA 的机器，它还会包含为容器保留的内存和大页的信息。"}
{"en": "Starting from Kubernetes v1.27, the `List` endpoint can provide information on resources\nof running pods allocated in `ResourceClaims` by the `DynamicResourceAllocation` API. To enable\nthis feature `kubelet` must be started with the following flags:", "zh": "从 Kubernetes v1.27 开始，`List` 端点可以通过 `DynamicResourceAllocation` API 提供在\n`ResourceClaims` 中分配的当前运行 Pod 的资源信息。\n要启用此特性，必须使用以下标志启动 `kubelet`：\n\n```\n--feature-gates=DynamicResourceAllocation=true,KubeletPodResourcesDynamicResources=true\n```"}
{"en": "```gRPC\n// ListPodResourcesResponse is the response returned by List function\nmessage ListPodResourcesResponse {\n    repeated PodResources pod_resources = 1;\n}\n\n// PodResources contains information about the node resources assigned to a pod\nmessage PodResources {\n    string name = 1;\n    string namespace = 2;\n    repeated ContainerResources containers = 3;\n}\n\n// ContainerResources contains information about the resources assigned to a container\nmessage ContainerResources {\n    string name = 1;\n    repeated ContainerDevices devices = 2;\n    repeated int64 cpu_ids = 3;\n    repeated ContainerMemory memory = 4;\n    repeated DynamicResource dynamic_resources = 5;\n}\n\n// ContainerMemory contains information about memory and hugepages assigned to a container\nmessage ContainerMemory {\n    string memory_type = 1;\n    uint64 size = 2;\n    TopologyInfo topology = 3;\n}\n\n// Topology describes hardware topology of the resource\nmessage TopologyInfo {\n        repeated NUMANode nodes = 1;\n}\n\n// NUMA representation of NUMA node\nmessage NUMANode {\n        int64 ID = 1;\n}\n\n// ContainerDevices contains information about the devices assigned to a container\nmessage ContainerDevices {\n    string resource_name = 1;\n    repeated string device_ids = 2;\n    TopologyInfo topology = 3;\n}\n\n// DynamicResource contains information about the devices assigned to a container by Dynamic Resource Allocation\nmessage DynamicResource {\n    string class_name = 1;\n    string claim_name = 2;\n    string claim_namespace = 3;\n    repeated ClaimResource claim_resources = 4;\n}\n\n// ClaimResource contains per-plugin resource information\nmessage ClaimResource {\n    repeated CDIDevice cdi_devices = 1 [(gogoproto.customname) = \"CDIDevices\"];\n}\n\n// CDIDevice specifies a CDI device information\nmessage CDIDevice {\n    // Fully qualified CDI device name\n    // for example: vendor.com/gpu=gpudevice1\n    // see more details in the CDI specification:\n    // https://github.com/container-orchestrated-devices/container-device-interface/blob/main/SPEC.md\n    string name = 1;\n}\n```", "zh": "```gRPC\n// ListPodResourcesResponse 是 List 函数的响应\nmessage ListPodResourcesResponse {\n    repeated PodResources pod_resources = 1;\n}\n\n// PodResources 包含关于分配给 Pod 的节点资源的信息\nmessage PodResources {\n    string name = 1;\n    string namespace = 2;\n    repeated ContainerResources containers = 3;\n}\n\n// ContainerResources 包含分配给容器的资源的信息\nmessage ContainerResources {\n    string name = 1;\n    repeated ContainerDevices devices = 2;\n    repeated int64 cpu_ids = 3;\n    repeated ContainerMemory memory = 4;\n    repeated DynamicResource dynamic_resources = 5;\n}\n\n// ContainerMemory 包含分配给容器的内存和大页信息\nmessage ContainerMemory {\n    string memory_type = 1;\n    uint64 size = 2;\n    TopologyInfo topology = 3;\n}\n\n// Topology 描述资源的硬件拓扑结构\nmessage TopologyInfo {\n        repeated NUMANode nodes = 1;\n}\n\n// NUMA 代表的是 NUMA 节点\nmessage NUMANode {\n        int64 ID = 1;\n}\n\n// ContainerDevices 包含分配给容器的设备信息\nmessage ContainerDevices {\n    string resource_name = 1;\n    repeated string device_ids = 2;\n    TopologyInfo topology = 3;\n}\n\n// DynamicResource 包含通过 Dynamic Resource Allocation 分配到容器的设备信息\nmessage DynamicResource {\n    string class_name = 1;\n    string claim_name = 2;\n    string claim_namespace = 3;\n    repeated ClaimResource claim_resources = 4;\n}\n\n// ClaimResource 包含每个插件的资源信息\nmessage ClaimResource {\n    repeated CDIDevice cdi_devices = 1 [(gogoproto.customname) = \"CDIDevices\"];\n}\n\n// CDIDevice 指定 CDI 设备信息\nmessage CDIDevice {\n    // 完全合格的 CDI 设备名称\n    // 例如：vendor.com/gpu=gpudevice1\n    // 参阅 CDI 规范中的更多细节：\n    // https://github.com/container-orchestrated-devices/container-device-interface/blob/main/SPEC.md\n    string name = 1;\n}\n```\n\n{{< note >}}"}
{"en": "cpu_ids in the `ContainerResources` in the `List` endpoint correspond to exclusive CPUs allocated\nto a particular container. If the goal is to evaluate CPUs that belong to the shared pool, the `List`\nendpoint needs to be used in conjunction with the `GetAllocatableResources` endpoint as explained\nbelow:\n1. Call `GetAllocatableResources` to get a list of all the allocatable CPUs\n2. Call `GetCpuIds` on all `ContainerResources` in the system\n3. Subtract out all of the CPUs from the `GetCpuIds` calls from the `GetAllocatableResources` call", "zh": "`List` 端点中的 `ContainerResources` 中的 cpu_ids 对应于分配给某个容器的专属 CPU。\n如果要统计共享池中的 CPU，`List` 端点需要与 `GetAllocatableResources` 端点一起使用，如下所述：\n\n1. 调用 `GetAllocatableResources` 获取所有可用的 CPU。\n2. 在系统中所有的 `ContainerResources` 上调用 `GetCpuIds`。\n3. 用 `GetAllocatableResources` 获取的 CPU 数减去 `GetCpuIds` 获取的 CPU 数。\n{{< /note >}}"}
{"en": "### `GetAllocatableResources` gRPC endpoint {#grpc-endpoint-getallocatableresources}", "zh": "### `GetAllocatableResources` gRPC 端点 {#grpc-endpoint-getallocatableresources}\n\n{{< feature-state state=\"stable\" for_k8s_version=\"v1.28\" >}}"}
{"en": "GetAllocatableResources provides information on resources initially available on the worker node.\nIt provides more information than kubelet exports to APIServer.", "zh": "端点 `GetAllocatableResources` 提供工作节点上原始可用的资源信息。\n此端点所提供的信息比导出给 API 服务器的信息更丰富。\n\n{{< note >}}"}
{"en": "`GetAllocatableResources` should only be used to evaluate [allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\nresources on a node. If the goal is to evaluate free/unallocated resources it should be used in\nconjunction with the List() endpoint. The result obtained by `GetAllocatableResources` would remain\nthe same unless the underlying resources exposed to kubelet change. This happens rarely but when\nit does (for example: hotplug/hotunplug, device health changes), client is expected to call\n`GetAlloctableResources` endpoint.\n\nHowever, calling `GetAllocatableResources` endpoint is not sufficient in case of cpu and/or memory\nupdate and Kubelet needs to be restarted to reflect the correct resource capacity and allocatable.", "zh": "`GetAllocatableResources` 应该仅被用于评估一个节点上的[可分配的](/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)资源。\n如果目标是评估空闲/未分配的资源，此调用应该与 `List()` 端点一起使用。\n除非暴露给 kubelet 的底层资源发生变化，否则 `GetAllocatableResources` 得到的结果将保持不变。\n这种情况很少发生，但当发生时（例如：热插拔，设备健康状况改变），客户端应该调用 `GetAlloctableResources` 端点。\n\n然而，调用 `GetAllocatableResources` 端点在 CPU、内存被更新的情况下是不够的，\nkubelet 需要重新启动以获取正确的资源容量和可分配的资源。\n{{< /note >}}\n\n```gRPC\n// AllocatableResourcesResponses 包含 kubelet 所了解到的所有设备的信息\nmessage AllocatableResourcesResponse {\n    repeated ContainerDevices devices = 1;\n    repeated int64 cpu_ids = 2;\n    repeated ContainerMemory memory = 3;\n}\n```"}
{"en": "`ContainerDevices` do expose the topology information declaring to which NUMA cells the device is\naffine. The NUMA cells are identified using a opaque integer ID, which value is consistent to\nwhat device plugins report\n[when they register themselves to the kubelet](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager).", "zh": "`ContainerDevices` 会向外提供各个设备所隶属的 NUMA 单元这类拓扑信息。\nNUMA 单元通过一个整数 ID 来标识，其取值与设备插件所报告的一致。\n[设备插件注册到 kubelet 时](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)\n会报告这类信息。"}
{"en": "The gRPC service is served over a unix socket at `/var/lib/kubelet/pod-resources/kubelet.sock`.\nMonitoring agents for device plugin resources can be deployed as a daemon, or as a DaemonSet.\nThe canonical directory `/var/lib/kubelet/pod-resources` requires privileged access, so monitoring\nagents must run in a privileged security context. If a device monitoring agent is running as a\nDaemonSet, `/var/lib/kubelet/pod-resources` must be mounted as a\n{{< glossary_tooltip term_id=\"volume\" >}} in the device monitoring agent's\n[PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core).", "zh": "gRPC 服务通过 `/var/lib/kubelet/pod-resources/kubelet.sock` 的 UNIX 套接字来提供服务。\n设备插件资源的监控代理程序可以部署为守护进程或者 DaemonSet。\n规范的路径 `/var/lib/kubelet/pod-resources` 需要特权来进入，\n所以监控代理程序必须要在获得授权的安全的上下文中运行。\n如果设备监控代理以 DaemonSet 形式运行，必须要在插件的\n[PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core)\n中声明将 `/var/lib/kubelet/pod-resources`\n目录以{{< glossary_tooltip text=\"卷\" term_id=\"volume\" >}}的形式被挂载到设备监控代理中。\n\n{{< note >}}"}
{"en": "When accessing the `/var/lib/kubelet/pod-resources/kubelet.sock` from DaemonSet\nor any other app deployed as a container on the host, which is mounting socket as\na volume, it is a good practice to mount directory `/var/lib/kubelet/pod-resources/`\ninstead of the `/var/lib/kubelet/pod-resources/kubelet.sock`. This will ensure\nthat after kubelet restart, container will be able to re-connect to this socket.", "zh": "在从 DaemonSet 或以容器形式部署在主机上的任何其他应用中访问\n`/var/lib/kubelet/pod-resources/kubelet.sock` 时，\n如果将套接字作为卷挂载，最好的做法是挂载目录 `/var/lib/kubelet/pod-resources/`\n而不是 `/var/lib/kubelet/pod-resources/kubelet.sock`。\n这样可以确保在 kubelet 重新启动后，容器将能够重新连接到此套接字。"}
{"en": "Container mounts are managed by inode referencing the socket or directory,\ndepending on what was mounted. When kubelet restarts, socket is deleted\nand a new socket is created, while directory stays untouched.\nSo the original inode for the socket become unusable. Inode to directory\nwill continue working.", "zh": "容器挂载是通过引用套接字或目录的 inode 进行管理的，具体取决于挂载的内容。\n当 kubelet 重新启动时，套接字会被删除并创建一个新的套接字，而目录则保持不变。\n因此，针对原始套接字的 inode 将变得无法使用，而到目录的 inode 将继续正常工作。\n\n{{< /note >}}"}
{"en": "### `Get` gRPC endpoint {#grpc-endpoint-get}", "zh": "### `Get` gRPC 端点   {#grpc-endpoint-get}\n\n{{< feature-state state=\"alpha\" for_k8s_version=\"v1.27\" >}}"}
{"en": "The `Get` endpoint provides information on resources of a running Pod. It exposes information\nsimilar to those described in the `List` endpoint. The `Get` endpoint requires `PodName`\nand `PodNamespace` of the running Pod.", "zh": "`Get` 端点提供了当前运行 Pod 的资源信息。它会暴露与 `List` 端点中所述类似的信息。\n`Get` 端点需要当前运行 Pod 的 `PodName` 和 `PodNamespace`。"}
{"en": "```gRPC\n// GetPodResourcesRequest contains information about the pod\nmessage GetPodResourcesRequest {\n    string pod_name = 1;\n    string pod_namespace = 2;\n}\n```", "zh": "```gRPC\n// GetPodResourcesRequest 包含 Pod 相关信息\nmessage GetPodResourcesRequest {\n    string pod_name = 1;\n    string pod_namespace = 2;\n}\n```"}
{"en": "To enable this feature, you must start your kubelet services with the following flag:", "zh": "要启用此特性，你必须使用以下标志启动 kubelet 服务：\n\n```\n--feature-gates=KubeletPodResourcesGet=true\n```"}
{"en": "The `Get` endpoint can provide Pod information related to dynamic resources\nallocated by the dynamic resource allocation API. To enable this feature, you must\nensure your kubelet services are started with the following flags:", "zh": "`Get` 端点可以提供与动态资源分配 API 所分配的动态资源相关的 Pod 信息。\n要启用此特性，你必须确保使用以下标志启动 kubelet 服务：\n\n```\n--feature-gates=KubeletPodResourcesGet=true,DynamicResourceAllocation=true,KubeletPodResourcesDynamicResources=true\n```"}
{"en": "## Device plugin integration with the Topology Manager", "zh": "## 设备插件与拓扑管理器的集成   {#device-plugin-integration-with-the-topology-manager}\n\n{{< feature-state for_k8s_version=\"v1.27\" state=\"stable\" >}}"}
{"en": "The Topology Manager is a Kubelet component that allows resources to be co-ordinated in a Topology\naligned manner. In order to do this, the Device Plugin API was extended to include a\n`TopologyInfo` struct.", "zh": "拓扑管理器是 kubelet 的一个组件，它允许以拓扑对齐方式来调度资源。\n为了做到这一点，设备插件 API 进行了扩展来包括一个 `TopologyInfo` 结构体。\n\n```gRPC\nmessage TopologyInfo {\n    repeated NUMANode nodes = 1;\n}\n\nmessage NUMANode {\n    int64 ID = 1;\n}\n```"}
{"en": "Device Plugins that wish to leverage the Topology Manager can send back a populated TopologyInfo\nstruct as part of the device registration, along with the device IDs and the health of the device.\nThe device manager will then use this information to consult with the Topology Manager and make\nresource assignment decisions.\n\n`TopologyInfo` supports setting a `nodes` field to either `nil` or a list of NUMA nodes. This\nallows the Device Plugin to advertise a device that spans multiple NUMA nodes.\n\nSetting `TopologyInfo` to `nil` or providing an empty list of NUMA nodes for a given device\nindicates that the Device Plugin does not have a NUMA affinity preference for that device.\n\nAn example `TopologyInfo` struct populated for a device by a Device Plugin:", "zh": "设备插件希望拓扑管理器可以将填充的 TopologyInfo 结构体作为设备注册的一部分以及设备 ID\n和设备的运行状况发送回去。然后设备管理器将使用此信息来咨询拓扑管理器并做出资源分配决策。\n\n`TopologyInfo` 支持将 `nodes` 字段设置为 `nil` 或一个 NUMA 节点的列表。\n这样就可以使设备插件通告跨越多个 NUMA 节点的设备。\n\n将 `TopologyInfo` 设置为 `nil` 或为给定设备提供一个空的\nNUMA 节点列表表示设备插件没有该设备的 NUMA 亲和偏好。\n\n下面是一个由设备插件为设备填充 `TopologyInfo` 结构体的示例：\n\n```\npluginapi.Device{ID: \"25102017\", Health: pluginapi.Healthy, Topology:&pluginapi.TopologyInfo{Nodes: []*pluginapi.NUMANode{&pluginapi.NUMANode{ID: 0,},}}}\n```"}
{"en": "## Device plugin examples {#examples}", "zh": "## 设备插件示例 {#examples}\n\n{{% thirdparty-content %}}"}
{"en": "Here are some examples of device plugin implementations:\n\n* [Akri](https://github.com/project-akri/akri), which lets you easily expose heterogeneous leaf devices (such as IP cameras and USB devices).\n* The [AMD GPU device plugin](https://github.com/ROCm/k8s-device-plugin)\n* The [generic device plugin](https://github.com/squat/generic-device-plugin) for generic Linux devices and USB devices\n* The [Intel device plugins](https://github.com/intel/intel-device-plugins-for-kubernetes) for\n  Intel GPU, FPGA, QAT, VPU, SGX, DSA, DLB and IAA devices\n* The [KubeVirt device plugins](https://github.com/kubevirt/kubernetes-device-plugins) for\n  hardware-assisted virtualization\n* The [NVIDIA GPU device plugin for Container-Optimized OS](https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu)\n* The [RDMA device plugin](https://github.com/hustcat/k8s-rdma-device-plugin)\n* The [SocketCAN device plugin](https://github.com/collabora/k8s-socketcan)\n* The [Solarflare device plugin](https://github.com/vikaschoudhary16/sfc-device-plugin)\n* The [SR-IOV Network device plugin](https://github.com/intel/sriov-network-device-plugin)\n* The [Xilinx FPGA device plugins](https://github.com/Xilinx/FPGA_as_a_Service/tree/master/k8s-device-plugin) for Xilinx FPGA devices", "zh": "下面是一些设备插件实现的示例：\n\n* [Akri](https://github.com/project-akri/akri)，它可以让你轻松公开异构叶子设备（例如 IP 摄像机和 USB 设备）。\n* [AMD GPU 设备插件](https://github.com/ROCm/k8s-device-plugin)\n* 适用于通用 Linux 设备和 USB 设备的[通用设备插件](https://github.com/squat/generic-device-plugin)\n* [Intel 设备插件](https://github.com/intel/intel-device-plugins-for-kubernetes)支持\n  Intel GPU、FPGA、QAT、VPU、SGX、DSA、DLB 和 IAA 设备\n* [KubeVirt 设备插件](https://github.com/kubevirt/kubernetes-device-plugins)用于硬件辅助的虚拟化\n* [为 Container-Optimized OS 所提供的 NVIDIA GPU 设备插件](https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu)\n* [RDMA 设备插件](https://github.com/hustcat/k8s-rdma-device-plugin)\n* [SocketCAN 设备插件](https://github.com/collabora/k8s-socketcan)\n* [Solarflare 设备插件](https://github.com/vikaschoudhary16/sfc-device-plugin)\n* [SR-IOV 网络设备插件](https://github.com/intel/sriov-network-device-plugin)\n* [Xilinx FPGA 设备插件](https://github.com/Xilinx/FPGA_as_a_Service/tree/master/k8s-device-plugin)\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn about [scheduling GPU resources](/docs/tasks/manage-gpus/scheduling-gpus/) using device\n  plugins\n* Learn about [advertising extended resources](/docs/tasks/administer-cluster/extended-resource-node/)\n  on a node\n* Learn about the [Topology Manager](/docs/tasks/administer-cluster/topology-manager/)\n* Read about using [hardware acceleration for TLS ingress](/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/)\n  with Kubernetes", "zh": "* 查看[调度 GPU 资源](/zh-cn/docs/tasks/manage-gpus/scheduling-gpus/)来学习使用设备插件\n* 查看在节点上如何[公布扩展资源](/zh-cn/docs/tasks/administer-cluster/extended-resource-node/)\n* 学习[拓扑管理器](/zh-cn/docs/tasks/administer-cluster/topology-manager/)\n* 阅读如何在 Kubernetes 中使用 [TLS Ingress 的硬件加速](/zh-cn/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/)"}
{"en": "This section covers extensions to your cluster that do not come as part as Kubernetes itself.\nYou can use these extensions to enhance the nodes in your cluster, or to provide the network\nfabric that links Pods together.\n\n* [CSI](/docs/concepts/storage/volumes/#csi) and [FlexVolume](/docs/concepts/storage/volumes/#flexvolume) storage plugins", "zh": "本节介绍不属于 Kubernetes 本身组成部分的一些集群扩展。\n你可以使用这些扩展来增强集群中的节点，或者提供将 Pod 关联在一起的网络结构。\n\n* [CSI](/zh-cn/docs/concepts/storage/volumes/#csi) 和\n  [FlexVolume](/zh-cn/docs/concepts/storage/volumes/#flexvolume) 存储插件"}
{"en": "{{< glossary_tooltip text=\"Container Storage Interface\" term_id=\"csi\" >}} (CSI) plugins\n  provide a way to extend Kubernetes with supports for new kinds of volumes. The volumes can\n  be backed by durable external storage, or provide ephemeral storage, or they might offer a\n  read-only interface to information using a filesystem paradigm.\n\n  Kubernetes also includes support for [FlexVolume](/docs/concepts/storage/volumes/#flexvolume)\n  plugins, which are deprecated since Kubernetes v1.23 (in favour of CSI).", "zh": "{{< glossary_tooltip text=\"容器存储接口\" term_id=\"csi\" >}} (CSI) 插件提供了一种扩展\n  Kubernetes 的方式使其支持新类别的卷。\n  这些卷可以由持久的外部存储提供支持，可以提供临时存储，还可以使用文件系统范型为信息提供只读接口。\n\n  Kubernetes 还包括对 [FlexVolume](/zh-cn/docs/concepts/storage/volumes/#flexvolume)\n  插件的扩展支持，该插件自 Kubernetes v1.23 起被弃用（被 CSI 替代）。"}
{"en": "FlexVolume plugins allow users to mount volume types that aren't natively\n  supported by Kubernetes. When you run a Pod that relies on FlexVolume\n  storage, the kubelet calls a binary plugin to mount the volume. The archived\n  [FlexVolume](https://git.k8s.io/design-proposals-archive/storage/flexvolume-deployment.md)\n  design proposal has more detail on this approach.\n\n  The [Kubernetes Volume Plugin FAQ for Storage Vendors](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors)\n  includes general information on storage plugins.", "zh": "FlexVolume 插件允许用户挂载 Kubernetes 本身不支持的卷类型。\n  当你运行依赖于 FlexVolume 存储的 Pod 时，kubelet 会调用一个二进制插件来挂载该卷。\n  归档的 [FlexVolume](https://git.k8s.io/design-proposals-archive/storage/flexvolume-deployment.md)\n  设计提案对此方法有更多详细说明。\n\n  [Kubernetes 存储供应商的卷插件 FAQ](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors)\n  包含了有关存储插件的通用信息。"}
{"en": "* [Device plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)\n\n  Device plugins allow a node to discover new Node facilities (in addition to the\n  built-in node resources such as `cpu` and `memory`), and provide these custom node-local\n  facilities to Pods that request them.", "zh": "* [设备插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)\n\n  设备插件允许一个节点发现新的 Node 设施（除了 `cpu` 和 `memory` 等内置的节点资源之外），\n  并向请求资源的 Pod 提供了这些自定义的节点本地设施。"}
{"en": "* [Network plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)\n\n  Network plugins allow Kubernetes to work with different networking topologies and technologies.\n  Your Kubernetes cluster needs a _network plugin_ in order to have a working Pod network\n  and to support other aspects of the Kubernetes network model.\n\n  Kubernetes {{< skew currentVersion >}} is compatible with {{< glossary_tooltip text=\"CNI\" term_id=\"cni\" >}}\n  network plugins.", "zh": "* [网络插件](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)\n\n  网络插件可以让 Kubernetes 使用不同的网络拓扑和技术。\n  你的 Kubernetes 集群需要一个 **网络插件** 才能拥有一个正常工作的 Pod 网络，\n  才能支持 Kubernetes 网络模型的其他方面。\n\n  Kubernetes {{< skew currentVersion >}} 兼容\n  {{< glossary_tooltip text=\"CNI\" term_id=\"cni\" >}} 网络插件。"}
{"en": "*Custom resources* are extensions of the Kubernetes API. This page discusses when to add a custom\nresource to your Kubernetes cluster and when to use a standalone service. It describes the two\nmethods for adding custom resources and how to choose between them.", "zh": "**定制资源（Custom Resource）** 是对 Kubernetes API 的扩展。\n本页讨论何时向 Kubernetes 集群添加定制资源，何时使用独立的服务。\n本页描述添加定制资源的两种方法以及怎样在二者之间做出抉择。"}
{"en": "## Custom resources\n\nA *resource* is an endpoint in the [Kubernetes API](/docs/concepts/overview/kubernetes-api/) that\nstores a collection of {{< glossary_tooltip text=\"API objects\" term_id=\"object\" >}}\nof a certain kind; for example, the built-in *pods* resource contains a collection of Pod objects.", "zh": "## 定制资源  {#custom-resources}\n\n**资源（Resource）** 是\n[Kubernetes API](/zh-cn/docs/concepts/overview/kubernetes-api/) 中的一个端点，\n其中存储的是某个类别的\n{{< glossary_tooltip text=\"API 对象\" term_id=\"object\" >}}的一个集合。\n例如内置的 **Pod** 资源包含一组 Pod 对象。"}
{"en": "A *custom resource* is an extension of the Kubernetes API that is not necessarily available in a default\nKubernetes installation. It represents a customization of a particular Kubernetes installation. However,\nmany core Kubernetes functions are now built using custom resources, making Kubernetes more modular.\n\nCustom resources can appear and disappear in a running cluster through dynamic registration,\nand cluster admins can update custom resources independently of the cluster itself.\nOnce a custom resource is installed, users can create and access its objects using\n{{< glossary_tooltip text=\"kubectl\" term_id=\"kubectl\" >}}, just as they do for built-in resources\nlike *Pods*.", "zh": "**定制资源（Custom Resource）** 是对 Kubernetes API 的扩展，不一定在默认的\nKubernetes 安装中就可用。定制资源所代表的是对特定 Kubernetes 安装的一种定制。\n不过，很多 Kubernetes 核心功能现在都用定制资源来实现，这使得 Kubernetes 更加模块化。\n\n定制资源可以通过动态注册的方式在运行中的集群内或出现或消失，集群管理员可以独立于集群更新定制资源。\n一旦某定制资源被安装，用户可以使用 {{< glossary_tooltip text=\"kubectl\" term_id=\"kubectl\" >}}\n来创建和访问其中的对象，就像他们为 **Pod** 这种内置资源所做的一样。"}
{"en": "## Custom controllers\n\nOn their own, custom resources let you store and retrieve structured data.\nWhen you combine a custom resource with a *custom controller*, custom resources\nprovide a true _declarative API_.", "zh": "## 定制控制器   {#custom-controllers}\n\n就定制资源本身而言，它只能用来存取结构化的数据。\n当你将定制资源与**定制控制器（Custom Controller）** 结合时，\n定制资源就能够提供真正的**声明式 API（Declarative API）**。"}
{"en": "The Kubernetes [declarative API](/docs/concepts/overview/kubernetes-api/)\nenforces a separation of responsibilities. You declare the desired state of\nyour resource. The Kubernetes controller keeps the current state of Kubernetes\nobjects in sync with your declared desired state. This is in contrast to an\nimperative API, where you *instruct* a server what to do.", "zh": "Kubernetes [声明式 API](/zh-cn/docs/concepts/overview/kubernetes-api/) 强制对职权做了一次分离操作。\n你声明所用资源的期望状态，而 Kubernetes 控制器使 Kubernetes 对象的当前状态与你所声明的期望状态保持同步。\n声明式 API 的这种机制与命令式 API（你**指示**服务器要做什么，服务器就去做什么）形成鲜明对比。"}
{"en": "You can deploy and update a custom controller on a running cluster, independently\nof the cluster's lifecycle. Custom controllers can work with any kind of resource,\nbut they are especially effective when combined with custom resources. The\n[Operator pattern](/docs/concepts/extend-kubernetes/operator/) combines custom\nresources and custom controllers. You can use custom controllers to encode domain knowledge\nfor specific applications into an extension of the Kubernetes API.", "zh": "你可以在一个运行中的集群上部署和更新定制控制器，这类操作与集群的生命周期无关。\n定制控制器可以用于任何类别的资源，不过它们与定制资源结合起来时最为有效。\n[Operator 模式](/zh-cn/docs/concepts/extend-kubernetes/operator/)就是将定制资源与定制控制器相结合的。\n你可以使用定制控制器来将特定于某应用的领域知识组织起来，以编码的形式构造对 Kubernetes API 的扩展。"}
{"en": "## Should I add a custom resource to my Kubernetes cluster?\n\nWhen creating a new API, consider whether to\n[aggregate your API with the Kubernetes cluster APIs](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)\nor let your API stand alone.", "zh": "## 我是否应该向我的 Kubernetes 集群添加定制资源？   {#should-i-add-a-cr-to-my-k8s-cluster}\n\n在创建新的 API 时，\n请考虑是[将你的 API 与 Kubernetes 集群 API 聚合起来](/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)，\n还是让你的 API 独立运行。"}
{"en": "| Consider API aggregation if: | Prefer a stand-alone API if: |\n| ---------------------------- | ---------------------------- |\n| Your API is [Declarative](#declarative-apis). | Your API does not fit the [Declarative](#declarative-apis) model. |\n| You want your new types to be readable and writable using `kubectl`.| `kubectl` support is not required |\n| You want to view your new types in a Kubernetes UI, such as dashboard, alongside built-in types. | Kubernetes UI support is not required. |\n| You are developing a new API. | You already have a program that serves your API and works well. |\n| You are willing to accept the format restriction that Kubernetes puts on REST resource paths, such as API Groups and Namespaces. (See the [API Overview](/docs/concepts/overview/kubernetes-api/).) | You need to have specific REST paths to be compatible with an already defined REST API. |\n| Your resources are naturally scoped to a cluster or namespaces of a cluster. | Cluster or namespace scoped resources are a poor fit; you need control over the specifics of resource paths. |\n| You want to reuse [Kubernetes API support features](#common-features).  | You don't need those features. |", "zh": "| 考虑 API 聚合的情况 | 优选独立 API 的情况 |\n| ---------------------------- | ---------------------------- |\n| 你的 API 是[声明式的](#declarative-apis)。 | 你的 API 不符合[声明式](#declarative-apis)模型。 |\n| 你希望可以是使用 `kubectl` 来读写你的新资源类别。 | 不要求 `kubectl` 支持。 |\n| 你希望在 Kubernetes UI （如仪表板）中和其他内置类别一起查看你的新资源类别。 | 不需要 Kubernetes UI 支持。 |\n| 你在开发新的 API。 | 你已经有一个提供 API 服务的程序并且工作良好。 |\n| 你有意愿取接受 Kubernetes 对 REST 资源路径所作的格式限制，例如 API 组和名字空间。（参阅 [API 概述](/zh-cn/docs/concepts/overview/kubernetes-api/)） | 你需要使用一些特殊的 REST 路径以便与已经定义的 REST API 保持兼容。 |\n| 你的资源可以自然地界定为集群作用域或集群中某个名字空间作用域。 | 集群作用域或名字空间作用域这种二分法很不合适；你需要对资源路径的细节进行控制。 |\n| 你希望复用 [Kubernetes API 支持特性](#common-features)。  | 你不需要这类特性。 |"}
{"en": "### Declarative APIs\n\nIn a Declarative API, typically:\n\n- Your API consists of a relatively small number of relatively small objects (resources).\n- The objects define configuration of applications or infrastructure.\n- The objects are updated relatively infrequently.\n- Humans often need to read and write the objects.\n- The main operations on the objects are CRUD-y (creating, reading, updating and deleting).\n- Transactions across objects are not required: the API represents a desired state, not an exact state.", "zh": "### 声明式 API   {#declarative-apis}\n\n典型地，在声明式 API 中：\n\n- 你的 API 包含相对而言为数不多的、尺寸较小的对象（资源）。\n- 对象定义了应用或者基础设施的配置信息。\n- 对象更新操作频率较低。\n- 通常需要人来读取或写入对象。\n- 对象的主要操作是 CRUD 风格的（创建、读取、更新和删除）。\n- 不需要跨对象的事务支持：API 对象代表的是期望状态而非确切实际状态。"}
{"en": "Imperative APIs are not declarative.\nSigns that your API might not be declarative include:\n\n- The client says \"do this\", and then gets a synchronous response back when it is done.\n- The client says \"do this\", and then gets an operation ID back, and has to check a separate\n  Operation object to determine completion of the request.\n- You talk about Remote Procedure Calls (RPCs).\n- Directly storing large amounts of data; for example, > a few kB per object, or > 1000s of objects.\n- High bandwidth access (10s of requests per second sustained) needed.\n- Store end-user data (such as images, PII, etc.) or other large-scale data processed by applications.\n- The natural operations on the objects are not CRUD-y.\n- The API is not easily modeled as objects.\n- You chose to represent pending operations with an operation ID or an operation object.", "zh": "命令式 API（Imperative API）与声明式有所不同。\n以下迹象表明你的 API 可能不是声明式的：\n\n- 客户端发出“做这个操作”的指令，之后在该操作结束时获得同步响应。\n- 客户端发出“做这个操作”的指令，并获得一个操作 ID，之后需要检查一个 Operation（操作）\n  对象来判断请求是否成功完成。\n- 你会将你的 API 类比为远程过程调用（Remote Procedure Call，RPC）。\n- 直接存储大量数据；例如每个对象几 kB，或者存储上千个对象。\n- 需要较高的访问带宽（长期保持每秒数十个请求）。\n- 存储有应用来处理的最终用户数据（如图片、个人标识信息（PII）等）或者其他大规模数据。\n- 在对象上执行的常规操作并非 CRUD 风格。\n- API 不太容易用对象来建模。\n- 你决定使用操作 ID 或者操作对象来表现悬决的操作。"}
{"en": "## Should I use a ConfigMap or a custom resource?\n\nUse a ConfigMap if any of the following apply:\n\n* There is an existing, well-documented configuration file format, such as a `mysql.cnf` or\n  `pom.xml`.\n* You want to put the entire configuration into one key of a ConfigMap.\n* The main use of the configuration file is for a program running in a Pod on your cluster to\n  consume the file to configure itself.\n* Consumers of the file prefer to consume via file in a Pod or environment variable in a pod,\n  rather than the Kubernetes API.\n* You want to perform rolling updates via Deployment, etc., when the file is updated.", "zh": "## 我应该使用一个 ConfigMap 还是一个定制资源？   {#should-i-use-a-configmap-or-a-cr}\n\n如果满足以下条件之一，应该使用 ConfigMap：\n\n* 存在一个已有的、文档完备的配置文件格式约定，例如 `mysql.cnf` 或 `pom.xml`。\n* 你希望将整个配置文件放到某 configMap 中的一个主键下面。\n* 配置文件的主要用途是针对运行在集群中 Pod 内的程序，供后者依据文件数据配置自身行为。\n* 文件的使用者期望以 Pod 内文件或者 Pod 内环境变量的形式来使用文件数据，\n  而不是通过 Kubernetes API。\n* 你希望当文件被更新时通过类似 Deployment 之类的资源完成滚动更新操作。\n\n{{< note >}}"}
{"en": "Use a {{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}} for sensitive data, which is similar\nto a ConfigMap but more secure.", "zh": "请使用 {{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}} 来保存敏感数据。\nSecret 类似于 configMap，但更为安全。\n{{< /note >}}"}
{"en": "Use a custom resource (CRD or Aggregated API) if most of the following apply:\n\n* You want to use Kubernetes client libraries and CLIs to create and update the new resource.\n* You want top-level support from `kubectl`; for example, `kubectl get my-object object-name`.\n* You want to build new automation that watches for updates on the new object, and then CRUD other\n  objects, or vice versa.\n* You want to write automation that handles updates to the object.\n* You want to use Kubernetes API conventions like `.spec`, `.status`, and `.metadata`.\n* You want the object to be an abstraction over a collection of controlled resources, or a\n  summarization of other resources.", "zh": "如果以下条件中大多数都被满足，你应该使用定制资源（CRD 或者 聚合 API）：\n\n* 你希望使用 Kubernetes 客户端库和 CLI 来创建和更改新的资源。\n* 你希望 `kubectl` 能够直接支持你的资源；例如，`kubectl get my-object object-name`。\n* 你希望构造新的自动化机制，监测新对象上的更新事件，并对其他对象执行 CRUD\n  操作，或者监测后者更新前者。\n* 你希望编写自动化组件来处理对对象的更新。\n* 你希望使用 Kubernetes API 对诸如 `.spec`、`.status` 和 `.metadata` 等字段的约定。\n* 你希望对象是对一组受控资源的抽象，或者对其他资源的归纳提炼。"}
{"en": "## Adding custom resources\n\nKubernetes provides two ways to add custom resources to your cluster:\n\n- CRDs are simple and can be created without any programming.\n- [API Aggregation](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)\n  requires programming, but allows more control over API behaviors like how data is stored and\n  conversion between API versions.", "zh": "## 添加定制资源   {#adding-custom-resources}\n\nKubernetes 提供了两种方式供你向集群中添加定制资源：\n\n- CRD 相对简单，创建 CRD 可以不必编程。\n- [API 聚合](/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)需要编程，\n  但支持对 API 行为进行更多的控制，例如数据如何存储以及在不同 API 版本间如何转换等。"}
{"en": "Kubernetes provides these two options to meet the needs of different users, so that neither ease\nof use nor flexibility is compromised.\n\nAggregated APIs are subordinate API servers that sit behind the primary API server, which acts as\na proxy. This arrangement is called [API Aggregation](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)(AA).\nTo users, the Kubernetes API appears extended.\n\nCRDs allow users to create new types of resources without adding another API server. You do not\nneed to understand API Aggregation to use CRDs.\n\nRegardless of how they are installed, the new resources are referred to as Custom Resources to\ndistinguish them from built-in Kubernetes resources (like pods).", "zh": "Kubernetes 提供这两种选项以满足不同用户的需求，这样就既不会牺牲易用性也不会牺牲灵活性。\n\n聚合 API 指的是一些下位的 API 服务器，运行在主 API 服务器后面；主 API\n服务器以代理的方式工作。这种组织形式称作\n[API 聚合（API Aggregation，AA）](/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/) 。\n对用户而言，看起来仅仅是 Kubernetes API 被扩展了。\n\nCRD 允许用户创建新的资源类别同时又不必添加新的 API 服务器。\n使用 CRD 时，你并不需要理解 API 聚合。\n\n无论以哪种方式安装定制资源，新的资源都会被当做定制资源，以便与内置的\nKubernetes 资源（如 Pods）相区分。\n\n{{< note >}}"}
{"en": "Avoid using a Custom Resource as data storage for application, end user, or monitoring data:\narchitecture designs that store application data within the Kubernetes API typically represent\na design that is too closely coupled.\n\nArchitecturally, [cloud native](https://www.cncf.io/about/faq/#what-is-cloud-native) application architectures\nfavor loose coupling between components. If part of your workload requires a backing service for\nits routine operation, run that backing service as a component or consume it as an external service.\nThis way, your workload does not rely on the Kubernetes API for its normal operation.", "zh": "避免将定制资源用于存储应用、最终用户或监控数据：\n将应用数据存储在 Kubernetes API 内的架构设计通常代表一种过于紧密耦合的设计。\n\n在架构上，[云原生](https://www.cncf.io/about/faq/#what-is-cloud-native)应用架构倾向于各组件之间的松散耦合。\n如果部分工作负载需要支持服务来维持其日常运转，则这种支持服务应作为一个组件运行或作为一个外部服务来使用。\n这样，工作负载的正常运转就不会依赖 Kubernetes API 了。\n{{< /note >}}"}
{"en": "## CustomResourceDefinitions\n\nThe [CustomResourceDefinition](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)\nAPI resource allows you to define custom resources.\nDefining a CRD object creates a new custom resource with a name and schema that you specify.\nThe Kubernetes API serves and handles the storage of your custom resource.\nThe name of the CRD object itself must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names) derived from the defined resource name and its API group; see [how to create a CRD](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions#create-a-customresourcedefinition) for more details.\nFurther, the name of an object whose kind/resource is defined by a CRD must also be a valid DNS subdomain name.", "zh": "## CustomResourceDefinitions\n\n[CustomResourceDefinition](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)\nAPI 资源允许你定义定制资源。\n定义 CRD 对象的操作会使用你所设定的名字和模式定义（Schema）创建一个新的定制资源，\nKubernetes API 负责为你的定制资源提供存储和访问服务。\nCRD 对象的名称必须是有效的 [DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)，\n该名称由定义的资源名称及其 API 组派生而来。有关详细信息，\n请参见[如何创建 CRD](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions#create-a-customresourcedefinition)。\n此外，由 CRD 定义的某种对象/资源的名称也必须是有效的 DNS 子域名。"}
{"en": "This frees you from writing your own API server to handle the custom resource,\nbut the generic nature of the implementation means you have less flexibility than with\n[API server aggregation](#api-server-aggregation).\n\nRefer to the [custom controller example](https://github.com/kubernetes/sample-controller)\nfor an example of how to register a new custom resource, work with instances of your new resource type,\nand use a controller to handle events.", "zh": "CRD 使得你不必编写自己的 API 服务器来处理定制资源，不过其背后实现的通用性也意味着你所获得的灵活性要比\n[API 服务器聚合](#api-server-aggregation)少很多。\n\n关于如何注册新的定制资源、使用新资源类别的实例以及如何使用控制器来处理事件，\n相关的例子可参见[定制控制器示例](https://github.com/kubernetes/sample-controller)。"}
{"en": "## API server aggregation\n\nUsually, each resource in the Kubernetes API requires code that handles REST requests and manages\npersistent storage of objects. The main Kubernetes API server handles built-in resources like\n*pods* and *services*, and can also generically handle custom resources through\n[CRDs](#customresourcedefinitions).\n\nThe [aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)\nallows you to provide specialized implementations for your custom resources by writing and\ndeploying your own API server.\nThe main API server delegates requests to your API server for the custom resources that you handle,\nmaking them available to all of its clients.", "zh": "## API 服务器聚合  {#api-server-aggregation}\n\n通常，Kubernetes API 中的每个资源都需要处理 REST 请求和管理对象持久性存储的代码。\nKubernetes API 主服务器能够处理诸如 **Pod** 和 **Service** 这些内置资源，\n也可以按通用的方式通过 [CRD](#customresourcedefinitions) 来处理定制资源。\n\n[聚合层（Aggregation Layer）](/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)\n使得你可以通过编写和部署你自己的 API 服务器来为定制资源提供特殊的实现。\n主 API 服务器将针对你要处理的定制资源的请求全部委托给你自己的 API 服务器来处理，\n同时将这些资源提供给其所有客户端。"}
{"en": "## Choosing a method for adding custom resources\n\nCRDs are easier to use. Aggregated APIs are more flexible. Choose the method that best meets your needs.\n\nTypically, CRDs are a good fit if:\n\n* You have a handful of fields\n* You are using the resource within your company, or as part of a small open-source project (as\n  opposed to a commercial product)", "zh": "## 选择添加定制资源的方法   {#choosing-a-method-for-adding-cr}\n\nCRD 更为易用；聚合 API 则更为灵活。请选择最符合你的需要的方法。\n\n通常，如果存在以下情况，CRD 可能更合适：\n\n* 定制资源的字段不多；\n* 你在组织内部使用该资源或者在一个小规模的开源项目中使用该资源，而不是在商业产品中使用。"}
{"en": "### Comparing ease of use\n\nCRDs are easier to create than Aggregated APIs.", "zh": "### 比较易用性  {#compare-ease-of-use}\n\nCRD 比聚合 API 更容易创建。"}
{"en": "| CRDs                        | Aggregated API |\n| --------------------------- | -------------- |\n| Do not require programming. Users can choose any language for a CRD controller. | Requires programming and building binary and image. |\n| No additional service to run; CRDs are handled by API server. | An additional service to create and that could fail. |\n| No ongoing support once the CRD is created. Any bug fixes are picked up as part of normal Kubernetes Master upgrades. | May need to periodically pickup bug fixes from upstream and rebuild and update the Aggregated API server. |\n| No need to handle multiple versions of your API; for example, when you control the client for this resource, you can upgrade it in sync with the API. | You need to handle multiple versions of your API; for example, when developing an extension to share with the world. |", "zh": "| CRD                        | 聚合 API       |\n| --------------------------- | -------------- |\n| 无需编程。用户可选择任何语言来实现 CRD 控制器。 | 需要编程，并构建可执行文件和镜像。 |\n| 无需额外运行服务；CRD 由 API 服务器处理。 | 需要额外创建服务，且该服务可能失效。 |\n| 一旦 CRD 被创建，不需要持续提供支持。Kubernetes 主控节点升级过程中自动会带入缺陷修复。 | 可能需要周期性地从上游提取缺陷修复并更新聚合 API 服务器。 |\n| 无需处理 API 的多个版本；例如，当你控制资源的客户端时，你可以更新它使之与 API 同步。 | 你需要处理 API 的多个版本；例如，在开发打算与很多人共享的扩展时。 |"}
{"en": "### Advanced features and flexibility\n\nAggregated APIs offer more advanced API features and customization of other features; for example, the storage layer.", "zh": "### 高级特性与灵活性  {#advanced-features-and-flexibility}\n\n聚合 API 可提供更多的高级 API 特性，也可对其他特性实行定制；例如，对存储层进行定制。"}
{"en": "| Feature | Description | CRDs | Aggregated API |\n| ------- | ----------- | ---- | -------------- |\n| Validation | Help users prevent errors and allow you to evolve your API independently of your clients. These features are most useful when there are many clients who can't all update at the same time. | Yes.  Most validation can be specified in the CRD using [OpenAPI v3.0 validation](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation). [CRDValidationRatcheting](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-ratcheting) feature gate allows failing validations specified using OpenAPI also can be ignored if the failing part of the resource was unchanged.  Any other validations supported by addition of a [Validating Webhook](/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook-alpha-in-1-8-beta-in-1-9). | Yes, arbitrary validation checks |\n| Defaulting | See above | Yes, either via [OpenAPI v3.0 validation](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting) `default` keyword (GA in 1.17), or via a [Mutating Webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook) (though this will not be run when reading from etcd for old objects). | Yes |\n| Multi-versioning | Allows serving the same object through two API versions. Can help ease API changes like renaming fields. Less important if you control your client versions. | [Yes](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning) | Yes |\n| Custom Storage | If you need storage with a different performance mode (for example, a time-series database instead of key-value store) or isolation for security (for example, encryption of sensitive information, etc.) | No | Yes |\n| Custom Business Logic | Perform arbitrary checks or actions when creating, reading, updating or deleting an object | Yes, using [Webhooks](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks). | Yes |\n| Scale Subresource | Allows systems like HorizontalPodAutoscaler and PodDisruptionBudget interact with your new resource | [Yes](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource)  | Yes |\n| Status Subresource | Allows fine-grained access control where user writes the spec section and the controller writes the status section. Allows incrementing object Generation on custom resource data mutation (requires separate spec and status sections in the resource) | [Yes](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#status-subresource) | Yes |\n| Other Subresources | Add operations other than CRUD, such as \"logs\" or \"exec\". | No | Yes |\n| strategic-merge-patch | The new endpoints support PATCH with `Content-Type: application/strategic-merge-patch+json`. Useful for updating objects that may be modified both locally, and by the server. For more information, see [\"Update API Objects in Place Using kubectl patch\"](/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/) | No | Yes |\n| Protocol Buffers | The new resource supports clients that want to use Protocol Buffers | No | Yes |\n| OpenAPI Schema | Is there an OpenAPI (swagger) schema for the types that can be dynamically fetched from the server? Is the user protected from misspelling field names by ensuring only allowed fields are set? Are types enforced (in other words, don't put an `int` in a `string` field?) | Yes, based on the [OpenAPI v3.0 validation](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation) schema (GA in 1.16). | Yes |\n| Instance Name | Does this extension mechanism impose any constraints on the names of objects whose kind/resource is defined this way? | Yes, such an object's name must be a valid DNS subdomain name. | No |", "zh": "| 特性    | 描述        | CRD | 聚合 API       |\n| ------- | ----------- | ---- | -------------- |\n| 合法性检查 | 帮助用户避免错误，允许你独立于客户端版本演化 API。这些特性对于由很多无法同时更新的客户端的场合。| 可以。大多数验证可以使用 [OpenAPI v3.0 合法性检查](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation) 来设定。[CRDValidationRatcheting](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-ratcheting) 特性门控允许在资源的失败部分未发生变化的情况下，忽略 OpenAPI 指定的失败验证。其他合法性检查操作可以通过添加[合法性检查 Webhook](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook-alpha-in-1-8-beta-in-1-9)来实现。 | 可以，可执行任何合法性检查。|\n| 默认值设置 | 同上 | 可以。可通过 [OpenAPI v3.0 合法性检查](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting)的 `default` 关键词（自 1.17 正式发布）或[更改性（Mutating）Webhook](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)来实现（不过从 etcd 中读取老的对象时不会执行这些 Webhook）。 | 可以。 |\n| 多版本支持 | 允许通过两个 API 版本同时提供同一对象。可帮助简化类似字段更名这类 API 操作。如果你能控制客户端版本，这一特性将不再重要。 | [可以](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning)。 | 可以。 |\n| 定制存储 | 支持使用具有不同性能模式的存储（例如，要使用时间序列数据库而不是键值存储），或者因安全性原因对存储进行隔离（例如对敏感信息执行加密）。 | 不可以。 | 可以。 |\n| 定制业务逻辑 | 在创建、读取、更新或删除对象时，执行任意的检查或操作。 | 可以。要使用 [Webhook](/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)。 | 可以。 |\n| 支持 scale 子资源 | 允许 HorizontalPodAutoscaler 和 PodDisruptionBudget 这类子系统与你的新资源交互。 | [可以](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource)。 | 可以。 |\n| 支持 status 子资源 | 允许在用户写入 spec 部分而控制器写入 status 部分时执行细粒度的访问控制。允许在对定制资源的数据进行更改时增加对象的代际（Generation）；这需要资源对 spec 和 status 部分有明确划分。| [可以](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#status-subresource)。 | 可以。 |\n| 其他子资源 | 添加 CRUD 之外的操作，例如 \"logs\" 或 \"exec\"。 | 不可以。 | 可以。 |\n| strategic-merge-patch | 新的端点要支持标记了 `Content-Type: application/strategic-merge-patch+json` 的 PATCH 操作。对于更新既可在本地更改也可在服务器端更改的对象而言是有用的。要了解更多信息，可参见[使用 `kubectl patch` 来更新 API 对象](/zh-cn/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/)。 | 不可以。 | 可以。 |\n| 支持协议缓冲区 | 新的资源要支持想要使用协议缓冲区（Protocol Buffer）的客户端。 | 不可以。 | 可以。 |\n| OpenAPI Schema | 是否存在新资源类别的 OpenAPI（Swagger）Schema 可供动态从服务器上读取？是否存在机制确保只能设置被允许的字段以避免用户犯字段拼写错误？是否实施了字段类型检查（换言之，不允许在 `string` 字段设置 `int` 值）？ | 可以，依据 [OpenAPI v3.0 合法性检查](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation) 模式（1.16 中进入正式发布状态）。 | 可以。|\n| 实例名称 | 这种扩展机制是否对通过这种方式定义的对象（类别/资源）的名称有任何限制? | 可以，此类对象的名称必须是一个有效的 DNS 子域名。 | 不可以|"}
{"en": "### Common Features\n\nWhen you create a custom resource, either via a CRD or an AA, you get many features for your API,\ncompared to implementing it outside the Kubernetes platform:", "zh": "### 公共特性  {#common-features}\n\n与在 Kubernetes 平台之外实现定制资源相比，\n无论是通过 CRD 还是通过聚合 API 来创建定制资源，你都会获得很多 API 特性："}
{"en": "| Feature | What it does |\n| ------- | ------------ |\n| CRUD | The new endpoints support CRUD basic operations via HTTP and `kubectl` |\n| Watch | The new endpoints support Kubernetes Watch operations via HTTP |\n| Discovery | Clients like `kubectl` and dashboard automatically offer list, display, and field edit operations on your resources |\n| json-patch | The new endpoints support PATCH with `Content-Type: application/json-patch+json` |\n| merge-patch | The new endpoints support PATCH with `Content-Type: application/merge-patch+json` |\n| HTTPS | The new endpoints uses HTTPS |\n| Built-in Authentication | Access to the extension uses the core API server (aggregation layer) for authentication |\n| Built-in Authorization | Access to the extension can reuse the authorization used by the core API server; for example, RBAC. |\n| Finalizers | Block deletion of extension resources until external cleanup happens. |\n| Admission Webhooks | Set default values and validate extension resources during any create/update/delete operation. |\n| UI/CLI Display | Kubectl, dashboard can display extension resources. |\n| Unset versus Empty | Clients can distinguish unset fields from zero-valued fields. |\n| Client Libraries Generation | Kubernetes provides generic client libraries, as well as tools to generate type-specific client libraries. |\n| Labels and annotations | Common metadata across objects that tools know how to edit for core and custom resources. |", "zh": "| 功能特性 | 具体含义     |\n| -------- | ------------ |\n| CRUD | 新的端点支持通过 HTTP 和 `kubectl` 发起的 CRUD 基本操作 |\n| 监测（Watch） | 新的端点支持通过 HTTP 发起的 Kubernetes Watch 操作 |\n| 发现（Discovery） | 类似 `kubectl` 和仪表盘（Dashboard）这类客户端能够自动提供列举、显示、在字段级编辑你的资源的操作 |\n| json-patch | 新的端点支持带 `Content-Type: application/json-patch+json` 的 PATCH 操作 |\n| merge-patch | 新的端点支持带 `Content-Type: application/merge-patch+json` 的 PATCH 操作 |\n| HTTPS | 新的端点使用 HTTPS |\n| 内置身份认证 | 对扩展的访问会使用核心 API 服务器（聚合层）来执行身份认证操作 |\n| 内置鉴权授权 | 对扩展的访问可以复用核心 API 服务器所使用的鉴权授权机制；例如，RBAC |\n| Finalizers | 在外部清除工作结束之前阻止扩展资源被删除 |\n| 准入 Webhooks | 在创建、更新和删除操作中对扩展资源设置默认值和执行合法性检查 |\n| UI/CLI 展示 | `kubectl` 和仪表盘（Dashboard）可以显示扩展资源 |\n| 区分未设置值和空值 | 客户端能够区分哪些字段是未设置的，哪些字段的值是被显式设置为零值的  |\n| 生成客户端库 | Kubernetes 提供通用的客户端库，以及用来生成特定类别客户端库的工具 |\n| 标签和注解 | 提供涵盖所有对象的公共元数据结构，且工具知晓如何编辑核心资源和定制资源的这些元数据 |"}
{"en": "## Preparing to install a custom resource\n\nThere are several points to be aware of before adding a custom resource to your cluster.", "zh": "## 准备安装定制资源   {#preparing-to-install-a-cr}\n\n在向你的集群添加定制资源之前，有些事情需要搞清楚。"}
{"en": "### Third party code and new points of failure\n\nWhile creating a CRD does not automatically add any new points of failure (for example, by causing\nthird party code to run on your API server), packages (for example, Charts) or other installation\nbundles often include CRDs as well as a Deployment of third-party code that implements the\nbusiness logic for a new custom resource.\n\nInstalling an Aggregated API server always involves running a new Deployment.", "zh": "### 第三方代码和新的失效点的问题   {#third-party-code-and-new-points-of-failure}\n\n尽管添加新的 CRD 不会自动带来新的失效点（Point of\nFailure），例如导致第三方代码被在 API 服务器上运行，\n类似 Helm Charts 这种软件包或者其他安装包通常在提供 CRD\n的同时还包含带有第三方代码的 Deployment，负责实现新的定制资源的业务逻辑。\n\n安装聚合 API 服务器时，也总会牵涉到运行一个新的 Deployment。"}
{"en": "### Storage\n\nCustom resources consume storage space in the same way that ConfigMaps do. Creating too many\ncustom resources may overload your API server's storage space.\n\nAggregated API servers may use the same storage as the main API server, in which case the same\nwarning applies.", "zh": "### 存储    {#storage}\n\n定制资源和 ConfigMap 一样也会消耗存储空间。创建过多的定制资源可能会导致\nAPI 服务器上的存储空间超载。\n\n聚合 API 服务器可以使用主 API 服务器相同的存储。如果是这样，你也要注意此警告。"}
{"en": "### Authentication, authorization, and auditing\n\nCRDs always use the same authentication, authorization, and audit logging as the built-in\nresources of your API server.\n\nIf you use RBAC for authorization, most RBAC roles will not grant access to the new resources\n(except the cluster-admin role or any role created with wildcard rules). You'll need to explicitly\ngrant access to the new resources. CRDs and Aggregated APIs often come bundled with new role\ndefinitions for the types they add.\n\nAggregated API servers may or may not use the same authentication, authorization, and auditing as\nthe primary API server.", "zh": "### 身份认证、鉴权授权以及审计    {#authentication-authorization-and-auditing}\n\nCRD 通常与 API 服务器上的内置资源一样使用相同的身份认证、鉴权授权和审计日志机制。\n\n如果你使用 RBAC 来执行鉴权授权，大多数 RBAC 角色都不会授权对新资源的访问\n（除了 cluster-admin 角色以及使用通配符规则创建的其他角色）。\n你要显式地为新资源的访问授权。CRD 和聚合 API 通常在交付时会包含针对所添加的类别的新的角色定义。\n\n聚合 API 服务器可能会使用主 API 服务器相同的身份认证、鉴权授权和审计机制，也可能不会。"}
{"en": "## Accessing a custom resource\n\nKubernetes [client libraries](/docs/reference/using-api/client-libraries/) can be used to access\ncustom resources. Not all client libraries support custom resources. The _Go_ and _Python_ client\nlibraries do.\n\nWhen you add a custom resource, you can access it using:\n\n- `kubectl`\n- The Kubernetes dynamic client.\n- A REST client that you write.\n- A client generated using [Kubernetes client generation tools](https://github.com/kubernetes/code-generator)\n  (generating one is an advanced undertaking, but some projects may provide a client along with\n  the CRD or AA).", "zh": "## 访问定制资源   {#accessing-a-custom-resources}\n\nKubernetes [客户端库](/zh-cn/docs/reference/using-api/client-libraries/)可用来访问定制资源。\n并非所有客户端库都支持定制资源。**Go** 和 **Python** 客户端库是支持的。\n\n当你添加了新的定制资源后，可以用如下方式之一访问它们：\n\n- `kubectl`\n- Kubernetes 动态客户端\n- 你所编写的 REST 客户端\n- 使用 [Kubernetes 客户端生成工具](https://github.com/kubernetes/code-generator)所生成的客户端。\n  生成客户端的工作有些难度，不过某些项目可能会随着 CRD 或聚合 API 一起提供一个客户端。"}
{"en": "## Custom resource field selectors\n\n[Field Selectors](/docs/concepts/overview/working-with-objects/field-selectors/)\nlet clients select custom resources based on the value of one or more resource\nfields.", "zh": "## 定制资源字段选择算符   {#custom-resource-field-selectors}\n\n[字段选择算符](/zh-cn/docs/concepts/overview/working-with-objects/field-selectors/)允许客户端根据一个或多个资源字段的值选择定制资源。"}
{"en": "All custom resources support the `metadata.name` and `metadata.namespace` field\nselectors.\n\nFields declared in a {{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CustomResourceDefinition\" >}}\nmay also be used with field selectors when included in the `spec.versions[*].selectableFields` field of the\n{{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CustomResourceDefinition\" >}}.", "zh": "所有定制资源都支持 `metadata.name` 和 `metadata.namespace` 字段选择算符。\n\n当 {{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CustomResourceDefinition\" >}}\n中声明的字段包含在 {{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CustomResourceDefinition\" >}}\n的 `spec.versions[*].selectableFields` 字段中时，也可以与字段选择算符一起使用。"}
{"en": "### Selectable fields for custom resources {#crd-selectable-fields}", "zh": "### 定制资源的可选择字段   {#crd-selectable-fields}\n\n{{< feature-state feature_gate_name=\"CustomResourceFieldSelectors\" >}}"}
{"en": "The `spec.versions[*].selectableFields` field of a {{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CustomResourceDefinition\" >}} may be used to\ndeclare which other fields in a custom resource may be used in field selectors\nwith the feature of `CustomResourceFieldSelectors`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) (This feature gate is enabled by default since Kubernetes v1.31).\nThe following example adds the `.spec.color` and `.spec.size` fields as\nselectable fields.", "zh": "你需要启用 `CustomResourceFieldSelectors`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)\n来使用此行为，然后将其应用到集群中的所有 CustomResourceDefinitions。\n\n{{< glossary_tooltip term_id=\"CustomResourceDefinition\" text=\"CustomResourceDefinition\" >}}\n字段可以用来在启用了 `CustomResourceFieldSelectors`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/) \n（自 Kubernetes v1.31 起，此特性默认启用）的集群中控制哪些字段可以用在字段选择算符中。\n\n以下示例将 `.spec.color` 和 `.spec.size` 字段添加为可选择字段。\n\n{{% code_sample file=\"customresourcedefinition/shirt-resource-definition.yaml\" %}}"}
{"en": "Field selectors can then be used to get only resources with a `color` of `blue`:", "zh": "字段选择算符随后可用于仅获取 `color` 为 `blue` 的资源：\n\n```shell\nkubectl get shirts.stable.example.com --field-selector spec.color=blue\n```"}
{"en": "The output should be:", "zh": "输出应该是：\n\n```\nNAME       COLOR  SIZE\nexample1   blue   S\nexample2   blue   M\n```\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn how to [Extend the Kubernetes API with the aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/).\n* Learn how to [Extend the Kubernetes API with CustomResourceDefinition](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/).", "zh": "* 了解如何[使用聚合层扩展 Kubernetes API](/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)\n* 了解如何[使用 CustomResourceDefinition 来扩展 Kubernetes API](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)"}
{"en": "The aggregation layer allows Kubernetes to be extended with additional APIs, beyond what is\noffered by the core Kubernetes APIs.\nThe additional APIs can either be ready-made solutions such as a\n[metrics server](https://github.com/kubernetes-sigs/metrics-server), or APIs that you develop yourself.", "zh": "使用聚合层（Aggregation Layer），用户可以通过附加的 API 扩展 Kubernetes，\n而不局限于 Kubernetes 核心 API 提供的功能。\n这里的附加 API 可以是现成的解决方案，比如\n[metrics server](https://github.com/kubernetes-sigs/metrics-server)，\n或者你自己开发的 API。"}
{"en": "The aggregation layer is different from\n[Custom Resource Definitions](/docs/concepts/extend-kubernetes/api-extension/custom-resources/),\nwhich are a way to make the {{< glossary_tooltip term_id=\"kube-apiserver\" text=\"kube-apiserver\" >}}\nrecognise new kinds of object.", "zh": "聚合层不同于\n[定制资源定义（Custom Resource Definitions）](/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources/)。\n后者的目的是让 {{< glossary_tooltip term_id=\"kube-apiserver\" text=\"kube-apiserver\" >}}\n能够识别新的对象类别（Kind）。"}
{"en": "## Aggregation layer\n\nThe aggregation layer runs in-process with the kube-apiserver. Until an extension resource is\nregistered, the aggregation layer will do nothing. To register an API, you add an _APIService_\nobject, which \"claims\" the URL path in the Kubernetes API. At that point, the aggregation layer\nwill proxy anything sent to that API path (e.g. `/apis/myextension.mycompany.io/v1/…`) to the\nregistered APIService.", "zh": "## 聚合层  {#aggregation-layer}\n\n聚合层在 kube-apiserver 进程内运行。在扩展资源注册之前，聚合层不做任何事情。\n要注册 API，你可以添加一个 **APIService** 对象，用它来 “申领” Kubernetes API 中的 URL 路径。\n自此以后，聚合层将把发给该 API 路径的所有内容（例如 `/apis/myextension.mycompany.io/v1/…`）\n转发到已注册的 APIService。"}
{"en": "The most common way to implement the APIService is to run an *extension API server* in Pod(s) that\nrun in your cluster. If you're using the extension API server to manage resources in your cluster,\nthe extension API server (also written as \"extension-apiserver\") is typically paired with one or\nmore {{< glossary_tooltip text=\"controllers\" term_id=\"controller\" >}}. The apiserver-builder\nlibrary provides a skeleton for both extension API servers and the associated controller(s).", "zh": "APIService 的最常见实现方式是在集群中某 Pod 内运行**扩展 API 服务器（Extension API Server）**。\n如果你在使用扩展 API 服务器来管理集群中的资源，该扩展 API 服务器（也被写成 \"extension-apiserver\"）\n一般需要和一个或多个{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}一起使用。\napiserver-builder 库同时提供构造扩展 API 服务器和控制器框架代码。"}
{"en": "### Response latency\n\nExtension API servers should have low latency networking to and from the kube-apiserver.\nDiscovery requests are required to round-trip from the kube-apiserver in five seconds or less.\n\nIf your extension API server cannot achieve that latency requirement, consider making changes that\nlet you meet it.", "zh": "### 响应延迟  {#response-latency}\n\n扩展 API 服务器（Extension API Server）与 kube-apiserver 之间需要存在低延迟的网络连接。\n发现请求需要在五秒钟或更短的时间内完成到 kube-apiserver 的往返。\n\n如果你的扩展 API 服务器无法满足这一延迟要求，应考虑如何更改配置以满足需要。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* To get the aggregator working in your environment, [configure the aggregation layer](/docs/tasks/extend-kubernetes/configure-aggregation-layer/).\n* Then, [setup an extension api-server](/docs/tasks/extend-kubernetes/setup-extension-api-server/) to work with the aggregation layer.\n* Read about [APIService](/docs/reference/kubernetes-api/cluster-resources/api-service-v1/) in the API reference\n\nAlternatively: learn how to\n[extend the Kubernetes API using Custom Resource Definitions](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/).", "zh": "* 阅读[配置聚合层](/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/)文档，\n  了解如何在自己的环境中启用聚合器。\n* 接下来，了解[安装扩展 API 服务器](/zh-cn/docs/tasks/extend-kubernetes/setup-extension-api-server/)，\n  开始使用聚合层。\n* 从 API 参考资料中研究关于 [APIService](/zh-cn/docs/reference/kubernetes-api/cluster-resources/api-service-v1/) 的内容。\n\n或者，学习如何[使用 CustomResourceDefinition 扩展 Kubernetes API](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)。"}
{"en": "Custom resources are extensions of the Kubernetes API. Kubernetes provides two ways to add custom resources to your cluster:", "zh": "自定义资源是 Kubernetes API 的扩展。\nKubernetes 提供了两种将自定义资源添加到集群的方法："}
{"en": "- The [CustomResourceDefinition](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\n  (CRD) mechanism allows you to declaratively define a new custom API with an API group, kind, and\n  schema that you specify.\n  The Kubernetes control plane serves and handles the storage of your custom resource. CRDs allow you to\n  create new types of resources for your cluster without writing and running a custom API server.", "zh": "- [CustomResourceDefinition](/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources/)（CRD）\n  机制允许你通过指定自己的 API 组、种类和模式以声明方式定义新的自定义 API。\n  Kubernetes 控制平面为自定义资源提供服务并为其提供存储。\n  CRD 允许你为集群创建新的资源类别，而无需编写和运行自定义 API 服务器。"}
{"en": "- The [aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)\n  sits behind the primary API server, which acts as a proxy.\n  This arrangement is called API Aggregation (AA), which allows you to provide\n  specialized implementations for your custom resources by writing and\n  deploying your own API server.\n  The main API server delegates requests to your API server for the custom APIs that you specify,\n  making them available to all of its clients.", "zh": "- [聚合层（Aggregation Layer）](/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)位于主\n  API 服务器后面，将 API 服务器用作代理。\n  这种安排称为 API 聚合（API Aggregation，AA），允许你通过编写和部署自己的 API 服务器来为自定义资源提供专门的实现。\n  主 API 服务器将你指定的自定义 API 的请求委托给你的 API 服务器，使其可供所有客户端使用。"}
{"en": "A container image represents binary data that encapsulates an application and all its\nsoftware dependencies. Container images are executable software bundles that can run\nstandalone and that make very well defined assumptions about their runtime environment.\n\nYou typically create a container image of your application and push it to a registry\nbefore referring to it in a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}.\n\nThis page provides an outline of the container image concept.", "zh": "容器镜像（Image）所承载的是封装了应用程序及其所有软件依赖的二进制数据。\n容器镜像是可执行的软件包，可以单独运行；该软件包对所处的运行时环境具有良定（Well Defined）的假定。\n\n你通常会创建应用的容器镜像并将其推送到某仓库（Registry），然后在\n{{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 中引用它。\n\n本页概要介绍容器镜像的概念。\n\n{{< note >}}"}
{"en": "If you are looking for the container images for a Kubernetes\nrelease (such as v{{< skew latestVersion >}}, the latest minor release),\nvisit [Download Kubernetes](https://kubernetes.io/releases/download/).", "zh": "如果你正在寻找 Kubernetes 某个发行版本（如最新次要版本 v{{< skew latestVersion >}}）\n的容器镜像，请访问[下载 Kubernetes](/zh-cn/releases/download/)。\n{{< /note >}}"}
{"en": "## Image names\n\nContainer images are usually given a name such as `pause`, `example/mycontainer`, or `kube-apiserver`.\nImages can also include a registry hostname; for example: `fictional.registry.example/imagename`,\nand possibly a port number as well; for example: `fictional.registry.example:10443/imagename`.\n\nIf you don't specify a registry hostname, Kubernetes assumes that you mean the [Docker public registry](https://hub.docker.com/).\n\nYou can change this behaviour by setting default image registry in \n[container runtime](/docs/setup/production-environment/container-runtimes/) configuration.", "zh": "## 镜像名称    {#image-names}\n\n容器镜像通常会被赋予 `pause`、`example/mycontainer` 或者 `kube-apiserver` 这类的名称。\n镜像名称也可以包含所在仓库的主机名。例如：`fictional.registry.example/imagename`。\n还可以包含仓库的端口号，例如：`fictional.registry.example:10443/imagename`。\n\n如果你不指定仓库的主机名，Kubernetes 认为你在使用 [Docker 公共仓库](https://hub.docker.com/)。\n\n你可以通过在[容器运行时](/zh-cn/docs/setup/production-environment/container-runtimes/)\n配置中设置默认镜像仓库来更改此行为。"}
{"en": "After the image name part you can add a _tag_ or _digest_ (in the same way you would when using with commands\nlike `docker` or `podman`). Tags let you identify different versions of the same series of images.\nDigests are a unique identifier for a specific version of an image. Digests are hashes of the image's content,\nand are immutable. Tags can be moved to point to different images, but digests are fixed.", "zh": "在镜像名称之后，你可以添加一个**标签（Tag）** 或 **摘要（digest）**\n（与使用 `docker` 或 `podman` 等命令时的方式相同）。\n使用标签能让你辨识同一镜像序列中的不同版本。\n摘要是特定版本镜像的唯一标识符，是镜像内容的哈希值，不可变。"}
{"en": "Image tags consist of lowercase and uppercase letters, digits, underscores (`_`),\nperiods (`.`), and dashes (`-`). It can be up to 128 characters long. And must follow the\nnext regex pattern: `[a-zA-Z0-9_][a-zA-Z0-9._-]{0,127}`\nYou can read more about and find validation regex in the\n[OCI Distribution Specification](https://github.com/opencontainers/distribution-spec/blob/master/spec.md#workflow-categories).\nIf you don't specify a tag, Kubernetes assumes you mean the tag `latest`.", "zh": "镜像标签可以包含小写字母、大写字母、数字、下划线（`_`）、句点（`.`）和连字符（`-`）。\n它的长度最多为 128 个字符，并且必须遵循正则表达式模式：`[a-zA-Z0-9_][a-zA-Z0-9._-]{0,127}`。\n你可以在 [OCI 分发规范](https://github.com/opencontainers/distribution-spec/blob/master/spec.md#workflow-categories)\n中阅读有关并找到验证正则表达式的更多信息。\n如果你不指定标签，Kubernetes 认为你想使用标签 `latest`。"}
{"en": "Image digests consists of a hash algorithm (such as `sha256`) and a hash value. For example:\n`sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07`\nYou can find more information about digests format in the \n[OCI Image Specification](https://github.com/opencontainers/image-spec/blob/master/descriptor.md#digests).", "zh": "图像摘要由哈希算法（例如 `sha256`）和哈希值组成，例如：\n`sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07`。\n你可以在 [OCI 镜像规范](https://github.com/opencontainers/image-spec/blob/master/descriptor.md#digests)\n中找到有关摘要格式的更多信息。"}
{"en": "Some image name examples that Kubernetes can use are:", "zh": "Kubernetes 可以使用的一些镜像名称示例包括："}
{"en": "- `busybox` - Image name only, no tag or digest. Kubernetes will use Docker public registry and latest tag. (Same as `docker.io/library/busybox:latest`)\n- `busybox:1.32.0` - Image name with tag. Kubernetes will use Docker public registry. (Same as `docker.io/library/busybox:1.32.0`)\n- `registry.k8s.io/pause:latest` - Image name with a custom registry and latest tag.\n- `registry.k8s.io/pause:3.5` - Image name with a custom registry and non-latest tag.\n- `registry.k8s.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07` - Image name with digest.\n- `registry.k8s.io/pause:3.5@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07` - Image name with tag and digest. Only digest will be used for pulling.", "zh": "- `busybox` - 仅包含镜像名称，没有标签或摘要，Kubernetes 将使用 Docker 公共镜像仓库和 `latest` 标签。\n  （例如 `docker.io/library/busybox:latest`）\n- `busybox:1.32.0` - 带标签的镜像名称，Kubernetes 将使用 Docker 公共镜像仓库。\n  （例如 `docker.io/library/busybox:1.32.0`）\n- `registry.k8s.io/pause:latest` - 带有自定义镜像仓库和 `latest` 标签的镜像名称。\n- `registry.k8s.io/pause:3.5` - 带有自定义镜像仓库和非 `latest` 标签的镜像名称。\n- `registry.k8s.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07` - 带摘要的镜像名称。\n- `registry.k8s.io/pause:3.5@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07` - 带有标签和摘要的镜像名称，镜像拉取仅参考摘要。"}
{"en": "## Updating images\n\nWhen you first create a {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}},\n{{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}, Pod, or other\nobject that includes a Pod template, then by default the pull policy of all\ncontainers in that pod will be set to `IfNotPresent` if it is not explicitly\nspecified. This policy causes the\n{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} to skip pulling an\nimage if it already exists.", "zh": "## 更新镜像  {#updating-images}\n\n当你最初创建一个 {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}、\n{{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}、Pod\n或者其他包含 Pod 模板的对象时，如果没有显式设定的话，\nPod 中所有容器的默认镜像拉取策略是 `IfNotPresent`。这一策略会使得\n{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}}\n在镜像已经存在的情况下直接略过拉取镜像的操作。"}
{"en": "### Image pull policy\n\nThe `imagePullPolicy` for a container and the tag of the image affect when the\n[kubelet](/docs/reference/command-line-tools-reference/kubelet/) attempts to pull (download) the specified image.\n\nHere's a list of the values you can set for `imagePullPolicy` and the effects\nthese values have:", "zh": "### 镜像拉取策略   {#image-pull-policy}\n\n容器的 `imagePullPolicy` 和镜像的标签会影响\n[kubelet](/zh-cn/docs/reference/command-line-tools-reference/kubelet/) 尝试拉取（下载）指定的镜像。\n\n以下列表包含了 `imagePullPolicy` 可以设置的值，以及这些值的效果："}
{"en": "`IfNotPresent`\n: the image is pulled only if it is not already present locally.\n\n`Always`\n: every time the kubelet launches a container, the kubelet queries the container\n  image registry to resolve the name to an image\n  [digest](https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier).\n  If the kubelet has a container image with that exact digest cached locally, the kubelet uses its\n  cached image; otherwise, the kubelet pulls the image with the resolved digest, and uses that image\n  to launch the container.\n\n`Never`\n: the kubelet does not try fetching the image. If the image is somehow already present\n  locally, the kubelet attempts to start the container; otherwise, startup fails.\n  See [pre-pulled images](#pre-pulled-images) for more details.", "zh": "`IfNotPresent`\n: 只有当镜像在本地不存在时才会拉取。\n\n`Always`\n: 每当 kubelet 启动一个容器时，kubelet 会查询容器的镜像仓库，\n  将名称解析为一个镜像[摘要](https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier)。\n  如果 kubelet 有一个容器镜像，并且对应的摘要已在本地缓存，kubelet 就会使用其缓存的镜像；\n  否则，kubelet 就会使用解析后的摘要拉取镜像，并使用该镜像来启动容器。\n\n`Never`\n: kubelet 不会尝试获取镜像。如果镜像已经以某种方式存在本地，\n  kubelet 会尝试启动容器；否则，会启动失败。\n  更多细节见[提前拉取镜像](#pre-pulled-images)。"}
{"en": "The caching semantics of the underlying image provider make even\n`imagePullPolicy: Always` efficient, as long as the registry is reliably accessible.\nYour container runtime can notice that the image layers already exist on the node\nso that they don't need to be downloaded again.", "zh": "只要能够可靠地访问镜像仓库，底层镜像提供者的缓存语义甚至可以使 `imagePullPolicy: Always` 高效。\n你的容器运行时可以注意到节点上已经存在的镜像层，这样就不需要再次下载。\n\n{{< note >}}"}
{"en": "You should avoid using the `:latest` tag when deploying containers in production as\nit is harder to track which version of the image is running and more difficult to\nroll back properly.\n\nInstead, specify a meaningful tag such as `v1.42.0` and/or a digest.", "zh": "在生产环境中部署容器时，你应该避免使用 `:latest` 标签，因为这使得正在运行的镜像的版本难以追踪，并且难以正确地回滚。\n\n相反，应指定一个有意义的标签，如 `v1.42.0`，和/或者一个摘要。\n{{< /note >}}"}
{"en": "To make sure the Pod always uses the same version of a container image, you can specify\nthe image's digest;\nreplace `<image-name>:<tag>` with `<image-name>@<digest>`\n(for example, `image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2`).", "zh": "为了确保 Pod 总是使用相同版本的容器镜像，你可以指定镜像的摘要；\n将 `<image-name>:<tag>` 替换为 `<image-name>@<digest>`，例如\n`image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2`。"}
{"en": "When using image tags, if the image registry were to change the code that the tag on that image\nrepresents, you might end up with a mix of Pods running the old and new code. An image digest\nuniquely identifies a specific version of the image, so Kubernetes runs the same code every time\nit starts a container with that image name and digest specified. Specifying an image by digest\nfixes the code that you run so that a change at the registry cannot lead to that mix of versions.\n\nThere are third-party [admission controllers](/docs/reference/access-authn-authz/admission-controllers/)\nthat mutate Pods (and pod templates) when they are created, so that the\nrunning workload is defined based on an image digest rather than a tag.\nThat might be useful if you want to make sure that all your workload is\nrunning the same code no matter what tag changes happen at the registry.", "zh": "当使用镜像标签时，如果镜像仓库修改了代码所对应的镜像标签，可能会出现新旧代码混杂在 Pod 中运行的情况。\n镜像摘要唯一标识了镜像的特定版本，因此 Kubernetes 每次启动具有指定镜像名称和摘要的容器时，都会运行相同的代码。\n通过摘要指定镜像可固定你运行的代码，这样镜像仓库的变化就不会导致版本的混杂。\n\n有一些第三方的[准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/)\n在创建 Pod（和 Pod 模板）时产生变更，这样运行的工作负载就是根据镜像摘要，而不是标签来定义的。\n无论镜像仓库上的标签发生什么变化，你都想确保你所有的工作负载都运行相同的代码，那么指定镜像摘要会很有用。"}
{"en": "#### Default image pull policy {#imagepullpolicy-defaulting}\n\nWhen you (or a controller) submit a new Pod to the API server, your cluster sets the\n`imagePullPolicy` field when specific conditions are met:\n\n- if you omit the `imagePullPolicy` field, and you specify the digest for the\n  container image, the `imagePullPolicy` is automatically set to `IfNotPresent`.", "zh": "#### 默认镜像拉取策略    {#imagepullpolicy-defaulting}\n\n当你（或控制器）向 API 服务器提交一个新的 Pod 时，你的集群会在满足特定条件时设置 `imagePullPolicy` 字段：\n\n- 如果你省略了 `imagePullPolicy` 字段，并且你为容器镜像指定了摘要，\n  那么 `imagePullPolicy` 会自动设置为 `IfNotPresent`。"}
{"en": "- if you omit the `imagePullPolicy` field, and the tag for the container image is\n  `:latest`, `imagePullPolicy` is automatically set to `Always`;\n- if you omit the `imagePullPolicy` field, and you don't specify the tag for the\n  container image, `imagePullPolicy` is automatically set to `Always`;\n- if you omit the `imagePullPolicy` field, and you specify the tag for the\n  container image that isn't `:latest`, the `imagePullPolicy` is automatically set to\n  `IfNotPresent`.", "zh": "- 如果你省略了 `imagePullPolicy` 字段，并且容器镜像的标签是 `:latest`，\n  `imagePullPolicy` 会自动设置为 `Always`。\n- 如果你省略了 `imagePullPolicy` 字段，并且没有指定容器镜像的标签，\n  `imagePullPolicy` 会自动设置为 `Always`。\n- 如果你省略了 `imagePullPolicy` 字段，并且为容器镜像指定了非 `:latest` 的标签，\n  `imagePullPolicy` 就会自动设置为 `IfNotPresent`。\n\n{{< note >}}"}
{"en": "The value of `imagePullPolicy` of the container is always set when the object is\nfirst _created_, and is not updated if the image's tag or digest later changes.\n\nFor example, if you create a Deployment with an image whose tag is _not_\n`:latest`, and later update that Deployment's image to a `:latest` tag, the\n`imagePullPolicy` field will _not_ change to `Always`. You must manually change\nthe pull policy of any object after its initial creation.", "zh": "容器的 `imagePullPolicy` 的值总是在对象初次**创建**时设置的，\n如果后来镜像的标签或摘要发生变化，则不会更新。\n\n例如，如果你用一个**非** `:latest` 的镜像标签创建一个 Deployment，\n并在随后更新该 Deployment 的镜像标签为 `:latest`，则 `imagePullPolicy` 字段**不会**变成 `Always`。\n你必须手动更改已经创建的资源的拉取策略。\n{{< /note >}}"}
{"en": "#### Required image pull\n\nIf you would like to always force a pull, you can do one of the following:\n\n- Set the `imagePullPolicy` of the container to `Always`.\n- Omit the `imagePullPolicy` and use `:latest` as the tag for the image to use;\n  Kubernetes will set the policy to `Always` when you submit the Pod.\n- Omit the `imagePullPolicy` and the tag for the image to use;\n  Kubernetes will set the policy to `Always` when you submit the Pod.\n- Enable the [AlwaysPullImages](/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages)\n  admission controller.", "zh": "#### 必要的镜像拉取   {#required-image-pull}\n\n如果你想总是强制执行拉取，你可以使用下述的一种方式：\n\n- 设置容器的 `imagePullPolicy` 为 `Always`。\n- 省略 `imagePullPolicy`，并使用 `:latest` 作为镜像标签；\n  当你提交 Pod 时，Kubernetes 会将策略设置为 `Always`。\n- 省略 `imagePullPolicy` 和镜像的标签；\n  当你提交 Pod 时，Kubernetes 会将策略设置为 `Always`。\n- 启用准入控制器 [AlwaysPullImages](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages)。"}
{"en": "### ImagePullBackOff\n\nWhen a kubelet starts creating containers for a Pod using a container runtime,\nit might be possible the container is in [Waiting](/docs/concepts/workloads/pods/pod-lifecycle/#container-state-waiting)\nstate because of `ImagePullBackOff`.", "zh": "### ImagePullBackOff\n\n当 kubelet 使用容器运行时创建 Pod 时，容器可能因为 `ImagePullBackOff` 导致状态为\n[Waiting](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#container-state-waiting)。"}
{"en": "The status `ImagePullBackOff` means that a container could not start because Kubernetes\ncould not pull a container image (for reasons such as invalid image name, or pulling\nfrom a private registry without `imagePullSecret`). The `BackOff` part indicates\nthat Kubernetes will keep trying to pull the image, with an increasing back-off delay.\n\nKubernetes raises the delay between each attempt until it reaches a compiled-in limit,\nwhich is 300 seconds (5 minutes).", "zh": "`ImagePullBackOff` 状态意味着容器无法启动，\n因为 Kubernetes 无法拉取容器镜像（原因包括无效的镜像名称，或从私有仓库拉取而没有 `imagePullSecret`）。\n`BackOff` 部分表示 Kubernetes 将继续尝试拉取镜像，并增加回退延迟。\n\nKubernetes 会增加每次尝试之间的延迟，直到达到编译限制，即 300 秒（5 分钟）。"}
{"en": "### Image pull per runtime class", "zh": "### 基于运行时类的镜像拉取  {#image-pull-per-runtime-class}\n\n{{< feature-state feature_gate_name=\"RuntimeClassInImageCriApi\" >}}"}
{"en": "Kubernetes includes alpha support for performing image pulls based on the RuntimeClass of a Pod.", "zh": "Kubernetes 包含了根据 Pod 的 RuntimeClass 来执行镜像拉取的 Alpha 支持。"}
{"en": "If you enable the `RuntimeClassInImageCriApi` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/),\nthe kubelet references container images by a tuple of (image name, runtime handler) rather than just the\nimage name or digest. Your {{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}\nmay adapt its behavior based on the selected runtime handler.\nPulling images based on runtime class will be helpful for VM based containers like windows hyperV containers.", "zh": "如果你启用了 `RuntimeClassInImageCriApi` [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)，\nkubelet 会通过一个元组（镜像名称，运行时处理程序）而不仅仅是镜像名称或镜像摘要来引用容器镜像。\n你的{{< glossary_tooltip text=\"容器运行时\" term_id=\"container-runtime\" >}}\n可能会根据选定的运行时处理程序调整其行为。\n基于运行时类来拉取镜像对于基于 VM 的容器（如 Windows Hyper-V 容器）会有帮助。"}
{"en": "## Serial and parallel image pulls", "zh": "## 串行和并行镜像拉取  {#serial-and-parallel-image-pulls}"}
{"en": "By default, kubelet pulls images serially. In other words, kubelet sends only\none image pull request to the image service at a time. Other image pull requests\nhave to wait until the one being processed is complete.", "zh": "默认情况下，kubelet 以串行方式拉取镜像。\n也就是说，kubelet 一次只向镜像服务发送一个镜像拉取请求。\n其他镜像拉取请求必须等待，直到正在处理的那个请求完成。"}
{"en": "Nodes make image pull decisions in isolation. Even when you use serialized image\npulls, two different nodes can pull the same image in parallel.", "zh": "节点独立地做出镜像拉取的决策。即使你使用串行的镜像拉取，两个不同的节点也可以并行拉取相同的镜像。"}
{"en": "If you would like to enable parallel image pulls, you can set the field\n`serializeImagePulls` to false in the [kubelet configuration](/docs/reference/config-api/kubelet-config.v1beta1/).\nWith `serializeImagePulls` set to false, image pull requests will be sent to the image service immediately,\nand multiple images will be pulled at the same time.", "zh": "如果你想启用并行镜像拉取，可以在 [kubelet 配置](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/)\n中将字段 `serializeImagePulls` 设置为 false。\n\n当`serializeImagePulls` 设置为 false 时，kubelet 会立即向镜像服务发送镜像拉取请求，多个镜像将同时被拉动。"}
{"en": "When enabling parallel image pulls, please make sure the image service of your\ncontainer runtime can handle parallel image pulls.", "zh": "启用并行镜像拉取时，请确保你的容器运行时的镜像服务可以处理并行镜像拉取。"}
{"en": "The kubelet never pulls multiple images in parallel on behalf of one Pod. For example,\nif you have a Pod that has an init container and an application container, the image\npulls for the two containers will not be parallelized. However, if you have two\nPods that use different images, the kubelet pulls the images in parallel on\nbehalf of the two different Pods, when parallel image pulls is enabled.", "zh": "kubelet 从不代表一个 Pod 并行地拉取多个镜像。\n\n例如，如果你有一个 Pod，它有一个初始容器和一个应用容器，那么这两个容器的镜像拉取将不会并行。\n但是，如果你有两个使用不同镜像的 Pod，当启用并行镜像拉取时，kubelet 会代表两个不同的 Pod 并行拉取镜像。"}
{"en": "### Maximum parallel image pulls", "zh": "### 最大并行镜像拉取数量  {#maximum-parallel-image-pulls}\n\n{{< feature-state for_k8s_version=\"v1.27\" state=\"alpha\" >}}"}
{"en": "When `serializeImagePulls` is set to false, the kubelet defaults to no limit on the\nmaximum number of images being pulled at the same time. If you would like to\nlimit the number of parallel image pulls, you can set the field `maxParallelImagePulls`\nin kubelet configuration. With `maxParallelImagePulls` set to _n_, only _n_ images\ncan be pulled at the same time, and any image pull beyond _n_ will have to wait\nuntil at least one ongoing image pull is complete.", "zh": "当 `serializeImagePulls` 被设置为 false 时，kubelet 默认对同时拉取的最大镜像数量没有限制。\n如果你想限制并行镜像拉取的数量，可以在 kubelet 配置中设置字段 `maxParallelImagePulls`。\n当 `maxParallelImagePulls` 设置为 **n** 时，只能同时拉取 **n** 个镜像，\n超过 **n** 的任何镜像都必须等到至少一个正在进行拉取的镜像拉取完成后，才能拉取。"}
{"en": "Limiting the number parallel image pulls would prevent image pulling from consuming\ntoo much network bandwidth or disk I/O, when parallel image pulling is enabled.", "zh": "当启用并行镜像拉取时，限制并行镜像拉取的数量可以防止镜像拉取消耗过多的网络带宽或磁盘 I/O。"}
{"en": "You can set `maxParallelImagePulls` to a positive number that is greater than or\nequal to 1. If you set `maxParallelImagePulls` to be greater than or equal to 2, you\nmust set the `serializeImagePulls` to false. The kubelet will fail to start with invalid\n`maxParallelImagePulls` settings.", "zh": "你可以将 `maxParallelImagePulls` 设置为大于或等于 1 的正数。\n如果将 `maxParallelImagePulls` 设置为大于等于 2，则必须将 `serializeImagePulls` 设置为 false。\nkubelet 在无效的 `maxParallelImagePulls` 设置下会启动失败。"}
{"en": "## Multi-architecture images with image indexes\n\nAs well as providing binary images, a container registry can also serve a\n[container image index](https://github.com/opencontainers/image-spec/blob/master/image-index.md).\nAn image index can point to multiple [image manifests](https://github.com/opencontainers/image-spec/blob/master/manifest.md)\nfor architecture-specific versions of a container. The idea is that you can have a name for an image\n(for example: `pause`, `example/mycontainer`, `kube-apiserver`) and allow different systems to\nfetch the right binary image for the machine architecture they are using.\n\nKubernetes itself typically names container images with a suffix `-$(ARCH)`. For backward\ncompatibility, please generate the older images with suffixes. The idea is to generate say `pause`\nimage which has the manifest for all the arch(es) and say `pause-amd64` which is backwards\ncompatible for older configurations or YAML files which may have hard coded the images with\nsuffixes.", "zh": "## 带镜像索引的多架构镜像  {#multi-architecture-images-with-image-indexes}\n\n除了提供二进制的镜像之外，\n容器仓库也可以提供[容器镜像索引](https://github.com/opencontainers/image-spec/blob/master/image-index.md)。\n镜像索引可以指向镜像的多个[镜像清单](https://github.com/opencontainers/image-spec/blob/master/manifest.md)，\n提供特定于体系结构版本的容器。\n这背后的理念是让你可以为镜像命名（例如：`pause`、`example/mycontainer`、`kube-apiserver`）\n的同时，允许不同的系统基于它们所使用的机器体系结构取回正确的二进制镜像。\n\nKubernetes 自身通常在命名容器镜像时添加后缀 `-$(ARCH)`。\n为了向前兼容，请在生成较老的镜像时也提供后缀。\n这里的理念是为某镜像（如 `pause`）生成针对所有平台都适用的清单时，\n生成 `pause-amd64` 这类镜像，以便较老的配置文件或者将镜像后缀硬编码到其中的\nYAML 文件也能兼容。"}
{"en": "## Using a private registry\n\nPrivate registries may require keys to read images from them.\nCredentials can be provided in several ways:", "zh": "## 使用私有仓库   {#using-a-private-registry}\n\n从私有仓库读取镜像时可能需要密钥。\n凭据可以用以下方式提供:"}
{"en": "- Configuring Nodes to Authenticate to a Private Registry\n  - all pods can read any configured private registries\n  - requires node configuration by cluster administrator\n- Kubelet Credential Provider to dynamically fetch credentials for private registries\n  - kubelet can be configured to use credential provider exec plugin\n    for the respective private registry.\n- Pre-pulled Images\n  - all pods can use any images cached on a node\n  - requires root access to all nodes to set up\n- Specifying ImagePullSecrets on a Pod\n  - only pods which provide own keys can access the private registry\n- Vendor-specific or local extensions\n  - if you're using a custom node configuration, you (or your cloud\n    provider) can implement your mechanism for authenticating the node\n    to the container registry.", "zh": "- 配置节点向私有仓库进行身份验证\n  - 所有 Pod 均可读取任何已配置的私有仓库\n  - 需要集群管理员配置节点\n- kubelet 凭据提供程序，动态获取私有仓库的凭据\n  - kubelet 可以被配置为使用凭据提供程序 exec 插件来访问对应的私有镜像库\n- 预拉镜像\n  - 所有 Pod 都可以使用节点上缓存的所有镜像\n  - 需要所有节点的 root 访问权限才能进行设置\n- 在 Pod 中设置 ImagePullSecrets\n  - 只有提供自己密钥的 Pod 才能访问私有仓库\n- 特定于厂商的扩展或者本地扩展\n  - 如果你在使用定制的节点配置，你（或者云平台提供商）可以实现让节点向容器仓库认证的机制"}
{"en": "These options are explained in more detail below.", "zh": "下面将详细描述每一项。"}
{"en": "### Configuring nodes to authenticate to a private registry\n\nSpecific instructions for setting credentials depends on the container runtime and registry you\nchose to use. You should refer to your solution's documentation for the most accurate information.", "zh": "### 配置 Node 对私有仓库认证  {#configuring-nodes-to-authenticate-to-a-private-registry}\n\n设置凭据的具体说明取决于你选择使用的容器运行时和仓库。\n你应该参考解决方案的文档来获取最准确的信息。"}
{"en": "For an example of configuring a private container image registry, see the\n[Pull an Image from a Private Registry](/docs/tasks/configure-pod-container/pull-image-private-registry)\ntask. That example uses a private registry in Docker Hub.", "zh": "有关配置私有容器镜像仓库的示例，\n请参阅任务[从私有镜像库中拉取镜像](/zh-cn/docs/tasks/configure-pod-container/pull-image-private-registry)。\n该示例使用 Docker Hub 中的私有镜像仓库。"}
{"en": "### Kubelet credential provider for authenticated image pulls {#kubelet-credential-provider}", "zh": "### 用于认证镜像拉取的 kubelet 凭据提供程序  {#kubelet-credential-provider}\n\n{{< note >}}"}
{"en": "This approach is especially suitable when kubelet needs to fetch registry credentials dynamically.\nMost commonly used for registries provided by cloud providers where auth tokens are short-lived.", "zh": "此方法尤其适合 kubelet 需要动态获取仓库凭据时。\n最常用于由云提供商提供的仓库，其中身份认证令牌的生命期是短暂的。\n{{< /note >}}"}
{"en": "You can configure the kubelet to invoke a plugin binary to dynamically fetch registry credentials for a container image.\nThis is the most robust and versatile way to fetch credentials for private registries, but also requires kubelet-level configuration to enable.\n\nSee [Configure a kubelet image credential provider](/docs/tasks/administer-cluster/kubelet-credential-provider/) for more details.", "zh": "你可以配置 kubelet，以调用插件可执行文件的方式来动态获取容器镜像的仓库凭据。\n这是为私有仓库获取凭据最稳健和最通用的方法，但也需要 kubelet 级别的配置才能启用。\n\n有关更多细节请参见[配置 kubelet 镜像凭据提供程序](/zh-cn/docs/tasks/administer-cluster/kubelet-credential-provider/)。"}
{"en": "### Interpretation of config.json {#config-json}", "zh": "### config.json 说明 {#config-json}"}
{"en": "The interpretation of `config.json` varies between the original Docker\nimplementation and the Kubernetes interpretation. In Docker, the `auths` keys\ncan only specify root URLs, whereas Kubernetes allows glob URLs as well as\nprefix-matched paths. The only limitation is that glob patterns (`*`) have to\ninclude the dot (`.`) for each subdomain. The amount of matched subdomains has\nto be equal to the amount of glob patterns (`*.`), for example:", "zh": "对于 `config.json` 的解释在原始 Docker 实现和 Kubernetes 的解释之间有所不同。\n在 Docker 中，`auths` 键只能指定根 URL，而 Kubernetes 允许 glob URL 以及前缀匹配的路径。\n唯一的限制是 glob 模式（`*`）必须为每个子域名包括点（`.`）。\n匹配的子域名数量必须等于 glob 模式（`*.`）的数量，例如："}
{"en": "- `*.kubernetes.io` will *not* match `kubernetes.io`, but `abc.kubernetes.io`\n- `*.*.kubernetes.io` will *not* match `abc.kubernetes.io`, but `abc.def.kubernetes.io`\n- `prefix.*.io` will match `prefix.kubernetes.io`\n- `*-good.kubernetes.io` will match `prefix-good.kubernetes.io`", "zh": "- `*.kubernetes.io` **不**会匹配 `kubernetes.io`，但会匹配 `abc.kubernetes.io`\n- `*.*.kubernetes.io` **不**会匹配 `abc.kubernetes.io`，但会匹配 `abc.def.kubernetes.io`\n- `prefix.*.io` 将匹配 `prefix.kubernetes.io`\n- `*-good.kubernetes.io` 将匹配 `prefix-good.kubernetes.io`"}
{"en": "This means that a `config.json` like this is valid:", "zh": "这意味着，像这样的 `config.json` 是有效的：\n\n```json\n{\n    \"auths\": {\n        \"my-registry.io/images\": { \"auth\": \"…\" },\n        \"*.my-registry.io/images\": { \"auth\": \"…\" }\n    }\n}\n```"}
{"en": "Image pull operations would now pass the credentials to the CRI container\nruntime for every valid pattern. For example the following container image names\nwould match successfully:", "zh": "现在镜像拉取操作会将每种有效模式的凭据都传递给 CRI 容器运行时。例如下面的容器镜像名称会匹配成功：\n\n- `my-registry.io/images`\n- `my-registry.io/images/my-image`\n- `my-registry.io/images/another-image`\n- `sub.my-registry.io/images/my-image`"}
{"en": "But not:", "zh": "但这些不会匹配成功：\n\n- `a.sub.my-registry.io/images/my-image`\n- `a.b.sub.my-registry.io/images/my-image`"}
{"en": "The kubelet performs image pulls sequentially for every found credential. This\nmeans, that multiple entries in `config.json` for different paths are possible, too:", "zh": "kubelet 为每个找到的凭据的镜像按顺序拉取。这意味着对于不同的路径在 `config.json` 中也可能有多项：\n\n```json\n{\n    \"auths\": {\n        \"my-registry.io/images\": {\n            \"auth\": \"…\"\n        },\n        \"my-registry.io/images/subpath\": {\n            \"auth\": \"…\"\n        }\n    }\n}\n```"}
{"en": "If now a container specifies an image `my-registry.io/images/subpath/my-image`\nto be pulled, then the kubelet will try to download them from both\nauthentication sources if one of them fails.", "zh": "如果一个容器指定了要拉取的镜像 `my-registry.io/images/subpath/my-image`，\n并且其中一个失败，kubelet 将尝试从另一个身份验证源下载镜像。"}
{"en": "### Pre-pulled images", "zh": "### 提前拉取镜像   {#pre-pulled-images}\n\n{{< note >}}"}
{"en": "This approach is suitable if you can control node configuration.  It\nwill not work reliably if your cloud provider manages nodes and replaces\nthem automatically.", "zh": "该方法适用于你能够控制节点配置的场合。\n如果你的云供应商负责管理节点并自动置换节点，这一方案无法可靠地工作。\n{{< /note >}}"}
{"en": "By default, the kubelet tries to pull each image from the specified registry.\nHowever, if the `imagePullPolicy` property of the container is set to `IfNotPresent` or `Never`,\nthen a local image is used (preferentially or exclusively, respectively).", "zh": "默认情况下，`kubelet` 会尝试从指定的仓库拉取每个镜像。\n但是，如果容器属性 `imagePullPolicy` 设置为 `IfNotPresent` 或者 `Never`，\n则会优先使用（对应 `IfNotPresent`）或者一定使用（对应 `Never`）本地镜像。"}
{"en": "If you want to rely on pre-pulled images as a substitute for registry authentication,\nyou must ensure all nodes in the cluster have the same pre-pulled images.\n\nThis can be used to preload certain images for speed or as an alternative to authenticating to a\nprivate registry.\n\nAll pods will have read access to any pre-pulled images.", "zh": "如果你希望使用提前拉取镜像的方法代替仓库认证，就必须保证集群中所有节点提前拉取的镜像是相同的。\n\n这一方案可以用来提前载入指定的镜像以提高速度，或者作为向私有仓库执行身份认证的一种替代方案。\n\n所有的 Pod 都可以使用节点上提前拉取的镜像。"}
{"en": "### Specifying imagePullSecrets on a Pod", "zh": "### 在 Pod 上指定 ImagePullSecrets   {#specifying-imagepullsecrets-on-a-pod}\n\n{{< note >}}"}
{"en": "This is the recommended approach to run containers based on images\nin private registries.", "zh": "运行使用私有仓库中镜像的容器时，建议使用这种方法。\n{{< /note >}}"}
{"en": "Kubernetes supports specifying container image registry keys on a Pod.\n`imagePullSecrets` must all be in the same namespace as the Pod. The referenced\nSecrets must be of type `kubernetes.io/dockercfg` or `kubernetes.io/dockerconfigjson`.", "zh": "Kubernetes 支持在 Pod 中设置容器镜像仓库的密钥。\n`imagePullSecrets` 必须全部与 Pod 位于同一个名字空间中。\n引用的 Secret 必须是 `kubernetes.io/dockercfg` 或 `kubernetes.io/dockerconfigjson` 类型。"}
{"en": "#### Creating a Secret with a Docker config\n\nYou need to know the username, registry password and client email address for authenticating\nto the registry, as well as its hostname.\nRun the following command, substituting the appropriate uppercase values:", "zh": "#### 使用 Docker Config 创建 Secret   {#creating-a-secret-with-docker-config}\n\n你需要知道用于向仓库进行身份验证的用户名、密码和客户端电子邮件地址，以及它的主机名。\n运行以下命令，注意替换适当的大写值：\n\n```shell\nkubectl create secret docker-registry <name> \\\n  --docker-server=DOCKER_REGISTRY_SERVER \\\n  --docker-username=DOCKER_USER \\\n  --docker-password=DOCKER_PASSWORD \\\n  --docker-email=DOCKER_EMAIL\n```"}
{"en": "If you already have a Docker credentials file then, rather than using the above\ncommand, you can import the credentials file as a Kubernetes\n{{< glossary_tooltip text=\"Secrets\" term_id=\"secret\" >}}.  \n[Create a Secret based on existing Docker credentials](/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials)\nexplains how to set this up.", "zh": "如果你已经有 Docker 凭据文件，则可以将凭据文件导入为 Kubernetes\n{{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}，\n而不是执行上面的命令。\n[基于已有的 Docker 凭据创建 Secret](/zh-cn/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials)\n解释了如何完成这一操作。"}
{"en": "This is particularly useful if you are using multiple private container\nregistries, as `kubectl create secret docker-registry` creates a Secret that\nonly works with a single private registry.", "zh": "如果你在使用多个私有容器仓库，这种技术将特别有用。\n原因是 `kubectl create secret docker-registry` 创建的是仅适用于某个私有仓库的 Secret。\n\n{{< note >}}"}
{"en": "Pods can only reference image pull secrets in their own namespace,\nso this process needs to be done one time per namespace.", "zh": "Pod 只能引用位于自身所在名字空间中的 Secret，因此需要针对每个名字空间重复执行上述过程。\n{{< /note >}}"}
{"en": "#### Referring to an imagePullSecrets on a Pod\n\nNow, you can create pods which reference that secret by adding an `imagePullSecrets`\nsection to a Pod definition. Each item in the `imagePullSecrets` array can only\nreference a Secret in the same namespace.\n\nFor example:", "zh": "#### 在 Pod 中引用 ImagePullSecrets {#referring-to-an-imagepullsecrets-on-a-pod}\n\n现在，在创建 Pod 时，可以在 Pod 定义中增加 `imagePullSecrets` 部分来引用该 Secret。\n`imagePullSecrets` 数组中的每一项只能引用同一名字空间中的 Secret。\n\n例如：\n\n```shell\ncat <<EOF > pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: foo\n  namespace: awesomeapps\nspec:\n  containers:\n    - name: foo\n      image: janedoe/awesomeapp:v1\n  imagePullSecrets:\n    - name: myregistrykey\nEOF\n\ncat <<EOF >> ./kustomization.yaml\nresources:\n- pod.yaml\nEOF\n```"}
{"en": "This needs to be done for each pod that is using a private registry.\n\nHowever, setting of this field can be automated by setting the imagePullSecrets\nin a [ServiceAccount](/docs/tasks/configure-pod-container/configure-service-account/) resource.\n\nCheck [Add ImagePullSecrets to a Service Account](/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account)\nfor detailed instructions.\n\nYou can use this in conjunction with a per-node `.docker/config.json`.  The credentials\nwill be merged.", "zh": "你需要对使用私有仓库的每个 Pod 执行以上操作。不过，\n设置该字段的过程也可以通过为[服务账号](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/)资源设置\n`imagePullSecrets` 来自动完成。有关详细指令，\n可参见[将 ImagePullSecrets 添加到服务账号](/zh-cn/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account)。\n\n你也可以将此方法与节点级别的 `.docker/config.json` 配置结合使用。\n来自不同来源的凭据会被合并。"}
{"en": "## Use cases\n\nThere are a number of solutions for configuring private registries.  Here are some\ncommon use cases and suggested solutions.", "zh": "## 使用案例  {#use-cases}\n\n配置私有仓库有多种方案，以下是一些常用场景和建议的解决方案。"}
{"en": "1. Cluster running only non-proprietary (e.g. open-source) images.  No need to hide images.\n   - Use public images from a public registry\n     - No configuration required.\n     - Some cloud providers automatically cache or mirror public images, which improves\n       availability and reduces the time to pull images.", "zh": "1. 集群运行非专有镜像（例如，开源镜像）。镜像不需要隐藏。\n   - 使用来自公共仓库的公共镜像\n     - 无需配置\n     - 某些云厂商会自动为公开镜像提供高速缓存，以便提升可用性并缩短拉取镜像所需时间"}
{"en": "1. Cluster running some proprietary images which should be hidden to those outside the company, but\n   visible to all cluster users.\n   - Use a hosted private registry\n     - Manual configuration may be required on the nodes that need to access to private registry\n   - Or, run an internal private registry behind your firewall with open read access.\n     - No Kubernetes configuration is required.\n   - Use a hosted container image registry service that controls image access\n     - It will work better with cluster autoscaling than manual node configuration.\n   - Or, on a cluster where changing the node configuration is inconvenient, use `imagePullSecrets`.", "zh": "2. 集群运行一些专有镜像，这些镜像需要对公司外部隐藏，对所有集群用户可见\n   - 使用托管的私有仓库\n     - 在需要访问私有仓库的节点上可能需要手动配置\n   - 或者，在防火墙内运行一个组织内部的私有仓库，并开放读取权限\n     - 不需要配置 Kubernetes\n   - 使用控制镜像访问的托管容器镜像仓库服务\n     - 与手动配置节点相比，这种方案能更好地处理集群自动扩缩容\n   - 或者，在不方便更改节点配置的集群中，使用 `imagePullSecrets`"}
{"en": "1. Cluster with proprietary images, a few of which require stricter access control.\n   - Ensure [AlwaysPullImages admission controller](/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages)\n     is active. Otherwise, all Pods potentially have access to all images.\n   - Move sensitive data into a \"Secret\" resource, instead of packaging it in an image.", "zh": "3. 集群使用专有镜像，且有些镜像需要更严格的访问控制\n   - 确保 [AlwaysPullImages 准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages)被启用。\n     否则，所有 Pod 都可以使用所有镜像。\n   - 确保将敏感数据存储在 Secret 资源中，而不是将其打包在镜像里。"}
{"en": "1. A multi-tenant cluster where each tenant needs own private registry.\n   - Ensure [AlwaysPullImages admission controller](/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages)\n     is active. Otherwise, all Pods of all tenants potentially have access to all images.\n   - Run a private registry with authorization required.\n   - Generate registry credential for each tenant, put into secret, and populate secret to each\n     tenant namespace.\n   - The tenant adds that secret to imagePullSecrets of each namespace.", "zh": "4. 集群是多租户的并且每个租户需要自己的私有仓库\n   - 确保 [AlwaysPullImages 准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages)。\n     否则，所有租户的所有的 Pod 都可以使用所有镜像。\n   - 为私有仓库启用鉴权。\n   - 为每个租户生成访问仓库的凭据，放置在 Secret 中，并将 Secret 发布到各租户的名字空间下。\n   - 租户将 Secret 添加到每个名字空间中的 imagePullSecrets。"}
{"en": "If you need access to multiple registries, you can create one secret for each registry.", "zh": "如果你需要访问多个仓库，可以为每个仓库创建一个 Secret。"}
{"en": "## Legacy built-in kubelet credential provider\n\nIn older versions of Kubernetes, the kubelet had a direct integration with cloud provider credentials.\nThis gave it the ability to dynamically fetch credentials for image registries.", "zh": "## 旧版的内置 kubelet 凭据提供程序    {#legacy-built-in-kubelet-credentials-provider}\n\n在旧版本的 Kubernetes 中，kubelet 与云提供商凭据直接集成。\n这使它能够动态获取镜像仓库的凭据。"}
{"en": "There were three built-in implementations of the kubelet credential provider integration:\nACR (Azure Container Registry), ECR (Elastic Container Registry), and GCR (Google Container Registry).", "zh": "kubelet 凭据提供程序集成存在三个内置实现：\nACR（Azure 容器仓库）、ECR（Elastic 容器仓库）和 GCR（Google 容器仓库）。"}
{"en": "For more information on the legacy mechanism, read the documentation for the version of Kubernetes that you\nare using. Kubernetes v1.26 through to v{{< skew latestVersion >}} do not include the legacy mechanism, so\nyou would need to either:\n- configure a kubelet image credential provider on each node\n- specify image pull credentials using `imagePullSecrets` and at least one Secret", "zh": "有关该旧版机制的更多信息，请阅读你正在使用的 Kubernetes 版本的文档。\n从 Kubernetes v1.26 到 v{{< skew latestVersion >}} 不再包含该旧版机制，因此你需要：\n\n- 在每个节点上配置一个 kubelet 镜像凭据提供程序\n- 使用 `imagePullSecrets` 和至少一个 Secret 指定镜像拉取凭据\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read the [OCI Image Manifest Specification](https://github.com/opencontainers/image-spec/blob/master/manifest.md).\n* Learn about [container image garbage collection](/docs/concepts/architecture/garbage-collection/#container-image-garbage-collection).\n* Learn more about [pulling an Image from a Private Registry](/docs/tasks/configure-pod-container/pull-image-private-registry).", "zh": "* 阅读 [OCI Image Manifest 规范](https://github.com/opencontainers/image-spec/blob/master/manifest.md)。\n* 了解[容器镜像垃圾收集](/zh-cn/docs/concepts/architecture/garbage-collection/#container-image-garbage-collection)。\n* 了解[从私有仓库拉取镜像](/zh-cn/docs/tasks/configure-pod-container/pull-image-private-registry)。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.20\" state=\"stable\" >}}"}
{"en": "This page describes the RuntimeClass resource and runtime selection mechanism.\n\nRuntimeClass is a feature for selecting the container runtime configuration. The container runtime\nconfiguration is used to run a Pod's containers.", "zh": "本页面描述了 RuntimeClass 资源和运行时的选择机制。\n\nRuntimeClass 是一个用于选择容器运行时配置的特性，容器运行时配置用于运行 Pod 中的容器。"}
{"en": "## Motivation\n\nYou can set a different RuntimeClass between different Pods to provide a balance of\nperformance versus security. For example, if part of your workload deserves a high\nlevel of information security assurance, you might choose to schedule those Pods so\nthat they run in a container runtime that uses hardware virtualization. You'd then\nbenefit from the extra isolation of the alternative runtime, at the expense of some\nadditional overhead.", "zh": "## 动机   {#motivation}\n\n你可以在不同的 Pod 设置不同的 RuntimeClass，以提供性能与安全性之间的平衡。\n例如，如果你的部分工作负载需要高级别的信息安全保证，你可以决定在调度这些 Pod\n时尽量使它们在使用硬件虚拟化的容器运行时中运行。\n这样，你将从这些不同运行时所提供的额外隔离中获益，代价是一些额外的开销。"}
{"en": "You can also use RuntimeClass to run different Pods with the same container runtime\nbut with different settings.", "zh": "你还可以使用 RuntimeClass 运行具有相同容器运行时但具有不同设置的 Pod。"}
{"en": "## Setup", "zh": "## 设置  {#setup}"}
{"en": "1. Configure the CRI implementation on nodes (runtime dependent)\n2. Create the corresponding RuntimeClass resources", "zh": "1. 在节点上配置 CRI 的实现（取决于所选用的运行时）\n2. 创建相应的 RuntimeClass 资源"}
{"en": "### 1. Configure the CRI implementation on nodes", "zh": "### 1. 在节点上配置 CRI 实现"}
{"en": "The configurations available through RuntimeClass are Container Runtime Interface (CRI)\nimplementation dependent. See the corresponding documentation ([below](#cri-configuration)) for your\nCRI implementation for how to configure.", "zh": "RuntimeClass 的配置依赖于运行时接口（CRI）的实现。\n根据你使用的 CRI 实现，查阅相关的文档（[下方](#cri-configuration)）来了解如何配置。\n\n{{< note >}}"}
{"en": "RuntimeClass assumes a homogeneous node configuration across the cluster by default (which means\nthat all nodes are configured the same way with respect to container runtimes). To support\nheterogeneous node configurations, see [Scheduling](#scheduling) below.", "zh": "RuntimeClass 假设集群中的节点配置是同构的（换言之，所有的节点在容器运行时方面的配置是相同的）。\n如果需要支持异构节点，配置方法请参阅下面的[调度](#scheduling)。\n{{< /note >}}"}
{"en": "The configurations have a corresponding `handler` name, referenced by the RuntimeClass. The\nhandler must be a valid [DNS label name](/docs/concepts/overview/working-with-objects/names/#dns-label-names).", "zh": "所有这些配置都具有相应的 `handler` 名，并被 RuntimeClass 引用。\nhandler 必须是有效的 [DNS 标签名](/zh-cn/docs/concepts/overview/working-with-objects/names/#dns-label-names)。"}
{"en": "### 2. Create the corresponding RuntimeClass resources\n\nThe configurations setup in step 1 should each have an associated `handler` name, which identifies\nthe configuration. For each handler, create a corresponding RuntimeClass object.", "zh": "### 2. 创建相应的 RuntimeClass 资源\n\n在上面步骤 1 中，每个配置都需要有一个用于标识配置的 `handler`。\n针对每个 handler 需要创建一个 RuntimeClass 对象。"}
{"en": "The RuntimeClass resource currently only has 2 significant fields: the RuntimeClass name\n(`metadata.name`) and the handler (`handler`). The object definition looks like this:", "zh": "RuntimeClass 资源当前只有两个重要的字段：RuntimeClass 名 (`metadata.name`) 和 handler (`handler`)。\n对象定义如下所示：\n\n```yaml\n# RuntimeClass 定义于 node.k8s.io API 组\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  # 用来引用 RuntimeClass 的名字\n  # RuntimeClass 是一个集群层面的资源\n  name: myclass\n# 对应的 CRI 配置的名称\nhandler: myconfiguration\n```"}
{"en": "The name of a RuntimeClass object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).", "zh": "RuntimeClass 对象的名称必须是有效的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。\n\n{{< note >}}"}
{"en": "It is recommended that RuntimeClass write operations (create/update/patch/delete) be\nrestricted to the cluster administrator. This is typically the default. See\n[Authorization Overview](/docs/reference/access-authn-authz/authorization/) for more details.", "zh": "建议将 RuntimeClass 写操作（create、update、patch 和 delete）限定于集群管理员使用。\n通常这是默认配置。参阅[授权概述](/zh-cn/docs/reference/access-authn-authz/authorization/)了解更多信息。\n{{< /note >}}"}
{"en": "## Usage\n\nOnce RuntimeClasses are configured for the cluster, you can specify a\n`runtimeClassName` in the Pod spec to use it. For example:", "zh": "## 使用说明  {#usage}\n\n一旦完成集群中 RuntimeClasses 的配置，\n你可以在 Pod spec 中指定 `runtimeClassName` 来使用它。例如:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  runtimeClassName: myclass\n  # ...\n```"}
{"en": "This will instruct the kubelet to use the named RuntimeClass to run this pod. If the named\nRuntimeClass does not exist, or the CRI cannot run the corresponding handler, the pod will enter the\n`Failed` terminal [phase](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase). Look for a\ncorresponding [event](/docs/tasks/debug/debug-application/debug-running-pod/) for an\nerror message.", "zh": "这一设置会告诉 kubelet 使用所指的 RuntimeClass 来运行该 Pod。\n如果所指的 RuntimeClass 不存在或者 CRI 无法运行相应的 handler，\n那么 Pod 将会进入 `Failed` 终止[阶段](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase)。\n你可以查看相应的[事件](/zh-cn/docs/tasks/debug/debug-application/debug-running-pod/)，\n获取执行过程中的错误信息。"}
{"en": "If no `runtimeClassName` is specified, the default RuntimeHandler will be used, which is equivalent\nto the behavior when the RuntimeClass feature is disabled.", "zh": "如果未指定 `runtimeClassName`，则将使用默认的 RuntimeHandler，相当于禁用 RuntimeClass 功能特性。"}
{"en": "### CRI Configuration\n\nFor more details on setting up CRI runtimes, see [CRI installation](/docs/setup/production-environment/container-runtimes/).", "zh": "### CRI 配置   {#cri-configuration}\n\n关于如何安装 CRI 运行时，请查阅\n[CRI 安装](/zh-cn/docs/setup/production-environment/container-runtimes/)。\n\n#### {{< glossary_tooltip term_id=\"containerd\" >}}"}
{"en": "Runtime handlers are configured through containerd's configuration at\n`/etc/containerd/config.toml`. Valid handlers are configured under the runtimes section:", "zh": "通过 containerd 的 `/etc/containerd/config.toml` 配置文件来配置运行时 handler。\nhandler 需要配置在 runtimes 块中：\n\n```\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.${HANDLER_NAME}]\n```"}
{"en": "See containerd's [config documentation](https://github.com/containerd/containerd/blob/main/docs/cri/config.md)\nfor more details:", "zh": "更详细信息，请查阅 containerd 的[配置指南](https://github.com/containerd/containerd/blob/main/docs/cri/config.md)\n\n#### {{< glossary_tooltip term_id=\"cri-o\" >}}"}
{"en": "Runtime handlers are configured through CRI-O's configuration at `/etc/crio/crio.conf`. Valid\nhandlers are configured under the\n[crio.runtime table](https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table):", "zh": "通过 CRI-O 的 `/etc/crio/crio.conf` 配置文件来配置运行时 handler。\nhandler 需要配置在\n[crio.runtime 表](https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table)之下：\n\n```\n[crio.runtime.runtimes.${HANDLER_NAME}]\n  runtime_path = \"${PATH_TO_BINARY}\"\n```"}
{"en": "See CRI-O's [config documentation](https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md) for more details.", "zh": "更详细信息，请查阅 CRI-O [配置文档](https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md)。"}
{"en": "## Scheduling", "zh": "## 调度  {#scheduling}\n\n{{< feature-state for_k8s_version=\"v1.16\" state=\"beta\" >}}"}
{"en": "By specifying the `scheduling` field for a RuntimeClass, you can set constraints to\nensure that Pods running with this RuntimeClass are scheduled to nodes that support it.\nIf `scheduling` is not set, this RuntimeClass is assumed to be supported by all nodes.", "zh": "通过为 RuntimeClass 指定 `scheduling` 字段，\n你可以通过设置约束，确保运行该 RuntimeClass 的 Pod 被调度到支持该 RuntimeClass 的节点上。\n如果未设置 `scheduling`，则假定所有节点均支持此 RuntimeClass。"}
{"en": "To ensure pods land on nodes supporting a specific RuntimeClass, that set of nodes should have a\ncommon label which is then selected by the `runtimeclass.scheduling.nodeSelector` field. The\nRuntimeClass's nodeSelector is merged with the pod's nodeSelector in admission, effectively taking\nthe intersection of the set of nodes selected by each. If there is a conflict, the pod will be\nrejected.", "zh": "为了确保 pod 会被调度到支持指定运行时的 node 上，每个 node 需要设置一个通用的 label 用于被\n`runtimeclass.scheduling.nodeSelector` 挑选。在 admission 阶段，RuntimeClass 的 nodeSelector 将会与\nPod 的 nodeSelector 合并，取二者的交集。如果有冲突，Pod 将会被拒绝。"}
{"en": "If the supported nodes are tainted to prevent other RuntimeClass pods from running on the node, you\ncan add `tolerations` to the RuntimeClass. As with the `nodeSelector`, the tolerations are merged\nwith the pod's tolerations in admission, effectively taking the union of the set of nodes tolerated\nby each.", "zh": "如果 node 需要阻止某些需要特定 RuntimeClass 的 Pod，可以在 `tolerations` 中指定。\n与 `nodeSelector` 一样，tolerations 也在 admission 阶段与 Pod 的 tolerations 合并，取二者的并集。"}
{"en": "To learn more about configuring the node selector and tolerations, see\n[Assigning Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/).", "zh": "更多有关 node selector 和 tolerations 的配置信息，请查阅\n[将 Pod 分派到节点](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/)。"}
{"en": "### Pod Overhead", "zh": "### Pod 开销   {#pod-overhead}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "You can specify _overhead_ resources that are associated with running a Pod. Declaring overhead allows\nthe cluster (including the scheduler) to account for it when making decisions about Pods and resources.", "zh": "你可以指定与运行 Pod 相关的**开销**资源。声明开销即允许集群（包括调度器）在决策 Pod 和资源时将其考虑在内。"}
{"en": "Pod overhead is defined in RuntimeClass through the `overhead` field. Through the use of this field,\nyou can specify the overhead of running pods utilizing this RuntimeClass and ensure these overheads\nare accounted for in Kubernetes.", "zh": "Pod 开销通过 RuntimeClass 的 `overhead` 字段定义。\n通过使用这个字段，你可以指定使用该 RuntimeClass 运行 Pod 时的开销并确保 Kubernetes 将这些开销计算在内。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- [RuntimeClass Design](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md)\n- [RuntimeClass Scheduling Design](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md#runtimeclass-scheduling)\n- Read about the [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/) concept\n- [PodOverhead Feature Design](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead)", "zh": "- [RuntimeClass 设计](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md)\n- [RuntimeClass 调度设计](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md#runtimeclass-scheduling)\n- 阅读关于 [Pod 开销](/zh-cn/docs/concepts/scheduling-eviction/pod-overhead/)的概念\n- [PodOverhead 特性设计](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead)"}
{"en": "This page describes the resources available to Containers in the Container environment.", "zh": "本页描述了在容器环境里容器可用的资源。"}
{"en": "## Container environment\n\nThe Kubernetes Container environment provides several important resources to Containers:\n\n* A filesystem, which is a combination of an [image](/docs/concepts/containers/images/) and one or more [volumes](/docs/concepts/storage/volumes/).\n* Information about the Container itself.\n* Information about other objects in the cluster.", "zh": "## 容器环境  {#container-environment}\n\nKubernetes 的容器环境给容器提供了几个重要的资源：\n\n* 文件系统，其中包含一个[镜像](/zh-cn/docs/concepts/containers/images/)\n  和一个或多个的[卷](/zh-cn/docs/concepts/storage/volumes/)\n* 容器自身的信息\n* 集群中其他对象的信息"}
{"en": "### Container information\n\nThe *hostname* of a Container is the name of the Pod in which the Container is running.\nIt is available through the `hostname` command or the\n[`gethostname`](https://man7.org/linux/man-pages/man2/gethostname.2.html)\nfunction call in libc.\n\nThe Pod name and namespace are available as environment variables through the\n[downward API](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/).\n\nUser defined environment variables from the Pod definition are also available to the Container,\nas are any environment variables specified statically in the container image.", "zh": "### 容器信息\n\n一个容器的 **hostname** 是该容器运行所在的 Pod 的名称。通过 `hostname` 命令或者调用 libc 中的\n[`gethostname`](https://man7.org/linux/man-pages/man2/gethostname.2.html) 函数可以获取该名称。\n\nPod 名称和命名空间可以通过\n[下行 API](/zh-cn/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)\n转换为环境变量。\n\nPod 定义中的用户所定义的环境变量也可在容器中使用，就像在 container 镜像中静态指定的任何环境变量一样。"}
{"en": "### Cluster information\n\nA list of all services that were running when a Container was created is available to that Container as environment variables.\nThis list is limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services.\n\nFor a service named *foo* that maps to a Container named *bar*,\nthe following variables are defined:", "zh": "### 集群信息\n\n创建容器时正在运行的所有服务都可用作该容器的环境变量。\n这里的服务仅限于新容器的 Pod 所在的名字空间中的服务，以及 Kubernetes 控制面的服务。\n\n对于名为 **foo** 的服务，当映射到名为 **bar** 的容器时，定义了以下变量：\n\n```shell\nFOO_SERVICE_HOST=<其上服务正运行的主机>\nFOO_SERVICE_PORT=<其上服务正运行的端口>\n```"}
{"en": "Services have dedicated IP addresses and are available to the Container via DNS,\nif [DNS addon](https://releases.k8s.io/v{{< skew currentPatchVersion >}}/cluster/addons/dns/) is enabled.", "zh": "服务具有专用的 IP 地址。如果启用了\n[DNS 插件](https://releases.k8s.io/v{{< skew currentPatchVersion >}}/cluster/addons/dns/)，\n可以在容器中通过 DNS 来访问服务。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn more about [Container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/).\n* Get hands-on experience\n  [attaching handlers to Container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).", "zh": "* 学习更多有关[容器生命周期回调](/zh-cn/docs/concepts/containers/container-lifecycle-hooks/)的知识。\n* 动手[为容器的生命周期事件设置处理函数](/zh-cn/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/)。"}
{"en": "This page describes how kubelet managed Containers can use the Container lifecycle hook framework\nto run code triggered by events during their management lifecycle.", "zh": "这个页面描述了 kubelet 管理的容器如何使用容器生命周期回调框架，\n藉由其管理生命周期中的事件触发，运行指定代码。"}
{"en": "## Overview\n\nAnalogous to many programming language frameworks that have component lifecycle hooks, such as Angular,\nKubernetes provides Containers with lifecycle hooks.\nThe hooks enable Containers to be aware of events in their management lifecycle\nand run code implemented in a handler when the corresponding lifecycle hook is executed.", "zh": "## 概述   {#overview}\n\n类似于许多具有生命周期回调组件的编程语言框架，例如 Angular、Kubernetes 为容器提供了生命周期回调。\n回调使容器能够了解其管理生命周期中的事件，并在执行相应的生命周期回调时运行在处理程序中实现的代码。"}
{"en": "## Container hooks\n\nThere are two hooks that are exposed to Containers:", "zh": "## 容器回调 {#container-hooks}\n\n有两个回调暴露给容器：\n\n`PostStart`"}
{"en": "This hook is executed immediately after a container is created.\nHowever, there is no guarantee that the hook will execute before the container ENTRYPOINT.\nNo parameters are passed to the handler.", "zh": "这个回调在容器被创建之后立即被执行。\n但是，不能保证回调会在容器入口点（ENTRYPOINT）之前执行。\n没有参数传递给处理程序。\n\n`PreStop`"}
{"en": "This hook is called immediately before a container is terminated due to an API request or management\nevent such as a liveness/startup probe failure, preemption, resource contention and others. A call\nto the `PreStop` hook fails if the container is already in a terminated or completed state and the\nhook must complete before the TERM signal to stop the container can be sent. The Pod's termination\ngrace period countdown begins before the `PreStop` hook is executed, so regardless of the outcome of\nthe handler, the container will eventually terminate within the Pod's termination grace period. No\nparameters are passed to the handler.", "zh": "在容器因 API 请求或者管理事件（诸如存活态探针、启动探针失败、资源抢占、资源竞争等）\n而被终止之前，此回调会被调用。\n如果容器已经处于已终止或者已完成状态，则对 preStop 回调的调用将失败。\n在用来停止容器的 TERM 信号被发出之前，回调必须执行结束。\nPod 的终止宽限周期在 `PreStop` 回调被执行之前即开始计数，\n所以无论回调函数的执行结果如何，容器最终都会在 Pod 的终止宽限期内被终止。\n没有参数会被传递给处理程序。"}
{"en": "A more detailed description of the termination behavior can be found in\n[Termination of Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination).", "zh": "有关终止行为的更详细描述，\n请参见[终止 Pod](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)。"}
{"en": "### Hook handler implementations\n\nContainers can access a hook by implementing and registering a handler for that hook.\nThere are three types of hook handlers that can be implemented for Containers:", "zh": "### 回调处理程序的实现   {#hook-handler-implementations}\n\n容器可以通过实现和注册该回调的处理程序来访问该回调。\n针对容器，有三种类型的回调处理程序可供实现："}
{"en": "* Exec - Executes a specific command, such as `pre-stop.sh`, inside the cgroups and namespaces of the Container.\nResources consumed by the command are counted against the Container.\n* HTTP - Executes an HTTP request against a specific endpoint on the Container.\n* Sleep - Pauses the container for a specified duration.\n  This is a beta-level feature default enabled by the `PodLifecycleSleepAction` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/).", "zh": "* Exec - 在容器的 cgroups 和名字空间中执行特定的命令（例如 `pre-stop.sh`）。\n  命令所消耗的资源计入容器的资源消耗。\n* HTTP - 对容器上的特定端点执行 HTTP 请求。\n* Sleep - 将容器暂停一段指定的时间。\n  这是由 `PodLifecycleSleepAction`\n  [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)默认启用的 Beta 级特性。"}
{"en": "### Hook handler execution\n\nWhen a Container lifecycle management hook is called,\nthe Kubernetes management system executes the handler according to the hook action,\n`httpGet` , `tcpSocket` and `sleep` are executed by the kubelet process, and `exec` is executed in the container.", "zh": "### 回调处理程序执行   {#hook-handler-execution}\n\n当调用容器生命周期管理回调时，Kubernetes 管理系统根据回调动作执行其处理程序，\n`httpGet`、`tcpSocket` 和 `sleep` 由 kubelet 进程执行，而 `exec` 在容器中执行。"}
{"en": "The `PostStart` hook handler call is initiated when a container is created,\nmeaning the container ENTRYPOINT and the `PostStart` hook are triggered simultaneously. \nHowever, if the `PostStart` hook takes too long to execute or if it hangs,\nit can prevent the container from transitioning to a `running` state.", "zh": "当容器创建时，会调用 `PostStart` 回调程序，\n这意味着容器的 ENTRYPOINT 和 `PostStart` 回调会同时触发。然而，\n如果 `PostStart` 回调程序执行时间过长或挂起，它可能会阻止容器进入 `running` 状态。"}
{"en": "`PreStop` hooks are not executed asynchronously from the signal\nto stop the Container; the hook must complete its execution before\nthe TERM signal can be sent.\nIf a `PreStop` hook hangs during execution,\nthe Pod's phase will be `Terminating` and remain there until the Pod is\nkilled after its `terminationGracePeriodSeconds` expires.\nThis grace period applies to the total time it takes for both\nthe `PreStop` hook to execute and for the Container to stop normally.\nIf, for example, `terminationGracePeriodSeconds` is 60, and the hook\ntakes 55 seconds to complete, and the Container takes 10 seconds to stop\nnormally after receiving the signal, then the Container will be killed\nbefore it can stop normally, since `terminationGracePeriodSeconds` is\nless than the total time (55+10) it takes for these two things to happen.", "zh": "`PreStop` 回调并不会与停止容器的信号处理程序异步执行；回调必须在可以发送信号之前完成执行。\n如果 `PreStop` 回调在执行期间停滞不前，Pod 的阶段会变成 `Terminating`并且一直处于该状态，\n直到其 `terminationGracePeriodSeconds` 耗尽为止，这时 Pod 会被杀死。\n这一宽限期是针对 `PreStop` 回调的执行时间及容器正常停止时间的总和而言的。\n例如，如果 `terminationGracePeriodSeconds` 是 60，回调函数花了 55 秒钟完成执行，\n而容器在收到信号之后花了 10 秒钟来正常结束，那么容器会在其能够正常结束之前即被杀死，\n因为 `terminationGracePeriodSeconds` 的值小于后面两件事情所花费的总时间（55+10）。"}
{"en": "If either a `PostStart` or `PreStop` hook fails,\nit kills the Container.", "zh": "如果 `PostStart` 或 `PreStop` 回调失败，它会杀死容器。"}
{"en": "Users should make their hook handlers as lightweight as possible.\nThere are cases, however, when long running commands make sense,\nsuch as when saving state prior to stopping a Container.", "zh": "用户应该使他们的回调处理程序尽可能的轻量级。\n但也需要考虑长时间运行的命令也很有用的情况，比如在停止容器之前保存状态。"}
{"en": "### Hook delivery guarantees\n\nHook delivery is intended to be *at least once*,\nwhich means that a hook may be called multiple times for any given event,\nsuch as for `PostStart` or `PreStop`.\nIt is up to the hook implementation to handle this correctly.", "zh": "### 回调递送保证   {#hook-delivery-guarantees}\n\n回调的递送应该是**至少一次**，这意味着对于任何给定的事件，\n例如 `PostStart` 或 `PreStop`，回调可以被调用多次。\n如何正确处理被多次调用的情况，是回调实现所要考虑的问题。"}
{"en": "Generally, only single deliveries are made.\nIf, for example, an HTTP hook receiver is down and is unable to take traffic,\nthere is no attempt to resend.\nIn some rare cases, however, double delivery may occur.\nFor instance, if a kubelet restarts in the middle of sending a hook,\nthe hook might be resent after the kubelet comes back up.", "zh": "通常情况下，只会进行单次递送。\n例如，如果 HTTP 回调接收器宕机，无法接收流量，则不会尝试重新发送。\n然而，偶尔也会发生重复递送的可能。\n例如，如果 kubelet 在发送回调的过程中重新启动，回调可能会在 kubelet 恢复后重新发送。"}
{"en": "### Debugging Hook handlers\n\nThe logs for a Hook handler are not exposed in Pod events.\nIf a handler fails for some reason, it broadcasts an event.\nFor `PostStart`, this is the `FailedPostStartHook` event,\nand for `PreStop`, this is the `FailedPreStopHook` event.\nTo generate a failed `FailedPostStartHook` event yourself, modify the [lifecycle-events.yaml](https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/lifecycle-events.yaml) file to change the postStart command to \"badcommand\" and apply it.\nHere is some example output of the resulting events you see from running `kubectl describe pod lifecycle-demo`:", "zh": "### 调试回调处理程序   {#debugging-hook-handlers}\n\n回调处理程序的日志不会在 Pod 事件中公开。\n如果处理程序由于某种原因失败，它将播放一个事件。\n对于 `PostStart`，这是 `FailedPostStartHook` 事件，对于 `PreStop`，这是 `FailedPreStopHook` 事件。\n要自己生成失败的 `FailedPostStartHook` 事件，请修改\n[lifecycle-events.yaml](https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/lifecycle-events.yaml)\n文件将 postStart 命令更改为 “badcommand” 并应用它。\n以下是通过运行 `kubectl describe pod lifecycle-demo` 后你看到的一些结果事件的示例输出：\n\n```\nEvents:\n  Type     Reason               Age              From               Message\n  ----     ------               ----             ----               -------\n  Normal   Scheduled            7s               default-scheduler  Successfully assigned default/lifecycle-demo to ip-XXX-XXX-XX-XX.us-east-2...\n  Normal   Pulled               6s               kubelet            Successfully pulled image \"nginx\" in 229.604315ms\n  Normal   Pulling              4s (x2 over 6s)  kubelet            Pulling image \"nginx\"\n  Normal   Created              4s (x2 over 5s)  kubelet            Created container lifecycle-demo-container\n  Normal   Started              4s (x2 over 5s)  kubelet            Started container lifecycle-demo-container\n  Warning  FailedPostStartHook  4s (x2 over 5s)  kubelet            Exec lifecycle hook ([badcommand]) for Container \"lifecycle-demo-container\" in Pod \"lifecycle-demo_default(30229739-9651-4e5a-9a32-a8f1688862db)\" failed - error: command 'badcommand' exited with 126: , message: \"OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \\\"badcommand\\\": executable file not found in $PATH: unknown\\r\\n\"\n  Normal   Killing              4s (x2 over 5s)  kubelet            FailedPostStartHook\n  Normal   Pulled               4s               kubelet            Successfully pulled image \"nginx\" in 215.66395ms\n  Warning  BackOff              2s (x2 over 3s)  kubelet            Back-off restarting failed container\n```\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn more about the [Container environment](/docs/concepts/containers/container-environment/).\n* Get hands-on experience\n  [attaching handlers to Container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).", "zh": "* 进一步了解[容器环境](/zh-cn/docs/concepts/containers/container-environment/)。\n* 动手[为容器的生命周期事件设置处理函数](/zh-cn/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/)。"}
{"en": "This page will discuss containers and container images, as well as their use in operations and solution development.\n\nThe word _container_ is an overloaded term. Whenever you use the word, check whether your audience uses the same definition.\n\nEach container that you run is repeatable; the standardization from having\ndependencies included means that you get the same behavior wherever you\nrun it.\n\nContainers decouple applications from the underlying host infrastructure.\nThis makes deployment easier in different cloud or OS environments.\n\nEach {{< glossary_tooltip text=\"node\" term_id=\"node\" >}} in a Kubernetes\ncluster runs the containers that form the\n[Pods](/docs/concepts/workloads/pods/) assigned to that node.\nContainers in a Pod are co-located and co-scheduled to run on the same node.", "zh": "本页将讨论容器和容器镜像，以及它们在运维和解决方案开发中的应用。\n\n**容器**是一个多义词。每当你使用这个词时，请确认你的受众是否使用相同的定义。\n\n每个运行的容器都是可重复的；\n包含依赖环境在内的标准，意味着无论你在哪里运行它都会得到相同的行为。\n\n容器将应用程序从底层的主机设施中解耦。\n这使得在不同的云或 OS 环境中部署更加容易。\n\nKubernetes 集群中的每个{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}都会运行容器，\n这些容器构成分配给该节点的 [Pod](/zh-cn/docs/concepts/workloads/pods/)。\n单个 Pod 中的容器会在共同调度下，于同一位置运行在相同的节点上。"}
{"en": "## Container images\nA [container image](/docs/concepts/containers/images/) is a ready-to-run\nsoftware package containing everything needed to run an application:\nthe code and any runtime it requires, application and system libraries,\nand default values for any essential settings.\n\nContainers are intended to be stateless and\n[immutable](https://glossary.cncf.io/immutable-infrastructure/):\nyou should not change\nthe code of a container that is already running. If you have a containerized\napplication and want to make changes, the correct process is to build a new\nimage that includes the change, then recreate the container to start from the\nupdated image.", "zh": "## 容器镜像 {#container-images}\n[容器镜像](/zh-cn/docs/concepts/containers/images/)是一个随时可以运行的软件包，\n包含运行应用程序所需的一切：代码和它需要的所有运行时、应用程序和系统库，以及一些基本设置的默认值。\n\n容器旨在设计成无状态且[不可变的](https://glossary.cncf.io/immutable-infrastructure/)：\n你不应更改已经运行的容器的代码。如果有一个容器化的应用程序需要修改，\n正确的流程是：先构建包含更改的新镜像，再基于新构建的镜像重新运行容器。"}
{"en": "## Container runtimes", "zh": "## 容器运行时  {#container-runtimes}\n\n{{< glossary_definition term_id=\"container-runtime\" length=\"all\" >}}"}
{"en": "Usually, you can allow your cluster to pick the default container runtime\nfor a Pod. If you need to use more than one container runtime in your cluster,\nyou can specify the [RuntimeClass](/docs/concepts/containers/runtime-class/)\nfor a Pod to make sure that Kubernetes runs those containers using a\nparticular container runtime.\n\nYou can also use RuntimeClass to run different Pods with the same container\nruntime but with different settings.", "zh": "通常，你可以允许集群为一个 Pod 选择其默认的容器运行时。如果你需要在集群中使用多个容器运行时，\n你可以为一个 Pod 指定 [RuntimeClass](/zh-cn/docs/concepts/containers/runtime-class/)，\n以确保 Kubernetes 会使用特定的容器运行时来运行这些容器。\n\n你还可以通过 RuntimeClass，使用相同的容器运行时，但使用不同设定的配置来运行不同的 Pod。"}
{"en": "Windows applications constitute a large portion of the services and applications that\nrun in many organizations. [Windows containers](https://aka.ms/windowscontainers)\nprovide a way to encapsulate processes and package dependencies, making it easier\nto use DevOps practices and follow cloud native patterns for Windows applications.\n\nOrganizations with investments in Windows-based applications and Linux-based\napplications don't have to look for separate orchestrators to manage their workloads,\nleading to increased operational efficiencies across their deployments, regardless\nof operating system.", "zh": "在许多组织中，所运行的很大一部分服务和应用是 Windows 应用。\n[Windows 容器](https://aka.ms/windowscontainers)提供了一种封装进程和包依赖项的方式，\n从而简化了 DevOps 实践，令 Windows 应用同样遵从云原生模式。\n\n对于同时投入基于 Windows 应用和 Linux 应用的组织而言，他们不必寻找不同的编排系统来管理其工作负载，\n使其跨部署的运营效率得以大幅提升，而不必关心所用的操作系统。"}
{"en": "## Windows nodes in Kubernetes\n\nTo enable the orchestration of Windows containers in Kubernetes, include Windows nodes\nin your existing Linux cluster. Scheduling Windows containers in\n{{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} on Kubernetes is similar to\nscheduling Linux-based containers.\n\nIn order to run Windows containers, your Kubernetes cluster must include\nmultiple operating systems.\nWhile you can only run the {{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} on Linux,\nyou can deploy worker nodes running either Windows or Linux.", "zh": "## Kubernetes 中的 Windows 节点   {#windows-nodes-in-k8s}\n\n若要在 Kubernetes 中启用对 Windows 容器的编排，可以在现有的 Linux 集群中包含 Windows 节点。\n在 Kubernetes 上调度 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 中的 Windows 容器与调度基于 Linux 的容器类似。\n\n为了运行 Windows 容器，你的 Kubernetes 集群必须包含多个操作系统。\n尽管你只能在 Linux 上运行{{< glossary_tooltip text=\"控制平面\" term_id=\"control-plane\" >}}，\n你可以部署运行 Windows 或 Linux 的工作节点。"}
{"en": "Windows {{< glossary_tooltip text=\"nodes\" term_id=\"node\" >}} are\n[supported](#windows-os-version-support) provided that the operating system is\nWindows Server 2019 or Windows Server 2022.\n\nThis document uses the term *Windows containers* to mean Windows containers with\nprocess isolation. Kubernetes does not support running Windows containers with\n[Hyper-V isolation](https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container).", "zh": "支持 Windows {{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}的前提是操作系统为\nWindows Server 2019 或 Windows Server 2022。\n\n本文使用术语 **Windows 容器**表示具有进程隔离能力的 Windows 容器。\nKubernetes 不支持使用\n[Hyper-V 隔离能力](https://docs.microsoft.com/zh-cn/virtualization/windowscontainers/manage-containers/hyperv-container)来运行\nWindows 容器。"}
{"en": "## Compatibility and limitations {#limitations}\n\nSome node features are only available if you use a specific\n[container runtime](#container-runtime); others are not available on Windows nodes,\nincluding:\n\n* HugePages: not supported for Windows containers\n* Privileged containers: not supported for Windows containers.\n  [HostProcess Containers](/docs/tasks/configure-pod-container/create-hostprocess-pod/) offer similar functionality.\n* TerminationGracePeriod: requires containerD", "zh": "## 兼容性与局限性   {#limitations}\n\n某些节点层面的功能特性仅在使用特定[容器运行时](#container-runtime)时才可用；\n另外一些特性则在 Windows 节点上不可用，包括：\n\n* 巨页（HugePages）：Windows 容器当前不支持。\n* 特权容器：Windows 容器当前不支持。\n  [HostProcess 容器](/zh-cn/docs/tasks/configure-pod-container/create-hostprocess-pod/)提供类似功能。\n* TerminationGracePeriod：需要 containerD。"}
{"en": "Not all features of shared namespaces are supported. See [API compatibility](#api)\nfor more details.\n\nSee [Windows OS version compatibility](#windows-os-version-support) for details on\nthe Windows versions that Kubernetes is tested against.\n\nFrom an API and kubectl perspective, Windows containers behave in much the same\nway as Linux-based containers. However, there are some notable differences in key\nfunctionality which are outlined in this section.", "zh": "Windows 节点并不支持共享命名空间的所有功能特性。\n有关更多详细信息，请参考 [API 兼容性](#api)。\n\n有关 Kubernetes 测试时所使用的 Windows 版本的详细信息，请参考\n[Windows 操作系统版本兼容性](#windows-os-version-support)。\n\n从 API 和 kubectl 的角度来看，Windows 容器的行为与基于 Linux 的容器非常相似。\n然而，在本节所概述的一些关键功能上，二者存在一些显著差异。"}
{"en": "### Comparison with Linux {#compatibility-linux-similarities}\n\nKey Kubernetes elements work the same way in Windows as they do in Linux. This\nsection refers to several key workload abstractions and how they map to Windows.", "zh": "### 与 Linux 比较   {#comparison-with-Linux-similarities}\n\nKubernetes 关键组件在 Windows 上的工作方式与在 Linux 上相同。\n本节介绍几个关键的工作负载抽象及其如何映射到 Windows。"}
{"en": "* [Pods](/docs/concepts/workloads/pods/)\n\n  A Pod is the basic building block of Kubernetes–the smallest and simplest unit in\n  the Kubernetes object model that you create or deploy. You may not deploy Windows and\n  Linux containers in the same Pod. All containers in a Pod are scheduled onto a single\n  Node where each Node represents a specific platform and architecture. The following\n  Pod capabilities, properties and events are supported with Windows containers:", "zh": "* [Pod](/zh-cn/docs/concepts/workloads/pods/)\n\n  Pod 是 Kubernetes 的基本构建块，是可以创建或部署的最小和最简单的单元。\n  你不可以在同一个 Pod 中部署 Windows 和 Linux 容器。\n  Pod 中的所有容器都调度到同一 Node 上，每个 Node 代表一个特定的平台和体系结构。\n  Windows 容器支持以下 Pod 能力、属性和事件："}
{"en": "* Single or multiple containers per Pod with process isolation and volume sharing\n  * Pod `status` fields\n  * Readiness, liveness, and startup probes\n  * postStart & preStop container lifecycle hooks\n  * ConfigMap, Secrets: as environment variables or volumes\n  * `emptyDir` volumes\n  * Named pipe host mounts\n  * Resource limits", "zh": "* 每个 Pod 有一个或多个容器，具有进程隔离和卷共享能力\n  * Pod `status` 字段\n  * 就绪、存活和启动探针\n  * postStart 和 preStop 容器生命周期回调\n  * ConfigMap 和 Secret：作为环境变量或卷\n  * `emptyDir` 卷\n  * 命名管道形式的主机挂载\n  * 资源限制"}
{"en": "* OS field:\n\n    The `.spec.os.name` field should be set to `windows` to indicate that the current Pod uses Windows containers.", "zh": "* 操作系统字段：\n\n    `.spec.os.name` 字段应设置为 `windows` 以表明当前 Pod 使用 Windows 容器。"}
{"en": "If you set the `.spec.os.name` field to `windows`,\n    you must not set the following fields in the `.spec` of that Pod:", "zh": "如果你将 `.spec.os.name` 字段设置为 `windows`，\n    则你必须不能在对应 Pod 的 `.spec` 中设置以下字段：\n\n    * `spec.hostPID`\n    * `spec.hostIPC`\n    * `spec.securityContext.seLinuxOptions`\n    * `spec.securityContext.seccompProfile`\n    * `spec.securityContext.fsGroup`\n    * `spec.securityContext.fsGroupChangePolicy`\n    * `spec.securityContext.sysctls`\n    * `spec.shareProcessNamespace`\n    * `spec.securityContext.runAsUser`\n    * `spec.securityContext.runAsGroup`\n    * `spec.securityContext.supplementalGroups`\n    * `spec.containers[*].securityContext.seLinuxOptions`\n    * `spec.containers[*].securityContext.seccompProfile`\n    * `spec.containers[*].securityContext.capabilities`\n    * `spec.containers[*].securityContext.readOnlyRootFilesystem`\n    * `spec.containers[*].securityContext.privileged`\n    * `spec.containers[*].securityContext.allowPrivilegeEscalation`\n    * `spec.containers[*].securityContext.procMount`\n    * `spec.containers[*].securityContext.runAsUser`\n    * `spec.containers[*].securityContext.runAsGroup`"}
{"en": "In the above list, wildcards (`*`) indicate all elements in a list.\n    For example, `spec.containers[*].securityContext` refers to the SecurityContext object\n    for all containers. If any of these fields is specified, the Pod will\n    not be admitted by the API server.", "zh": "在上述列表中，通配符（`*`）表示列表中的所有项。\n    例如，`spec.containers[*].securityContext` 指代所有容器的 SecurityContext 对象。\n    如果指定了这些字段中的任意一个，则 API 服务器不会接受此 Pod。"}
{"en": "* [Workload resources](/docs/concepts/workloads/controllers/) including:\n  * ReplicaSet\n  * Deployment\n  * StatefulSet\n  * DaemonSet\n  * Job\n  * CronJob\n  * ReplicationController\n* {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}}\n  See [Load balancing and Services](/docs/concepts/services-networking/windows-networking/#load-balancing-and-services) for more details.", "zh": "* [工作负载资源](/zh-cn/docs/concepts/workloads/controllers/)包括：\n  \n  * ReplicaSet\n  * Deployment\n  * StatefulSet\n  * DaemonSet\n  * Job\n  * CronJob\n  * ReplicationController\n\n* {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}}\n\n  有关更多详细信息，请参考[负载均衡和 Service](/zh-cn/docs/concepts/services-networking/windows-networking/#load-balancing-and-services)。"}
{"en": "Pods, workload resources, and Services are critical elements to managing Windows\nworkloads on Kubernetes. However, on their own they are not enough to enable\nthe proper lifecycle management of Windows workloads in a dynamic cloud native\nenvironment.\n\n* `kubectl exec`\n* Pod and container metrics\n* {{< glossary_tooltip text=\"Horizontal pod autoscaling\" term_id=\"horizontal-pod-autoscaler\" >}}\n* {{< glossary_tooltip text=\"Resource quotas\" term_id=\"resource-quota\" >}}\n* Scheduler preemption", "zh": "Pod、工作负载资源和 Service 是在 Kubernetes 上管理 Windows 工作负载的关键元素。\n然而，它们本身还不足以在动态的云原生环境中对 Windows 工作负载进行恰当的生命周期管理。\n\n* `kubectl exec`\n* Pod 和容器度量指标\n* {{< glossary_tooltip text=\"Pod 水平自动扩缩容\" term_id=\"horizontal-pod-autoscaler\" >}}\n* {{< glossary_tooltip text=\"资源配额\" term_id=\"resource-quota\" >}}\n* 调度器抢占"}
{"en": "### Command line options for the kubelet {#kubelet-compatibility}\n\nSome kubelet command line options behave differently on Windows, as described below:", "zh": "### kubelet 的命令行选项   {#kubelet-compatibility}\n\n某些 kubelet 命令行选项在 Windows 上的行为不同，如下所述："}
{"en": "* The `--windows-priorityclass` lets you set the scheduling priority of the kubelet process\n  (see [CPU resource management](/docs/concepts/configuration/windows-resource-management/#resource-management-cpu))\n* The `--kube-reserved`, `--system-reserved` , and `--eviction-hard` flags update\n  [NodeAllocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\n* Eviction by using `--enforce-node-allocable` is not implemented\n* When running on a Windows node the kubelet does not have memory or CPU\n  restrictions. `--kube-reserved` and `--system-reserved` only subtract from `NodeAllocatable`\n  and do not guarantee resource provided for workloads.\n  See [Resource Management for Windows nodes](/docs/concepts/configuration/windows-resource-management/#resource-reservation)\n  for more information.\n* The `PIDPressure` Condition is not implemented\n* The kubelet does not take OOM eviction actions", "zh": "* `--windows-priorityclass` 允许你设置 kubelet 进程的调度优先级\n  （参考 [CPU 资源管理](/zh-cn/docs/concepts/configuration/windows-resource-management/#resource-management-cpu)）。\n* `--kube-reserved`、`--system-reserved` 和 `--eviction-hard` 标志更新\n  [NodeAllocatable](/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)。\n* 未实现使用 `--enforce-node-allocable` 驱逐。\n* 在 Windows 节点上运行时，kubelet 没有内存或 CPU 限制。\n  `--kube-reserved` 和 `--system-reserved` 仅从 `NodeAllocatable` 中减去，并且不保证为工作负载提供的资源。\n  有关更多信息，请参考 [Windows 节点的资源管理](/zh-cn/docs/concepts/configuration/windows-resource-management/#resource-reservation)。\n* 未实现 `PIDPressure` 条件。\n* kubelet 不会执行 OOM 驱逐操作。"}
{"en": "### API compatibility {#api}\n\nThere are subtle differences in the way the Kubernetes APIs work for Windows due to the OS\nand container runtime. Some workload properties were designed for Linux, and fail to run on Windows.\n\nAt a high level, these OS concepts are different:", "zh": "### API 兼容性   {#api}\n\n由于操作系统和容器运行时的缘故，Kubernetes API 在 Windows 上的工作方式存在细微差异。\n某些工作负载属性是为 Linux 设计的，无法在 Windows 上运行。\n\n从较高的层面来看，以下操作系统概念是不同的："}
{"en": "* Identity - Linux uses userID (UID) and groupID (GID) which\n  are represented as integer types. User and group names\n  are not canonical - they are just an alias in `/etc/groups`\n  or `/etc/passwd` back to UID+GID. Windows uses a larger binary\n  [security identifier](https://docs.microsoft.com/en-us/windows/security/identity-protection/access-control/security-identifiers) (SID)\n  which is stored in the Windows Security Access Manager (SAM) database. This\n  database is not shared between the host and containers, or between containers.\n* File permissions - Windows uses an access control list based on (SIDs), whereas\n  POSIX systems such as Linux use a bitmask based on object permissions and UID+GID,\n  plus _optional_ access control lists.\n* File paths - the convention on Windows is to use `\\` instead of `/`. The Go IO\n  libraries typically accept both and just make it work, but when you're setting a\n  path or command line that's interpreted inside a container, `\\` may be needed.", "zh": "* 身份 - Linux 使用 userID（UID）和 groupID（GID），表示为整数类型。\n  用户名和组名是不规范的，它们只是 `/etc/groups` 或 `/etc/passwd` 中的别名，\n  作为 UID+GID 的后备标识。\n  Windows 使用更大的二进制[安全标识符](https://docs.microsoft.com/zh-cn/windows/security/identity-protection/access-control/security-identifiers)（SID），\n  存放在 Windows 安全访问管理器（Security Access Manager，SAM）数据库中。\n  此数据库在主机和容器之间或容器之间不共享。\n* 文件权限 - Windows 使用基于 SID 的访问控制列表，\n  而像 Linux 使用基于对象权限和 UID+GID 的位掩码（POSIX 系统）以及**可选的**访问控制列表。\n* 文件路径 - Windows 上的约定是使用 `\\` 而不是 `/`。\n  Go IO 库通常接受两者，能让其正常工作，但当你设置要在容器内解读的路径或命令行时，\n  可能需要用 `\\`。"}
{"en": "* Signals - Windows interactive apps handle termination differently, and can\n  implement one or more of these:\n  * A UI thread handles well-defined messages including `WM_CLOSE`.\n  * Console apps handle Ctrl-C or Ctrl-break using a Control Handler.\n  * Services register a Service Control Handler function that can accept\n    `SERVICE_CONTROL_STOP` control codes.\n\nContainer exit codes follow the same convention where 0 is success, and nonzero is failure.\nThe specific error codes may differ across Windows and Linux. However, exit codes\npassed from the Kubernetes components (kubelet, kube-proxy) are unchanged.", "zh": "* 信号 - Windows 交互式应用处理终止的方式不同，可以实现以下一种或多种：\n  * UI 线程处理包括 `WM_CLOSE` 在内准确定义的消息。\n  * 控制台应用使用控制处理程序（Control Handler）处理 Ctrl-C 或 Ctrl-Break。\n  * 服务会注册可接受 `SERVICE_CONTROL_STOP` 控制码的服务控制处理程序（Service Control Handler）函数。\n\n容器退出码遵循相同的约定，其中 0 表示成功，非零表示失败。\n具体的错误码在 Windows 和 Linux 中可能不同。\n但是，从 Kubernetes 组件（kubelet、kube-proxy）传递的退出码保持不变。"}
{"en": "#### Field compatibility for container specifications {#compatibility-v1-pod-spec-containers}\n\nThe following list documents differences between how Pod container specifications\nwork between Windows and Linux:\n\n* Huge pages are not implemented in the Windows container\n  runtime, and are not available. They require [asserting a user\n  privilege](https://docs.microsoft.com/en-us/windows/desktop/Memory/large-page-support)\n  that's not configurable for containers.\n* `requests.cpu` and `requests.memory` - requests are subtracted\n  from node available resources, so they can be used to avoid overprovisioning a\n  node. However, they cannot be used to guarantee resources in an overprovisioned\n  node. They should be applied to all containers as a best practice if the operator\n  wants to avoid overprovisioning entirely.", "zh": "#### 容器规约的字段兼容性   {#compatibility-v1-pod-spec-containers}\n\n以下列表记录了 Pod 容器规约在 Windows 和 Linux 之间的工作方式差异：\n\n* 巨页（Huge page）在 Windows 容器运行时中未实现，且不可用。\n  巨页需要不可为容器配置的[用户特权生效](https://docs.microsoft.com/zh-cn/windows/win32/memory/large-page-support)。\n* `requests.cpu` 和 `requests.memory` -\n  从节点可用资源中减去请求，因此请求可用于避免一个节点过量供应。\n  但是，请求不能用于保证已过量供应的节点中的资源。\n  如果运营商想要完全避免过量供应，则应将设置请求作为最佳实践应用到所有容器。"}
{"en": "* `securityContext.allowPrivilegeEscalation` -\n   not possible on Windows; none of the capabilities are hooked up\n* `securityContext.capabilities` -\n   POSIX capabilities are not implemented on Windows\n* `securityContext.privileged` -\n   Windows doesn't support privileged containers, use [HostProcess Containers](/docs/tasks/configure-pod-container/create-hostprocess-pod/) instead\n* `securityContext.procMount` -\n   Windows doesn't have a `/proc` filesystem\n* `securityContext.readOnlyRootFilesystem` -\n   not possible on Windows; write access is required for registry & system\n   processes to run inside the container\n* `securityContext.runAsGroup` -\n   not possible on Windows as there is no GID support", "zh": "* `securityContext.allowPrivilegeEscalation` -\n  不能在 Windows 上使用；所有权能字都无法生效。\n* `securityContext.capabilities` - POSIX 权能未在 Windows 上实现。\n* `securityContext.privileged` - Windows 不支持特权容器，\n  可使用 [HostProcess 容器](/zh-cn/docs/tasks/configure-pod-container/create-hostprocess-pod/)代替。\n* `securityContext.procMount` - Windows 没有 `/proc` 文件系统。\n* `securityContext.readOnlyRootFilesystem` -\n  不能在 Windows 上使用；对于容器内运行的注册表和系统进程，写入权限是必需的。\n* `securityContext.runAsGroup` - 不能在 Windows 上使用，因为不支持 GID。"}
{"en": "* `securityContext.runAsNonRoot` -\n   this setting will prevent containers from running as `ContainerAdministrator`\n   which is the closest equivalent to a root user on Windows.\n* `securityContext.runAsUser` -\n   use [`runAsUserName`](/docs/tasks/configure-pod-container/configure-runasusername)\n   instead\n* `securityContext.seLinuxOptions` -\n   not possible on Windows as SELinux is Linux-specific\n* `terminationMessagePath` -\n   this has some limitations in that Windows doesn't support mapping single files. The\n   default value is `/dev/termination-log`, which does work because it does not\n   exist on Windows by default.", "zh": "* `securityContext.runAsNonRoot` -\n  此设置将阻止以 `ContainerAdministrator` 身份运行容器，这是 Windows 上与 root 用户最接近的身份。\n* `securityContext.runAsUser` - 改用 [`runAsUserName`](/zh-cn/docs/tasks/configure-pod-container/configure-runasusername)。\n* `securityContext.seLinuxOptions` - 不能在 Windows 上使用，因为 SELinux 特定于 Linux。\n* `terminationMessagePath` - 这个字段有一些限制，因为 Windows 不支持映射单个文件。\n  默认值为 `/dev/termination-log`，因为默认情况下它在 Windows 上不存在，所以能生效。"}
{"en": "#### Field compatibility for Pod specifications {#compatibility-v1-pod}\n\nThe following list documents differences between how Pod specifications work between Windows and Linux:\n\n* `hostIPC` and `hostpid` - host namespace sharing is not possible on Windows\n* `hostNetwork` - [see below](#compatibility-v1-pod-spec-containers-hostnetwork)\n* `dnsPolicy` - setting the Pod `dnsPolicy` to `ClusterFirstWithHostNet` is\n   not supported on Windows because host networking is not provided. Pods always\n   run with a container network.\n* `podSecurityContext` [see below](#compatibility-v1-pod-spec-containers-securitycontext)\n* `shareProcessNamespace` - this is a beta feature, and depends on Linux namespaces\n  which are not implemented on Windows. Windows cannot share process namespaces or\n  the container's root filesystem. Only the network can be shared.", "zh": "#### Pod 规约的字段兼容性   {#compatibility-v1-pod}\n\n以下列表记录了 Pod 规约在 Windows 和 Linux 之间的工作方式差异：\n\n* `hostIPC` 和 `hostpid` - 不能在 Windows 上共享主机命名空间。\n* `hostNetwork` - [参见下文](#compatibility-v1-pod-spec-containers-hostnetwork)\n* `dnsPolicy` - Windows 不支持将 Pod `dnsPolicy` 设为 `ClusterFirstWithHostNet`，\n  因为未提供主机网络。Pod 始终用容器网络运行。\n* `podSecurityContext` [参见下文](#compatibility-v1-pod-spec-containers-securitycontext)\n* `shareProcessNamespace` - 这是一个 Beta 版功能特性，依赖于 Windows 上未实现的 Linux 命名空间。\n  Windows 无法共享进程命名空间或容器的根文件系统（root filesystem）。\n  只能共享网络。"}
{"en": "* `terminationGracePeriodSeconds` - this is not fully implemented in Docker on Windows,\n  see the [GitHub issue](https://github.com/moby/moby/issues/25982).\n  The behavior today is that the ENTRYPOINT process is sent CTRL_SHUTDOWN_EVENT,\n  then Windows waits 5 seconds by default, and finally shuts down\n  all processes using the normal Windows shutdown behavior. The 5\n  second default is actually in the Windows registry\n  [inside the container](https://github.com/moby/moby/issues/25982#issuecomment-426441183),\n  so it can be overridden when the container is built.\n* `volumeDevices` - this is a beta feature, and is not implemented on Windows.\n  Windows cannot attach raw block devices to pods.\n* `volumes`\n  * If you define an `emptyDir` volume, you cannot set its volume source to `memory`.\n* You cannot enable `mountPropagation` for volume mounts as this is not\n  supported on Windows.", "zh": "* `terminationGracePeriodSeconds` - 这在 Windows 上的 Docker 中没有完全实现，\n  请参考 [GitHub issue](https://github.com/moby/moby/issues/25982)。\n  目前的行为是通过 CTRL_SHUTDOWN_EVENT 发送 ENTRYPOINT 进程，然后 Windows 默认等待 5 秒，\n  最后使用正常的 Windows 关机行为终止所有进程。\n  5 秒默认值实际上位于[容器内](https://github.com/moby/moby/issues/25982#issuecomment-426441183)的\n  Windows 注册表中，因此在构建容器时可以覆盖这个值。\n* `volumeDevices` - 这是一个 Beta 版功能特性，未在 Windows 上实现。\n  Windows 无法将原始块设备挂接到 Pod。\n* `volumes`\n  * 如果你定义一个 `emptyDir` 卷，则你无法将卷源设为 `memory`。\n* 你无法为卷挂载启用 `mountPropagation`，因为这在 Windows 上不支持。"}
{"en": "#### Field compatibility for hostNetwork {#compatibility-v1-pod-spec-containers-hostnetwork}\n\n{{< feature-state for_k8s_version=\"v1.26\" state=\"alpha\" >}}\n\nThe kubelet can now request that pods running on Windows nodes use the host's network namespace instead\nof creating a new pod network namespace. To enable this functionality pass `--feature-gates=WindowsHostNetwork=true` to the kubelet.", "zh": "#### hostNetwork 的字段兼容性   {#compatibility-v1-pod-spec-containers-hostnetwork}\n\n{{< feature-state for_k8s_version=\"v1.26\" state=\"alpha\" >}}\n\n现在，kubelet 可以请求在 Windows 节点上运行的 Pod 使用主机的网络命名空间，而不是创建新的 Pod 网络命名空间。\n要启用此功能，请将 `--feature-gates=WindowsHostNetwork=true` 传递给 kubelet。\n\n{{< note >}}"}
{"en": "This functionality requires a container runtime that supports this functionality.", "zh": "此功能需要支持该功能的容器运行时。\n{{< /note >}}"}
{"en": "#### Field compatibility for Pod security context {#compatibility-v1-pod-spec-containers-securitycontext}\n\nOnly the `securityContext.runAsNonRoot` and `securityContext.windowsOptions` from the Pod\n[`securityContext`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context) fields work on Windows.", "zh": "#### Pod 安全上下文的字段兼容性   {#compatibility-v1-pod-spec-containers-securitycontext}\n\nPod 的 [`securityContext`](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context)\n中只有 `securityContext.runAsNonRoot` 和 `securityContext.windowsOptions` 字段在 Windows 上生效。"}
{"en": "## Node problem detector\n\nThe node problem detector (see\n[Monitor Node Health](/docs/tasks/debug/debug-cluster/monitor-node-health/))\nhas preliminary support for Windows.\nFor more information, visit the project's [GitHub page](https://github.com/kubernetes/node-problem-detector#windows).", "zh": "## 节点问题检测器   {#node-problem-detector}\n\n节点问题检测器（参考[节点健康监测](/zh-cn/docs/tasks/debug/debug-cluster/monitor-node-health/)）初步支持 Windows。\n有关更多信息，请访问该项目的 [GitHub 页面](https://github.com/kubernetes/node-problem-detector#windows)。"}
{"en": "## Pause container\n\nIn a Kubernetes Pod, an infrastructure or “pause” container is first created\nto host the container. In Linux, the cgroups and namespaces that make up a pod\nneed a process to maintain their continued existence; the pause process provides\nthis. Containers that belong to the same pod, including infrastructure and worker\ncontainers, share a common network endpoint (same IPv4 and / or IPv6 address, same\nnetwork port spaces). Kubernetes uses pause containers to allow for worker containers\ncrashing or restarting without losing any of the networking configuration.", "zh": "## Pause 容器   {#pause-container}\n\n在 Kubernetes Pod 中，首先创建一个基础容器或 “pause” 容器来承载容器。\n在 Linux 中，构成 Pod 的 cgroup 和命名空间维持持续存在需要一个进程；\n而 pause 进程就提供了这个功能。\n属于同一 Pod 的容器（包括基础容器和工作容器）共享一个公共网络端点\n（相同的 IPv4 和/或 IPv6 地址，相同的网络端口空间）。\nKubernetes 使用 pause 容器以允许工作容器崩溃或重启，而不会丢失任何网络配置。"}
{"en": "Kubernetes maintains a multi-architecture image that includes support for Windows.\nFor Kubernetes v{{< skew currentPatchVersion >}} the recommended pause image is `registry.k8s.io/pause:3.6`.\nThe [source code](https://github.com/kubernetes/kubernetes/tree/master/build/pause)\nis available on GitHub.\n\nMicrosoft maintains a different multi-architecture image, with Linux and Windows\namd64 support, that you can find as `mcr.microsoft.com/oss/kubernetes/pause:3.6`.\nThis image is built from the same source as the Kubernetes maintained image but\nall of the Windows binaries are [authenticode signed](https://docs.microsoft.com/en-us/windows-hardware/drivers/install/authenticode) by Microsoft.\nThe Kubernetes project recommends using the Microsoft maintained image if you are\ndeploying to a production or production-like environment that requires signed\nbinaries.", "zh": "Kubernetes 维护一个多体系结构的镜像，包括对 Windows 的支持。\n对于 Kubernetes v{{< skew currentPatchVersion >}}，推荐的 pause 镜像为 `registry.k8s.io/pause:3.6`。\n可在 GitHub 上获得[源代码](https://github.com/kubernetes/kubernetes/tree/master/build/pause)。\n\nMicrosoft 维护一个不同的多体系结构镜像，支持 Linux 和 Windows amd64，\n你可以找到的镜像类似 `mcr.microsoft.com/oss/kubernetes/pause:3.6`。\n此镜像的构建与 Kubernetes 维护的镜像同源，但所有 Windows 可执行文件均由\nMicrosoft 进行了[验证码签名](https://docs.microsoft.com/zh-cn/windows-hardware/drivers/install/authenticode)。\n如果你正部署到一个需要签名可执行文件的生产或类生产环境，\nKubernetes 项目建议使用 Microsoft 维护的镜像。"}
{"en": "## Container runtimes {#container-runtime}\n\nYou need to install a\n{{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}\ninto each node in the cluster so that Pods can run there.\n\nThe following container runtimes work with Windows:", "zh": "## 容器运行时   {#container-runtime}\n\n你需要将{{< glossary_tooltip text=\"容器运行时\" term_id=\"container-runtime\" >}}安装到集群中的每个节点，\n这样 Pod 才能在这些节点上运行。\n\n以下容器运行时适用于 Windows：\n\n{{% thirdparty-content %}}"}
{"en": "### ContainerD\n\n{{< feature-state for_k8s_version=\"v1.20\" state=\"stable\" >}}\n\nYou can use {{< glossary_tooltip term_id=\"containerd\" text=\"ContainerD\" >}} 1.4.0+\nas the container runtime for Kubernetes nodes that run Windows.\n\nLearn how to [install ContainerD on a Windows node](/docs/setup/production-environment/container-runtimes/#containerd).", "zh": "### ContainerD\n\n{{< feature-state for_k8s_version=\"v1.20\" state=\"stable\" >}}\n\n对于运行 Windows 的 Kubernetes 节点，你可以使用\n{{< glossary_tooltip term_id=\"containerd\" text=\"ContainerD\" >}} 1.4.0+ 作为容器运行时。\n\n学习如何[在 Windows 上安装 ContainerD](/zh-cn/docs/setup/production-environment/container-runtimes/#containerd)。\n\n{{< note >}}"}
{"en": "There is a [known limitation](/docs/tasks/configure-pod-container/configure-gmsa/#gmsa-limitations)\nwhen using GMSA with containerd to access Windows network shares, which requires a\nkernel patch.", "zh": "将 GMSA 和 containerd 一起用于访问 Windows\n网络共享时存在[已知限制](/zh-cn/docs/tasks/configure-pod-container/configure-gmsa/#gmsa-limitations)，\n这需要一个内核补丁。\n{{< /note >}}"}
{"en": "### Mirantis Container Runtime {#mcr}\n\n[Mirantis Container Runtime](https://docs.mirantis.com/mcr/20.10/overview.html) (MCR)\nis available as a container runtime for all Windows Server 2019 and later versions.\n\nSee [Install MCR on Windows Servers](https://docs.mirantis.com/mcr/20.10/install/mcr-windows.html) for more information.", "zh": "### Mirantis 容器运行时   {#mcr}\n\n[Mirantis 容器运行时](https://docs.mirantis.com/mcr/20.10/overview.html)（MCR）\n可作为所有 Windows Server 2019 和更高版本的容器运行时。\n\n有关更多信息，请参考[在 Windows Server 上安装 MCR](https://docs.mirantis.com/mcr/20.10/install/mcr-windows.html)。"}
{"en": "## Windows OS version compatibility {#windows-os-version-support}\n\nOn Windows nodes, strict compatibility rules apply where the host OS version must\nmatch the container base image OS version. Only Windows containers with a container\noperating system of Windows Server 2019 are fully supported.\n\nFor Kubernetes v{{< skew currentVersion >}}, operating system compatibility for Windows nodes (and Pods)\nis as follows:", "zh": "## Windows 操作系统版本兼容性   {#windows-os-version-support}\n\n在 Windows 节点上，如果主机操作系统版本必须与容器基础镜像操作系统版本匹配，\n则会应用严格的兼容性规则。\n仅 Windows Server 2019 作为容器操作系统时，才能完全支持 Windows 容器。\n\n对于 Kubernetes v{{< skew currentVersion >}}，Windows 节点（和 Pod）的操作系统兼容性如下：\n\nWindows Server LTSC release\n: Windows Server 2019\n: Windows Server 2022\n\nWindows Server SAC release\n: Windows Server version 20H2"}
{"en": "The Kubernetes [version-skew policy](/docs/setup/release/version-skew-policy/) also applies.", "zh": "也适用 Kubernetes [版本偏差策略](/zh-cn/releases/version-skew-policy/)。"}
{"en": "## Hardware recommendations and considerations {#windows-hardware-recommendations}", "zh": "## 硬件建议和注意事项   {#windows-hardware-recommendations}\n\n{{% thirdparty-content %}}\n\n{{< note >}}"}
{"en": "The following hardware specifications outlined here should be regarded as sensible default values. \nThey are not intended to represent minimum requirements or specific recommendations for production environments.\nDepending on the requirements for your workload these values may need to be adjusted.", "zh": "这里列出的硬件规格应被视为合理的默认值。\n它们并不代表生产环境的最低要求或具体推荐。\n根据你的工作负载要求，这些值可能需要进行调整。\n{{< /note >}}"}
{"en": "- 64-bit processor 4 CPU cores or more, capable of supporting virtualization\n- 8GB or more of RAM\n- 50GB or more of free disk space", "zh": "- 64 位处理器，4 核或更多的 CPU，能够支持虚拟化\n- 8GB 或更多的 RAM\n- 50GB 或更多的可用磁盘空间"}
{"en": "Refer to\n[Hardware requirements for Windows Server Microsoft documentation](https://learn.microsoft.com/en-us/windows-server/get-started/hardware-requirements)\nfor the most up-to-date information on minimum hardware requirements. For guidance on deciding on resources for\nproduction worker nodes refer to [Production worker nodes Kubernetes documentation](/docs/setup/production-environment/#production-worker-nodes).", "zh": "有关最新的最低硬件要求信息，\n请参考[微软文档：Windows Server 的硬件要求](https://learn.microsoft.com/zh-cn/windows-server/get-started/hardware-requirements)。\n有关决定生产工作节点资源的指导信息，\n请参考 [Kubernetes 文档：生产用工作节点](/zh-cn/docs/setup/production-environment/#production-worker-nodes)。"}
{"en": "To optimize system resources, if a graphical user interface is not required,\nit may be preferable to use a Windows Server OS installation that excludes\nthe [Windows Desktop Experience](https://learn.microsoft.com/en-us/windows-server/get-started/install-options-server-core-desktop-experience)\ninstallation option, as this configuration typically frees up more system \nresources.", "zh": "为了优化系统资源，如果图形用户界面不是必需的，最好选择一个不包含\n[Windows 桌面体验](https://learn.microsoft.com/zh-cn/windows-server/get-started/install-options-server-core-desktop-experience)安装选项的\nWindows Server 操作系统安装包，因为这种配置通常会释放更多的系统资源。"}
{"en": "In assessing disk space for Windows worker nodes, take note that Windows container images are typically larger than\nLinux container images, with container image sizes ranging\nfrom [300MB to over 10GB](https://techcommunity.microsoft.com/t5/containers/nano-server-x-server-core-x-server-which-base-image-is-the-right/ba-p/2835785)\nfor a single image. Additionally, take note that the `C:` drive in Windows containers represents a virtual free size of\n20GB by default, which is not the actual consumed space, but rather the disk size for which a single container can grow\nto occupy when using local storage on the host.\nSee [Containers on Windows - Container Storage Documentation](https://learn.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/container-storage#storage-limits)\nfor more detail.", "zh": "在估算 Windows 工作节点的磁盘空间时，需要注意 Windows 容器镜像通常比 Linux 容器镜像更大，\n单个镜像的容器大小范围从 [300MB 到超过 10GB](https://techcommunity.microsoft.com/t5/containers/nano-server-x-server-core-x-server-which-base-image-is-the-right/ba-p/2835785)。\n此外，需要注意 Windows 容器中的 `C:` 驱动器默认呈现的虚拟剩余空间为 20GB，\n这不是实际的占用空间，而是使用主机上的本地存储时单个容器可以最多占用的磁盘大小。\n有关更多详细信息，\n请参见[在 Windows 上运行容器 - 容器存储文档](https://learn.microsoft.com/zh-cn/virtualization/windowscontainers/manage-containers/container-storage#storage-limits)。"}
{"en": "## Getting help and troubleshooting {#troubleshooting}\n\nYour main source of help for troubleshooting your Kubernetes cluster should start\nwith the [Troubleshooting](/docs/tasks/debug/)\npage.\n\nSome additional, Windows-specific troubleshooting help is included\nin this section. Logs are an important element of troubleshooting\nissues in Kubernetes. Make sure to include them any time you seek\ntroubleshooting assistance from other contributors. Follow the\ninstructions in the\nSIG Windows [contributing guide on gathering logs](https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs).", "zh": "## 获取帮助和故障排查   {#troubleshooting}\n\n对 Kubernetes 集群进行故障排查的主要帮助来源应始于[故障排查](/zh-cn/docs/tasks/debug/)页面。\n\n本节包括了一些其他特定于 Windows 的故障排查帮助。\n日志是解决 Kubernetes 中问题的重要元素。\n确保在任何时候向其他贡献者寻求故障排查协助时随附了日志信息。\n遵照 SIG Windows\n[日志收集贡献指南](https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs)中的指示说明。"}
{"en": "### Reporting issues and feature requests\n\nIf you have what looks like a bug, or you would like to\nmake a feature request, please follow the [SIG Windows contributing guide](https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#reporting-issues-and-feature-requests) to create a new issue.\nYou should first search the list of issues in case it was\nreported previously and comment with your experience on the issue and add additional\nlogs. SIG Windows channel on the Kubernetes Slack is also a great avenue to get some initial support and\ntroubleshooting ideas prior to creating a ticket.", "zh": "### 报告问题和功能请求   {#report-issue-and-feature-request}\n\n如果你发现疑似 bug，或者你想提出功能请求，请按照\n[SIG Windows 贡献指南](https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#reporting-issues-and-feature-requests)\n新建一个 Issue。你应该先搜索 Issue 列表，以防之前报告过这个问题，凭你对该问题的经验添加评论，\n并随附日志信息。Kubernetes Slack 上的 SIG Windows 频道也是一个很好的途径，\n可以在创建工单之前获得一些初始支持和故障排查思路。"}
{"en": "### Validating the Windows cluster operability\n\nThe Kubernetes project provides a _Windows Operational Readiness_ specification,\naccompanied by a structured test suite. This suite is split into two sets of tests,\ncore and extended, each containing categories aimed at testing specific areas.\nIt can be used to validate all the functionalities of a Windows and hybrid system\n(mixed with Linux nodes) with full coverage.\n\nTo set up the project on a newly created cluster, refer to the instructions in the\n[project guide](https://github.com/kubernetes-sigs/windows-operational-readiness/blob/main/README.md).", "zh": "### 验证 Windows 集群的操作性  {#validating-windows-cluster-operability}\n\nKubernetes 项目提供了 **Windows 操作准备**规范，配备了结构化的测试套件。\n这个套件分为两组测试：核心和扩展。每组测试都包含了针对特定场景的分类测试。\n它可以用来验证 Windows 和混合系统（混合了 Linux 节点）的所有功能，实现全面覆盖。\n\n要在新创建的集群上搭建此项目，\n请参考[项目指南](https://github.com/kubernetes-sigs/windows-operational-readiness/blob/main/README.md)中的说明。"}
{"en": "## Deployment tools\n\nThe kubeadm tool helps you to deploy a Kubernetes cluster, providing the control\nplane to manage the cluster it, and nodes to run your workloads.\n\nThe Kubernetes [cluster API](https://cluster-api.sigs.k8s.io/) project also provides means to automate deployment of Windows nodes.", "zh": "## 部署工具   {#deployment-tools}\n\nkubeadm 工具帮助你部署 Kubernetes 集群，提供管理集群的控制平面以及运行工作负载的节点。\n\nKubernetes [集群 API](https://cluster-api.sigs.k8s.io/) 项目也提供了自动部署\nWindows 节点的方式。"}
{"en": "## Windows distribution channels\n\nFor a detailed explanation of Windows distribution channels see the\n[Microsoft documentation](https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19).\n\nInformation on the different Windows Server servicing channels\nincluding their support models can be found at\n[Windows Server servicing channels](https://docs.microsoft.com/en-us/windows-server/get-started/servicing-channels-comparison).", "zh": "## Windows 分发渠道   {#windows-distribution-channels}\n\n有关 Windows 分发渠道的详细阐述，请参考\n[Microsoft 文档](https://docs.microsoft.com/zh-cn/windows-server/get-started-19/servicing-channels-19)。\n\n有关支持模型在内的不同 Windows Server 服务渠道的信息，请参考\n[Windows Server 服务渠道](https://docs.microsoft.com/zh-cn/windows-server/get-started/servicing-channels-comparison)。"}
{"en": "Windows applications constitute a large portion of the services and applications that run in many organizations.\nThis guide walks you through the steps to configure and deploy Windows containers in Kubernetes.", "zh": "在许多组织中运行的服务和应用程序中，Windows 应用程序构成了很大一部分。\n本指南将引导你完成在 Kubernetes 中配置和部署 Windows 容器的步骤。"}
{"en": "## Objectives\n\n* Configure an example deployment to run Windows containers on the Windows node\n* Highlight Windows specific functionality in Kubernetes", "zh": "## 目标  {#objectives}\n\n* 配置 Deployment 样例以在 Windows 节点上运行 Windows 容器\n* 在 Kubernetes 中突出 Windows 特定的功能"}
{"en": "## Before you begin\n\n* Create a Kubernetes cluster that includes a control plane and a worker node running Windows Server\n* It is important to note that creating and deploying services and workloads on Kubernetes\n  behaves in much the same way for Linux and Windows containers.\n  [Kubectl commands](/docs/reference/kubectl/) to interface with the cluster are identical.\n  The example in the section below is provided to jumpstart your experience with Windows containers.", "zh": "## 在你开始之前  {#before-you-begin}\n\n* 创建一个 Kubernetes 集群，其中包含一个控制平面和一个运行 Windows Server 的工作节点。\n* 务必请注意，在 Kubernetes 上创建和部署服务和工作负载的行为方式与 Linux 和 Windows 容器的行为方式大致相同。\n  与集群交互的 [kubectl 命令](/zh-cn/docs/reference/kubectl/)是一致的。\n  下一小节的示例旨在帮助你快速开始使用 Windows 容器。"}
{"en": "## Getting Started: Deploying a Windows container\n\nThe example YAML file below deploys a simple webserver application running inside a Windows container.\n\nCreate a service spec named `win-webserver.yaml` with the contents below:", "zh": "## 快速开始：部署 Windows 容器  {#getting-started-deploying-a-windows-container}\n\n以下示例 YAML 文件部署了一个在 Windows 容器内运行的简单 Web 服务器的应用程序。\n\n创建一个名为 `win-webserver.yaml` 的 Service 规约，其内容如下：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: win-webserver\n  labels:\n    app: win-webserver\nspec:\n  ports:\n    # 此 Service 服务的端口\n    - port: 80\n      targetPort: 80\n  selector:\n    app: win-webserver\n  type: NodePort\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: win-webserver\n  name: win-webserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: win-webserver\n  template:\n    metadata:\n      labels:\n        app: win-webserver\n      name: win-webserver\n    spec:\n     containers:\n      - name: windowswebserver\n        image: mcr.microsoft.com/windows/servercore:ltsc2019\n        command:\n        - powershell.exe\n        - -command\n        - \"<#code used from https://gist.github.com/19WAS85/5424431#> ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add('http://*:80/') ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host('Listening at http://*:80/') ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host '' ;Write-Host('> {0}' -f $$requestUrl) ;  ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header='<html><body><H1>Windows Container Web Server</H1>' ;$$callerCountsString='' ;$$callerCounts.Keys | % { $$callerCountsString+='<p>IP {0} callerCount {1} ' -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer='</body></html>' ;$$content='{0}{1}{2}' -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host('< {0}' -f $$responseStatus)  } ; \"\n     nodeSelector:\n      kubernetes.io/os: windows\n```\n\n{{< note >}}"}
{"en": "Port mapping is also supported, but for simplicity this example exposes\nport 80 of the container directly to the Service.", "zh": "端口映射也是支持的，但为简单起见，此示例将容器的端口 80 直接暴露给服务。\n{{< /note >}}"}
{"en": "1. Check that all nodes are healthy:", "zh": "1. 检查所有节点是否健康\n\n   ```bash\n   kubectl get nodes\n   ```"}
{"en": "1. Deploy the service and watch for pod updates:", "zh": "1. 部署 Service 并监视 Pod 更新：\n\n   ```bash\n   kubectl apply -f win-webserver.yaml\n   kubectl get pods -o wide -w\n   ```"}
{"en": "When the service is deployed correctly both Pods are marked as Ready. To exit the watch command, press Ctrl+C.", "zh": "当 Service 被正确部署时，两个 Pod 都被标记为就绪（Ready）。要退出 watch 命令，请按 Ctrl+C。"}
{"en": "1. Check that the deployment succeeded. To verify:\n\n    * Two pods listed from the Linux control plane node, use `kubectl get pods`\n    * Node-to-pod communication across the network, `curl` port 80 of your pod IPs from the Linux control plane node\n      to check for a web server response\n    * Pod-to-pod communication, ping between pods (and across hosts, if you have more than one Windows node)\n      using `docker exec` or `kubectl exec`\n    * Service-to-pod communication, `curl` the virtual service IP (seen under `kubectl get services`)\n      from the Linux control plane node and from individual pods\n    * Service discovery, `curl` the service name with the Kubernetes [default DNS suffix](/docs/concepts/services-networking/dns-pod-service/#services)\n    * Inbound connectivity, `curl` the NodePort from the Linux control plane node or machines outside of the cluster\n    * Outbound connectivity, `curl` external IPs from inside the pod using `kubectl exec`", "zh": "1. 检查部署是否成功。请验证：\n\n   * 当执行 `kubectl get pods` 命令时，能够从 Linux 控制平面所在的节点上列出两个 Pod。\n   * 跨网络的节点到 Pod 通信，从 Linux 控制平面所在的节点上执行 `curl` 命令来访问\n     Pod IP 的 80 端口以检查 Web 服务器响应。\n   * Pod 间通信，使用 `docker exec` 或 `kubectl exec`\n     命令进入容器，并在 Pod 之间（以及跨主机，如果你有多个 Windows 节点）相互进行 ping 操作。\n   * Service 到 Pod 的通信，在 Linux 控制平面所在的节点以及独立的 Pod 中执行 `curl`\n     命令来访问虚拟的服务 IP（在 `kubectl get services` 命令下查看）。\n   * 服务发现，执行 `curl` 命令来访问带有 Kubernetes\n     [默认 DNS 后缀](/zh-cn/docs/concepts/services-networking/dns-pod-service/#services)的服务名称。\n   * 入站连接，在 Linux 控制平面所在的节点上或集群外的机器上执行 `curl` 命令来访问 NodePort 服务。\n   * 出站连接，使用 `kubectl exec`，从 Pod 内部执行 `curl` 访问外部 IP。\n\n{{< note >}}"}
{"en": "Windows container hosts are not able to access the IP of services scheduled on them due to current platform limitations of the Windows networking stack.\nOnly Windows pods are able to access service IPs.", "zh": "由于当前 Windows 平台的网络堆栈限制，Windows 容器主机无法访问调度到其上的 Service 的 IP。\n只有 Windows Pod 能够访问 Service IP。\n{{< /note >}}"}
{"en": "## Observability\n\n### Capturing logs from workloads\n\nLogs are an important element of observability; they enable users to gain insights\ninto the operational aspect of workloads and are a key ingredient to troubleshooting issues.\nBecause Windows containers and workloads inside Windows containers behave differently from Linux containers,\nusers had a hard time collecting logs, limiting operational visibility.\nWindows workloads for example are usually configured to log to ETW (Event Tracing for Windows)\nor push entries to the application event log.\n[LogMonitor](https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor), an open source tool by Microsoft,\nis the recommended way to monitor configured log sources inside a Windows container.\nLogMonitor supports monitoring event logs, ETW providers, and custom application logs,\npiping them to STDOUT for consumption by `kubectl logs <pod>`.\n\nFollow the instructions in the LogMonitor GitHub page to copy its binaries and configuration files\nto all your containers and add the necessary entrypoints for LogMonitor to push your logs to STDOUT.", "zh": "## 可观察性  {#observability}\n\n### 捕捉来自工作负载的日志  {#capturing-logs-from-workloads}\n\n日志是可观察性的重要元素；它们使用户能够深入了解工作负载的运行情况，并且是解决问题的关键因素。\n由于 Windows 容器和 Windows 容器中的工作负载与 Linux 容器的行为不同，因此用户很难收集日志，从而限制了操作可见性。\n例如，Windows 工作负载通常配置为记录到 ETW（Windows 事件跟踪）或向应用程序事件日志推送条目。\n[LogMonitor](https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor)\n是一个微软开源的工具，是监视 Windows 容器内所配置的日志源的推荐方法。\nLogMonitor 支持监视事件日志、ETW 提供程序和自定义应用程序日志，将它们传送到 STDOUT 以供 `kubectl logs <pod>` 使用。\n\n按照 LogMonitor GitHub 页面中的说明，将其二进制文件和配置文件复制到所有容器，\n并为 LogMonitor 添加必要的入口点以将日志推送到标准输出（STDOUT）。"}
{"en": "## Configuring container user\n\n### Using configurable Container usernames\n\nWindows containers can be configured to run their entrypoints and processes\nwith different usernames than the image defaults.\nLearn more about it [here](/docs/tasks/configure-pod-container/configure-runasusername/).", "zh": "## 配置容器用户  {#configuring-container-user}\n\n### 使用可配置的容器用户名  {#using-configurable-container-usernames}\n\nWindows 容器可以配置为使用不同于镜像默认值的用户名来运行其入口点和进程。\n[在这里](/zh-cn/docs/tasks/configure-pod-container/configure-runasusername/)了解更多信息。"}
{"en": "### Managing Workload Identity with Group Managed Service Accounts\n\nWindows container workloads can be configured to use Group Managed Service Accounts (GMSA).\nGroup Managed Service Accounts are a specific type of Active Directory account that provide automatic password management,\nsimplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.\nContainers configured with a GMSA can access external Active Directory Domain resources while carrying the identity configured with the GMSA.\nLearn more about configuring and using GMSA for Windows containers [here](/docs/tasks/configure-pod-container/configure-gmsa/).", "zh": "### 使用组托管服务帐户（GMSA）管理工作负载身份  {#managing-workload-identity-with-group-managed-service-accounts}\n\nWindows 容器工作负载可以配置为使用组托管服务帐户（Group Managed Service Accounts，GMSA）。\n组托管服务帐户是一种特定类型的活动目录（Active Directory）帐户，可提供自动密码管理、\n简化的服务主体名称（Service Principal Name，SPN）管理，以及将管理委派给多个服务器上的其他管理员的能力。\n配置了 GMSA 的容器可以携带使用 GMSA 配置的身份访问外部活动目录域资源。\n在[此处](/zh-cn/docs/tasks/configure-pod-container/configure-gmsa/)了解有关为 Windows\n容器配置和使用 GMSA 的更多信息。"}
{"en": "## Taints and Tolerations\n\nUsers need to use some combination of taints and node selectors in order to\nschedule Linux and Windows workloads to their respective OS-specific nodes.\nThe recommended approach is outlined below,\nwith one of its main goals being that this approach should not break compatibility for existing Linux workloads.\n\nStarting from 1.25, you can (and should) set `.spec.os.name` for each Pod, to indicate the operating system\nthat the containers in that Pod are designed for. For Pods that run Linux containers, set\n`.spec.os.name` to `linux`. For Pods that run Windows containers, set `.spec.os.name`\nto `windows`.", "zh": "## 污点和容忍度  {#taints-and-tolerations}\n\n用户需要使用某种污点（Taint）和节点选择器的组合，以便将 Linux 和 Windows 工作负载各自调度到特定操作系统的节点。\n下面概述了推荐的方法，其主要目标之一是该方法不应破坏现有 Linux 工作负载的兼容性。\n\n从 1.25 开始，你可以（并且应该）将每个 Pod 的 `.spec.os.name` 设置为 Pod 中的容器设计所用于的操作系统。\n对于运行 Linux 容器的 Pod，将 `.spec.os.name` 设置为 `linux`。\n对于运行 Windows 容器的 Pod，将 `.spec.os.name` 设置为 `windows`。"}
{"en": "The scheduler does not use the value of `.spec.os.name` when assigning Pods to nodes. You should\nuse normal Kubernetes mechanisms for\n[assigning pods to nodes](/docs/concepts/scheduling-eviction/assign-pod-node/)\nto ensure that the control plane for your cluster places pods onto nodes that are running the\nappropriate operating system.\n\nThe `.spec.os.name` value has no effect on the scheduling of the Windows pods,\nso taints and tolerations and node selectors are still required\n to ensure that the Windows pods land onto appropriate Windows nodes.", "zh": "调度器在将 Pod 分配到节点时并不使用 `.spec.os.name` 的值。\n你应该使用正常的 Kubernetes 机制[将 Pod 分配给节点](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/)，\n以确保集群的控制平面将 Pod 放置到运行适当操作系统的节点上。\n\n`.spec.os.name` 值对 Windows Pod 的调度没有影响，\n因此仍然需要污点和容忍以及节点选择器来确保 Windows Pod 落在适当的 Windows 节点。"}
{"en": "### Ensuring OS-specific workloads land on the appropriate container host\n\nUsers can ensure Windows containers can be scheduled on the appropriate host using Taints and Tolerations.\nAll Kubernetes nodes today have the following default labels:\n\n* kubernetes.io/os = [windows|linux]\n* kubernetes.io/arch = [amd64|arm64|...]", "zh": "### 确保特定于操作系统的工作负载落到合适的容器主机上  {#ensuring-os-specific-workloads-land-on-the-appropriate-container-host}\n\n用户可以使用污点（Taint）和容忍度（Toleration）确保将 Windows 容器调度至合适的主机上。\n现在，所有的 Kubernetes 节点都有以下默认标签：\n\n* kubernetes.io/os = [windows|linux]\n* kubernetes.io/arch = [amd64|arm64|...]"}
{"en": "If a Pod specification does not specify a nodeSelector like `\"kubernetes.io/os\": windows`,\nit is possible the Pod can be scheduled on any host, Windows or Linux.\nThis can be problematic since a Windows container can only run on Windows and a Linux container can only run on Linux.\nThe best practice is to use a nodeSelector.", "zh": "如果 Pod 规约没有指定像 `\"kubernetes.io/os\": windows` 这样的 nodeSelector，\n则 Pod 可以被调度到任何主机上，Windows 或 Linux。\n这可能会有问题，因为 Windows 容器只能在 Windows 上运行，而 Linux 容器只能在 Linux 上运行。\n最佳实践是使用 nodeSelector。"}
{"en": "However, we understand that in many cases users have a pre-existing large number of deployments for Linux containers,\nas well as an ecosystem of off-the-shelf configurations, such as community Helm charts, and programmatic Pod generation cases, such as with Operators.\nIn those situations, you may be hesitant to make the configuration change to add nodeSelectors.\nThe alternative is to use Taints. Because the kubelet can set Taints during registration,\nit could easily be modified to automatically add a taint when running on Windows only.", "zh": "但是，我们了解到，在许多情况下，用户已经预先存在大量 Linux 容器部署，\n以及现成配置的生态系统，例如社区中的 Helm Chart 包和程序化的 Pod 生成案例，例如 Operator。\n在这些情况下，你可能不愿更改配置来添加节点选择器。\n另一种方法是使用污点。因为 kubelet 可以在注册过程中设置污点，\n所以可以很容易地修改为，当只能在 Windows 上运行时，自动添加污点。"}
{"en": "For example:  `--register-with-taints='os=windows:NoSchedule'`\n\nBy adding a taint to all Windows nodes, nothing will be scheduled on them (that includes existing Linux Pods).\nIn order for a Windows Pod to be scheduled on a Windows node, \nit would need both the nodeSelector and the appropriate matching toleration to choose Windows.", "zh": "例如：`--register-with-taints='os=windows:NoSchedule'`\n\n通过向所有 Windows 节点添加污点，任何负载都不会被调度到这些节点上（包括现有的 Linux Pod）。\n为了在 Windows 节点上调度 Windows Pod，它需要 nodeSelector 和匹配合适的容忍度来选择 Windows。\n\n```yaml\nnodeSelector:\n    kubernetes.io/os: windows\n    node.kubernetes.io/windows-build: '10.0.17763'\ntolerations:\n    - key: \"os\"\n      operator: \"Equal\"\n      value: \"windows\"\n      effect: \"NoSchedule\"\n```"}
{"en": "### Handling multiple Windows versions in the same cluster\n\nThe Windows Server version used by each pod must match that of the node. If you want to use multiple Windows\nServer versions in the same cluster, then you should set additional node labels and nodeSelectors.\n\nKubernetes 1.17 automatically adds a new label `node.kubernetes.io/windows-build` to simplify this.\nIf you're running an older version, then it's recommended to add this label manually to Windows nodes.\n\nThis label reflects the Windows major, minor, and build number that need to match for compatibility.\nHere are values used today for each Windows Server version.", "zh": "### 处理同一集群中的多个 Windows 版本  {#handling-multiple-windows-versions-in-the-same-cluster}\n\n每个 Pod 使用的 Windows Server 版本必须与节点的版本匹配。\n如果要在同一个集群中使用多个 Windows Server 版本，则应设置额外的节点标签和节点选择器。\n\nKubernetes 1.17 自动添加了一个新标签 `node.kubernetes.io/windows-build` 来简化这一点。\n如果你运行的是旧版本，则建议手动将此标签添加到 Windows 节点。\n\n此标签反映了需要匹配以实现兼容性的 Windows 主要、次要和内部版本号。\n以下是目前用于每个 Windows Server 版本的值。"}
{"en": "| Product Name                         |   Build Number(s)      |", "zh": "| 产品名称                              |   构建号                |\n|--------------------------------------|------------------------|\n| Windows Server 2019                  | 10.0.17763             |\n| Windows Server, Version 20H2         | 10.0.19042             |\n| Windows Server 2022                  | 10.0.20348             |"}
{"en": "### Simplifying with RuntimeClass\n\n[RuntimeClass] can be used to simplify the process of using taints and tolerations.\nA cluster administrator can create a `RuntimeClass` object which is used to encapsulate these taints and tolerations.\n\n1. Save this file to `runtimeClasses.yml`. It includes the appropriate `nodeSelector`\nfor the Windows OS, architecture, and version.", "zh": "### 使用 RuntimeClass 进行简化  {#simplifying-with-runtimeclass}\n\n[RuntimeClass] 可用于简化使用污点和容忍度的流程。\n集群管理员可以创建一个用于封装这些污点和容忍度的 `RuntimeClass` 对象。\n\n1. 将此文件保存到 `runtimeClasses.yml`。它包括针对 Windows 操作系统、架构和版本的 `nodeSelector`。\n\n   ```yaml\n   ---\n   apiVersion: node.k8s.io/v1\n   kind: RuntimeClass\n   metadata:\n     name: windows-2019\n   handler: example-container-runtime-handler\n   scheduling:\n     nodeSelector:\n       kubernetes.io/os: 'windows'\n       kubernetes.io/arch: 'amd64'\n       node.kubernetes.io/windows-build: '10.0.17763'\n     tolerations:\n     - effect: NoSchedule\n       key: os\n       operator: Equal\n       value: \"windows\"\n   ```"}
{"en": "1. Run `kubectl create -f runtimeClasses.yml` using as a cluster administrator\n1. Add `runtimeClassName: windows-2019` as appropriate to Pod specs\n\n   For example:", "zh": "1. 以集群管理员身份运行 `kubectl create -f runtimeClasses.yml`\n1. 根据情况，向 Pod 规约中添加 `runtimeClassName: windows-2019`\n\n   例如：\n\n   ```yaml\n   ---\n   apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: iis-2019\n     labels:\n       app: iis-2019\n   spec:\n     replicas: 1\n     template:\n       metadata:\n         name: iis-2019\n         labels:\n           app: iis-2019\n       spec:\n         runtimeClassName: windows-2019\n         containers:\n         - name: iis\n           image: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019\n           resources:\n             limits:\n               cpu: 1\n               memory: 800Mi\n             requests:\n               cpu: .1\n               memory: 300Mi\n           ports:\n             - containerPort: 80\n    selector:\n       matchLabels:\n         app: iis-2019\n   ---\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: iis\n   spec:\n     type: LoadBalancer\n     ports:\n     - protocol: TCP\n       port: 80\n     selector:\n       app: iis-2019\n   ```\n\n[RuntimeClass]: /zh-cn/docs/concepts/containers/runtime-class/"}
{"en": "Kubernetes supports worker {{< glossary_tooltip text=\"nodes\" term_id=\"node\" >}}\nrunning either Linux or Microsoft Windows.", "zh": "Kubernetes 支持运行 Linux 或 Microsoft Windows\n的工作{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}。\n\n{{% thirdparty-content single=\"true\" %}}"}
{"en": "The CNCF and its parent the Linux Foundation take a vendor-neutral approach\ntowards compatibility. It is possible to join your [Windows server](https://www.microsoft.com/en-us/windows-server)\nas a worker node to a Kubernetes cluster.", "zh": "CNCF 及其母公司 Linux 基金会采用供应商中立的方法来实现兼容性。可以将你的\n[Windows 服务器](https://www.microsoft.com/en-us/windows-server)作为工作节点加入\nKubernetes 集群。"}
{"en": "You can [install and set up kubectl on Windows](/docs/tasks/tools/install-kubectl-windows/)\nno matter what operating system you use within your cluster.\n\nIf you are using Windows nodes, you can read:", "zh": "无论你的集群使用什么操作系统，\n都可以[在 Windows 上安装和设置 kubectl](/zh-cn/docs/tasks/tools/install-kubectl-windows/)。\n\n如果你使用的是 Windows 节点，你可以阅读："}
{"en": "* [Networking On Windows](/docs/concepts/services-networking/windows-networking/)\n* [Windows Storage In Kubernetes](/docs/concepts/storage/windows-storage/)\n* [Resource Management for Windows Nodes](/docs/concepts/configuration/windows-resource-management/)\n* [Configure RunAsUserName for Windows Pods and Containers](/docs/tasks/configure-pod-container/configure-runasusername/)\n* [Create A Windows HostProcess Pod](/docs/tasks/configure-pod-container/create-hostprocess-pod/)\n* [Configure Group Managed Service Accounts for Windows Pods and Containers](/docs/tasks/configure-pod-container/configure-gmsa/)\n* [Security For Windows Nodes](/docs/concepts/security/windows-security/)\n* [Windows Debugging Tips](/docs/tasks/debug/debug-cluster/windows/)\n* [Guide for Scheduling Windows Containers in Kubernetes](/docs/concepts/windows/user-guide)\n\nor, for an overview, read:", "zh": "* [Windows 上的网络](/zh-cn/docs/concepts/services-networking/windows-networking/)\n* [Kubernetes 中的 Windows 存储](/zh-cn/docs/concepts/storage/windows-storage/)\n* [Windows 节点的资源管理](/zh-cn/docs/concepts/configuration/windows-resource-management/)\n* [为 Windows Pod 和容器配置 RunAsUserName](/zh-cn/docs/tasks/configure-pod-container/configure-runasusername/)\n* [创建 Windows HostProcess Pod](/zh-cn/docs/tasks/configure-pod-container/create-hostprocess-pod/)\n* [为 Windows Pod 和容器配置组托管服务帐户](/zh-cn/docs/tasks/configure-pod-container/configure-gmsa/)\n* [Windows 节点的安全性](/zh-cn/docs/concepts/security/windows-security/)\n* [Windows 调试技巧](/zh-cn/docs/tasks/debug/debug-cluster/windows/)\n* [在 Kubernetes 中调度 Windows 容器指南](/zh-cn/docs/concepts/windows/user-guide)\n\n或者，要了解概览，请阅读："}
{"en": "The core of Kubernetes' {{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}}\nis the {{< glossary_tooltip text=\"API server\" term_id=\"kube-apiserver\" >}}. The API server\nexposes an HTTP API that lets end users, different parts of your cluster, and\nexternal components communicate with one another.\n\nThe Kubernetes API lets you query and manipulate the state of API objects in Kubernetes\n(for example: Pods, Namespaces, ConfigMaps, and Events).", "zh": "Kubernetes {{< glossary_tooltip text=\"控制面\" term_id=\"control-plane\" >}}的核心是\n{{< glossary_tooltip text=\"API 服务器\" term_id=\"kube-apiserver\" >}}。\nAPI 服务器负责提供 HTTP API，以供用户、集群中的不同部分和集群外部组件相互通信。\n\nKubernetes API 使你可以在 Kubernetes 中查询和操纵 API 对象\n（例如 Pod、Namespace、ConfigMap 和 Event）的状态。"}
{"en": "Most operations can be performed through the [kubectl](/docs/reference/kubectl/)\ncommand-line interface or other command-line tools, such as\n[kubeadm](/docs/reference/setup-tools/kubeadm/), which in turn use the API.\nHowever, you can also access the API directly using REST calls. Kubernetes\nprovides a set of [client libraries](/docs/reference/using-api/client-libraries/)\nfor those looking to\nwrite applications using the Kubernetes API.", "zh": "大部分操作都可以通过 [kubectl](/zh-cn/docs/reference/kubectl/) 命令行接口或类似\n[kubeadm](/zh-cn/docs/reference/setup-tools/kubeadm/) 这类命令行工具来执行，\n这些工具在背后也是调用 API。不过，你也可以使用 REST 调用来访问这些 API。\nKubernetes 为那些希望使用 Kubernetes API\n编写应用的开发者提供一组[客户端库](/zh-cn/docs/reference/using-api/client-libraries/)。"}
{"en": "Each Kubernetes cluster publishes the specification of the APIs that the cluster serves.\nThere are two mechanisms that Kubernetes uses to publish these API specifications; both are useful\nto enable automatic interoperability. For example, the `kubectl` tool fetches and caches the API\nspecification for enabling command-line completion and other features.\nThe two supported mechanisms are as follows:", "zh": "每个 Kubernetes 集群都会发布集群所使用的 API 规范。\nKubernetes 使用两种机制来发布这些 API 规范；这两种机制都有助于实现自动互操作。\n例如，`kubectl` 工具获取并缓存 API 规范，以实现命令行补全和其他特性。所支持的两种机制如下："}
{"en": "- [The Discovery API](#discovery-api) provides information about the Kubernetes APIs:\n  API names, resources, versions, and supported operations. This is a Kubernetes\n  specific term as it is a separate API from the Kubernetes OpenAPI.\n  It is intended to be a brief summary of the available resources and it does not\n  detail specific schema for the resources. For reference about resource schemas,\n  please refer to the OpenAPI document.", "zh": "- [发现 API](#discovery-api) 提供有关 Kubernetes API 的信息：API 名称、资源、版本和支持的操作。\n  此 API 是特定于 Kubernetes 的一个术语，因为它是一个独立于 Kubernetes OpenAPI 的 API。\n  其目的是为可用的资源提供简要总结，不详细说明资源的具体模式。有关资源模式的参考，请参阅 OpenAPI 文档。"}
{"en": "- The [Kubernetes OpenAPI Document](#openapi-interface-definition) provides (full)\n  [OpenAPI v2.0 and 3.0 schemas](https://www.openapis.org/) for all Kubernetes API\nendpoints.\n  The OpenAPI v3 is the preferred method for accessing OpenAPI as it\nprovides\n  a more comprehensive and accurate view of the API. It includes all the available\n  API paths, as well as all resources consumed and produced for every operations\n  on every endpoints. It also includes any extensibility components that a cluster supports.\n  The data is a complete specification and is significantly larger than that from the\n  Discovery API.", "zh": "- [Kubernetes OpenAPI 文档](#openapi-interface-definition)为所有 Kubernetes API 端点提供（完整的）\n  [OpenAPI v2.0 和 v3.0 模式](https://www.openapis.org/)。OpenAPI v3 是访问 OpenAPI 的首选方法，\n  因为它提供了更全面和准确的 API 视图。其中包括所有可用的 API 路径，以及每个端点上每个操作所接收和生成的所有资源。\n  它还包括集群支持的所有可扩展组件。这些数据是完整的规范，比 Discovery API 提供的规范要大得多。"}
{"en": "## Discovery API\n\nKubernetes publishes a list of all group versions and resources supported via\nthe Discovery API. This includes the following for each resource:", "zh": "## Discovery API\n\nKubernetes 通过 Discovery API 发布集群所支持的所有组版本和资源列表。对于每个资源，包括以下内容："}
{"en": "- Name\n- Cluster or namespaced scope\n- Endpoint URL and supported verbs\n- Alternative names\n- Group, version, kind", "zh": "- 名称\n- 集群作用域还是名字空间作用域\n- 端点 URL 和所支持的动词\n- 别名\n- 组、版本、类别"}
{"en": "The API is available in both aggregated and unaggregated form. The aggregated\ndiscovery serves two endpoints while the unaggregated discovery serves a\nseparate endpoint for each group version.", "zh": "API 以聚合和非聚合形式提供。聚合的发现提供两个端点，而非聚合的发现为每个组版本提供单独的端点。"}
{"en": "### Aggregated discovery", "zh": "### 聚合的发现   {#aggregated-discovery}\n\n{{< feature-state feature_gate_name=\"AggregatedDiscoveryEndpoint\" >}}"}
{"en": "Kubernetes offers beta support for _aggregated discovery_, publishing\nall resources supported by a cluster through two endpoints (`/api` and\n`/apis`). Requesting this\nendpoint drastically reduces the number of requests sent to fetch the\ndiscovery data from the cluster. You can access the data by\nrequesting the respective endpoints with an `Accept` header indicating\nthe aggregated discovery resource:\n`Accept: application/json;v=v2beta1;g=apidiscovery.k8s.io;as=APIGroupDiscoveryList`.", "zh": "Kubernetes 为**聚合的发现**提供了 Beta 支持，通过两个端点（`/api` 和 `/apis`）发布集群所支持的所有资源。\n请求这个端点会大大减少从集群获取发现数据时发送的请求数量。你可以通过带有\n`Accept` 头（`Accept: application/json;v=v2beta1;g=apidiscovery.k8s.io;as=APIGroupDiscoveryList`）\n的请求发送到不同端点，来指明聚合发现的资源。"}
{"en": "Without indicating the resource type using the `Accept` header, the default\nresponse for the `/api` and `/apis` endpoint is an unaggregated discovery\ndocument.", "zh": "如果没有使用 `Accept` 头指示资源类型，对于 `/api` 和 `/apis` 端点的默认响应将是一个非聚合的发现文档。"}
{"en": "The [discovery document](https://github.com/kubernetes/kubernetes/blob/release-{{< skew currentVersion >}}/api/discovery/aggregated_v2beta1.json)\nfor the built-in resources can be found in the Kubernetes GitHub repository.\nThis Github document can be used as a reference of the base set of the available resources\nif a Kubernetes cluster is not available to query.\n\nThe endpoint also supports ETag and protobuf encoding.", "zh": "内置资源的[发现文档](https://github.com/kubernetes/kubernetes/blob/release-{{< skew currentVersion >}}/api/discovery/aggregated_v2beta1.json)可以在\nKubernetes GitHub 代码仓库中找到。如果手头没有 Kubernetes 集群可供查询，\n此 Github 文档可用作可用资源的基础集合的参考。端点还支持 ETag 和 protobuf 编码。"}
{"en": "### Unaggregated discovery\n\nWithout discovery aggregation, discovery is published in levels, with the root\nendpoints publishing discovery information for downstream documents.\n\nA list of all group versions supported by a cluster is published at\nthe `/api` and `/apis` endpoints. Example:", "zh": "### 非聚合的发现   {#unaggregated-discovery}\n\n在不使用聚合发现的情况下，发现 API 以不同级别发布，同时根端点为下游文档发布发现信息。\n\n集群支持的所有组版本列表发布在 `/api` 和 `/apis` 端点。例如：\n\n```\n{\n  \"kind\": \"APIGroupList\",\n  \"apiVersion\": \"v1\",\n  \"groups\": [\n    {\n      \"name\": \"apiregistration.k8s.io\",\n      \"versions\": [\n        {\n          \"groupVersion\": \"apiregistration.k8s.io/v1\",\n          \"version\": \"v1\"\n        }\n      ],\n      \"preferredVersion\": {\n        \"groupVersion\": \"apiregistration.k8s.io/v1\",\n        \"version\": \"v1\"\n      }\n    },\n    {\n      \"name\": \"apps\",\n      \"versions\": [\n        {\n          \"groupVersion\": \"apps/v1\",\n          \"version\": \"v1\"\n        }\n      ],\n      \"preferredVersion\": {\n        \"groupVersion\": \"apps/v1\",\n        \"version\": \"v1\"\n      }\n    },\n    ...\n}\n```"}
{"en": "Additional requests are needed to obtain the discovery document for each group version at\n`/apis/<group>/<version>` (for example:\n`/apis/rbac.authorization.k8s.io/v1alpha1`), which advertises the list of\nresources served under a particular group version. These endpoints are used by\nkubectl to fetch the list of resources supported by a cluster.", "zh": "用户需要发出额外的请求才能在 `/apis/<group>/<version>`（例如 `/apis/rbac.authorization.k8s.io/v1alpha1`）\n获取每个组版本的发现文档。这些发现文档会公布在特定组版本下所提供的资源列表。\nkubectl 使用这些端点来获取某集群所支持的资源列表。"}
{"en": "body", "zh": "<a id=\"#api-specification\" />"}
{"en": "## OpenAPI interface definition\n\nFor details about the OpenAPI specifications, see the [OpenAPI documentation](https://www.openapis.org/).\n\nKubernetes serves both OpenAPI v2.0 and OpenAPI v3.0. OpenAPI v3 is the\npreferred method of accessing the OpenAPI because it offers a more comprehensive\n(lossless) representation of Kubernetes resources. Due to limitations of OpenAPI\nversion 2, certain fields are dropped from the published OpenAPI including but not\nlimited to `default`, `nullable`, `oneOf`.", "zh": "## OpenAPI 接口定义   {#openapi-interface-definition}\n\n有关 OpenAPI 规范的细节，参阅 [OpenAPI 文档](https://www.openapis.org/)。\n\nKubernetes 同时提供 OpenAPI v2.0 和 OpenAPI v3.0。OpenAPI v3 是访问 OpenAPI 的首选方法，\n因为它提供了对 Kubernetes 资源更全面（无损）的表示。由于 OpenAPI v2 的限制，\n所公布的 OpenAPI 中会丢弃掉一些字段，包括但不限于 `default`、`nullable`、`oneOf`。"}
{"en": "### OpenAPI V2\n\nThe Kubernetes API server serves an aggregated OpenAPI v2 spec via the\n`/openapi/v2` endpoint. You can request the response format using\nrequest headers as follows:", "zh": "### OpenAPI v2\n\nKubernetes API 服务器通过 `/openapi/v2` 端点提供聚合的 OpenAPI v2 规范。\n你可以按照下表所给的请求头部，指定响应的格式："}
{"en": "<table>\n  <caption style=\"display:none\">Valid request header values for OpenAPI v2 queries</caption>\n  <thead>\n     <tr>\n        <th>Header</th>\n        <th style=\"min-width: 50%;\">Possible values</th>\n        <th>Notes</th>\n     </tr>\n  </thead>\n  <tbody>\n     <tr>\n        <td><code>Accept-Encoding</code></td>\n        <td><code>gzip</code></td>\n        <td><em>not supplying this header is also acceptable</em></td>\n     </tr>\n     <tr>\n        <td rowspan=\"3\"><code>Accept</code></td>\n        <td><code>application/com.github.proto-openapi.spec.v2@v1.0+protobuf</code></td>\n        <td><em>mainly for intra-cluster use</em></td>\n     </tr>\n     <tr>\n        <td><code>application/json</code></td>\n        <td><em>default</em></td>\n     </tr>\n     <tr>\n        <td><code>*</code></td>\n        <td><em>serves </em><code>application/json</code></td>\n     </tr>\n  </tbody>\n</table>", "zh": "<table>\n  <caption style=\"display:none\">OpenAPI v2 查询请求的合法头部值</caption>\n  <thead>\n     <tr>\n        <th>头部</th>\n        <th style=\"min-width: 50%;\">可选值</th>\n        <th>说明</th>\n     </tr>\n  </thead>\n  <tbody>\n     <tr>\n        <td><code>Accept-Encoding</code></td>\n        <td><code>gzip</code></td>\n        <td><em>不指定此头部也是可以的</em></td>\n     </tr>\n     <tr>\n        <td rowspan=\"3\"><code>Accept</code></td>\n        <td><code>application/com.github.proto-openapi.spec.v2@v1.0+protobuf</code></td>\n        <td><em>主要用于集群内部</em></td>\n     </tr>\n     <tr>\n        <td><code>application/json</code></td>\n        <td><em>默认值</em></td>\n     </tr>\n     <tr>\n        <td><code>*</code></td>\n        <td><em>提供</em><code>application/json</code></td>\n     </tr>\n  </tbody>\n</table>\n\n### OpenAPI v3\n\n{{< feature-state feature_gate_name=\"OpenAPIV3\" >}}"}
{"en": "Kubernetes supports publishing a description of its APIs as OpenAPI v3.", "zh": "Kubernetes 支持将其 API 的描述以 OpenAPI v3 形式发布。"}
{"en": "A discovery endpoint `/openapi/v3` is provided to see a list of all\ngroup/versions available. This endpoint only returns JSON. These\ngroup/versions are provided in the following format:", "zh": "发现端点 `/openapi/v3` 被提供用来查看可用的所有组、版本列表。\n此列表仅返回 JSON。这些组、版本以下面的格式提供：\n\n```yaml\n{\n    \"paths\": {\n        ...,\n        \"api/v1\": {\n            \"serverRelativeURL\": \"/openapi/v3/api/v1?hash=CC0E9BFD992D8C59AEC98A1E2336F899E8318D3CF4C68944C3DEC640AF5AB52D864AC50DAA8D145B3494F75FA3CFF939FCBDDA431DAD3CA79738B297795818CF\"\n        },\n        \"apis/admissionregistration.k8s.io/v1\": {\n            \"serverRelativeURL\": \"/openapi/v3/apis/admissionregistration.k8s.io/v1?hash=E19CC93A116982CE5422FC42B590A8AFAD92CDE9AE4D59B5CAAD568F083AD07946E6CB5817531680BCE6E215C16973CD39003B0425F3477CFD854E89A9DB6597\"\n        },\n        ....\n    }\n}\n```"}
{"en": "The relative URLs are pointing to immutable OpenAPI descriptions, in\norder to improve client-side caching. The proper HTTP caching headers\nare also set by the API server for that purpose (`Expires` to 1 year in\nthe future, and `Cache-Control` to `immutable`). When an obsolete URL is\nused, the API server returns a redirect to the newest URL.", "zh": "为了改进客户端缓存，相对的 URL 会指向不可变的 OpenAPI 描述。\n为了此目的，API 服务器也会设置正确的 HTTP 缓存标头\n（`Expires` 为未来 1 年，和 `Cache-Control` 为 `immutable`）。\n当一个过时的 URL 被使用时，API 服务器会返回一个指向最新 URL 的重定向。"}
{"en": "The Kubernetes API server publishes an OpenAPI v3 spec per Kubernetes\ngroup version at the `/openapi/v3/apis/<group>/<version>?hash=<hash>`\nendpoint.\n\nRefer to the table below for accepted request headers.", "zh": "Kubernetes API 服务器会在端点 `/openapi/v3/apis/<group>/<version>?hash=<hash>`\n发布一个 Kubernetes 组版本的 OpenAPI v3 规范。\n\n请参阅下表了解可接受的请求头部。\n\n<table>\n  <caption style=\"display:none\">"}
{"en": "Valid request header values for OpenAPI v3 queries", "zh": "OpenAPI v3 查询的合法请求头部值</caption>\n  <thead>\n     <tr>\n        <th>"}
{"en": "Header", "zh": "头部</th>\n        <th style=\"min-width: 50%;\">"}
{"en": "Possible values", "zh": "可选值</th>\n        <th>"}
{"en": "Notes", "zh": "说明</th>\n     </tr>\n  </thead>\n  <tbody>\n     <tr>\n        <td><code>Accept-Encoding</code></td>\n        <td><code>gzip</code></td>\n        <td><em>"}
{"en": "not supplying this header is also acceptable", "zh": "不提供此头部也是可接受的</em></td>\n     </tr>\n     <tr>\n        <td rowspan=\"3\"><code>Accept</code></td>\n        <td><code>application/com.github.proto-openapi.spec.v3@v1.0+protobuf</code></td>\n        <td><em>"}
{"en": "mainly for intra-cluster use", "zh": "主要用于集群内部使用</em></td>\n     </tr>\n     <tr>\n        <td><code>application/json</code></td>\n        <td><em>"}
{"en": "default", "zh": "默认</em></td>\n     </tr>\n     <tr>\n        <td><code>*</code></td>\n        <td><em>"}
{"en": "serves", "zh": "以</em> <code>application/json</code> 形式返回</td>\n     </tr>\n  </tbody>\n</table>"}
{"en": "A Golang implementation to fetch the OpenAPI V3 is provided in the package\n[`k8s.io/client-go/openapi3`](https://pkg.go.dev/k8s.io/client-go/openapi3).\n\nKubernetes {{< skew currentVersion >}} publishes\nOpenAPI v2.0 and v3.0; there are no plans to support 3.1 in the near future.", "zh": "[`k8s.io/client-go/openapi3`](https://pkg.go.dev/k8s.io/client-go/openapi3)\n包中提供了获取 OpenAPI v3 的 Golang 实现。\n\nKubernetes {{< skew currentVersion >}} 发布了 OpenAPI v2.0 和 v3.0；\n近期没有支持 v3.1 的计划。"}
{"en": "### Protobuf serialization\n\nKubernetes implements an alternative Protobuf based serialization format that\nis primarily intended for intra-cluster communication. For more information\nabout this format, see the [Kubernetes Protobuf serialization](https://git.k8s.io/design-proposals-archive/api-machinery/protobuf.md)\ndesign proposal and the\nInterface Definition Language (IDL) files for each schema located in the Go\npackages that define the API objects.", "zh": "### Protobuf 序列化   {#protobuf-serialization}\n\nKubernetes 为 API 实现了一种基于 Protobuf 的序列化格式，主要用于集群内部通信。\n关于此格式的详细信息，可参考\n[Kubernetes Protobuf 序列化](https://git.k8s.io/design-proposals-archive/api-machinery/protobuf.md)设计提案。\n每种模式对应的接口描述语言（IDL）位于定义 API 对象的 Go 包中。"}
{"en": "## Persistence\n\nKubernetes stores the serialized state of objects by writing them into\n{{< glossary_tooltip term_id=\"etcd\" >}}.", "zh": "## 持久化   {#persistence}\n\nKubernetes 通过将序列化状态的对象写入到 {{< glossary_tooltip term_id=\"etcd\" >}} 中完成存储操作。"}
{"en": "## API groups and versioning\n\nTo make it easier to eliminate fields or restructure resource representations,\nKubernetes supports multiple API versions, each at a different API path, such\nas `/api/v1` or `/apis/rbac.authorization.k8s.io/v1alpha1`.\n\nVersioning is done at the API level rather than at the resource or field level\nto ensure that the API presents a clear, consistent view of system resources\nand behavior, and to enable controlling access to end-of-life and/or\nexperimental APIs.", "zh": "## API 组和版本控制   {#api-groups-and-versioning}\n\n为了更容易消除字段或重组资源的呈现方式，Kubernetes 支持多个 API 版本，每个版本位于不同的 API 路径，\n例如 `/api/v1` 或 `/apis/rbac.authorization.k8s.io/v1alpha1`。\n\n版本控制是在 API 级别而不是在资源或字段级别完成的，以确保 API 呈现出清晰、一致的系统资源和行为视图，\n并能够控制对生命结束和/或实验性 API 的访问。"}
{"en": "To make it easier to evolve and to extend its API, Kubernetes implements\n[API groups](/docs/reference/using-api/#api-groups) that can be\n[enabled or disabled](/docs/reference/using-api/#enabling-or-disabling).\n\nAPI resources are distinguished by their API group, resource type, namespace\n(for namespaced resources), and name. The API server handles the conversion between\nAPI versions transparently: all the different versions are actually representations\nof the same persisted data. The API server may serve the same underlying data\nthrough multiple API versions.", "zh": "为了更容易演进和扩展其 API，Kubernetes 实现了 [API 组](/zh-cn/docs/reference/using-api/#api-groups)，\n这些 API 组可以被[启用或禁用](/zh-cn/docs/reference/using-api/#enabling-or-disabling)。\n\nAPI 资源通过其 API 组、资源类型、名字空间（用于名字空间作用域的资源）和名称来区分。\nAPI 服务器透明地处理 API 版本之间的转换：所有不同的版本实际上都是相同持久化数据的呈现。\nAPI 服务器可以通过多个 API 版本提供相同的底层数据。"}
{"en": "For example, suppose there are two API versions, `v1` and `v1beta1`, for the same\nresource. If you originally created an object using the `v1beta1` version of its\nAPI, you can later read, update, or delete that object using either the `v1beta1`\nor the `v1` API version, until the `v1beta1` version is deprecated and removed.\nAt that point you can continue accessing and modifying the object using the `v1` API.", "zh": "例如，假设针对相同的资源有两个 API 版本：`v1` 和 `v1beta1`。\n如果你最初使用其 API 的 `v1beta1` 版本创建了一个对象，\n你稍后可以使用 `v1beta1` 或 `v1` API 版本来读取、更新或删除该对象，\n直到 `v1beta1` 版本被废弃和移除为止。此后，你可以使用 `v1` API 继续访问和修改该对象。"}
{"en": "### API changes\n\nAny system that is successful needs to grow and change as new use cases emerge or existing ones change.\nTherefore, Kubernetes has designed the Kubernetes API to continuously change and grow.\nThe Kubernetes project aims to _not_ break compatibility with existing clients, and to maintain that\ncompatibility for a length of time so that other projects have an opportunity to adapt.", "zh": "### API 变更     {#api-changes}\n\n任何成功的系统都要随着新的使用案例的出现和现有案例的变化来成长和变化。\n为此，Kubernetes 已设计了 Kubernetes API 来持续变更和成长。\nKubernetes 项目的目标是**不要**给现有客户端带来兼容性问题，并在一定的时期内维持这种兼容性，\n以便其他项目有机会作出适应性变更。"}
{"en": "In general, new API resources and new resource fields can be added often and frequently.\nElimination of resources or fields requires following the\n[API deprecation policy](/docs/reference/using-api/deprecation-policy/).", "zh": "一般而言，新的 API 资源和新的资源字段可以被频繁地添加进来。\n删除资源或者字段则要遵从\n[API 废弃策略](/zh-cn/docs/reference/using-api/deprecation-policy/)。"}
{"en": "Kubernetes makes a strong commitment to maintain compatibility for official Kubernetes APIs\nonce they reach general availability (GA), typically at API version `v1`. Additionally,\nKubernetes maintains compatibility with data persisted via _beta_ API versions of official Kubernetes APIs,\nand ensures that data can be converted and accessed via GA API versions when the feature goes stable.", "zh": "Kubernetes 对维护达到正式发布（GA）阶段的官方 API 的兼容性有着很强的承诺，通常这一 API 版本为 `v1`。\n此外，Kubernetes 保持与 Kubernetes 官方 API 的 **Beta** API 版本持久化数据的兼容性，\n并确保在该功能特性已进入稳定期时数据可以通过 GA API 版本进行转换和访问。"}
{"en": "If you adopt a beta API version, you will need to transition to a subsequent beta or stable API version\nonce the API graduates. The best time to do this is while the beta API is in its deprecation period,\nsince objects are simultaneously accessible via both API versions. Once the beta API completes its\ndeprecation period and is no longer served, the replacement API version must be used.", "zh": "如果你采用一个 Beta API 版本，一旦该 API 进阶，你将需要转换到后续的 Beta 或稳定的 API 版本。\n执行此操作的最佳时间是 Beta API 处于弃用期，因为此时可以通过两个 API 版本同时访问那些对象。\n一旦 Beta API 结束其弃用期并且不再提供服务，则必须使用替换的 API 版本。\n\n{{< note >}}"}
{"en": "Although Kubernetes also aims to maintain compatibility for _alpha_ APIs versions, in some\ncircumstances this is not possible. If you use any alpha API versions, check the release notes\nfor Kubernetes when upgrading your cluster, in case the API did change in incompatible\nways that require deleting all existing alpha objects prior to upgrade.", "zh": "尽管 Kubernetes 也努力为 **Alpha** API 版本维护兼容性，在有些场合兼容性是无法做到的。\n如果你使用了任何 Alpha API 版本，需要在升级集群时查看 Kubernetes 发布说明，\n如果 API 确实以不兼容的方式发生变更，则需要在升级之前删除所有现有的 Alpha 对象。\n{{< /note >}}"}
{"en": "Refer to [API versions reference](/docs/reference/using-api/#api-versioning)\nfor more details on the API version level definitions.", "zh": "关于 API 版本分级的定义细节，请参阅\n[API 版本参考](/zh-cn/docs/reference/using-api/#api-versioning)页面。"}
{"en": "## API Extension\n\nThe Kubernetes API can be extended in one of two ways:", "zh": "## API 扩展  {#api-extension}\n\n有两种途径来扩展 Kubernetes API："}
{"en": "1. [Custom resources](/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\n   let you declaratively define how the API server should provide your chosen resource API.\n1. You can also extend the Kubernetes API by implementing an\n   [aggregation layer](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/).", "zh": "1. 你可以使用[自定义资源](/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources/)来以声明式方式定义\n   API 服务器如何提供你所选择的资源 API。\n1. 你也可以选择实现自己的[聚合层](/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)来扩展\n   Kubernetes API。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Learn how to extend the Kubernetes API by adding your own\n  [CustomResourceDefinition](/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/).\n- [Controlling Access To The Kubernetes API](/docs/concepts/security/controlling-access/) describes\n  how the cluster manages authentication and authorization for API access.\n- Learn about API endpoints, resource types and samples by reading\n  [API Reference](/docs/reference/kubernetes-api/).\n- Learn about what constitutes a compatible change, and how to change the API, from\n  [API changes](https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#readme).", "zh": "- 了解如何通过添加你自己的\n  [CustomResourceDefinition](/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)\n  来扩展 Kubernetes API。\n- [控制 Kubernetes API 访问](/zh-cn/docs/concepts/security/controlling-access/)页面描述了集群如何针对\n  API 访问管理身份认证和鉴权。\n- 通过阅读 [API 参考](/zh-cn/docs/reference/kubernetes-api/)了解 API 端点、资源类型以及示例。\n- 阅读 [API 变更（英文）](https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#readme)\n  以了解什么是兼容性的变更以及如何变更 API。"}
{"en": "本页面概述了组成 Kubernetes 集群的基本组件。\n\n{{ < figure src=\"/images/docs/components-of-kubernetes.svg\" alt=\"Components of Kubernetes\" caption=\"The components of a Kubernetes cluster\" class=\"diagram-large\" clicktozoom=\"true\" >}}", "zh": "本文档概述了一个正常运行的 Kubernetes 集群所需的各种组件。\n\n{{< figure src=\"/images/docs/components-of-kubernetes.svg\" alt=\"Kubernetes 的组件\" caption=\"Kubernetes 集群的组件\" class=\"diagram-large\" clicktozoom=\"true\" >}}"}
{"en": "## Core Components\n\nA Kubernetes cluster consists of a control plane and one or more worker nodes.\nHere's a brief overview of the main components:", "zh": "## 核心组件\n\nKubernetes 集群由控制平面和一个或多个工作节点组成。以下是主要组件的简要概述："}
{"en": "### Control Plane Components\n\nManage the overall state of the cluster:\n\n[kube-apiserver](/docs/concepts/architecture/#kube-apiserver)\n: The core component server that exposes the Kubernetes HTTP API\n\n[etcd](/docs/concepts/architecture/#etcd)\n: Consistent and highly-available key value store for all API server data\n\n[kube-scheduler](/docs/concepts/architecture/#kube-scheduler)\n: Looks for Pods not yet bound to a node, and assigns each Pod to a suitable node.\n\n[kube-controller-manager](/docs/concepts/architecture/#kube-controller-manager)\n: Runs {{< glossary_tooltip text=\"controllers\" term_id=\"controller\" >}} to implement Kubernetes API behavior.\n\n[cloud-controller-manager](/docs/concepts/architecture/#cloud-controller-manager) (optional)\n: Integrates with underlying cloud provider(s).", "zh": "## 控制平面组件（Control Plane Components）    {#control-plane-components}\n\n管理集群的整体状态：\n\n[kube-apiserver](/zh-cn/docs/concepts/architecture/#kube-apiserver)\n: 公开 Kubernetes HTTP API 的核心组件服务器\n\n[etcd](/zh-cn/docs/concepts/architecture/#etcd)\n: 具备一致性和高可用性的键值存储，用于所有 API 服务器的数据存储\n\n[kube-scheduler](/zh-cn/docs/concepts/architecture/#kube-scheduler)\n: 查找尚未绑定到节点的 Pod，并将每个 Pod 分配给合适的节点。\n\n[kube-controller-manager](/zh-cn/docs/concepts/architecture/#kube-controller-manager)\n: 运行{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}来实现 Kubernetes API 行为。\n\n[cloud-controller-manager](/zh-cn/docs/concepts/architecture/#cloud-controller-manager) (optional)\n: 与底层云驱动集成"}
{"en": "### Node Components\n\nRun on every node, maintaining running pods and providing the Kubernetes runtime environment:\n\n[kubelet](/docs/concepts/architecture/#kubelet)\n: Ensures that Pods are running, including their containers.\n\n[kube-proxy](/docs/concepts/architecture/#kube-proxy) (optional)\n: Maintains network rules on nodes to implement {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}}.\n\n\n[Container runtime](/docs/concepts/architecture/#container-runtime)\n: Software responsible for running containers. Read\n  [Container Runtimes](/docs/setup/production-environment/container-runtimes/) to learn more.", "zh": "## Node 组件  {#node-components}\n\n在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行时环境：\n\n[kubelet](/zh-cn/docs/concepts/architecture/#kubelet)\n: 确保 Pod 及其容器正常运行。\n\n[kube-proxy](/zh-cn/docs/concepts/architecture/#kube-proxy)（可选）\n: 维护节点上的网络规则以实现 Service 的功能。\n\n[容器运行时（Container runtime）](/zh-cn/docs/concepts/architecture/#container-runtime)\n: 负责运行容器的软件，阅读[容器运行时](/zh-cn/docs/setup/production-environment/container-runtimes/)以了解更多信息。\n\n{{% thirdparty-content single=\"true\" %}}"}
{"en": "Your cluster may require additional software on each node; for example, you might also\nrun [systemd](https://systemd.io/) on a Linux node to supervise local components.", "zh": "你的集群可能需要每个节点上运行额外的软件；例如，你可能还需要在 Linux\n节点上运行 [systemd](https://systemd.io/) 来监督本地组件。"}
{"en": "## Addons\n\nAddons extend the functionality of Kubernetes. A few important examples include:", "zh": "## 插件（Addons）    {#addons}\n\n插件扩展了 Kubernetes 的功能。一些重要的例子包括："}
{"en": "[DNS](/docs/concepts/architecture/#dns)\n: For cluster-wide DNS resolution\n\n[Web UI](/docs/concepts/architecture/#web-ui-dashboard) (Dashboard)\n: For cluster management via a web interface\n\n[Container Resource Monitoring](/docs/concepts/architecture/#container-resource-monitoring)\n: For collecting and storing container metrics\n\n[Cluster-level Logging](/docs/concepts/architecture/#cluster-level-logging)\n: For saving container logs to a central log store", "zh": "[DNS](/zh-cn/docs/concepts/architecture/#dns)\n: 集群范围内的 DNS 解析\n\n[Web 界面](/zh-cn/docs/concepts/architecture/#web-ui-dashboard)（Dashboard）\n: 通过 Web 界面进行集群管理\n\n[容器资源监控](/zh-cn/docs/concepts/architecture/#container-resource-monitoring)\n: 用于收集和存储容器指标\n\n[集群层面日志](/zh-cn/docs/concepts/architecture/#cluster-level-logging)\n: 用于将容器日志保存到中央日志存储"}
{"en": "## Flexibility in Architecture\n\nKubernetes allows for flexibility in how these components are deployed and managed.\nThe architecture can be adapted to various needs, from small development environments\nto large-scale production deployments.\n\nFor more detailed information about each component and various ways to configure your\ncluster architecture, see the [Cluster Architecture](/docs/concepts/architecture/) page.", "zh": "## 架构灵活性    {#flexibility-in-architecture}\n\nKubernetes 允许灵活地部署和管理这些组件。此架构可以适应各种需求，从小型开发环境到大规模生产部署。\n\n有关每个组件的详细信息以及配置集群架构的各种方法，\n请参阅[集群架构](/zh-cn/docs/concepts/architecture/)页面。"}
{"en": "This page is an overview of Kubernetes.", "zh": "此页面是 Kubernetes 的概述。"}
{"en": "The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation\nresults from counting the eight letters between the \"K\" and the \"s\". Google open-sourced the\nKubernetes project in 2014. Kubernetes combines\n[over 15 years of Google's experience](/blog/2015/04/borg-predecessor-to-kubernetes/) running\nproduction workloads at scale with best-of-breed ideas and practices from the community.", "zh": "**Kubernetes** 这个名字源于希腊语，意为“舵手”或“飞行员”。K8s 这个缩写是因为 K 和 s 之间有 8 个字符的关系。\nGoogle 在 2014 年开源了 Kubernetes 项目。\nKubernetes 建立在 [Google 大规模运行生产工作负载十几年经验](https://research.google/pubs/pub43438)的基础上，\n结合了社区中最优秀的想法和实践。"}
{"en": "## Why you need Kubernetes and what it can do {#why-you-need-kubernetes-and-what-can-it-do}", "zh": "## 为什么需要 Kubernetes，它能做什么？   {#why-you-need-kubernetes-and-what-can-it-do}"}
{"en": "Containers are a good way to bundle and run your applications. In a production\nenvironment, you need to manage the containers that run the applications and\nensure that there is no downtime. For example, if a container goes down, another\ncontainer needs to start. Wouldn't it be easier if this behavior was handled by a system?", "zh": "容器是打包和运行应用程序的好方式。在生产环境中，\n你需要管理运行着应用程序的容器，并确保服务不会下线。\n例如，如果一个容器发生故障，则你需要启动另一个容器。\n如果此行为交由给系统处理，是不是会更容易一些？"}
{"en": "That's how Kubernetes comes to the rescue! Kubernetes provides you with a framework\nto run distributed systems resiliently. It takes care of scaling and failover for\nyour application, provides deployment patterns, and more. For example: Kubernetes\ncan easily manage a canary deployment for your system.", "zh": "这就是 Kubernetes 要来做的事情！\nKubernetes 为你提供了一个可弹性运行分布式系统的框架。\nKubernetes 会满足你的扩展要求、故障转移你的应用、提供部署模式等。\n例如，Kubernetes 可以轻松管理系统的 Canary (金丝雀) 部署。"}
{"en": "Kubernetes provides you with:", "zh": "Kubernetes 为你提供："}
{"en": "* **Service discovery and load balancing**\n  Kubernetes can expose a container using the DNS name or using their own IP address.\n  If traffic to a container is high, Kubernetes is able to load balance and distribute\n  the network traffic so that the deployment is stable.", "zh": "* **服务发现和负载均衡**\n\n  Kubernetes 可以使用 DNS 名称或自己的 IP 地址来暴露容器。\n  如果进入容器的流量很大，\n  Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。"}
{"en": "* **Storage orchestration**\n  Kubernetes allows you to automatically mount a storage system of your choice, such as\n  local storages, public cloud providers, and more.", "zh": "* **存储编排**\n\n  Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。"}
{"en": "* **Automated rollouts and rollbacks**\n  You can describe the desired state for your deployed containers using Kubernetes,\n  and it can change the actual state to the desired state at a controlled rate.\n  For example, you can automate Kubernetes to create new containers for your\n  deployment, remove existing containers and adopt all their resources to the new container.", "zh": "* **自动部署和回滚**\n\n  你可以使用 Kubernetes 描述已部署容器的所需状态，\n  它可以以受控的速率将实际状态更改为期望状态。\n  例如，你可以自动化 Kubernetes 来为你的部署创建新容器，\n  删除现有容器并将它们的所有资源用于新容器。"}
{"en": "* **Automatic bin packing**\n  You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks.\n  You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit\n  containers onto your nodes to make the best use of your resources.", "zh": "* **自动完成装箱计算**\n\n  你为 Kubernetes 提供许多节点组成的集群，在这个集群上运行容器化的任务。\n  你告诉 Kubernetes 每个容器需要多少 CPU 和内存 (RAM)。\n  Kubernetes 可以将这些容器按实际情况调度到你的节点上，以最佳方式利用你的资源。"}
{"en": "* **Self-healing**\n  Kubernetes restarts containers that fail, replaces containers, kills containers that don't\n  respond to your user-defined health check, and doesn't advertise them to clients until they\n  are ready to serve.", "zh": "* **自我修复**\n\n  Kubernetes 将重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器，\n  并且在准备好服务之前不将其通告给客户端。"}
{"en": "* **Secret and configuration management**\n  Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens,\n  and SSH keys. You can deploy and update secrets and application configuration without\n  rebuilding your container images, and without exposing secrets in your stack configuration.", "zh": "* **密钥与配置管理**\n\n  Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 SSH 密钥。\n  你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。"}
{"en": "* **Batch execution**\n  In addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.\n* **Horizontal scaling**\n  Scale your application up and down with a simple command, with a UI, or automatically based on CPU usage.\n* **IPv4/IPv6 dual-stack**\n  Allocation of IPv4 and IPv6 addresses to Pods and Services\n* **Designed for extensibility**\n  Add features to your Kubernetes cluster without changing upstream source code.", "zh": "* **批处理执行**\n  除了服务外，Kubernetes 还可以管理你的批处理和 CI（持续集成）工作负载，如有需要，可以替换失败的容器。\n* **水平扩缩**\n  使用简单的命令、用户界面或根据 CPU 使用率自动对你的应用进行扩缩。\n* **IPv4/IPv6 双栈**\n  为 Pod（容器组）和 Service（服务）分配 IPv4 和 IPv6 地址。\n* **为可扩展性设计**\n  在不改变上游源代码的情况下为你的 Kubernetes 集群添加功能。"}
{"en": "## What Kubernetes is not", "zh": "## Kubernetes 不是什么   {#what-kubernetes-is-not}"}
{"en": "Kubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system.\nSince Kubernetes operates at the container level rather than at the hardware level,\nit provides some generally applicable features common to PaaS offerings, such as\ndeployment, scaling, load balancing, and lets users integrate their logging, monitoring,\nand alerting solutions. However, Kubernetes is not monolithic, and these default solutions\nare optional and pluggable. Kubernetes provides the building blocks for building developer\nplatforms, but preserves user choice and flexibility where it is important.", "zh": "Kubernetes 不是传统的、包罗万象的 PaaS（平台即服务）系统。\n由于 Kubernetes 是在容器级别运行，而非在硬件级别，它提供了 PaaS 产品共有的一些普遍适用的功能，\n例如部署、扩展、负载均衡，允许用户集成他们的日志记录、监控和警报方案。\n但是，Kubernetes 不是单体式（monolithic）系统，那些默认解决方案都是可选、可插拔的。\nKubernetes 为构建开发人员平台提供了基础，但是在重要的地方保留了用户选择权，能有更高的灵活性。"}
{"en": "Kubernetes:", "zh": "Kubernetes："}
{"en": "* Does not limit the types of applications supported. Kubernetes aims to support an\n  extremely diverse variety of workloads, including stateless, stateful, and data-processing\n  workloads. If an application can run in a container, it should run great on Kubernetes.\n* Does not deploy source code and does not build your application. Continuous Integration,\n  Delivery, and Deployment (CI/CD) workflows are determined by organization cultures and\n  preferences as well as technical requirements.\n* Does not provide application-level services, such as middleware (for example, message buses),\n  data-processing frameworks (for example, Spark), databases (for example, MySQL), caches, nor\n  cluster storage systems (for example, Ceph) as built-in services. Such components can run on\n  Kubernetes, and/or can be accessed by applications running on Kubernetes through portable\n  mechanisms, such as the [Open Service Broker](https://openservicebrokerapi.org/).", "zh": "* 不限制支持的应用程序类型。\n  Kubernetes 旨在支持极其多种多样的工作负载，包括无状态、有状态和数据处理工作负载。\n  如果应用程序可以在容器中运行，那么它应该可以在 Kubernetes 上很好地运行。\n* 不部署源代码，也不构建你的应用程序。\n  持续集成（CI）、交付和部署（CI/CD）工作流取决于组织的文化和偏好以及技术要求。\n* 不提供应用程序级别的服务作为内置服务，例如中间件（例如消息中间件）、\n  数据处理框架（例如 Spark）、数据库（例如 MySQL）、缓存、集群存储系统\n  （例如 Ceph）。这样的组件可以在 Kubernetes 上运行，并且/或者可以由运行在\n  Kubernetes 上的应用程序通过可移植机制（例如[开放服务代理](https://openservicebrokerapi.org/)）来访问。"}
{"en": "* Does not dictate logging, monitoring, or alerting solutions. It provides some integrations\n  as proof of concept, and mechanisms to collect and export metrics.\n* Does not provide nor mandate a configuration language/system (for example, Jsonnet). It provides\n  a declarative API that may be targeted by arbitrary forms of declarative specifications.\n* Does not provide nor adopt any comprehensive machine configuration, maintenance, management,\n  or self-healing systems.\n* Additionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need\n  for orchestration. The technical definition of orchestration is execution of a defined workflow:\n  first do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable\n  control processes that continuously drive the current state towards the provided desired state.\n  It shouldn't matter how you get from A to C. Centralized control is also not required. This\n  results in a system that is easier to use and more powerful, robust, resilient, and extensible.", "zh": "* 不是日志记录、监视或警报的解决方案。\n  它集成了一些功能作为概念证明，并提供了收集和导出指标的机制。\n* 不提供也不要求配置用的语言、系统（例如 jsonnet），它提供了声明性 API，\n  该声明性 API 可以由任意形式的声明性规范所构成。\n* 不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。\n* 此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。\n  编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。\n  而 Kubernetes 包含了一组独立可组合的控制过程，可以持续地将当前状态驱动到所提供的预期状态。\n  你不需要在乎如何从 A 移动到 C，也不需要集中控制，这使得系统更易于使用且功能更强大、\n  系统更健壮，更为弹性和可扩展。"}
{"en": "## Historical context for Kubernetes {#going-back-in-time}\n\nLet's take a look at why Kubernetes is so useful by going back in time.", "zh": "## Kubernetes 的历史背景   {#going-back-in-time}\n\n让我们回顾一下为何 Kubernetes 能够裨益四方。"}
{"en": "![Deployment evolution](/images/docs/Container_Evolution.svg)", "zh": "![部署演进](/zh-cn/docs/images/Container_Evolution.svg)"}
{"en": "**Traditional deployment era:**\n\nEarly on, organizations ran applications on physical servers. There was no way to define\nresource boundaries for applications in a physical server, and this caused resource\nallocation issues. For example, if multiple applications run on a physical server, there\ncan be instances where one application would take up most of the resources, and as a result,\nthe other applications would underperform. A solution for this would be to run each application\non a different physical server. But this did not scale as resources were underutilized, and it\nwas expensive for organizations to maintain many physical servers.", "zh": "**传统部署时代：**\n\n早期，各个组织是在物理服务器上运行应用程序。\n由于无法限制在物理服务器中运行的应用程序资源使用，因此会导致资源分配问题。\n例如，如果在同一台物理服务器上运行多个应用程序，\n则可能会出现一个应用程序占用大部分资源的情况，而导致其他应用程序的性能下降。\n一种解决方案是将每个应用程序都运行在不同的物理服务器上，\n但是当某个应用程序资源利用率不高时，剩余资源无法被分配给其他应用程序，\n而且维护许多物理服务器的成本很高。"}
{"en": "**Virtualized deployment era:**\n\nAs a solution, virtualization was introduced. It allows you\nto run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization\nallows applications to be isolated between VMs and provides a level of security as the\ninformation of one application cannot be freely accessed by another application.", "zh": "**虚拟化部署时代：**\n\n因此，虚拟化技术被引入了。虚拟化技术允许你在单个物理服务器的 CPU 上运行多台虚拟机（VM）。\n虚拟化能使应用程序在不同 VM 之间被彼此隔离，且能提供一定程度的安全性，\n因为一个应用程序的信息不能被另一应用程序随意访问。"}
{"en": "Virtualization allows better utilization of resources in a physical server and allows\nbetter scalability because an application can be added or updated easily, reduces\nhardware costs, and much more. With virtualization you can present a set of physical\nresources as a cluster of disposable virtual machines.\n\nEach VM is a full machine running all the components, including its own operating\nsystem, on top of the virtualized hardware.", "zh": "虚拟化技术能够更好地利用物理服务器的资源，并且因为可轻松地添加或更新应用程序，\n而因此可以具有更高的可扩缩性，以及降低硬件成本等等的好处。\n通过虚拟化，你可以将一组物理资源呈现为可丢弃的虚拟机集群。\n\n每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。"}
{"en": "**Container deployment era:**\n\nContainers are similar to VMs, but they have relaxed\nisolation properties to share the Operating System (OS) among the applications.\nTherefore, containers are considered lightweight. Similar to a VM, a container\nhas its own filesystem, share of CPU, memory, process space, and more. As they\nare decoupled from the underlying infrastructure, they are portable across clouds\nand OS distributions.", "zh": "**容器部署时代：**\n\n容器类似于 VM，但是更宽松的隔离特性，使容器之间可以共享操作系统（OS）。\n因此，容器比起 VM 被认为是更轻量级的。且与 VM 类似，每个容器都具有自己的文件系统、CPU、内存、进程空间等。\n由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。"}
{"en": "Containers have become popular because they provide extra benefits, such as:", "zh": "容器因具有许多优势而变得流行起来，例如："}
{"en": "* Agile application creation and deployment: increased ease and efficiency of\n  container image creation compared to VM image use.\n* Continuous development, integration, and deployment: provides for reliable\n  and frequent container image build and deployment with quick and efficient\n  rollbacks (due to image immutability).\n* Dev and Ops separation of concerns: create application container images at\n  build/release time rather than deployment time, thereby decoupling\n  applications from infrastructure.\n* Observability: not only surfaces OS-level information and metrics, but also\n  application health and other signals.", "zh": "* 敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。\n* 持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性），\n  提供可靠且频繁的容器镜像构建和部署。\n* 关注开发与运维的分离：在构建、发布时创建应用程序容器镜像，而不是在部署时，\n  从而将应用程序与基础架构分离。\n* 可观察性：不仅可以显示 OS 级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。"}
{"en": "* Environmental consistency across development, testing, and production: runs\n  the same on a laptop as it does in the cloud.\n* Cloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises,\n  on major public clouds, and anywhere else.\n* Application-centric management: raises the level of abstraction from running an\n  OS on virtual hardware to running an application on an OS using logical resources.\n* Loosely coupled, distributed, elastic, liberated micro-services: applications are\n  broken into smaller, independent pieces and can be deployed and managed dynamically –\n  not a monolithic stack running on one big single-purpose machine.\n* Resource isolation: predictable application performance.\n* Resource utilization: high efficiency and density.", "zh": "* 跨开发、测试和生产的环境一致性：在笔记本计算机上也可以和在云中运行一样的应用程序。\n* 跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、\n  Google Kubernetes Engine 和其他任何地方运行。\n* 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。\n* 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分，\n  并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。\n* 资源隔离：可预测的应用程序性能。\n* 资源利用：高效率和高密度。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Take a look at the [Kubernetes Components](/docs/concepts/overview/components/)\n* Take a look at the [The Kubernetes API](/docs/concepts/overview/kubernetes-api/)\n* Take a look at the [Cluster Architecture](/docs/concepts/architecture/)\n* Ready to [Get Started](/docs/setup/)?", "zh": "* 查阅 [Kubernetes 组件](/zh-cn/docs/concepts/overview/components/)\n* 查阅 [Kubernetes API](/zh-cn/docs/concepts/overview/kubernetes-api/)\n* 查阅 [Cluster 架构](/zh-cn/docs/concepts/architecture/)\n* 开始 [Kubernetes 的建置](/zh-cn/docs/setup/)吧！"}
{"en": "Each {{< glossary_tooltip text=\"object\" term_id=\"object\" >}} in your cluster has a [_Name_](#names) that is unique for that type of resource.\nEvery Kubernetes object also has a [_UID_](#uids) that is unique across your whole cluster.\n\nFor example, you can only have one Pod named `myapp-1234` within the same [namespace](/docs/concepts/overview/working-with-objects/namespaces/), but you can have one Pod and one Deployment that are each named `myapp-1234`.", "zh": "集群中的每一个{{< glossary_tooltip text=\"对象\" term_id=\"object\" >}}都有一个[**名称**](#names)来标识在同类资源中的唯一性。\n\n每个 Kubernetes 对象也有一个 [**UID**](#uids) 来标识在整个集群中的唯一性。\n\n比如，在同一个[名字空间](/zh-cn/docs/concepts/overview/working-with-objects/namespaces/)\n中只能有一个名为 `myapp-1234` 的 Pod，但是可以命名一个 Pod 和一个 Deployment 同为 `myapp-1234`。"}
{"en": "For non-unique user-provided attributes, Kubernetes provides [labels](/docs/concepts/overview/working-with-objects/labels/) and [annotations](/docs/concepts/overview/working-with-objects/annotations/).", "zh": "对于用户提供的非唯一性的属性，Kubernetes\n提供了[标签（Label）](/zh-cn/docs/concepts/overview/working-with-objects/labels/)和\n[注解（Annotation）](/zh-cn/docs/concepts/overview/working-with-objects/annotations/)机制。"}
{"en": "## Names", "zh": "## 名称  {#names}\n\n{{< glossary_definition term_id=\"name\" length=\"all\" >}}"}
{"en": "**Names must be unique across all [API versions](/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning)\nof the same resource. API resources are distinguished by their API group, resource type, namespace\n(for namespaced resources), and name. In other words, API version is irrelevant in this context.**", "zh": "**名称在同一资源的所有\n[API 版本](/zh-cn/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning)中必须是唯一的。\n这些 API 资源通过各自的 API 组、资源类型、名字空间（对于划分名字空间的资源）和名称来区分。\n换言之，API 版本在此上下文中是不相关的。**\n\n{{< note >}}"}
{"en": "In cases when objects represent a physical entity, like a Node representing a physical host, when the host is re-created under the same name without deleting and re-creating the Node, Kubernetes treats the new host as the old one, which may lead to inconsistencies.", "zh": "当对象所代表的是一个物理实体（例如代表一台物理主机的 Node）时，\n如果在 Node 对象未被删除并重建的条件下，重新创建了同名的物理主机，\n则 Kubernetes 会将新的主机看作是老的主机，这可能会带来某种不一致性。\n{{< /note >}}"}
{"en": "Below are four types of commonly used name constraints for resources.", "zh": "以下是比较常见的四种资源命名约束。"}
{"en": "### DNS Subdomain Names\n\nMost resource types require a name that can be used as a DNS subdomain name\nas defined in [RFC 1123](https://tools.ietf.org/html/rfc1123).\nThis means the name must:\n\n- contain no more than 253 characters\n- contain only lowercase alphanumeric characters, '-' or '.'\n- start with an alphanumeric character\n- end with an alphanumeric character", "zh": "### DNS 子域名  {#dns-subdomain-names}\n\n很多资源类型需要可以用作 DNS 子域名的名称。\nDNS 子域名的定义可参见 [RFC 1123](https://tools.ietf.org/html/rfc1123)。\n这一要求意味着名称必须满足如下规则：\n\n- 不能超过 253 个字符\n- 只能包含小写字母、数字，以及 '-' 和 '.'\n- 必须以字母数字开头\n- 必须以字母数字结尾"}
{"en": "### RFC 1123 Label Names {#dns-label-names}\n\nSome resource types require their names to follow the DNS\nlabel standard as defined in [RFC 1123](https://tools.ietf.org/html/rfc1123).\nThis means the name must:\n\n- contain at most 63 characters\n- contain only lowercase alphanumeric characters or '-'\n- start with an alphanumeric character\n- end with an alphanumeric character", "zh": "### RFC 1123 标签名    {#dns-label-names}\n\n某些资源类型需要其名称遵循 [RFC 1123](https://tools.ietf.org/html/rfc1123)\n所定义的 DNS 标签标准。也就是命名必须满足如下规则：\n\n- 最多 63 个字符\n- 只能包含小写字母、数字，以及 '-'\n- 必须以字母数字开头\n- 必须以字母数字结尾"}
{"en": "### RFC 1035 Label Names\n\nSome resource types require their names to follow the DNS\nlabel standard as defined in [RFC 1035](https://tools.ietf.org/html/rfc1035).\nThis means the name must:\n\n- contain at most 63 characters\n- contain only lowercase alphanumeric characters or '-'\n- start with an alphabetic character\n- end with an alphanumeric character", "zh": "### RFC 1035 标签名   {#rfc-1035-label-names}\n\n某些资源类型需要其名称遵循 [RFC 1035](https://tools.ietf.org/html/rfc1035)\n所定义的 DNS 标签标准。也就是命名必须满足如下规则：\n\n- 最多 63 个字符\n- 只能包含小写字母、数字，以及 '-'\n- 必须以字母开头\n- 必须以字母数字结尾\n\n{{< note >}}"}
{"en": "The only difference between the RFC 1035 and RFC 1123\nlabel standards is that RFC 1123 labels are allowed to\nstart with a digit, whereas RFC 1035 labels can start\nwith a lowercase alphabetic character only.", "zh": "RFC 1035 和 RFC 1123 标签标准之间的唯一区别是 RFC 1123\n标签允许以数字开头，而 RFC 1035 标签只能以小写字母字符开头。\n{{< /note >}}"}
{"en": "### Path Segment Names\n\nSome resource types require their names to be able to be safely encoded as a\npath segment. In other words, the name may not be \".\" or \"..\" and the name may\nnot contain \"/\" or \"%\".", "zh": "### 路径分段名称    {#path-segment-names}\n\n某些资源类型要求名称能被安全地用作路径中的片段。\n换句话说，其名称不能是 `.`、`..`，也不可以包含 `/` 或 `%` 这些字符。"}
{"en": "Here's an example manifest for a Pod named `nginx-demo`.", "zh": "下面是一个名为 `nginx-demo` 的 Pod 的配置清单：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-demo\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.2\n    ports:\n    - containerPort: 80\n```\n\n{{< note >}}"}
{"en": "Some resource types have additional restrictions on their names.", "zh": "某些资源类型可能具有额外的命名约束。\n{{< /note >}}\n\n## UID\n\n{{< glossary_definition term_id=\"uid\" length=\"all\" >}}"}
{"en": "Kubernetes UIDs are universally unique identifiers (also known as UUIDs).\nUUIDs are standardized as ISO/IEC 9834-8 and as ITU-T X.667.", "zh": "Kubernetes UID 是全局唯一标识符（也叫 UUID）。\nUUID 是标准化的，见 ISO/IEC 9834-8 和 ITU-T X.667。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about [labels](/docs/concepts/overview/working-with-objects/labels/) and [annotations](/docs/concepts/overview/working-with-objects/annotations/) in Kubernetes.\n* See the [Identifiers and Names in Kubernetes](https://git.k8s.io/design-proposals-archive/architecture/identifiers.md) design document.", "zh": "* 进一步了解 Kubernetes [标签](/zh-cn/docs/concepts/overview/working-with-objects/labels/)和[注解](/zh-cn/docs/concepts/overview/working-with-objects/annotations/)。\n* 参阅 [Kubernetes 标识符和名称](https://git.k8s.io/design-proposals-archive/architecture/identifiers.md)的设计文档。"}
{"en": "_Field selectors_ let you [select Kubernetes resources](/docs/concepts/overview/working-with-objects/kubernetes-objects) based on the value of one or more resource fields. Here are some examples of field selector queries:\n\n_Field selectors_ let you select Kubernetes {{< glossary_tooltip text=\"objects\" term_id=\"object\" >}} based on the\nvalue of one or more resource fields. Here are some examples of field selector queries:", "zh": "“字段选择器（Field selectors）”允许你根据一个或多个资源字段的值筛选\nKubernetes {{< glossary_tooltip text=\"对象\" term_id=\"object\" >}}。\n下面是一些使用字段选择器查询的例子：\n\n* `metadata.name=my-service`\n* `metadata.namespace!=default`\n* `status.phase=Pending`"}
{"en": "This `kubectl` command selects all Pods for which the value of the [`status.phase`](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase) field is `Running`:", "zh": "下面这个 `kubectl` 命令将筛选出\n[`status.phase`](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase)\n字段值为 `Running` 的所有 Pod：\n\n```shell\nkubectl get pods --field-selector status.phase=Running\n```\n\n{{< note >}}"}
{"en": "Field selectors are essentially resource *filters*. By default, no selectors/filters are applied, meaning that all resources of the specified type are selected. This makes the `kubectl` queries `kubectl get pods` and `kubectl get pods --field-selector \"\"` equivalent.", "zh": "字段选择器本质上是资源“过滤器（Filters）”。默认情况下，字段选择器/过滤器是未被应用的，\n这意味着指定类型的所有资源都会被筛选出来。\n这使得 `kubectl get pods` 和 `kubectl get pods --field-selector \"\"`\n这两个 `kubectl` 查询是等价的。\n\n{{< /note >}}"}
{"en": "## Supported fields\n\nSupported field selectors vary by Kubernetes resource type. All resource types support the `metadata.name` and `metadata.namespace` fields. Using unsupported field selectors produces an error. For example:", "zh": "## 支持的字段  {#supported-fields}\n\n不同的 Kubernetes 资源类型支持不同的字段选择器。\n所有资源类型都支持 `metadata.name` 和 `metadata.namespace` 字段。\n使用不被支持的字段选择器会产生错误。例如：\n\n```shell\nkubectl get ingress --field-selector foo.bar=baz\n```\n\n```\nError from server (BadRequest): Unable to find \"ingresses\" that match label selector \"\", field selector \"foo.bar=baz\": \"foo.bar\" is not a known field selector: only \"metadata.name\", \"metadata.namespace\"\n```"}
{"en": "### List of supported fields\n\n| Kind                      | Fields                                                                                                                                                                                                                                                          |", "zh": "### 支持字段列表\n\n| 类别                       | 字段                                                                                                                                                                                                                                                            |\n| ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Pod                       | `spec.nodeName`<br>`spec.restartPolicy`<br>`spec.schedulerName`<br>`spec.serviceAccountName`<br>`spec.hostNetwork`<br>`status.phase`<br>`status.podIP`<br>`status.nominatedNodeName`                                                                            |\n| Event                     | `involvedObject.kind`<br>`involvedObject.namespace`<br>`involvedObject.name`<br>`involvedObject.uid`<br>`involvedObject.apiVersion`<br>`involvedObject.resourceVersion`<br>`involvedObject.fieldPath`<br>`reason`<br>`reportingComponent`<br>`source`<br>`type` |\n| Secret                    | `type`                                                                                                                                                                                                                                                          |\n| Namespace                 | `status.phase`                                                                                                                                                                                                                                                  |\n| ReplicaSet                | `status.replicas`                                                                                                                                                                                                                                               |\n| ReplicationController     | `status.replicas`                                                                                                                                                                                                                                               |\n| Job                       | `status.successful`                                                                                                                                                                                                                                             |\n| Node                      | `spec.unschedulable`                                                                                                                                                                                                                                            |\n| CertificateSigningRequest | `spec.signerName`                                                                                                                                                                                                                                               |"}
{"en": "## Supported operators\n\nYou can use the `=`, `==`, and `!=` operators with field selectors (`=` and `==` mean the same thing). This `kubectl` command, for example, selects all Kubernetes Services that aren't in the `default` namespace:", "zh": "## 支持的操作符   {#supported-operators}\n\n你可在字段选择器中使用 `=`、`==` 和 `!=`（`=` 和 `==` 的意义是相同的）操作符。\n例如，下面这个 `kubectl` 命令将筛选所有不属于 `default` 命名空间的 Kubernetes 服务：\n\n```shell\nkubectl get services  --all-namespaces --field-selector metadata.namespace!=default\n```\n\n{{< note >}}"}
{"en": "[Set-based operators](/docs/concepts/overview/working-with-objects/labels/#set-based-requirement)\n(`in`, `notin`, `exists`) are not supported for field selectors.", "zh": "[基于集合的操作符](/zh-cn/docs/concepts/overview/working-with-objects/labels/#set-based-requirement)\n（`in`、`notin`、`exists`）不支持字段选择算符。\n{{< /note >}}"}
{"en": "## Chained selectors\n\nAs with [label](/docs/concepts/overview/working-with-objects/labels) and other selectors, field selectors can be chained together as a comma-separated list. This `kubectl` command selects all Pods for which the `status.phase` does not equal `Running` and the `spec.restartPolicy` field equals `Always`:", "zh": "## 链式选择器   {#chained-selectors}\n\n同[标签](/zh-cn/docs/concepts/overview/working-with-objects/labels/)和其他选择器一样，\n字段选择器可以通过使用逗号分隔的列表组成一个选择链。\n下面这个 `kubectl` 命令将筛选 `status.phase` 字段不等于 `Running` 同时\n`spec.restartPolicy` 字段等于 `Always` 的所有 Pod：\n\n```shell\nkubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always\n```"}
{"en": "## Multiple resource types\n\nYou can use field selectors across multiple resource types. This `kubectl` command selects all Statefulsets and Services that are not in the `default` namespace:", "zh": "## 多种资源类型   {#multiple-resource-types}\n\n你能够跨多种资源类型来使用字段选择器。\n下面这个 `kubectl` 命令将筛选出所有不在 `default` 命名空间中的 StatefulSet 和 Service：\n\n```shell\nkubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default\n```"}
{"en": "overview", "zh": "{{<glossary_definition term_id=\"finalizer\" length=\"long\">}}"}
{"en": "You can use finalizers to control {{<glossary_tooltip text=\"garbage collection\" term_id=\"garbage-collection\">}}\nof {{< glossary_tooltip text=\"objects\" term_id=\"object\" >}} by alerting {{<glossary_tooltip text=\"controllers\" term_id=\"controller\">}}\nto perform specific cleanup tasks before deleting the target resource.", "zh": "你可以使用 Finalizers 来控制{{< glossary_tooltip text=\"对象\" term_id=\"object\" >}}的{{<glossary_tooltip text=\"垃圾回收\" term_id=\"garbage-collection\">}}，\n方法是在删除目标资源之前提醒{{<glossary_tooltip text=\"控制器\" term_id=\"controller\">}}执行特定的清理任务。"}
{"en": "Finalizers don't usually specify the code to execute. Instead, they are\ntypically lists of keys on a specific resource similar to annotations.\nKubernetes specifies some finalizers automatically, but you can also specify\nyour own.", "zh": "Finalizers 通常不指定要执行的代码。\n相反，它们通常是特定资源上的键的列表，类似于注解。\nKubernetes 自动指定了一些 Finalizers，但你也可以指定你自己的。"}
{"en": "## How finalizers work\n\nWhen you create a resource using a manifest file, you can specify finalizers in\nthe `metadata.finalizers` field. When you attempt to delete the resource, the\nAPI server handling the delete request notices the values in the `finalizers` field\nand does the following: \n\n  * Modifies the object to add a `metadata.deletionTimestamp` field with the\n    time you started the deletion.\n  * Prevents the object from being removed until its `metadata.finalizers` field is empty.\n  * Prevents the object from being removed until all items are removed from its `metadata.finalizers` field\n  * Returns a `202` status code (HTTP \"Accepted\")", "zh": "## Finalizers 如何工作   {#how-finalizers-work}\n\n当你使用清单文件创建资源时，你可以在 `metadata.finalizers` 字段指定 Finalizers。\n当你试图删除该资源时，处理删除请求的 API 服务器会注意到 `finalizers` 字段中的值，\n并进行以下操作：\n\n  * 修改对象，将你开始执行删除的时间添加到 `metadata.deletionTimestamp` 字段。\n  * 禁止对象被删除，直到其 `metadata.finalizers` 字段内的所有项被删除。\n  * 返回 `202` 状态码（HTTP \"Accepted\"）。"}
{"en": "The controller managing that finalizer notices the update to the object setting the\n`metadata.deletionTimestamp`, indicating deletion of the object has been requested.\nThe controller then attempts to satisfy the requirements of the finalizers\nspecified for that resource. Each time a finalizer condition is satisfied, the\ncontroller removes that key from the resource's `finalizers` field. When the\n`finalizers` field is emptied, an object with a `deletionTimestamp` field set\nis automatically deleted. You can also use finalizers to prevent deletion of unmanaged resources.", "zh": "管理 finalizer 的控制器注意到对象上发生的更新操作，对象的 `metadata.deletionTimestamp`\n被设置，意味着已经请求删除该对象。然后，控制器会试图满足资源的 Finalizers 的条件。\n每当一个 Finalizer 的条件被满足时，控制器就会从资源的 `finalizers` 字段中删除该键。\n当 `finalizers` 字段为空时，`deletionTimestamp` 字段被设置的对象会被自动删除。\n你也可以使用 Finalizers 来阻止删除未被管理的资源。"}
{"en": "A common example of a finalizer is `kubernetes.io/pv-protection`, which prevents\naccidental deletion of `PersistentVolume` objects. When a `PersistentVolume`\nobject is in use by a Pod, Kubernetes adds the `pv-protection` finalizer. If you\ntry to delete the `PersistentVolume`, it enters a `Terminating` status, but the\ncontroller can't delete it because the finalizer exists. When the Pod stops\nusing the `PersistentVolume`, Kubernetes clears the `pv-protection` finalizer,\nand the controller deletes the volume.", "zh": "一个常见的 Finalizer 的例子是 `kubernetes.io/pv-protection`，\n它用来防止意外删除 `PersistentVolume` 对象。\n当一个 `PersistentVolume` 对象被 Pod 使用时，\nKubernetes 会添加 `pv-protection` Finalizer。\n如果你试图删除 `PersistentVolume`，它将进入 `Terminating` 状态，\n但是控制器因为该 Finalizer 存在而无法删除该资源。\n当 Pod 停止使用 `PersistentVolume` 时，\nKubernetes 清除 `pv-protection` Finalizer，控制器就会删除该卷。\n\n{{<note>}}"}
{"en": "* When you `DELETE` an object, Kubernetes adds the deletion timestamp for that object and then\nimmediately starts to restrict changes to the `.metadata.finalizers` field for the object that is\nnow pending deletion. You can remove existing finalizers (deleting an entry from the `finalizers`\nlist) but you cannot add a new finalizer. You also cannot modify the `deletionTimestamp` for an\nobject once it is set.\n\n* After the deletion is requested, you can not resurrect this object. The only way is to delete it and make a new similar object.", "zh": "* 当你 `DELETE` 一个对象时，Kubernetes 为该对象增加删除时间戳，然后立即开始限制\n对这个正处于待删除状态的对象的 `.metadata.finalizers` 字段进行修改。\n你可以删除现有的 finalizers （从 `finalizers` 列表删除条目），但你不能添加新的 finalizer。\n对象的 `deletionTimestamp` 被设置后也不能修改。\n* 删除请求已被发出之后，你无法复活该对象。唯一的方法是删除它并创建一个新的相似对象。\n{{</note>}}"}
{"en": "## Owner references, labels, and finalizers {#owners-labels-finalizers}\n\nLike {{<glossary_tooltip text=\"labels\" term_id=\"label\">}},\n[owner references](/docs/concepts/overview/working-with-objects/owners-dependents/)\ndescribe the relationships between objects in Kubernetes, but are used for a\ndifferent purpose. When a\n{{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} manages objects\nlike Pods, it uses labels to track changes to groups of related objects. For\nexample, when a {{<glossary_tooltip text=\"Job\" term_id=\"job\">}} creates one or\nmore Pods, the Job controller applies labels to those pods and tracks changes to\nany Pods in the cluster with the same label.", "zh": "## 属主引用、标签和 Finalizers {#owners-labels-finalizers}\n\n与{{<glossary_tooltip text=\"标签\" term_id=\"label\">}}类似，\n[属主引用](/zh-cn/docs/concepts/overview/working-with-objects/owners-dependents/)\n描述了 Kubernetes 中对象之间的关系，但它们作用不同。\n当一个{{<glossary_tooltip text=\"控制器\" term_id=\"controller\">}}\n管理类似于 Pod 的对象时，它使用标签来跟踪相关对象组的变化。\n例如，当 {{<glossary_tooltip text=\"Job\" term_id=\"job\">}} 创建一个或多个 Pod 时，\nJob 控制器会给这些 Pod 应用上标签，并跟踪集群中的具有相同标签的 Pod 的变化。"}
{"en": "The Job controller also adds *owner references* to those Pods, pointing at the\nJob that created the Pods. If you delete the Job while these Pods are running,\nKubernetes uses the owner references (not labels) to determine which Pods in the\ncluster need cleanup.\n\nKubernetes also processes finalizers when it identifies owner references on a\nresource targeted for deletion. \n\nIn some situations, finalizers can block the deletion of dependent objects,\nwhich can cause the targeted owner object to remain for\nlonger than expected without being fully deleted. In these situations, you\nshould check finalizers and owner references on the target owner and dependent\nobjects to troubleshoot the cause.", "zh": "Job 控制器还为这些 Pod 添加了“属主引用”，指向创建 Pod 的 Job。\n如果你在这些 Pod 运行的时候删除了 Job，\nKubernetes 会使用属主引用（而不是标签）来确定集群中哪些 Pod 需要清理。\n\n当 Kubernetes 识别到要删除的资源上的属主引用时，它也会处理 Finalizers。\n\n在某些情况下，Finalizers 会阻止依赖对象的删除，\n这可能导致目标属主对象被保留的时间比预期的长，而没有被完全删除。\n在这些情况下，你应该检查目标属主和附属对象上的 Finalizers 和属主引用，来排查原因。\n\n{{< note >}}"}
{"en": "In cases where objects are stuck in a deleting state, avoid manually\nremoving finalizers to allow deletion to continue. Finalizers are usually added\nto resources for a reason, so forcefully removing them can lead to issues in\nyour cluster. This should only be done when the purpose of the finalizer is\nunderstood and is accomplished in another way (for example, manually cleaning\nup some dependent object).", "zh": "在对象卡在删除状态的情况下，要避免手动移除 Finalizers，以允许继续删除操作。\nFinalizers 通常因为特殊原因被添加到资源上，所以强行删除它们会导致集群出现问题。\n只有了解 finalizer 的用途时才能这样做，并且应该通过一些其他方式来完成\n（例如，手动清除其余的依赖对象）。\n{{< /note >}}\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read [Using Finalizers to Control Deletion](/blog/2021/05/14/using-finalizers-to-control-deletion/)\n  on the Kubernetes blog.", "zh": "* 在 Kubernetes 博客上阅读[使用 Finalizers 控制删除](/blog/2021/05/14/using-finalizers-to-control-deletion/)。"}
{"en": "In Kubernetes, _namespaces_ provide a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced {{< glossary_tooltip text=\"objects\" term_id=\"object\" >}} _(e.g. Deployments, Services, etc)_ and not for cluster-wide objects _(e.g. StorageClass, Nodes, PersistentVolumes, etc.)_.", "zh": "在 Kubernetes 中，**名字空间（Namespace）** 提供一种机制，将同一集群中的资源划分为相互隔离的组。\n同一名字空间内的资源名称要唯一，但跨名字空间时没有这个要求。\n名字空间作用域仅针对带有名字空间的{{< glossary_tooltip text=\"对象\" term_id=\"object\" >}}，\n（例如 Deployment、Service 等），这种作用域对集群范围的对象\n（例如 StorageClass、Node、PersistentVolume 等）不适用。"}
{"en": "## When to Use Multiple Namespaces", "zh": "## 何时使用多个名字空间    {#when-to-use-multiple-namespaces}"}
{"en": "Namespaces are intended for use in environments with many users spread across multiple\nteams, or projects.  For clusters with a few to tens of users, you should not\nneed to create or think about namespaces at all.  Start using namespaces when you\nneed the features they provide.", "zh": "名字空间适用于存在很多跨多个团队或项目的用户的场景。对于只有几到几十个用户的集群，根本不需要创建或考虑名字空间。当需要名字空间提供的功能时，请开始使用它们。"}
{"en": "Namespaces provide a scope for names.  Names of resources need to be unique within a namespace,\nbut not across namespaces. Namespaces cannot be nested inside one another and each Kubernetes\nresource can only be in one namespace.", "zh": "名字空间为名称提供了一个范围。资源的名称需要在名字空间内是唯一的，但不能跨名字空间。\n名字空间不能相互嵌套，每个 Kubernetes 资源只能在一个名字空间中。"}
{"en": "Namespaces are a way to divide cluster resources between multiple users (via [resource quota](/docs/concepts/policy/resource-quotas/)).", "zh": "名字空间是在多个用户之间划分集群资源的一种方法（通过[资源配额](/zh-cn/docs/concepts/policy/resource-quotas/)）。"}
{"en": "It is not necessary to use multiple namespaces to separate slightly different\nresources, such as different versions of the same software: use\n{{< glossary_tooltip text=\"labels\" term_id=\"label\" >}} to distinguish\nresources within the same namespace.", "zh": "不必使用多个名字空间来分隔仅仅轻微不同的资源，例如同一软件的不同版本：\n应该使用{{< glossary_tooltip text=\"标签\" term_id=\"label\" >}}来区分同一名字空间中的不同资源。\n\n{{< note >}}"}
{"en": "For a production cluster, consider _not_ using the `default` namespace. Instead, make other namespaces and use those.", "zh": "对于生产集群，请考虑**不要**使用 `default` 名字空间，而是创建其他名字空间来使用。\n{{< /note >}}"}
{"en": "## Initial namespaces", "zh": "## 初始名字空间   {#initial-namespaces}"}
{"en": "Kubernetes starts with four initial namespaces:\n\n`default`\n: Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.\n\n`kube-node-lease`\n: This namespace holds [Lease](/docs/concepts/architecture/leases/) objects associated with each node. Node leases allow the kubelet to send [heartbeats](/docs/concepts/architecture/nodes/#node-heartbeats) so that the control plane can detect node failure.\n\n`kube-public`\n: This namespace is readable by *all* clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.\n\n`kube-system`\n: The namespace for objects created by the Kubernetes system.", "zh": "Kubernetes 启动时会创建四个初始名字空间：\n\n`default`\n: Kubernetes 包含这个名字空间，以便于你无需创建新的名字空间即可开始使用新集群。\n\n`kube-node-lease`\n: 该名字空间包含用于与各个节点关联的 [Lease（租约）](/zh-cn/docs/concepts/architecture/leases/)对象。\n  节点租约允许 kubelet 发送[心跳](/zh-cn/docs/concepts/architecture/nodes/#node-heartbeats)，\n  由此控制面能够检测到节点故障。\n\n`kube-public`\n: **所有**的客户端（包括未经身份验证的客户端）都可以读取该名字空间。\n  该名字空间主要预留为集群使用，以便某些资源需要在整个集群中可见可读。\n  该名字空间的公共属性只是一种约定而非要求。\n\n`kube-system`\n: 该名字空间用于 Kubernetes 系统创建的对象。"}
{"en": "## Working with Namespaces\n\nCreation and deletion of namespaces are described in the\n[Admin Guide documentation for namespaces](/docs/tasks/administer-cluster/namespaces).", "zh": "## 使用名字空间    {#working-with-namespaces}\n\n名字空间的创建和删除在[名字空间的管理指南文档](/zh-cn/docs/tasks/administer-cluster/namespaces/)描述。\n\n{{< note >}}"}
{"en": "Avoid creating namespaces with the prefix `kube-`, since it is reserved for Kubernetes system namespaces.", "zh": "避免使用前缀 `kube-` 创建名字空间，因为它是为 Kubernetes 系统名字空间保留的。\n{{< /note >}}"}
{"en": "### Viewing namespaces\n\nYou can list the current namespaces in a cluster using:", "zh": "### 查看名字空间    {#viewing-namespaces}\n\n你可以使用以下命令列出集群中现存的名字空间：\n\n```shell\nkubectl get namespace\n```\n```\nNAME              STATUS   AGE\ndefault           Active   1d\nkube-node-lease   Active   1d\nkube-public       Active   1d\nkube-system       Active   1d\n```"}
{"en": "### Setting the namespace for a request\n\nTo set the namespace for a current request, use the `--namespace` flag.\n\nFor example:", "zh": "### 为请求设置名字空间    {#setting-the-namespace-for-a-request}\n\n要为当前请求设置名字空间，请使用 `--namespace` 参数。\n\n例如：\n\n```shell\nkubectl run nginx --image=nginx --namespace=<名字空间名称>\nkubectl get pods --namespace=<名字空间名称>\n```"}
{"en": "### Setting the namespace preference\n\nYou can permanently save the namespace for all subsequent kubectl commands in that\ncontext.", "zh": "### 设置名字空间偏好    {#setting-the-namespace-preference}\n\n你可以永久保存名字空间，以用于对应上下文中所有后续 kubectl 命令。\n\n```shell\nkubectl config set-context --current --namespace=<名字空间名称>\n# 验证\nkubectl config view --minify | grep namespace:\n```"}
{"en": "## Namespaces and DNS\n\nWhen you create a [Service](/docs/concepts/services-networking/service/),\nit creates a corresponding [DNS entry](/docs/concepts/services-networking/dns-pod-service/).", "zh": "## 名字空间和 DNS   {#namespaces-and-dns}\n\n当你创建一个[服务](/zh-cn/docs/concepts/services-networking/service/)时，\nKubernetes 会创建一个相应的 [DNS 条目](/zh-cn/docs/concepts/services-networking/dns-pod-service/)。"}
{"en": "This entry is of the form `<service-name>.<namespace-name>.svc.cluster.local`, which means\nthat if a container only uses `<service-name>`, it will resolve to the service which\nis local to a namespace.  This is useful for using the same configuration across\nmultiple namespaces such as Development, Staging and Production.  If you want to reach\nacross namespaces, you need to use the fully qualified domain name (FQDN).", "zh": "该条目的形式是 `<服务名称>.<名字空间名称>.svc.cluster.local`，这意味着如果容器只使用\n`<服务名称>`，它将被解析到本地名字空间的服务。这对于跨多个名字空间（如开发、测试和生产）\n使用相同的配置非常有用。如果你希望跨名字空间访问，则需要使用完全限定域名（FQDN）。"}
{"en": "As a result, all namespace names must be valid\n[RFC 1123 DNS labels](/docs/concepts/overview/working-with-objects/names/#dns-label-names).", "zh": "因此，所有的名字空间名称都必须是合法的\n[RFC 1123 DNS 标签](/zh-cn/docs/concepts/overview/working-with-objects/names/#dns-label-names)。\n\n{{< warning >}}"}
{"en": "By creating namespaces with the same name as [public top-level\ndomains](https://data.iana.org/TLD/tlds-alpha-by-domain.txt), Services in these\nnamespaces can have short DNS names that overlap with public DNS records.\nWorkloads from any namespace performing a DNS lookup without a [trailing dot](https://datatracker.ietf.org/doc/html/rfc1034#page-8) will\nbe redirected to those services, taking precedence over public DNS.", "zh": "通过创建与[公共顶级域名](https://data.iana.org/TLD/tlds-alpha-by-domain.txt)同名的名字空间，\n这些名字空间中的服务可以拥有与公共 DNS 记录重叠的、较短的 DNS 名称。\n所有名字空间中的负载在执行 DNS 查找时，\n如果查找的名称没有[尾部句点](https://datatracker.ietf.org/doc/html/rfc1034#page-8)，\n就会被重定向到这些服务上，因此呈现出比公共 DNS 更高的优先序。"}
{"en": "To mitigate this, limit privileges for creating namespaces to trusted users. If\nrequired, you could additionally configure third-party security controls, such\nas [admission\nwebhooks](/docs/reference/access-authn-authz/extensible-admission-controllers/),\nto block creating any namespace with the name of [public\nTLDs](https://data.iana.org/TLD/tlds-alpha-by-domain.txt).", "zh": "为了缓解这类问题，需要将创建名字空间的权限授予可信的用户。\n如果需要，你可以额外部署第三方的安全控制机制，\n例如以[准入 Webhook](/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/)\n的形式，阻止用户创建与公共 [TLD](https://data.iana.org/TLD/tlds-alpha-by-domain.txt)\n同名的名字空间。\n{{< /warning >}}"}
{"en": "## Not all objects are in a namespace", "zh": "## 并非所有对象都在名字空间中    {#not-all-objects-are-in-a-namespace}"}
{"en": "Most Kubernetes resources (e.g. pods, services, replication controllers, and others) are\nin some namespaces.  However namespace resources are not themselves in a namespace.\nAnd low-level resources, such as\n[nodes](/docs/concepts/architecture/nodes/) and\n[persistentVolumes](/docs/concepts/storage/persistent-volumes/), are not in any namespace.", "zh": "大多数 kubernetes 资源（例如 Pod、Service、副本控制器等）都位于某些名字空间中。\n但是名字空间资源本身并不在名字空间中。而且底层资源，\n例如[节点](/zh-cn/docs/concepts/architecture/nodes/)和[持久化卷](/zh-cn/docs/concepts/storage/persistent-volumes/)不属于任何名字空间。"}
{"en": "To see which Kubernetes resources are and aren't in a namespace:", "zh": "查看哪些 Kubernetes 资源在名字空间中，哪些不在名字空间中：\n\n```shell\n# 位于名字空间中的资源\nkubectl api-resources --namespaced=true\n\n# 不在名字空间中的资源\nkubectl api-resources --namespaced=false\n```"}
{"en": "## Automatic labelling", "zh": "## 自动打标签   {#automatic-labelling}\n\n{{< feature-state for_k8s_version=\"1.22\" state=\"stable\" >}}"}
{"en": "The Kubernetes control plane sets an immutable {{< glossary_tooltip text=\"label\" term_id=\"label\" >}}\n`kubernetes.io/metadata.name` on all namespaces.\nThe value of the label is the namespace name.", "zh": "Kubernetes 控制面会为所有名字空间设置一个不可变更的{{< glossary_tooltip text=\"标签\" term_id=\"label\" >}}\n`kubernetes.io/metadata.name`。\n标签的值是名字空间的名称。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn more about [creating a new namespace](/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace).\n* Learn more about [deleting a namespace](/docs/tasks/administer-cluster/namespaces/#deleting-a-namespace).", "zh": "* 进一步了解[建立新的名字空间](/zh-cn/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace)。\n* 进一步了解[删除名字空间](/zh-cn/docs/tasks/administer-cluster/namespaces/#deleting-a-namespace)。"}
{"en": "You can visualize and manage Kubernetes objects with more tools than kubectl and\nthe dashboard. A common set of labels allows tools to work interoperably, describing\nobjects in a common manner that all tools can understand.", "zh": "除了 kubectl 和 dashboard 之外，你还可以使用其他工具来可视化和管理 Kubernetes 对象。\n一组通用的标签可以让多个工具之间相互操作，用所有工具都能理解的通用方式描述对象。"}
{"en": "In addition to supporting tooling, the recommended labels describe applications\nin a way that can be queried.", "zh": "除了支持工具外，推荐的标签还以一种可以查询的方式描述了应用程序。"}
{"en": "The metadata is organized around the concept of an _application_. Kubernetes is not\na platform as a service (PaaS) and doesn't have or enforce a formal notion of an application.\nInstead, applications are informal and described with metadata. The definition of\nwhat an application contains is loose.", "zh": "元数据围绕 **应用（application）** 的概念进行组织。Kubernetes\n不是平台即服务（PaaS），没有或强制执行正式的应用程序概念。\n相反，应用程序是非正式的，并使用元数据进行描述。应用程序包含的定义是松散的。\n\n{{< note >}}"}
{"en": "These are recommended labels. They make it easier to manage applications\nbut aren't required for any core tooling.", "zh": "这些是推荐的标签。它们使管理应用程序变得更容易但不是任何核心工具所必需的。\n{{< /note >}}"}
{"en": "Shared labels and annotations share a common prefix: `app.kubernetes.io`. Labels\nwithout a prefix are private to users. The shared prefix ensures that shared labels\ndo not interfere with custom user labels.", "zh": "共享标签和注解都使用同一个前缀：`app.kubernetes.io`。没有前缀的标签是用户私有的。\n共享前缀可以确保共享标签不会干扰用户自定义的标签。"}
{"en": "## Labels\n\nIn order to take full advantage of using these labels, they should be applied\non every resource object.", "zh": "## 标签   {#labels}\n\n为了充分利用这些标签，应该在每个资源对象上都使用它们。"}
{"en": "| Key                                 | Description           | Example  | Type |\n| ----------------------------------- | --------------------- | -------- | ---- |\n| `app.kubernetes.io/name`            | The name of the application | `mysql` | string |\n| `app.kubernetes.io/instance`        | A unique name identifying the instance of an application | `mysql-abcxyz` | string |\n| `app.kubernetes.io/version`         | The current version of the application (e.g., a [SemVer 1.0](https://semver.org/spec/v1.0.0.html), revision hash, etc.) | `5.7.21` | string |\n| `app.kubernetes.io/component`       | The component within the architecture | `database` | string |\n| `app.kubernetes.io/part-of`         | The name of a higher level application this one is part of | `wordpress` | string |\n| `app.kubernetes.io/managed-by`      | The tool being used to manage the operation of an application | `Helm` | string |", "zh": "| 键                                 | 描述           | 示例  | 类型 |\n| ----------------------------------- | --------------------- | -------- | ---- |\n| `app.kubernetes.io/name`            | 应用程序的名称 | `mysql` | 字符串 |\n| `app.kubernetes.io/instance`        | 用于唯一确定应用实例的名称 | `mysql-abcxyz` | 字符串 |\n| `app.kubernetes.io/version`         | 应用程序的当前版本（例如[语义版本 1.0](https://semver.org/spec/v1.0.0.html)、修订版哈希等） | `5.7.21` | 字符串 |\n| `app.kubernetes.io/component`       | 架构中的组件 | `database` | 字符串 |\n| `app.kubernetes.io/part-of`         | 此级别的更高级别应用程序的名称 | `wordpress` | 字符串 |\n| `app.kubernetes.io/managed-by`      | 用于管理应用程序的工具 | `Helm` | 字符串 |"}
{"en": "To illustrate these labels in action, consider the following {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}} object:", "zh": "为说明这些标签的实际使用情况，请看下面的 {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}} 对象：\n\n```yaml\n# 这是一段节选\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/instance: mysql-abcxyz\n    app.kubernetes.io/version: \"5.7.21\"\n    app.kubernetes.io/component: database\n    app.kubernetes.io/part-of: wordpress\n    app.kubernetes.io/managed-by: Helm\n```"}
{"en": "## Applications And Instances Of Applications\n\nAn application can be installed one or more times into a Kubernetes cluster and,\nin some cases, the same namespace. For example, WordPress can be installed more\nthan once where different websites are different installations of WordPress.\n\nThe name of an application and the instance name are recorded separately. For\nexample, WordPress has a `app.kubernetes.io/name` of `wordpress` while it has\nan instance name, represented as `app.kubernetes.io/instance` with a value of\n`wordpress-abcxyz`. This enables the application and instance of the application\nto be identifiable. Every instance of an application must have a unique name.", "zh": "## 应用和应用实例   {#application-and-instances-of-applications}\n\n应用可以在 Kubernetes 集群中安装一次或多次。在某些情况下，可以安装在同一命名空间中。\n例如，可以不止一次地为不同的站点安装不同的 WordPress。\n\n应用的名称和实例的名称是分别记录的。例如，WordPress 应用的 \n`app.kubernetes.io/name` 为 `wordpress`，而其实例名称 \n`app.kubernetes.io/instance` 为 `wordpress-abcxyz`。\n这使得应用和应用的实例均可被识别，应用的每个实例都必须具有唯一的名称。"}
{"en": "## Examples", "zh": "## 示例   {#examples}"}
{"en": "To illustrate different ways to use these labels the following examples have varying complexity.", "zh": "为了说明使用这些标签的不同方式，以下示例具有不同的复杂性。"}
{"en": "### A Simple Stateless Service", "zh": "### 一个简单的无状态服务"}
{"en": "Consider the case for a simple stateless service deployed using `Deployment` and `Service` objects. The following two snippets represent how the labels could be used in their simplest form.", "zh": "考虑使用 `Deployment` 和 `Service` 对象部署的简单无状态服务的情况。\n以下两个代码段表示如何以最简单的形式使用标签。"}
{"en": "The `Deployment` is used to oversee the pods running the application itself.", "zh": "下面的 `Deployment` 用于监督运行应用本身的那些 Pod。\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: myservice\n    app.kubernetes.io/instance: myservice-abcxyz\n...\n```"}
{"en": "The `Service` is used to expose the application.", "zh": "下面的 `Service` 用于暴露应用。\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: myservice\n    app.kubernetes.io/instance: myservice-abcxyz\n...\n```"}
{"en": "### Web Application With A Database", "zh": "### 带有一个数据库的 Web 应用程序"}
{"en": "Consider a slightly more complicated application: a web application (WordPress)\nusing a database (MySQL), installed using Helm. The following snippets illustrate\nthe start of objects used to deploy this application.\n\nThe start to the following `Deployment` is used for WordPress:", "zh": "考虑一个稍微复杂的应用：一个使用 Helm 安装的 Web 应用（WordPress），\n其中使用了数据库（MySQL）。以下代码片段说明用于部署此应用程序的对象的开始。\n\n以下 `Deployment` 的开头用于 WordPress：\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/instance: wordpress-abcxyz\n    app.kubernetes.io/version: \"4.9.4\"\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: server\n    app.kubernetes.io/part-of: wordpress\n...\n```"}
{"en": "The `Service` is used to expose WordPress:", "zh": "这个 `Service` 用于暴露 WordPress：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/instance: wordpress-abcxyz\n    app.kubernetes.io/version: \"4.9.4\"\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: server\n    app.kubernetes.io/part-of: wordpress\n...\n```"}
{"en": "MySQL is exposed as a `StatefulSet` with metadata for both it and the larger application it belongs to:", "zh": "MySQL 作为一个 `StatefulSet` 暴露，包含它和它所属的较大应用程序的元数据：\n\n```yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/instance: mysql-abcxyz\n    app.kubernetes.io/version: \"5.7.21\"\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: database\n    app.kubernetes.io/part-of: wordpress\n...\n```"}
{"en": "The `Service` is used to expose MySQL as part of WordPress:", "zh": "`Service` 用于将 MySQL 作为 WordPress 的一部分暴露：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/instance: mysql-abcxyz\n    app.kubernetes.io/version: \"5.7.21\"\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: database\n    app.kubernetes.io/part-of: wordpress\n...\n```"}
{"en": "With the MySQL `StatefulSet` and `Service` you'll notice information about both MySQL and WordPress, the broader application, are included.", "zh": "使用 MySQL `StatefulSet` 和 `Service`，你会注意到有关 MySQL 和 WordPress 的信息，包括更广泛的应用程序。"}
{"en": "You can use Kubernetes annotations to attach arbitrary non-identifying metadata\nto {{< glossary_tooltip text=\"objects\" term_id=\"object\" >}}.\nClients such as tools and libraries can retrieve this metadata.", "zh": "你可以使用 Kubernetes 注解为{{< glossary_tooltip text=\"对象\" term_id=\"object\" >}}附加任意的非标识的元数据。\n客户端程序（例如工具和库）能够获取这些元数据信息。"}
{"en": "## Attaching metadata to objects\n\nYou can use either labels or annotations to attach metadata to Kubernetes\nobjects. Labels can be used to select objects and to find\ncollections of objects that satisfy certain conditions. In contrast, annotations\nare not used to identify and select objects. The metadata\nin an annotation can be small or large, structured or unstructured, and can\ninclude characters not permitted by labels. It is possible to use labels as \nwell as annotations in the metadata of the same object.\n\nAnnotations, like labels, are key/value maps:", "zh": "## 为对象附加元数据\n\n你可以使用标签或注解将元数据附加到 Kubernetes 对象。\n标签可以用来选择对象和查找满足某些条件的对象集合。 相反，注解不用于标识和选择对象。\n注解中的元数据，可以很小，也可以很大，可以是结构化的，也可以是非结构化的，能够包含标签不允许的字符。\n可以在同一对象的元数据中同时使用标签和注解。\n\n注解和标签一样，是键/值对：\n\n```json\n\"metadata\": {\n  \"annotations\": {\n    \"key1\" : \"value1\",\n    \"key2\" : \"value2\"\n  }\n}\n```\n\n{{<note>}}"}
{"en": "The keys and the values in the map must be strings. In other words, you cannot use\nnumeric, boolean, list or other types for either the keys or the values.", "zh": "Map 中的键和值必须是字符串。\n换句话说，你不能使用数字、布尔值、列表或其他类型的键或值。\n{{</note>}}"}
{"en": "Here are some examples of information that could be recorded in annotations:", "zh": "以下是一些例子，用来说明哪些信息可以使用注解来记录："}
{"en": "* Fields managed by a declarative configuration layer. Attaching these fields\n  as annotations distinguishes them from default values set by clients or\n  servers, and from auto-generated fields and fields set by\n  auto-sizing or auto-scaling systems.\n\n* Build, release, or image information like timestamps, release IDs, git branch,\n  PR numbers, image hashes, and registry address.\n\n* Pointers to logging, monitoring, analytics, or audit repositories.", "zh": "* 由声明性配置所管理的字段。\n  将这些字段附加为注解，能够将它们与客户端或服务端设置的默认值、\n  自动生成的字段以及通过自动调整大小或自动伸缩系统设置的字段区分开来。\n* 构建、发布或镜像信息（如时间戳、发布 ID、Git 分支、PR 数量、镜像哈希、仓库地址）。\n* 指向日志记录、监控、分析或审计仓库的指针。"}
{"en": "* Client library or tool information that can be used for debugging purposes:\n  for example, name, version, and build information.\n\n* User or tool/system provenance information, such as URLs of related objects\n  from other ecosystem components.\n\n* Lightweight rollout tool metadata: for example, config or checkpoints.\n\n* Phone or pager numbers of persons responsible, or directory entries that\n  specify where that information can be found, such as a team web site.\n\n* Directives from the end-user to the implementations to modify behavior or\n  engage non-standard features.", "zh": "* 可用于调试目的的客户端库或工具信息：例如，名称、版本和构建信息。\n\n* 用户或者工具/系统的来源信息，例如来自其他生态系统组件的相关对象的 URL。\n\n* 轻量级上线工具的元数据信息：例如，配置或检查点。\n\n* 负责人员的电话或呼机号码，或指定在何处可以找到该信息的目录条目，如团队网站。\n\n* 从用户到最终运行的指令，以修改行为或使用非标准功能。"}
{"en": "Instead of using annotations, you could store this type of information in an\nexternal database or directory, but that would make it much harder to produce\nshared client libraries and tools for deployment, management, introspection,\nand the like.", "zh": "你可以将这类信息存储在外部数据库或目录中而不使用注解，\n但这样做就使得开发人员很难生成用于部署、管理、自检的客户端共享库和工具。"}
{"en": "## Syntax and character set\n\n_Annotations_ are key/value pairs. Valid annotation keys have two segments: an optional prefix and name, separated by a slash (`/`). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character (`[a-z0-9A-Z]`) with dashes (`-`), underscores (`_`), dots (`.`), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (`.`), not longer than 253 characters in total, followed by a slash (`/`).\n\nIf the prefix is omitted, the annotation Key is presumed to be private to the user. Automated system components (e.g. `kube-scheduler`, `kube-controller-manager`, `kube-apiserver`, `kubectl`, or other third-party automation) which add annotations to end-user objects must specify a prefix.", "zh": "## 语法和字符集\n\n**注解（Annotations）** 存储的形式是键/值对。有效的注解键分为两部分：\n可选的前缀和名称，以斜杠（`/`）分隔。 \n名称段是必需项，并且必须在 63 个字符以内，以字母数字字符（`[a-z0-9A-Z]`）开头和结尾，\n并允许使用破折号（`-`），下划线（`_`），点（`.`）和字母数字。 \n前缀是可选的。如果指定，则前缀必须是 DNS 子域：一系列由点（`.`）分隔的 DNS 标签，\n总计不超过 253 个字符，后跟斜杠（`/`）。\n如果省略前缀，则假定注解键对用户是私有的。 由系统组件添加的注解\n（例如，`kube-scheduler`，`kube-controller-manager`，`kube-apiserver`，`kubectl`\n或其他第三方组件），必须为终端用户添加注解前缀。"}
{"en": "The `kubernetes.io/` and `k8s.io/` prefixes are reserved for Kubernetes core components.\n\nFor example, here's a manifest for a Pod that has the annotation `imageregistry: https://hub.docker.com/` :", "zh": "`kubernetes.io/` 和 `k8s.io/` 前缀是为 Kubernetes 核心组件保留的。\n\n例如，下面是一个 Pod 的清单，其注解中包含 `imageregistry: https://hub.docker.com/`：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: annotations-demo\n  annotations:\n    imageregistry: \"https://hub.docker.com/\"\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.2\n    ports:\n    - containerPort: 80\n```\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Learn more about [Labels and Selectors](/docs/concepts/overview/working-with-objects/labels/).\n- Find [Well-known labels, Annotations and Taints](/docs/reference/labels-annotations-taints/)", "zh": "- 进一步了解[标签和选择算符](/zh-cn/docs/concepts/overview/working-with-objects/labels/)。\n- 查找[众所周知的标签、注解和污点](/zh-cn/docs/reference/labels-annotations-taints/)。"}
{"en": "In Kubernetes, some {{< glossary_tooltip text=\"objects\" term_id=\"object\" >}} are\n*owners* of other objects. For example, a\n{{<glossary_tooltip text=\"ReplicaSet\" term_id=\"replica-set\">}} is the owner\nof a set of Pods. These owned objects are *dependents* of their owner.", "zh": "在 Kubernetes 中，一些{{< glossary_tooltip text=\"对象\" term_id=\"Object\" >}}是其他对象的“属主（Owner）”。\n例如，{{<glossary_tooltip text=\"ReplicaSet\" term_id=\"replica-set\">}} 是一组 Pod 的属主。\n具有属主的对象是属主的“附属（Dependent）”。"}
{"en": "Ownership is different from the [labels and selectors](/docs/concepts/overview/working-with-objects/labels/)\nmechanism that some resources also use. For example, consider a Service that\ncreates `EndpointSlice` objects. The Service uses {{<glossary_tooltip text=\"labels\" term_id=\"label\">}} to allow the control plane to\ndetermine which `EndpointSlice` objects are used for that Service. In addition\nto the labels, each `EndpointSlice` that is managed on behalf of a Service has\nan owner reference. Owner references help different parts of Kubernetes avoid\ninterfering with objects they don’t control.", "zh": "属主关系不同于一些资源使用的[标签和选择算符](/zh-cn/docs/concepts/overview/working-with-objects/labels/)机制。\n例如，有一个创建 `EndpointSlice` 对象的 Service，\n该 Service 使用{{<glossary_tooltip text=\"标签\" term_id=\"label\">}}来让控制平面确定哪些\n`EndpointSlice` 对象属于该 Service。除开标签，每个代表 Service 所管理的\n`EndpointSlice` 都有一个属主引用。属主引用避免 Kubernetes\n的不同部分干扰到不受它们控制的对象。"}
{"en": "## Owner references in object specifications\n\nDependent objects have a `metadata.ownerReferences` field that references their\nowner object. A valid owner reference consists of the object name and a {{<glossary_tooltip text=\"UID\" term_id=\"uid\">}}\nwithin the same {{<glossary_tooltip text=\"namespace\" term_id=\"namespace\">}} as the dependent object. Kubernetes sets the value of\nthis field automatically for objects that are dependents of other objects like\nReplicaSets, DaemonSets, Deployments, Jobs and CronJobs, and ReplicationControllers.\nYou can also configure these relationships manually by changing the value of\nthis field. However, you usually don't need to and can allow Kubernetes to\nautomatically manage the relationships.", "zh": "## 对象规约中的属主引用   {#owner-references-in-object-specifications}\n\n附属对象有一个 `metadata.ownerReferences` 字段，用于引用其属主对象。一个有效的属主引用，\n包含与附属对象同在一个{{<glossary_tooltip text=\"命名空间\" term_id=\"namespace\">}}下的对象名称和一个\n{{<glossary_tooltip text=\"UID\" term_id=\"uid\">}}。\nKubernetes 自动为一些对象的附属资源设置属主引用的值，\n这些对象包含 ReplicaSet、DaemonSet、Deployment、Job、CronJob、ReplicationController 等。\n你也可以通过改变这个字段的值，来手动配置这些关系。\n然而，通常不需要这么做，你可以让 Kubernetes 自动管理附属关系。"}
{"en": "Dependent objects also have an `ownerReferences.blockOwnerDeletion` field that\ntakes a boolean value and controls whether specific dependents can block garbage\ncollection from deleting their owner object. Kubernetes automatically sets this\nfield to `true` if a {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} \n(for example, the Deployment controller) sets the value of the\n`metadata.ownerReferences` field. You can also set the value of the\n`blockOwnerDeletion` field manually to control which dependents block garbage\ncollection.\n\nA Kubernetes admission controller controls user access to change this field for\ndependent resources, based on the delete permissions of the owner. This control\nprevents unauthorized users from delaying owner object deletion.", "zh": "附属对象还有一个 `ownerReferences.blockOwnerDeletion` 字段，该字段使用布尔值，\n用于控制特定的附属对象是否可以阻止垃圾收集删除其属主对象。\n如果{{<glossary_tooltip text=\"控制器\" term_id=\"controller\">}}（例如 Deployment 控制器）\n设置了 `metadata.ownerReferences` 字段的值，Kubernetes 会自动设置\n`blockOwnerDeletion` 的值为 `true`。\n你也可以手动设置 `blockOwnerDeletion` 字段的值，以控制哪些附属对象会阻止垃圾收集。\n\nKubernetes 准入控制器根据属主的删除权限控制用户访问，以便为附属资源更改此字段。\n这种控制机制可防止未经授权的用户延迟属主对象的删除。\n\n{{< note >}}"}
{"en": "Cross-namespace owner references are disallowed by design.\nNamespaced dependents can specify cluster-scoped or namespaced owners.\nA namespaced owner **must** exist in the same namespace as the dependent.\nIf it does not, the owner reference is treated as absent, and the dependent\nis subject to deletion once all owners are verified absent.\n\nCluster-scoped dependents can only specify cluster-scoped owners.\nIn v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner,\nit is treated as having an unresolvable owner reference, and is not able to be garbage collected.\n\nIn v1.20+, if the garbage collector detects an invalid cross-namespace `ownerReference`,\nor a cluster-scoped dependent with an `ownerReference` referencing a namespaced kind, a warning Event \nwith a reason of `OwnerRefInvalidNamespace` and an `involvedObject` of the invalid dependent is reported.\nYou can check for that kind of Event by running\n`kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace`.", "zh": "根据设计，kubernetes 不允许跨名字空间指定属主。\n名字空间范围的附属可以指定集群范围的或者名字空间范围的属主。\n名字空间范围的属主**必须**和该附属处于相同的名字空间。\n如果名字空间范围的属主和附属不在相同的名字空间，那么该属主引用就会被认为是缺失的，\n并且当附属的所有属主引用都被确认不再存在之后，该附属就会被删除。\n\n集群范围的附属只能指定集群范围的属主。\n在 v1.20+ 版本，如果一个集群范围的附属指定了一个名字空间范围类型的属主，\n那么该附属就会被认为是拥有一个不可解析的属主引用，并且它不能够被垃圾回收。\n\n在 v1.20+ 版本，如果垃圾收集器检测到无效的跨名字空间的属主引用，\n或者一个集群范围的附属指定了一个名字空间范围类型的属主，\n那么它就会报告一个警告事件。该事件的原因是 `OwnerRefInvalidNamespace`，\n`involvedObject` 属性中包含无效的附属。\n你可以运行 `kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace`\n来获取该类型的事件。\n{{< /note >}}"}
{"en": "## Ownership and finalizers\n\nWhen you tell Kubernetes to delete a resource, the API server allows the\nmanaging controller to process any [finalizer rules](/docs/concepts/overview/working-with-objects/finalizers/)\nfor the resource. {{<glossary_tooltip text=\"Finalizers\" term_id=\"finalizer\">}}\nprevent accidental deletion of resources your cluster may still need to function\ncorrectly. For example, if you try to delete a [PersistentVolume](/docs/concepts/storage/persistent-volumes/) that is still\nin use by a Pod, the deletion does not happen immediately because the\n`PersistentVolume` has the `kubernetes.io/pv-protection` finalizer on it.\nInstead, the [volume](/docs/concepts/storage/volumes/) remains in the `Terminating` status until Kubernetes clears\nthe finalizer, which only happens after the `PersistentVolume` is no longer\nbound to a Pod.", "zh": "## 属主关系与 Finalizer   {#ownership-and-finalizers}\n\n当你告诉 Kubernetes 删除一个资源，API 服务器允许管理控制器处理该资源的任何\n[Finalizer 规则](/zh-cn/docs/concepts/overview/working-with-objects/finalizers/)。\n{{<glossary_tooltip text=\"Finalizer\" term_id=\"finalizer\">}}\n防止意外删除你的集群所依赖的、用于正常运作的资源。\n例如，如果你试图删除一个仍被 Pod 使用的 `PersistentVolume`，该资源不会被立即删除，\n因为 [PersistentVolume](/zh-cn/docs/concepts/storage/persistent-volumes/) 有\n`kubernetes.io/pv-protection` Finalizer。\n相反，[数据卷](/zh-cn/docs/concepts/storage/volumes/)将进入 `Terminating` 状态，\n直到 Kubernetes 清除这个 Finalizer，而这种情况只会发生在 `PersistentVolume`\n不再被挂载到 Pod 上时。"}
{"en": "Kubernetes also adds finalizers to an owner resource when you use either\n[foreground or orphan cascading deletion](/docs/concepts/architecture/garbage-collection/#cascading-deletion).\nIn foreground deletion, it adds the `foreground` finalizer so that the\ncontroller must delete dependent resources that also have\n`ownerReferences.blockOwnerDeletion=true` before it deletes the owner. If you\nspecify an orphan deletion policy, Kubernetes adds the `orphan` finalizer so\nthat the controller ignores dependent resources after it deletes the owner\nobject.", "zh": "当你使用[前台或孤立级联删除](/zh-cn/docs/concepts/architecture/garbage-collection/#cascading-deletion)时，\nKubernetes 也会向属主资源添加 Finalizer。\n在前台删除中，会添加 `foreground` Finalizer，这样控制器必须在删除了拥有\n`ownerReferences.blockOwnerDeletion=true` 的附属资源后，才能删除属主对象。\n如果你指定了孤立删除策略，Kubernetes 会添加 `orphan` Finalizer，\n这样控制器在删除属主对象后，会忽略附属资源。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Learn more about [Kubernetes finalizers](/docs/concepts/overview/working-with-objects/finalizers/).\n* Learn about [garbage collection](/docs/concepts/architecture/garbage-collection).\n* Read the API reference for [object metadata](/docs/reference/kubernetes-api/common-definitions/object-meta/#System).", "zh": "* 了解更多关于 [Kubernetes Finalizer](/zh-cn/docs/concepts/overview/working-with-objects/finalizers/)。\n* 了解关于[垃圾收集](/zh-cn/docs/concepts/architecture/garbage-collection)。\n* 阅读[对象元数据](/zh-cn/docs/reference/kubernetes-api/common-definitions/object-meta/#System)的 API 参考文档。"}
{"en": "_Labels_ are key/value pairs that are attached to\n{{< glossary_tooltip text=\"objects\" term_id=\"object\" >}} such as pods.\nLabels are intended to be used to specify identifying attributes of objects\nthat are meaningful and relevant to users, but do not directly imply semantics\nto the core system. Labels can be used to organize and to select subsets of\nobjects. Labels can be attached to objects at creation time and subsequently\nadded and modified at any time. Each object can have a set of key/value labels\ndefined. Each Key must be unique for a given object.", "zh": "**标签（Labels）** 是附加到 Kubernetes\n{{< glossary_tooltip text=\"对象\" term_id=\"object\" >}}（比如 Pod）上的键值对。\n标签旨在用于指定对用户有意义且相关的对象的标识属性，但不直接对核心系统有语义含义。\n标签可以用于组织和选择对象的子集。标签可以在创建时附加到对象，随后可以随时添加和修改。\n每个对象都可以定义一组键/值标签。每个键对于给定对象必须是唯一的。\n\n```json\n\"metadata\": {\n  \"labels\": {\n    \"key1\" : \"value1\",\n    \"key2\" : \"value2\"\n  }\n}\n```"}
{"en": "Labels allow for efficient queries and watches and are ideal for use in UIs\nand CLIs. Non-identifying information should be recorded using\n[annotations](/docs/concepts/overview/working-with-objects/annotations/).", "zh": "标签能够支持高效的查询和监听操作，对于用户界面和命令行是很理想的。\n应使用[注解](/zh-cn/docs/concepts/overview/working-with-objects/annotations/)记录非识别信息。"}
{"en": "## Motivation\n\nLabels enable users to map their own organizational structures onto system objects\nin a loosely coupled fashion, without requiring clients to store these mappings.", "zh": "## 动机   {#motivation}\n\n标签使用户能够以松散耦合的方式将他们自己的组织结构映射到系统对象，而无需客户端存储这些映射。"}
{"en": "Service deployments and batch processing pipelines are often multi-dimensional entities\n(e.g., multiple partitions or deployments, multiple release tracks, multiple tiers,\nmultiple micro-services per tier). Management often requires cross-cutting operations,\nwhich breaks encapsulation of strictly hierarchical representations, especially rigid\nhierarchies determined by the infrastructure rather than by users.\n\nExample labels:", "zh": "服务部署和批处理流水线通常是多维实体（例如，多个分区或部署、多个发行序列、多个层，每层多个微服务）。\n管理通常需要交叉操作，这打破了严格的层次表示的封装，特别是由基础设施而不是用户确定的严格的层次结构。\n\n示例标签：\n\n* `\"release\" : \"stable\"`, `\"release\" : \"canary\"`\n* `\"environment\" : \"dev\"`, `\"environment\" : \"qa\"`, `\"environment\" : \"production\"`\n* `\"tier\" : \"frontend\"`, `\"tier\" : \"backend\"`, `\"tier\" : \"cache\"`\n* `\"partition\" : \"customerA\"`, `\"partition\" : \"customerB\"`\n* `\"track\" : \"daily\"`, `\"track\" : \"weekly\"`"}
{"en": "These are examples of\n[commonly used labels](/docs/concepts/overview/working-with-objects/common-labels/);\nyou are free to develop your own conventions.\nKeep in mind that label Key must be unique for a given object.", "zh": "有一些[常用标签](/zh-cn/docs/concepts/overview/working-with-objects/common-labels/)的例子；你可以任意制定自己的约定。\n请记住，标签的 Key 对于给定对象必须是唯一的。"}
{"en": "## Syntax and character set\n\n_Labels_ are key/value pairs. Valid label keys have two segments: an optional\nprefix and name, separated by a slash (`/`). The name segment is required and\nmust be 63 characters or less, beginning and ending with an alphanumeric\ncharacter (`[a-z0-9A-Z]`) with dashes (`-`), underscores (`_`), dots (`.`),\nand alphanumerics between. The prefix is optional. If specified, the prefix\nmust be a DNS subdomain: a series of DNS labels separated by dots (`.`),\nnot longer than 253 characters in total, followed by a slash (`/`).\n\nIf the prefix is omitted, the label Key is presumed to be private to the user.\nAutomated system components (e.g. `kube-scheduler`, `kube-controller-manager`,\n`kube-apiserver`, `kubectl`, or other third-party automation) which add labels\nto end-user objects must specify a prefix.\n\nThe `kubernetes.io/` and `k8s.io/` prefixes are\n[reserved](/docs/reference/labels-annotations-taints/) for Kubernetes core components.", "zh": "## 语法和字符集   {#syntax-and-character-set}\n\n**标签**是键值对。有效的标签键有两个段：可选的前缀和名称，用斜杠（`/`）分隔。\n名称段是必需的，必须小于等于 63 个字符，以字母数字字符（`[a-z0-9A-Z]`）开头和结尾，\n带有破折号（`-`），下划线（`_`），点（ `.`）和之间的字母数字。\n前缀是可选的。如果指定，前缀必须是 DNS 子域：由点（`.`）分隔的一系列 DNS 标签，总共不超过 253 个字符，\n后跟斜杠（`/`）。\n\n如果省略前缀，则假定标签键对用户是私有的。\n向最终用户对象添加标签的自动系统组件（例如 `kube-scheduler`、`kube-controller-manager`、\n`kube-apiserver`、`kubectl` 或其他第三方自动化工具）必须指定前缀。\n\n`kubernetes.io/` 和 `k8s.io/` 前缀是为 Kubernetes 核心组件[保留的](/zh-cn/docs/reference/labels-annotations-taints/)。"}
{"en": "Valid label value:\n\n* must be 63 characters or less (can be empty),\n* unless empty, must begin and end with an alphanumeric character (`[a-z0-9A-Z]`),\n* could contain dashes (`-`), underscores (`_`), dots (`.`), and alphanumerics between.", "zh": "有效标签值：\n\n* 必须为 63 个字符或更少（可以为空）\n* 除非标签值为空，必须以字母数字字符（`[a-z0-9A-Z]`）开头和结尾\n* 包含破折号（`-`）、下划线（`_`）、点（`.`）和字母或数字"}
{"en": "For example, here's a manifest for a Pod that has two labels\n`environment: production` and `app: nginx`:", "zh": "例如，以下是一个清单 (manifest)，适用于具有 `environment: production` 和 `app: nginx` 这两个标签的 Pod：\n\n```yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: label-demo\n  labels:\n    environment: production\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.2\n    ports:\n    - containerPort: 80\n\n```"}
{"en": "## Label selectors\n\nUnlike [names and UIDs](/docs/concepts/overview/working-with-objects/names/), labels\ndo not provide uniqueness. In general, we expect many objects to carry the same label(s).", "zh": "## 标签选择算符   {#label-selectors}\n\n与[名称和 UID](/zh-cn/docs/concepts/overview/working-with-objects/names/) 不同，\n标签不支持唯一性。通常，我们希望许多对象携带相同的标签。"}
{"en": "Via a _label selector_, the client/user can identify a set of objects.\nThe label selector is the core grouping primitive in Kubernetes.", "zh": "通过**标签选择算符**，客户端/用户可以识别一组对象。标签选择算符是 Kubernetes 中的核心分组原语。"}
{"en": "The API currently supports two types of selectors: _equality-based_ and _set-based_.\nA label selector can be made of multiple _requirements_ which are comma-separated.\nIn the case of multiple requirements, all must be satisfied so the comma separator\nacts as a logical _AND_ (`&&`) operator.", "zh": "API 目前支持两种类型的选择算符：**基于等值的**和**基于集合的**。\n标签选择算符可以由逗号分隔的多个**需求**组成。\n在多个需求的情况下，必须满足所有要求，因此逗号分隔符充当逻辑**与**（`&&`）运算符。"}
{"en": "The semantics of empty or non-specified selectors are dependent on the context,\nand API types that use selectors should document the validity and meaning of\nthem.", "zh": "空标签选择算符或者未指定的选择算符的语义取决于上下文，\n支持使用选择算符的 API 类别应该将算符的合法性和含义用文档记录下来。\n\n{{< note >}}"}
{"en": "For some API types, such as ReplicaSets, the label selectors of two instances must\nnot overlap within a namespace, or the controller can see that as conflicting\ninstructions and fail to determine how many replicas should be present.", "zh": "对于某些 API 类别（例如 ReplicaSet）而言，两个实例的标签选择算符不得在命名空间内重叠，\n否则它们的控制器将互相冲突，无法确定应该存在的副本个数。\n{{< /note >}}\n\n{{< caution >}}"}
{"en": "For both equality-based and set-based conditions there is no logical _OR_ (`||`) operator.\nEnsure your filter statements are structured accordingly.", "zh": "对于基于等值的和基于集合的条件而言，不存在逻辑或（`||`）操作符。\n你要确保你的过滤语句按合适的方式组织。\n{{< /caution >}}"}
{"en": "### _Equality-based_ requirement\n\n_Equality-_ or _inequality-based_ requirements allow filtering by label keys and values.\nMatching objects must satisfy all of the specified label constraints, though they may\nhave additional labels as well. Three kinds of operators are admitted `=`,`==`,`!=`.\nThe first two represent _equality_ (and are synonyms), while the latter represents _inequality_.\nFor example:", "zh": "### **基于等值的**需求\n\n**基于等值**或**基于不等值**的需求允许按标签键和值进行过滤。\n匹配对象必须满足所有指定的标签约束，尽管它们也可能具有其他标签。\n可接受的运算符有 `=`、`==` 和 `!=` 三种。\n前两个表示**相等**（并且是同义词），而后者表示**不相等**。例如：\n\n```\nenvironment = production\ntier != frontend\n```"}
{"en": "The former selects all resources with key equal to `environment` and value equal to `production`.\nThe latter selects all resources with key equal to `tier` and value distinct from `frontend`,\nand all resources with no labels with the `tier` key. One could filter for resources in `production`\nexcluding `frontend` using the comma operator: `environment=production,tier!=frontend`", "zh": "前者选择所有资源，其键名等于 `environment`，值等于 `production`。\n后者选择所有资源，其键名等于 `tier`，值不同于 `frontend`，所有资源都没有带有 `tier` 键的标签。\n可以使用逗号运算符来过滤 `production` 环境中的非 `frontend` 层资源：`environment=production,tier!=frontend`。"}
{"en": "One usage scenario for equality-based label requirement is for Pods to specify\nnode selection criteria. For example, the sample Pod below selects nodes where\nthe  `accelerator` label exists and is set to `nvidia-tesla-p100`.", "zh": "基于等值的标签要求的一种使用场景是 Pod 要指定节点选择标准。\n例如，下面的示例 Pod 选择存在 `accelerator` 标签且值为 `nvidia-tesla-p100` 的节点。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-test\nspec:\n  containers:\n    - name: cuda-test\n      image: \"registry.k8s.io/cuda-vector-add:v0.1\"\n      resources:\n        limits:\n          nvidia.com/gpu: 1\n  nodeSelector:\n    accelerator: nvidia-tesla-p100\n```"}
{"en": "### _Set-based_ requirement\n\n_Set-based_ label requirements allow filtering keys according to a set of values.\nThree kinds of operators are supported: `in`,`notin` and `exists` (only the key identifier).\nFor example:", "zh": "### **基于集合**的需求\n\n**基于集合**的标签需求允许你通过一组值来过滤键。\n支持三种操作符：`in`、`notin` 和 `exists`（只可以用在键标识符上）。例如：\n\n```\nenvironment in (production, qa)\ntier notin (frontend, backend)\npartition\n!partition\n```"}
{"en": "- The first example selects all resources with key equal to `environment` and value\n  equal to `production` or `qa`.\n- The second example selects all resources with key equal to `tier` and values other\n  than `frontend` and `backend`, and all resources with no labels with the `tier` key.\n- The third example selects all resources including a label with key `partition`;\n  no values are checked.\n- The fourth example selects all resources without a label with key `partition`;\n  no values are checked.\n\nSimilarly the comma separator acts as an _AND_ operator. So filtering resources\nwith a `partition` key (no matter the value) and with `environment` different\nthan `qa` can be achieved using `partition,environment notin (qa)`.", "zh": "- 第一个示例选择了所有键等于 `environment` 并且值等于 `production` 或者 `qa` 的资源。\n- 第二个示例选择了所有键等于 `tier` 并且值不等于 `frontend` 或者 `backend` 的资源，以及所有没有 `tier` 键标签的资源。\n- 第三个示例选择了所有包含了有 `partition` 标签的资源；没有校验它的值。\n- 第四个示例选择了所有没有 `partition` 标签的资源；没有校验它的值。\n\n类似地，逗号分隔符充当**与**运算符。因此，使用 `partition` 键（无论为何值）和\n`environment` 不同于 `qa` 来过滤资源可以使用 `partition, environment notin (qa)` 来实现。"}
{"en": "The _set-based_ label selector is a general form of equality since\n`environment=production` is equivalent to `environment in (production)`;\nsimilarly for `!=` and `notin`.", "zh": "**基于集合**的标签选择算符是相等标签选择算符的一般形式，因为 `environment=production`\n等同于 `environment in (production)`；`!=` 和 `notin` 也是类似的。"}
{"en": "_Set-based_ requirements can be mixed with _equality-based_ requirements.\nFor example: `partition in (customerA, customerB),environment!=qa`.", "zh": "**基于集合**的要求可以与基于**相等**的要求混合使用。例如：`partition in (customerA, customerB),environment!=qa`。\n\n## API"}
{"en": "### LIST and WATCH filtering\n\nFor **list** and **watch** operations, you can specify label selectors to filter the sets of objects\nreturned; you specify the filter using a query parameter.\n(To learn in detail about watches in Kubernetes, read\n[efficient detection of changes](/docs/reference/using-api/api-concepts/#efficient-detection-of-changes)).\nBoth requirements are permitted\n(presented here as they would appear in a URL query string):", "zh": "### LIST 和 WATCH 过滤\n\n对于 **list** 和 **watch** 操作，你可以指定标签选择算符过滤返回的对象集；你可以使用查询参数来指定过滤条件。\n（了解 Kubernetes 中的 watch 操作细节，请参阅\n[高效检测变更](/zh-cn/docs/reference/using-api/api-concepts/#efficient-detection-of-changes)）。\n两种需求都是允许的。（这里显示的是它们出现在 URL 查询字符串中）"}
{"en": "* _equality-based_ requirements: `?labelSelector=environment%3Dproduction,tier%3Dfrontend`\n* _set-based_ requirements: `?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29`", "zh": "* **基于等值**的需求：`?labelSelector=environment%3Dproduction,tier%3Dfrontend`\n* **基于集合**的需求：`?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29`"}
{"en": "Both label selector styles can be used to list or watch resources via a REST client.\nFor example, targeting `apiserver` with `kubectl` and using _equality-based_ one may write:", "zh": "两种标签选择算符都可以通过 REST 客户端用于 list 或者 watch 资源。\n例如，使用 `kubectl` 定位 `apiserver`，可以使用**基于等值**的标签选择算符可以这么写：\n\n\n```shell\nkubectl get pods -l environment=production,tier=frontend\n```"}
{"en": "or using _set-based_ requirements:", "zh": "或者使用**基于集合的**需求：\n\n```shell\nkubectl get pods -l 'environment in (production),tier in (frontend)'\n```"}
{"en": "As already mentioned _set-based_ requirements are more expressive.\nFor instance, they can implement the _OR_ operator on values:", "zh": "正如刚才提到的，**基于集合**的需求更具有表达力。例如，它们可以实现值的**或**操作：\n\n```shell\nkubectl get pods -l 'environment in (production, qa)'\n```"}
{"en": "or restricting negative matching via _notin_ operator:", "zh": "或者通过**notin**运算符限制不匹配：\n\n```shell\nkubectl get pods -l 'environment,environment notin (frontend)'\n```"}
{"en": "### Set references in API objects\n\nSome Kubernetes objects, such as [`services`](/docs/concepts/services-networking/service/)\nand [`replicationcontrollers`](/docs/concepts/workloads/controllers/replicationcontroller/),\nalso use label selectors to specify sets of other resources, such as\n[pods](/docs/concepts/workloads/pods/).", "zh": "### 在 API 对象中设置引用\n\n一些 Kubernetes 对象，例如 [`services`](/zh-cn/docs/concepts/services-networking/service/)\n和 [`replicationcontrollers`](/zh-cn/docs/concepts/workloads/controllers/replicationcontroller/)，\n也使用了标签选择算符去指定了其他资源的集合，例如\n[pods](/zh-cn/docs/concepts/workloads/pods/)。"}
{"en": "#### Service and ReplicationController\n\nThe set of pods that a `service` targets is defined with a label selector.\nSimilarly, the population of pods that a `replicationcontroller` should\nmanage is also defined with a label selector.\n\nLabel selectors for both objects are defined in `json` or `yaml` files using maps,\nand only _equality-based_ requirement selectors are supported:", "zh": "#### Service 和 ReplicationController\n\n一个 `Service` 指向的一组 Pod 是由标签选择算符定义的。同样，一个 `ReplicationController`\n应该管理的 Pod 的数量也是由标签选择算符定义的。\n\n两个对象的标签选择算符都是在 `json` 或者 `yaml` 文件中使用映射定义的，并且只支持\n**基于等值**需求的选择算符：\n\n```json\n\"selector\": {\n    \"component\" : \"redis\",\n}\n```"}
{"en": "or", "zh": "或者\n\n```yaml\nselector:\n  component: redis\n```"}
{"en": "-\nThis selector (respectively in `json` or `yaml` format) is equivalent to\n`component=redis` or `component in (redis)`.", "zh": "这个选择算符（分别在 `json` 或者 `yaml` 格式中）等价于 `component=redis` 或 `component in (redis)`。"}
{"en": "#### Resources that support set-based requirements\n\nNewer resources, such as [`Job`](/docs/concepts/workloads/controllers/job/),\n[`Deployment`](/docs/concepts/workloads/controllers/deployment/),\n[`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/), and\n[`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/),\nsupport _set-based_ requirements as well.", "zh": "#### 支持基于集合需求的资源\n\n比较新的资源，例如 [`Job`](/zh-cn/docs/concepts/workloads/controllers/job/)、\n[`Deployment`](/zh-cn/docs/concepts/workloads/controllers/deployment/)、\n[`ReplicaSet`](/zh-cn/docs/concepts/workloads/controllers/replicaset/) 和\n[`DaemonSet`](/zh-cn/docs/concepts/workloads/controllers/daemonset/)，\n也支持**基于集合的**需求。\n\n```yaml\nselector:\n  matchLabels:\n    component: redis\n  matchExpressions:\n    - { key: tier, operator: In, values: [cache] }\n    - { key: environment, operator: NotIn, values: [dev] }\n```"}
{"en": "`matchLabels` is a map of `{key,value}` pairs. A single `{key,value}` in the\n`matchLabels` map is equivalent to an element of `matchExpressions`, whose `key`\nfield is \"key\", the `operator` is \"In\", and the `values` array contains only \"value\".\n`matchExpressions` is a list of pod selector requirements. Valid operators include\nIn, NotIn, Exists, and DoesNotExist. The values set must be non-empty in the case of\nIn and NotIn. All of the requirements, from both `matchLabels` and `matchExpressions`\nare ANDed together -- they must all be satisfied in order to match.", "zh": "`matchLabels` 是由 `{key,value}` 对组成的映射。\n`matchLabels` 映射中的单个 `{key,value}` 等同于 `matchExpressions` 的元素，\n其 `key` 字段为 \"key\"，`operator` 为 \"In\"，而 `values` 数组仅包含 \"value\"。\n`matchExpressions` 是 Pod 选择算符需求的列表。\n有效的运算符包括 `In`、`NotIn`、`Exists` 和 `DoesNotExist`。\n在 `In` 和 `NotIn` 的情况下，设置的值必须是非空的。\n来自 `matchLabels` 和 `matchExpressions` 的所有要求都按逻辑与的关系组合到一起\n-- 它们必须都满足才能匹配。"}
{"en": "#### Selecting sets of nodes\n\nOne use case for selecting over labels is to constrain the set of nodes onto which\na pod can schedule. See the documentation on\n[node selection](/docs/concepts/scheduling-eviction/assign-pod-node/) for more information.", "zh": "#### 选择节点集\n\n通过标签进行选择的一个用例是确定节点集，方便 Pod 调度。\n有关更多信息，请参阅[选择节点](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/)文档。"}
{"en": "## Using labels effectively\n\nThe examples we've used so far apply at most a single label to any resource. There are many\nscenarios where multiple labels should be used to distinguish sets from one another.", "zh": "## 有效地使用标签  {#using-labels-effectively}\n\n到目前为止我们使用的示例中的资源最多使用了一个标签。\n在许多情况下，应使用多个标签来区分不同集合。"}
{"en": "For instance, different applications would use different values for the `app` label, but a\nmulti-tier application, such as the [guestbook example](https://github.com/kubernetes/examples/tree/master/guestbook/),\nwould additionally need to distinguish each tier. The frontend could carry the following labels:", "zh": "例如，不同的应用可能会为 `app` 标签设置不同的值。\n但是，类似 [guestbook 示例](https://github.com/kubernetes/examples/tree/master/guestbook/)\n这样的多层应用，还需要区分每一层。前端可能会带有以下标签：\n\n```yaml\nlabels:\n  app: guestbook\n  tier: frontend\n```"}
{"en": "while the Redis master and replica would have different `tier` labels, and perhaps even an\nadditional `role` label:", "zh": "Redis 的主从节点会有不同的 `tier` 标签，甚至还有一个额外的 `role` 标签：\n\n```yaml\nlabels:\n  app: guestbook\n  tier: backend\n  role: master\n```"}
{"en": "and", "zh": "以及\n\n```yaml\nlabels:\n  app: guestbook\n  tier: backend\n  role: replica\n```"}
{"en": "The labels allow us to slice and dice our resources along any dimension specified by a label:", "zh": "标签使得我们能够按照所指定的任何维度对我们的资源进行切片和切块：\n\n```shell\nkubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml\nkubectl get pods -Lapp -Ltier -Lrole\n```\n\n```none\nNAME                           READY  STATUS    RESTARTS   AGE   APP         TIER       ROLE\nguestbook-fe-4nlpb             1/1    Running   0          1m    guestbook   frontend   <none>\nguestbook-fe-ght6d             1/1    Running   0          1m    guestbook   frontend   <none>\nguestbook-fe-jpy62             1/1    Running   0          1m    guestbook   frontend   <none>\nguestbook-redis-master-5pg3b   1/1    Running   0          1m    guestbook   backend    master\nguestbook-redis-replica-2q2yf  1/1    Running   0          1m    guestbook   backend    replica\nguestbook-redis-replica-qgazl  1/1    Running   0          1m    guestbook   backend    replica\nmy-nginx-divi2                 1/1    Running   0          29m   nginx       <none>     <none>\nmy-nginx-o0ef1                 1/1    Running   0          29m   nginx       <none>     <none>\n```\n\n```shell\nkubectl get pods -lapp=guestbook,role=replica\n```\n\n```none\nNAME                           READY  STATUS   RESTARTS  AGE\nguestbook-redis-replica-2q2yf  1/1    Running  0         3m\nguestbook-redis-replica-qgazl  1/1    Running  0         3m\n```"}
{"en": "## Updating labels\n\nSometimes existing pods and other resources need to be relabeled before creating new resources.\nThis can be done with `kubectl label`.\nFor example, if you want to label all your nginx pods as frontend tier, run:", "zh": "## 更新标签  {#updating-labels}\n\n有时需要要在创建新资源之前对现有的 Pod 和其它资源重新打标签。\n这可以用 `kubectl label` 完成。\n例如，如果想要将所有 NGINX Pod 标记为前端层，运行：\n\n```shell\nkubectl label pods -l app=nginx tier=fe\n```\n\n```none\npod/my-nginx-2035384211-j5fhi labeled\npod/my-nginx-2035384211-u2c7e labeled\npod/my-nginx-2035384211-u3t6x labeled\n```"}
{"en": "This first filters all pods with the label \"app=nginx\", and then labels them with the \"tier=fe\".\nTo see the pods you labeled, run:", "zh": "首先用标签 \"app=nginx\" 过滤所有的 Pod，然后用 \"tier=fe\" 标记它们。\n想要查看你刚设置了标签的 Pod，请运行：\n\n```shell\nkubectl get pods -l app=nginx -L tier\n```\n\n```none\nNAME                        READY     STATUS    RESTARTS   AGE       TIER\nmy-nginx-2035384211-j5fhi   1/1       Running   0          23m       fe\nmy-nginx-2035384211-u2c7e   1/1       Running   0          23m       fe\nmy-nginx-2035384211-u3t6x   1/1       Running   0          23m       fe\n```"}
{"en": "This outputs all \"app=nginx\" pods, with an additional label column of pods' tier (specified with\n`-L` or `--label-columns`).\n\nFor more information, please see [labels](/docs/concepts/overview/working-with-objects/labels/)\nand [kubectl label](/docs/reference/generated/kubectl/kubectl-commands/#label).", "zh": "此命令将输出所有 \"app=nginx\" 的 Pod，并有一个额外的描述 Pod 所在分层的标签列\n（用参数 `-L` 或者 `--label-columns` 标明）。\n\n想要了解更多信息，请参考[标签](/zh-cn/docs/concepts/overview/working-with-objects/labels/)和\n[`kubectl label`](/docs/reference/generated/kubectl/kubectl-commands/#label)\n命令文档。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Learn how to [add a label to a node](/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node)\n- Find [Well-known labels, Annotations and Taints](/docs/reference/labels-annotations-taints/)\n- See [Recommended labels](/docs/concepts/overview/working-with-objects/common-labels/)\n- [Enforce Pod Security Standards with Namespace Labels](/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/)\n- Read a blog on [Writing a Controller for Pod Labels](/blog/2021/06/21/writing-a-controller-for-pod-labels/)", "zh": "- 学习如何[给节点添加标签](/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node)\n- 查阅[众所周知的标签、注解和污点](/zh-cn/docs/reference/labels-annotations-taints/)\n- 参见[推荐使用的标签](/zh-cn/docs/concepts/overview/working-with-objects/common-labels/)\n- [使用名字空间标签来实施 Pod 安全性标准](/zh-cn/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/)\n- 阅读[为 Pod 标签编写控制器](/blog/2021/06/21/writing-a-controller-for-pod-labels/)的博文"}
{"en": "The `kubectl` command-line tool supports several different ways to create and manage\nKubernetes {{< glossary_tooltip text=\"objects\" term_id=\"object\" >}}. This document provides an overview of the different\napproaches. Read the [Kubectl book](https://kubectl.docs.kubernetes.io) for\ndetails of managing objects by Kubectl.", "zh": "`kubectl` 命令行工具支持多种不同的方式来创建和管理 Kubernetes\n{{< glossary_tooltip text=\"对象\" term_id=\"object\" >}}。\n本文档概述了不同的方法。\n阅读 [Kubectl book](https://kubectl.docs.kubernetes.io/zh/) 来了解 kubectl\n管理对象的详细信息。"}
{"en": "## Management techniques", "zh": "## 管理技巧\n\n{{< warning >}}"}
{"en": "A Kubernetes object should be managed using only one technique. Mixing\nand matching techniques for the same object results in undefined behavior.", "zh": "应该只使用一种技术来管理 Kubernetes 对象。混合和匹配技术作用在同一对象上将导致未定义行为。\n{{< /warning >}}"}
{"en": "| Management technique             | Operates on          |Recommended environment | Supported writers  | Learning curve |\n|----------------------------------|----------------------|------------------------|--------------------|----------------|\n| Imperative commands              | Live objects         | Development projects   | 1+                 | Lowest         |\n| Imperative object configuration  | Individual files     | Production projects    | 1                  | Moderate       |\n| Declarative object configuration | Directories of files | Production projects    | 1+                 | Highest        |", "zh": "| 管理技术       | 作用于   | 建议的环境 | 支持的写者 | 学习难度 |\n|----------------|----------|------------|------------|----------|\n| 指令式命令     | 活跃对象 | 开发项目   | 1+         | 最低     |\n| 指令式对象配置 | 单个文件 | 生产项目   | 1          | 中等     |\n| 声明式对象配置 | 文件目录 | 生产项目   | 1+         | 最高     |"}
{"en": "## Imperative commands", "zh": "## 指令式命令"}
{"en": "When using imperative commands, a user operates directly on live objects\nin a cluster. The user provides operations to\nthe `kubectl` command as arguments or flags.", "zh": "使用指令式命令时，用户可以在集群中的活动对象上进行操作。用户将操作传给\n`kubectl` 命令作为参数或标志。"}
{"en": "This is the recommended way to get started or to run a one-off task in\na cluster. Because this technique operates directly on live\nobjects, it provides no history of previous configurations.", "zh": "这是开始或者在集群中运行一次性任务的推荐方法。因为这个技术直接在活跃对象\n上操作，所以它不提供以前配置的历史记录。"}
{"en": "### Examples", "zh": "### 例子"}
{"en": "Run an instance of the nginx container by creating a Deployment object:", "zh": "通过创建 Deployment 对象来运行 nginx 容器的实例：\n\n```sh\nkubectl create deployment nginx --image nginx\n```"}
{"en": "### Trade-offs", "zh": "### 权衡"}
{"en": "Advantages compared to object configuration:\n\n- Commands are expressed as a single action word.\n- Commands require only a single step to make changes to the cluster.", "zh": "与对象配置相比的优点：\n\n- 命令用单个动词表示。\n- 命令仅需一步即可对集群进行更改。"}
{"en": "Disadvantages compared to object configuration:\n\n- Commands do not integrate with change review processes.\n- Commands do not provide an audit trail associated with changes.\n- Commands do not provide a source of records except for what is live.\n- Commands do not provide a template for creating new objects.", "zh": "与对象配置相比的缺点：\n\n- 命令不与变更审查流程集成。\n- 命令不提供与更改关联的审核跟踪。\n- 除了实时内容外，命令不提供记录源。\n- 命令不提供用于创建新对象的模板。"}
{"en": "## Imperative object configuration", "zh": "## 指令式对象配置"}
{"en": "In imperative object configuration, the kubectl command specifies the\noperation (create, replace, etc.), optional flags and at least one file\nname. The file specified must contain a full definition of the object\nin YAML or JSON format.", "zh": "在指令式对象配置中，kubectl 命令指定操作（创建，替换等），可选标志和\n至少一个文件名。指定的文件必须包含 YAML 或 JSON 格式的对象的完整定义。"}
{"en": "See the [API reference](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/)\nfor more details on object definitions.", "zh": "有关对象定义的详细信息，请查看\n[API 参考](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/)。\n\n{{< warning >}}"}
{"en": "The imperative `replace` command replaces the existing\nspec with the newly provided one, dropping all changes to the object missing from\nthe configuration file.  This approach should not be used with resource\ntypes whose specs are updated independently of the configuration file.\nServices of type `LoadBalancer`, for example, have their `externalIPs` field updated\nindependently from the configuration by the cluster.", "zh": "`replace` 指令式命令将现有规范替换为新提供的规范，并放弃对配置文件中\n缺少的对象的所有更改。此方法不应与对象规约被独立于配置文件进行更新的\n资源类型一起使用。比如类型为 `LoadBalancer` 的服务，它的 `externalIPs` \n字段就是独立于集群配置进行更新。\n{{< /warning >}}"}
{"en": "### Examples\n\nCreate the objects defined in a configuration file:", "zh": "### 例子\n\n创建配置文件中定义的对象：\n\n```sh\nkubectl create -f nginx.yaml\n```"}
{"en": "Delete the objects defined in two configuration files:", "zh": "删除两个配置文件中定义的对象：\n\n```sh\nkubectl delete -f nginx.yaml -f redis.yaml\n```"}
{"en": "Update the objects defined in a configuration file by overwriting\nthe live configuration:", "zh": "通过覆盖活动配置来更新配置文件中定义的对象：\n\n```sh\nkubectl replace -f nginx.yaml\n```"}
{"en": "### Trade-offs", "zh": "### 权衡"}
{"en": "Advantages compared to imperative commands:\n\n- Object configuration can be stored in a source control system such as Git.\n- Object configuration can integrate with processes such as reviewing changes before push and audit trails.\n- Object configuration provides a template for creating new objects.", "zh": "与指令式命令相比的优点：\n\n- 对象配置可以存储在源控制系统中，比如 Git。\n- 对象配置可以与流程集成，例如在推送和审计之前检查更新。\n- 对象配置提供了用于创建新对象的模板。"}
{"en": "Disadvantages compared to imperative commands:\n\n- Object configuration requires basic understanding of the object schema.\n- Object configuration requires the additional step of writing a YAML file.", "zh": "与指令式命令相比的缺点：\n\n- 对象配置需要对对象架构有基本的了解。\n- 对象配置需要额外的步骤来编写 YAML 文件。"}
{"en": "Advantages compared to declarative object configuration:\n\n- Imperative object configuration behavior is simpler and easier to understand.\n- As of Kubernetes version 1.5, imperative object configuration is more mature.", "zh": "与声明式对象配置相比的优点：\n\n- 指令式对象配置行为更加简单易懂。\n- 从 Kubernetes 1.5 版本开始，指令对象配置更加成熟。"}
{"en": "Disadvantages compared to declarative object configuration:\n\n- Imperative object configuration works best on files, not directories.\n- Updates to live objects must be reflected in configuration files, or they will be lost during the next replacement.", "zh": "与声明式对象配置相比的缺点：\n\n- 指令式对象配置更适合文件，而非目录。\n- 对活动对象的更新必须反映在配置文件中，否则会在下一次替换时丢失。"}
{"en": "## Declarative object configuration", "zh": "## 声明式对象配置"}
{"en": "When using declarative object configuration, a user operates on object\nconfiguration files stored locally, however the user does not define the\noperations to be taken on the files. Create, update, and delete operations\nare automatically detected per-object by `kubectl`. This enables working on\ndirectories, where different operations might be needed for different objects.", "zh": "使用声明式对象配置时，用户对本地存储的对象配置文件进行操作，但是用户\n未定义要对该文件执行的操作。\n`kubectl` 会自动检测每个文件的创建、更新和删除操作。\n这使得配置可以在目录上工作，根据目录中配置文件对不同的对象执行不同的操作。\n\n{{< note >}}"}
{"en": "Declarative object configuration retains changes made by other\nwriters, even if the changes are not merged back to the object configuration file.\nThis is possible by using the `patch` API operation to write only\nobserved differences, instead of using the `replace`\nAPI operation to replace the entire object configuration.", "zh": "声明式对象配置保留其他编写者所做的修改，即使这些更改并未合并到对象配置文件中。\n可以通过使用 `patch` API 操作仅写入观察到的差异，而不是使用 `replace` API\n操作来替换整个对象配置来实现。\n{{< /note >}}"}
{"en": "### Examples", "zh": "### 例子"}
{"en": "Process all object configuration files in the `configs` directory, and create or\npatch the live objects. You can first `diff` to see what changes are going to be\nmade, and then apply:", "zh": "处理 `configs` 目录中的所有对象配置文件，创建并更新活跃对象。\n可以首先使用 `diff` 子命令查看将要进行的更改，然后在进行应用：\n\n```sh\nkubectl diff -f configs/\nkubectl apply -f configs/\n```"}
{"en": "Recursively process directories:", "zh": "递归处理目录：\n\n```sh\nkubectl diff -R -f configs/\nkubectl apply -R -f configs/\n```"}
{"en": "### Trade-offs\n\nAdvantages compared to imperative object configuration:\n\n- Changes made directly to live objects are retained, even if they are not merged back into the configuration files.\n- Declarative object configuration has better support for operating on directories and automatically detecting operation types (create, patch, delete) per-object.", "zh": "### 权衡\n\n与指令式对象配置相比的优点：\n\n- 对活动对象所做的更改即使未合并到配置文件中，也会被保留下来。\n- 声明性对象配置更好地支持对目录进行操作并自动检测每个文件的操作类型（创建，修补，删除）。"}
{"en": "Disadvantages compared to imperative object configuration:\n\n- Declarative object configuration is harder to debug and understand results when they are unexpected.\n- Partial updates using diffs create complex merge and patch operations.", "zh": "与指令式对象配置相比的缺点：\n\n- 声明式对象配置难于调试并且出现异常时结果难以理解。\n- 使用 diff 产生的部分更新会创建复杂的合并和补丁操作。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- [Managing Kubernetes Objects Using Imperative Commands](/docs/tasks/manage-kubernetes-objects/imperative-command/)\n- [Imperative Management of Kubernetes Objects Using Configuration Files](/docs/tasks/manage-kubernetes-objects/imperative-config/)\n- [Declarative Management of Kubernetes Objects Using Configuration Files](/docs/tasks/manage-kubernetes-objects/declarative-config/)\n- [Declarative Management of Kubernetes Objects Using Kustomize](/docs/tasks/manage-kubernetes-objects/kustomization/)\n- [Kubectl Command Reference](/docs/reference/generated/kubectl/kubectl-commands/)\n- [Kubectl Book](https://kubectl.docs.kubernetes.io)\n- [Kubernetes API Reference](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/)", "zh": "- [使用指令式命令管理 Kubernetes 对象](/zh-cn/docs/tasks/manage-kubernetes-objects/imperative-command/)\n- [使用配置文件对 Kubernetes 对象进行命令式管理](/zh-cn/docs/tasks/manage-kubernetes-objects/imperative-config/)\n- [使用配置文件对 Kubernetes 对象进行声明式管理](/zh-cn/docs/tasks/manage-kubernetes-objects/declarative-config/)\n- [使用 Kustomize 对 Kubernetes 对象进行声明式管理](/zh-cn/docs/tasks/manage-kubernetes-objects/kustomization/)\n- [Kubectl 命令参考](/docs/reference/generated/kubectl/kubectl-commands/)\n- [Kubectl Book](https://kubectl.docs.kubernetes.io/zh/)\n- [Kubernetes API 参考](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/)"}
{"en": "This page explains how Kubernetes objects are represented in the Kubernetes API, and how you can\nexpress them in `.yaml` format.", "zh": "本页说明了在 Kubernetes API 中是如何表示 Kubernetes 对象的，\n以及如何使用 `.yaml` 格式的文件表示 Kubernetes 对象。"}
{"en": "## Understanding Kubernetes objects {#kubernetes-objects}\n\n*Kubernetes objects* are persistent entities in the Kubernetes system. Kubernetes uses these\nentities to represent the state of your cluster. Specifically, they can describe:\n\n* What containerized applications are running (and on which nodes)\n* The resources available to those applications\n* The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance", "zh": "## 理解 Kubernetes 对象    {#kubernetes-objects}\n\n在 Kubernetes 系统中，**Kubernetes 对象**是持久化的实体。\nKubernetes 使用这些实体去表示整个集群的状态。\n具体而言，它们描述了如下信息：\n\n* 哪些容器化应用正在运行（以及在哪些节点上运行）\n* 可以被应用使用的资源\n* 关于应用运行时行为的策略，比如重启策略、升级策略以及容错策略"}
{"en": "A Kubernetes object is a \"record of intent\"--once you create the object, the Kubernetes system\nwill constantly work to ensure that the object exists. By creating an object, you're effectively\ntelling the Kubernetes system what you want your cluster's workload to look like; this is your\ncluster's *desired state*.", "zh": "Kubernetes 对象是一种“意向表达（Record of Intent）”。一旦创建该对象，\nKubernetes 系统将不断工作以确保该对象存在。通过创建对象，你本质上是在告知\nKubernetes 系统，你想要的集群工作负载状态看起来应是什么样子的，\n这就是 Kubernetes 集群所谓的**期望状态（Desired State）**。"}
{"en": "To work with Kubernetes objects—whether to create, modify, or delete them—you'll need to use the\n[Kubernetes API](/docs/concepts/overview/kubernetes-api/). When you use the `kubectl` command-line\ninterface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use\nthe Kubernetes API directly in your own programs using one of the\n[Client Libraries](/docs/reference/using-api/client-libraries/).", "zh": "操作 Kubernetes 对象 —— 无论是创建、修改或者删除 —— 需要使用\n[Kubernetes API](/zh-cn/docs/concepts/overview/kubernetes-api)。\n比如，当使用 `kubectl` 命令行接口（CLI）时，CLI 会调用必要的 Kubernetes API；\n也可以在程序中使用[客户端库](/zh-cn/docs/reference/using-api/client-libraries/)，\n来直接调用 Kubernetes API。"}
{"en": "### Object spec and status\n\nAlmost every Kubernetes object includes two nested object fields that govern\nthe object's configuration: the object *`spec`* and the object *`status`*.\nFor objects that have a `spec`, you have to set this when you create the object,\nproviding a description of the characteristics you want the resource to have:\nits _desired state_.", "zh": "### 对象规约（Spec）与状态（Status）    {#object-spec-and-status}\n\n几乎每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置：\n对象 **`spec`（规约）** 和对象 **`status`（状态）**。\n对于具有 `spec` 的对象，你必须在创建对象时设置其内容，描述你希望对象所具有的特征：\n**期望状态（Desired State）**。"}
{"en": "The `status` describes the _current state_ of the object, supplied and updated\nby the Kubernetes system and its components. The Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} continually\nand actively manages every object's actual state to match the desired state you\nsupplied.", "zh": "`status` 描述了对象的**当前状态（Current State）**，它是由 Kubernetes\n系统和组件设置并更新的。在任何时刻，Kubernetes\n{{< glossary_tooltip text=\"控制平面\" term_id=\"control-plane\" >}}\n都一直在积极地管理着对象的实际状态，以使之达成期望状态。"}
{"en": "For example: in Kubernetes, a Deployment is an object that can represent an\napplication running on your cluster. When you create the Deployment, you\nmight set the Deployment `spec` to specify that you want three replicas of\nthe application to be running. The Kubernetes system reads the Deployment\nspec and starts three instances of your desired application--updating\nthe status to match your spec. If any of those instances should fail\n(a status change), the Kubernetes system responds to the difference\nbetween spec and status by making a correction--in this case, starting\na replacement instance.", "zh": "例如，Kubernetes 中的 Deployment 对象能够表示运行在集群中的应用。\n当创建 Deployment 时，你可能会设置 Deployment 的 `spec`，指定该应用要有 3 个副本运行。\nKubernetes 系统读取 Deployment 的 `spec`，\n并启动我们所期望的应用的 3 个实例 —— 更新状态以与规约相匹配。\n如果这些实例中有的失败了（一种状态变更），Kubernetes 系统会通过执行修正操作来响应\n`spec` 和 `status` 间的不一致 —— 意味着它会启动一个新的实例来替换。"}
{"en": "For more information on the object spec, status, and metadata, see the\n[Kubernetes API Conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md).", "zh": "关于对象 spec、status 和 metadata 的更多信息，可参阅\n[Kubernetes API 约定](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md)。"}
{"en": "### Describing a Kubernetes object\n\nWhen you create an object in Kubernetes, you must provide the object spec that describes its\ndesired state, as well as some basic information about the object (such as a name). When you use\nthe Kubernetes API to create the object (either directly or via `kubectl`), that API request must\ninclude that information as JSON in the request body.\nMost often, you provide the information to `kubectl` in a file known as a _manifest_.\nBy convention, manifests are YAML (you could also use JSON format).\nTools such as `kubectl` convert the information from a manifest into JSON or another supported\nserialization format when making the API request over HTTP.", "zh": "### 描述 Kubernetes 对象    {#describing-a-kubernetes-object}\n\n创建 Kubernetes 对象时，必须提供对象的 `spec`，用来描述该对象的期望状态，\n以及关于对象的一些基本信息（例如名称）。\n当使用 Kubernetes API 创建对象时（直接创建或经由 `kubectl` 创建），\nAPI 请求必须在请求主体中包含 JSON 格式的信息。\n大多数情况下，你会通过 **清单（Manifest）** 文件为 `kubectl` 提供这些信息。\n按照惯例，清单是 YAML 格式的（你也可以使用 JSON 格式）。\n像 `kubectl` 这样的工具在通过 HTTP 进行 API 请求时，\n会将清单中的信息转换为 JSON 或其他受支持的序列化格式。"}
{"en": "Here's an example manifest that shows the required fields and object spec for a Kubernetes\nDeployment:", "zh": "这里有一个清单示例文件，展示了 Kubernetes Deployment 的必需字段和对象 `spec`：\n\n{{% code_sample file=\"application/deployment.yaml\" %}}"}
{"en": "One way to create a Deployment using a manifest file like the one above is to use the\n[`kubectl apply`](/docs/reference/generated/kubectl/kubectl-commands#apply) command\nin the `kubectl` command-line interface, passing the `.yaml` file as an argument. Here's an example:", "zh": "与上面使用清单文件来创建 Deployment 类似，另一种方式是使用 `kubectl` 命令行接口（CLI）的\n[`kubectl apply`](/docs/reference/generated/kubectl/kubectl-commands#apply) 命令，\n将 `.yaml` 文件作为参数。下面是一个示例：\n\n```shell\nkubectl apply -f https://k8s.io/examples/application/deployment.yaml\n```"}
{"en": "The output is similar to this:", "zh": "输出类似下面这样：\n\n```\ndeployment.apps/nginx-deployment created\n```"}
{"en": "### Required fields\n\nIn the manifest (YAML or JSON file) for the Kubernetes object you want to create, you'll need to set values for\nthe following fields:\n\n* `apiVersion` - Which version of the Kubernetes API you're using to create this object\n* `kind` - What kind of object you want to create\n* `metadata` - Data that helps uniquely identify the object, including a `name` string, `UID`, and optional `namespace`\n* `spec` - What state you desire for the object", "zh": "### 必需字段    {#required-fields}\n\n在想要创建的 Kubernetes 对象所对应的清单（YAML 或 JSON 文件）中，需要配置的字段如下：\n\n* `apiVersion` - 创建该对象所使用的 Kubernetes API 的版本\n* `kind` - 想要创建的对象的类别\n* `metadata` - 帮助唯一标识对象的一些数据，包括一个 `name` 字符串、`UID` 和可选的 `namespace`\n* `spec` - 你所期望的该对象的状态"}
{"en": "The precise format of the object `spec` is different for every Kubernetes object, and contains\nnested fields specific to that object. The [Kubernetes API Reference](/docs/reference/kubernetes-api/)\ncan help you find the spec format for all of the objects you can create using Kubernetes.", "zh": "对每个 Kubernetes 对象而言，其 `spec` 之精确格式都是不同的，包含了特定于该对象的嵌套字段。\n[Kubernetes API 参考](/zh-cn/docs/reference/kubernetes-api/)可以帮助你找到想要使用\nKubernetes 创建的所有对象的规约格式。"}
{"en": "For example, see the [`spec` field](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec)\nfor the Pod API reference.\nFor each Pod, the `.spec` field specifies the pod and its desired state (such as the container image name for\neach container within that pod).\nAnother example of an object specification is the\n[`spec` field](/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/#StatefulSetSpec)\nfor the StatefulSet API. For StatefulSet, the `.spec` field specifies the StatefulSet and\nits desired state.\nWithin the `.spec` of a StatefulSet is a [template](/docs/concepts/workloads/pods/#pod-templates)\nfor Pod objects. That template describes Pods that the StatefulSet controller will create in order to\nsatisfy the StatefulSet specification.\nDifferent kinds of objects can also have different `.status`; again, the API reference pages\ndetail the structure of that `.status` field, and its content for each different type of object.", "zh": "例如，参阅 Pod API 参考文档中\n[`spec` 字段](/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec)。\n对于每个 Pod，其 `.spec` 字段设置了 Pod 及其期望状态（例如 Pod 中每个容器的容器镜像名称）。\n另一个对象规约的例子是 StatefulSet API 中的\n[`spec` 字段](/zh-cn/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/#StatefulSetSpec)。\n对于 StatefulSet 而言，其 `.spec` 字段设置了 StatefulSet 及其期望状态。\n在 StatefulSet 的 `.spec` 内，有一个为 Pod 对象提供的[模板](/zh-cn/docs/concepts/workloads/pods/#pod-templates)。\n该模板描述了 StatefulSet 控制器为了满足 StatefulSet 规约而要创建的 Pod。\n不同类型的对象可以有不同的 `.status` 信息。API 参考页面给出了 `.status` 字段的详细结构，\n以及针对不同类型 API 对象的具体内容。\n\n{{< note >}}"}
{"en": "See [Configuration Best Practices](/docs/concepts/configuration/overview/) for additional\ninformation on writing YAML configuration files.", "zh": "请查看[配置最佳实践](/zh-cn/docs/concepts/configuration/overview/)来获取有关编写 YAML 配置文件的更多信息。\n{{< /note >}}"}
{"en": "## Server side field validation\n\nStarting with Kubernetes v1.25, the API server offers server side\n[field validation](/docs/reference/using-api/api-concepts/#field-validation)\nthat detects unrecognized or duplicate fields in an object. It provides all the functionality\nof `kubectl --validate` on the server side.", "zh": "## 服务器端字段验证   {#server-side-field-validation}\n\n从 Kubernetes v1.25 开始，API\n服务器提供了服务器端[字段验证](/zh-cn/docs/reference/using-api/api-concepts/#field-validation)，\n可以检测对象中未被识别或重复的字段。它在服务器端提供了 `kubectl --validate` 的所有功能。"}
{"en": "The `kubectl` tool uses the `--validate` flag to set the level of field validation. It accepts the\nvalues `ignore`, `warn`, and `strict` while also accepting the values `true` (equivalent to `strict`)\nand `false` (equivalent to `ignore`). The default validation setting for `kubectl` is `--validate=true`.", "zh": "`kubectl` 工具使用 `--validate` 标志来设置字段验证级别。它接受值\n`ignore`、`warn` 和 `strict`，同时还接受值 `true`（等同于 `strict`）和\n`false`（等同于 `ignore`）。`kubectl` 的默认验证设置为 `--validate=true`。"}
{"en": "`Strict`\n: Strict field validation, errors on validation failure\n\n`Warn`\n: Field validation is performed, but errors are exposed as warnings rather than failing the request\n\n`Ignore`\n: No server side field validation is performed", "zh": "`Strict`\n: 严格的字段验证，验证失败时会报错\n\n`Warn`\n: 执行字段验证，但错误会以警告形式提供而不是拒绝请求\n\n`Ignore`\n: 不执行服务器端字段验证"}
{"en": "When `kubectl` cannot connect to an API server that supports field validation it will fall back\nto using client-side validation. Kubernetes 1.27 and later versions always offer field validation;\nolder Kubernetes releases might not. If your cluster is older than v1.27, check the documentation\nfor your version of Kubernetes.", "zh": "当 `kubectl` 无法连接到支持字段验证的 API 服务器时，它将回退为使用客户端验证。\nKubernetes 1.27 及更高版本始终提供字段验证；较早的 Kubernetes 版本可能没有此功能。\n如果你的集群版本低于 v1.27，可以查阅适用于你的 Kubernetes 版本的文档。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "If you're new to Kubernetes, read more about the following:\n\n* [Pods](/docs/concepts/workloads/pods/) which are the most important basic Kubernetes objects.\n* [Deployment](/docs/concepts/workloads/controllers/deployment/) objects.\n* [Controllers](/docs/concepts/architecture/controller/) in Kubernetes.\n* [kubectl](/docs/reference/kubectl/) and [kubectl commands](/docs/reference/generated/kubectl/kubectl-commands).\n\n[Kubernetes Object Management](/docs/concepts/overview/working-with-objects/object-management/)\nexplains how to use `kubectl` to manage objects.\nYou might need to [install kubectl](/docs/tasks/tools/#kubectl) if you don't already have it available.", "zh": "如果你刚开始学习 Kubernetes，可以进一步阅读以下信息：\n\n* 最重要的 Kubernetes 基本对象 [Pod](/zh-cn/docs/concepts/workloads/pods/)。\n* [Deployment](/zh-cn/docs/concepts/workloads/controllers/deployment/) 对象。\n* Kubernetes 中的[控制器](/zh-cn/docs/concepts/architecture/controller/)。\n* [kubectl](/zh-cn/docs/reference/kubectl/) 和\n  [kubectl 命令](/docs/reference/generated/kubectl/kubectl-commands)。\n\n[Kubernetes 对象管理](/zh-cn/docs/concepts/overview/working-with-objects/object-management/)\n介绍了如何使用 `kubectl` 来管理对象。\n如果你还没有安装 `kubectl`，你可能需要[安装 kubectl](/zh-cn/docs/tasks/tools/#kubectl)。"}
{"en": "To learn about the Kubernetes API in general, visit:\n\n* [Kubernetes API overview](/docs/reference/using-api/)\n\nTo learn about objects in Kubernetes in more depth, read other pages in this section:", "zh": "从总体上了解 Kubernetes API，可以查阅：\n\n* [Kubernetes API 概述](/zh-cn/docs/reference/using-api/)\n\n若要更深入地了解 Kubernetes 对象，可以阅读本节的其他页面："}
{"en": "By default, containers run with unbounded [compute resources](/docs/concepts/configuration/manage-resources-containers/) on a Kubernetes cluster.\nUsing  Kubernetes [resource quotas](/docs/concepts/policy/resource-quotas/),\nadministrators (also termed _cluster operators_) can restrict consumption and creation\nof cluster resources (such as CPU time, memory, and persistent storage) within a specified\n{{< glossary_tooltip text=\"namespace\" term_id=\"namespace\" >}}.\nWithin a namespace, a {{< glossary_tooltip text=\"Pod\" term_id=\"Pod\" >}} can consume as much CPU and memory as is allowed by the ResourceQuotas that apply to that namespace. As a cluster operator, or as a namespace-level administrator, you might also be concerned about making sure that a single object cannot monopolize all available resources within a namespace.\n\nA LimitRange is a policy to constrain the resource allocations (limits and requests) that you can specify for each applicable object kind (such as Pod or {{< glossary_tooltip text=\"PersistentVolumeClaim\" term_id=\"persistent-volume-claim\" >}}) in a namespace.", "zh": "默认情况下， Kubernetes 集群上的容器运行使用的[计算资源](/zh-cn/docs/concepts/configuration/manage-resources-containers/)没有限制。\n使用 Kubernetes [资源配额](/zh-cn/docs/concepts/policy/resource-quotas/)，\n管理员（也称为 **集群操作者**）可以在一个指定的{{< glossary_tooltip text=\"命名空间\" term_id=\"namespace\" >}}内限制集群资源的使用与创建。\n在命名空间中，一个 {{< glossary_tooltip text=\"Pod\" term_id=\"Pod\" >}} 最多能够使用命名空间的资源配额所定义的 CPU 和内存用量。\n作为集群操作者或命名空间级的管理员，你可能也会担心如何确保一个 Pod 不会垄断命名空间内所有可用的资源。\n\nLimitRange 是限制命名空间内可为每个适用的对象类别\n（例如 Pod 或 {{< glossary_tooltip text=\"PersistentVolumeClaim\" term_id=\"persistent-volume-claim\" >}}）\n指定的资源分配量（限制和请求）的策略对象。"}
{"en": "A _LimitRange_ provides constraints that can:\n\n- Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.\n- Enforce minimum and maximum storage request per {{< glossary_tooltip text=\"PersistentVolumeClaim\" term_id=\"persistent-volume-claim\" >}} in a namespace.\n- Enforce a ratio between request and limit for a resource in a namespace.\n- Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime.", "zh": "一个 **LimitRange（限制范围）** 对象提供的限制能够做到：\n\n- 在一个命名空间中实施对每个 Pod 或 Container 最小和最大的资源使用量的限制。\n- 在一个命名空间中实施对每个 {{< glossary_tooltip text=\"PersistentVolumeClaim\" term_id=\"persistent-volume-claim\" >}}\n  能申请的最小和最大的存储空间大小的限制。\n- 在一个命名空间中实施对一种资源的申请值和限制值的比值的控制。\n- 设置一个命名空间中对计算资源的默认申请/限制值，并且自动的在运行时注入到多个 Container 中。"}
{"en": "A LimitRange is enforced in a particular namespace when there is a\nLimitRange object in that namespace.", "zh": "当某命名空间中有一个 LimitRange 对象时，将在该命名空间中实施 LimitRange 限制。"}
{"en": "The name of a LimitRange object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).", "zh": "LimitRange 的名称必须是合法的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。"}
{"en": "## Constraints on resource limits and requests\n\n- The administrator creates a LimitRange in a namespace.\n- Users create (or try to create) objects in that namespace, such as Pods or PersistentVolumeClaims.\n- First, the `LimitRange` admission controller applies default request and limit values for all Pods (and their containers) that do not set compute resource requirements.\n- Second, the `LimitRange` tracks usage to ensure it does not exceed resource minimum, maximum and ratio defined in any `LimitRange` present in the namespace.\n- If you attempt to create or update an object (Pod or PersistentVolumeClaim) that violates a `LimitRange` constraint, your request to the API server will fail with an HTTP status code `403 Forbidden` and a message explaining the constraint that has been violated.\n- If you add a `LimitRange` in a namespace that applies to compute-related resources such as\n `cpu` and `memory`, you must specify\n  requests or limits for those values. Otherwise, the system may reject Pod creation.\n- `LimitRange` validations occur only at Pod admission stage, not on running Pods.\n  If you add or modify a LimitRange, the Pods that already exist in that namespace\n  continue unchanged.\n- If two or more `LimitRange` objects exist in the namespace, it is not deterministic which default value will be applied.", "zh": "## 资源限制和请求的约束   {#constraints-on-resource-limits-and-requests}\n\n- 管理员在一个命名空间内创建一个 `LimitRange` 对象。\n- 用户在此命名空间内创建（或尝试创建） Pod 和 PersistentVolumeClaim 等对象。\n- 首先，`LimitRange` 准入控制器对所有没有设置计算资源需求的所有 Pod（及其容器）设置默认请求值与限制值。\n- 其次，`LimitRange` 跟踪其使用量以保证没有超出命名空间中存在的任意 `LimitRange` 所定义的最小、最大资源使用量以及使用量比值。\n- 若尝试创建或更新的对象（Pod 和 PersistentVolumeClaim）违反了 `LimitRange` 的约束，\n  向 API 服务器的请求会失败，并返回 HTTP 状态码 `403 Forbidden` 以及描述哪一项约束被违反的消息。\n- 若你在命名空间中添加 `LimitRange` 启用了对 `cpu` 和 `memory` 等计算相关资源的限制，\n  你必须指定这些值的请求使用量与限制使用量。否则，系统将会拒绝创建 Pod。\n- `LimitRange` 的验证仅在 Pod 准入阶段进行，不对正在运行的 Pod 进行验证。\n  如果你添加或修改 LimitRange，命名空间中已存在的 Pod 将继续不变。\n- 如果命名空间中存在两个或更多 `LimitRange` 对象，应用哪个默认值是不确定的。"}
{"en": "## LimitRange and admission checks for Pods\n\nA `LimitRange` does **not** check the consistency of the default values it applies. This means that a default value for the _limit_ that is set by `LimitRange` may be less than the _request_ value specified for the container in the spec that a client submits to the API server. If that happens, the final Pod will not be schedulable.\n\nFor example, you define a `LimitRange` with this manifest:", "zh": "## Pod 的 LimitRange 和准入检查     {#limitrange-and-admission-checks-for-pod}\n\n`LimitRange` **不** 检查所应用的默认值的一致性。\n这意味着 `LimitRange` 设置的 **limit** 的默认值可能小于客户端提交给 API 服务器的规约中为容器指定的 **request** 值。\n如果发生这种情况，最终 Pod 将无法调度。\n\n例如，你使用如下清单定义一个 `LimitRange`：\n\n{{% code_sample file=\"concepts/policy/limit-range/problematic-limit-range.yaml\" %}}"}
{"en": "along with a Pod that declares a CPU resource request of `700m`, but not a limit:", "zh": "以及一个声明 CPU 资源请求为 `700m` 但未声明限制值的 Pod：\n\n{{% code_sample file=\"concepts/policy/limit-range/example-conflict-with-limitrange-cpu.yaml\" %}}"}
{"en": "then that Pod will not be scheduled, failing with an error similar to:", "zh": "那么该 Pod 将不会被调度，失败并出现类似以下的错误：\n\n```\nPod \"example-conflict-with-limitrange-cpu\" is invalid: spec.containers[0].resources.requests: Invalid value: \"700m\": must be less than or equal to cpu limit\n```"}
{"en": "If you set both `request` and `limit`, then that new Pod will be scheduled successfully even with the same `LimitRange` in place:", "zh": "如果你同时设置了 `request` 和 `limit`，那么即使使用相同的 `LimitRange`，新 Pod 也会被成功调度：\n\n{{% code_sample file=\"concepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yaml\" %}}"}
{"en": "## Example resource constraints\n\nExamples of policies that could be created using `LimitRange` are:\n\n- In a 2 node cluster with a capacity of 8 GiB RAM and 16 cores, constrain Pods in a namespace to request 100m of CPU with a max limit of 500m for CPU and request 200Mi for Memory with a max limit of 600Mi for Memory.\n- Define default CPU limit and request to 150m and memory default request to 300Mi for Containers started with no cpu and memory requests in their specs.", "zh": "## 资源约束示例   {#example-resource-constraints}\n\n能够使用限制范围创建的策略示例有：\n\n- 在一个有两个节点，8 GiB 内存与16个核的集群中，限制一个命名空间的 Pod 申请\n  100m 单位，最大 500m 单位的 CPU，以及申请 200Mi，最大 600Mi 的内存。\n- 为 spec 中没有 cpu 和内存需求值的 Container 定义默认 CPU 限制值与需求值\n  150m，内存默认需求值 300Mi。"}
{"en": "In the case where the total limits of the namespace is less than the sum of the limits of the Pods/Containers,\nthere may be contention for resources. In this case, the Containers or Pods will not be created.", "zh": "在命名空间的总限制值小于 Pod 或 Container 的限制值的总和的情况下，可能会产生资源竞争。\n在这种情况下，将不会创建 Container 或 Pod。"}
{"en": "Neither contention nor changes to a LimitRange will affect already created resources.", "zh": "竞争和对 LimitRange 的改变都不会影响任何已经创建了的资源。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "For examples on using limits, see:\n\n- [how to configure minimum and maximum CPU constraints per namespace](/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/).\n- [how to configure minimum and maximum Memory constraints per namespace](/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/).\n- [how to configure default CPU Requests and Limits per namespace](/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/).\n- [how to configure default Memory Requests and Limits per namespace](/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/).\n- [how to configure minimum and maximum Storage consumption per namespace](/docs/tasks/administer-cluster/limit-storage-consumption/#limitrange-to-limit-requests-for-storage).\n- a [detailed example on configuring quota per namespace](/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/).\n\nRefer to the [LimitRanger design document](https://git.k8s.io/design-proposals-archive/resource-management/admission_control_limit_range.md) for context and historical information.", "zh": "关于使用限值的例子，可参阅：\n\n- [如何配置每个命名空间最小和最大的 CPU 约束](/zh-cn/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/)。\n- [如何配置每个命名空间最小和最大的内存约束](/zh-cn/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/)。\n- [如何配置每个命名空间默认的 CPU 申请值和限制值](/zh-cn/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/)。\n- [如何配置每个命名空间默认的内存申请值和限制值](/zh-cn/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/)。\n- [如何配置每个命名空间最小和最大存储使用量](/zh-cn/docs/tasks/administer-cluster/limit-storage-consumption/#limitrange-to-limit-requests-for-storage)。\n- [配置每个命名空间的配额的详细例子](/zh-cn/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/)。\n\n有关上下文和历史信息，请参阅 [LimitRanger 设计文档](https://git.k8s.io/design-proposals-archive/resource-management/admission_control_limit_range.md)。"}
{"en": "When several users or teams share a cluster with a fixed number of nodes,\nthere is a concern that one team could use more than its fair share of resources.\n\nResource quotas are a tool for administrators to address this concern.", "zh": "当多个用户或团队共享具有固定节点数目的集群时，人们会担心有人使用超过其基于公平原则所分配到的资源量。\n\n资源配额是帮助管理员解决这一问题的工具。"}
{"en": "A resource quota, defined by a `ResourceQuota` object, provides constraints that limit\naggregate resource consumption per namespace.  It can limit the quantity of objects that can\nbe created in a namespace by type, as well as the total amount of compute resources that may\nbe consumed by resources in that namespace.", "zh": "资源配额，通过 `ResourceQuota` 对象来定义，对每个命名空间的资源消耗总量提供限制。\n它可以限制命名空间中某种类型的对象的总数目上限，也可以限制命名空间中的 Pod 可以使用的计算资源的总上限。"}
{"en": "Resource quotas work like this:", "zh": "资源配额的工作方式如下："}
{"en": "- Different teams work in different namespaces. This can be enforced with [RBAC](/docs/reference/access-authn-authz/rbac/).\n- The administrator creates one ResourceQuota for each namespace.\n- Users create resources (pods, services, etc.) in the namespace, and the quota system\n  tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.\n- If creating or updating a resource violates a quota constraint, the request will fail with HTTP\n  status code `403 FORBIDDEN` with a message explaining the constraint that would have been violated.\n- If quota is enabled in a namespace for compute resources like `cpu` and `memory`, users must specify\n  requests or limits for those values; otherwise, the quota system may reject pod creation.  Hint: Use\n  the `LimitRanger` admission controller to force defaults for pods that make no compute resource requirements.\n\n  See the [walkthrough](/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/)\n  for an example of how to avoid this problem.", "zh": "- 不同的团队可以在不同的命名空间下工作，这可以通过\n  [RBAC](/zh-cn/docs/reference/access-authn-authz/rbac/) 强制执行。\n- 集群管理员可以为每个命名空间创建一个或多个 ResourceQuota 对象。\n- 当用户在命名空间下创建资源（如 Pod、Service 等）时，Kubernetes 的配额系统会跟踪集群的资源使用情况，\n  以确保使用的资源用量不超过 ResourceQuota 中定义的硬性资源限额。\n- 如果资源创建或者更新请求违反了配额约束，那么该请求会报错（HTTP 403 FORBIDDEN），\n  并在消息中给出有可能违反的约束。\n- 如果命名空间下的计算资源（如 `cpu` 和 `memory`）的配额被启用，\n  则用户必须为这些资源设定请求值（request）和约束值（limit），否则配额系统将拒绝 Pod 的创建。\n  提示: 可使用 `LimitRanger` 准入控制器来为没有设置计算资源需求的 Pod 设置默认值。\n  \n  若想避免这类问题，请参考\n  [演练](/zh-cn/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/)示例。\n\n{{< note >}}"}
{"en": "- For `cpu` and `memory` resources, ResourceQuotas enforce that **every**\n(new) pod in that namespace sets a limit for that resource.\nIf you enforce a resource quota in a namespace for either `cpu` or `memory`,\nyou, and other clients, **must** specify either `requests` or `limits` for that resource,\nfor every new Pod you submit. If you don't, the control plane may reject admission\nfor that Pod.\n- For other resources: ResourceQuota works and will ignore pods in the namespace without setting a limit or request for that resource. It means that you can create a new pod without limit/request ephemeral storage if the resource quota limits the ephemeral storage of this namespace.\nYou can use a [LimitRange](/docs/concepts/policy/limit-range/) to automatically set\na default request for these resources.", "zh": "- 对于 `cpu` 和 `memory` 资源：ResourceQuota 强制该命名空间中的**每个**（新）Pod 为该资源设置限制。\n  如果你在命名空间中为 `cpu` 和 `memory` 实施资源配额，\n  你或其他客户端**必须**为你提交的每个新 Pod 指定该资源的 `requests` 或 `limits`。\n  否则，控制平面可能会拒绝接纳该 Pod。\n- 对于其他资源：ResourceQuota 可以工作，并且会忽略命名空间中的 Pod，而无需为该资源设置限制或请求。\n  这意味着，如果资源配额限制了此命名空间的临时存储，则可以创建没有限制/请求临时存储的新 Pod。\n  你可以使用[限制范围](/zh-cn/docs/concepts/policy/limit-range/)自动设置对这些资源的默认请求。\n{{< /note >}}"}
{"en": "The name of a ResourceQuota object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).", "zh": "ResourceQuota 对象的名称必须是合法的\n[DNS 子域名](/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)。"}
{"en": "Examples of policies that could be created using namespaces and quotas are:", "zh": "下面是使用命名空间和配额构建策略的示例："}
{"en": "- In a cluster with a capacity of 32 GiB RAM, and 16 cores, let team A use 20 GiB and 10 cores,\n  let B use 10GiB and 4 cores, and hold 2GiB and 2 cores in reserve for future allocation.\n- Limit the \"testing\" namespace to using 1 core and 1GiB RAM.  Let the \"production\" namespace\n  use any amount.", "zh": "- 在具有 32 GiB 内存和 16 核 CPU 资源的集群中，允许 A 团队使用 20 GiB 内存 和 10 核的 CPU 资源，\n  允许 B 团队使用 10 GiB 内存和 4 核的 CPU 资源，并且预留 2 GiB 内存和 2 核的 CPU 资源供将来分配。\n- 限制 \"testing\" 命名空间使用 1 核 CPU 资源和 1GiB 内存。允许 \"production\" 命名空间使用任意数量。"}
{"en": "In the case where the total capacity of the cluster is less than the sum of the quotas of the namespaces,\nthere may be contention for resources.  This is handled on a first-come-first-served basis.\n\nNeither contention nor changes to quota will affect already created resources.", "zh": "在集群容量小于各命名空间配额总和的情况下，可能存在资源竞争。资源竞争时，Kubernetes 系统会遵循先到先得的原则。\n\n不管是资源竞争还是配额的修改，都不会影响已经创建的资源使用对象。"}
{"en": "## Enabling Resource Quota\n\nResource Quota support is enabled by default for many Kubernetes distributions. It is\nenabled when the {{< glossary_tooltip text=\"API server\" term_id=\"kube-apiserver\" >}}\n`--enable-admission-plugins=` flag has `ResourceQuota` as\none of its arguments.", "zh": "## 启用资源配额  {#enabling-resource-quota}\n\n资源配额的支持在很多 Kubernetes 版本中是默认启用的。\n当 {{< glossary_tooltip text=\"API 服务器\" term_id=\"kube-apiserver\" >}}\n的命令行标志 `--enable-admission-plugins=` 中包含 `ResourceQuota` 时，\n资源配额会被启用。"}
{"en": "A resource quota is enforced in a particular namespace when there is a\nResourceQuota in that namespace.", "zh": "当命名空间中存在一个 ResourceQuota 对象时，对于该命名空间而言，资源配额就是开启的。"}
{"en": "## Compute Resource Quota\n\nYou can limit the total sum of\n[compute resources](/docs/concepts/configuration/manage-resources-containers/)\nthat can be requested in a given namespace.", "zh": "## 计算资源配额  {#compute-resource-quota}\n\n用户可以对给定命名空间下的可被请求的\n[计算资源](/zh-cn/docs/concepts/configuration/manage-resources-containers/)\n总量进行限制。"}
{"en": "The following resource types are supported:", "zh": "配额机制所支持的资源类型："}
{"en": "| Resource Name | Description |\n| --------------------- | --------------------------------------------------------- |\n| `limits.cpu` | Across all pods in a non-terminal state, the sum of CPU limits cannot exceed this value. |\n| `limits.memory` | Across all pods in a non-terminal state, the sum of memory limits cannot exceed this value. |\n| `requests.cpu` | Across all pods in a non-terminal state, the sum of CPU requests cannot exceed this value. |\n| `requests.memory` | Across all pods in a non-terminal state, the sum of memory requests cannot exceed this value. |\n| `hugepages-<size>` | Across all pods in a non-terminal state, the number of huge page requests of the specified size cannot exceed this value. |\n| `cpu` | Same as `requests.cpu` |\n| `memory` | Same as `requests.memory` |", "zh": "| 资源名称 | 描述 |\n| --------------------- | --------------------------------------------- |\n| `limits.cpu` | 所有非终止状态的 Pod，其 CPU 限额总量不能超过该值。 |\n| `limits.memory` | 所有非终止状态的 Pod，其内存限额总量不能超过该值。 |\n| `requests.cpu` | 所有非终止状态的 Pod，其 CPU 需求总量不能超过该值。 |\n| `requests.memory` | 所有非终止状态的 Pod，其内存需求总量不能超过该值。 |\n| `hugepages-<size>` | 对于所有非终止状态的 Pod，针对指定尺寸的巨页请求总数不能超过此值。 |\n| `cpu` | 与 `requests.cpu` 相同。 |\n| `memory` | 与 `requests.memory` 相同。 |"}
{"en": "### Resource Quota For Extended Resources\n\nIn addition to the resources mentioned above, in release 1.10, quota support for\n[extended resources](/docs/concepts/configuration/manage-resources-containers/#extended-resources) is added.", "zh": "### 扩展资源的资源配额  {#resource-quota-for-extended-resources}\n\n除上述资源外，在 Kubernetes 1.10 版本中，\n还添加了对[扩展资源](/zh-cn/docs/concepts/configuration/manage-resources-containers/#extended-resources)\n的支持。"}
{"en": "As overcommit is not allowed for extended resources, it makes no sense to specify both `requests`\nand `limits` for the same extended resource in a quota. So for extended resources, only quota items\nwith prefix `requests.` is allowed for now.", "zh": "由于扩展资源不可超量分配，因此没有必要在配额中为同一扩展资源同时指定 `requests` 和 `limits`。\n对于扩展资源而言，目前仅允许使用前缀为 `requests.` 的配额项。"}
{"en": "Take the GPU resource as an example, if the resource name is `nvidia.com/gpu`, and you want to\nlimit the total number of GPUs requested in a namespace to 4, you can define a quota as follows:", "zh": "以 GPU 拓展资源为例，如果资源名称为 `nvidia.com/gpu`，并且要将命名空间中请求的 GPU\n资源总数限制为 4，则可以如下定义配额：\n\n* `requests.nvidia.com/gpu: 4`"}
{"en": "See [Viewing and Setting Quotas](#viewing-and-setting-quotas) for more detail information.", "zh": "有关更多详细信息，请参阅[查看和设置配额](#viewing-and-setting-quotas)。"}
{"en": "## Storage Resource Quota\n\nYou can limit the total sum of [storage resources](/docs/concepts/storage/persistent-volumes/) that can be requested in a given namespace.\n\nIn addition, you can limit consumption of storage resources based on associated storage-class.", "zh": "## 存储资源配额  {#storage-resource-quota}\n\n用户可以对给定命名空间下的[存储资源](/zh-cn/docs/concepts/storage/persistent-volumes/)\n总量进行限制。\n\n此外，还可以根据相关的存储类（Storage Class）来限制存储资源的消耗。"}
{"en": "| Resource Name | Description |\n| --------------------- | --------------------------------------------------------- |\n| `requests.storage` | Across all persistent volume claims, the sum of storage requests cannot exceed this value. |\n| `persistentvolumeclaims` | The total number of [PersistentVolumeClaims](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) that can exist in the namespace. |\n| `<storage-class-name>.storageclass.storage.k8s.io/requests.storage` | Across all persistent volume claims associated with the `<storage-class-name>`, the sum of storage requests cannot exceed this value. |\n| `<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims` | Across all persistent volume claims associated with the `<storage-class-name>`, the total number of [persistent volume claims](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) that can exist in the namespace. |", "zh": "| 资源名称 | 描述 |\n| --------------------- | ----------------------------------------------------------- |\n| `requests.storage` | 所有 PVC，存储资源的需求总量不能超过该值。 |\n| `persistentvolumeclaims` | 在该命名空间中所允许的 [PVC](/zh-cn/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) 总量。 |\n| `<storage-class-name>.storageclass.storage.k8s.io/requests.storage` | 在所有与 `<storage-class-name>` 相关的持久卷申领中，存储请求的总和不能超过该值。 |\n| `<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims` | 在与 storage-class-name 相关的所有持久卷申领中，命名空间中可以存在的[持久卷申领](/zh-cn/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)总数。 |"}
{"en": "For example, if an operator wants to quota storage with `gold` storage class separate from `bronze` storage class, the operator can\ndefine a quota as follows:", "zh": "例如，如果一个操作人员针对 `gold` 存储类型与 `bronze` 存储类型设置配额，\n操作人员可以定义如下配额：\n\n* `gold.storageclass.storage.k8s.io/requests.storage: 500Gi`\n* `bronze.storageclass.storage.k8s.io/requests.storage: 100Gi`"}
{"en": "In release 1.8, quota support for local ephemeral storage is added as an alpha feature:", "zh": "在 Kubernetes 1.8 版本中，本地临时存储的配额支持已经是 Alpha 功能："}
{"en": "| Resource Name | Description |\n| ------------------------------- |----------------------------------------------------------- |\n| `requests.ephemeral-storage` | Across all pods in the namespace, the sum of local ephemeral storage requests cannot exceed this value. |\n| `limits.ephemeral-storage` | Across all pods in the namespace, the sum of local ephemeral storage limits cannot exceed this value. |\n| `ephemeral-storage` | Same as `requests.ephemeral-storage`. |", "zh": "| 资源名称 | 描述 |\n| ------------------------------- |----------------------------------------------------------- |\n| `requests.ephemeral-storage` | 在命名空间的所有 Pod 中，本地临时存储请求的总和不能超过此值。 |\n| `limits.ephemeral-storage` | 在命名空间的所有 Pod 中，本地临时存储限制值的总和不能超过此值。 |\n| `ephemeral-storage` | 与 `requests.ephemeral-storage` 相同。 |\n\n{{< note >}}"}
{"en": "When using a CRI container runtime, container logs will count against the ephemeral storage quota.\nThis can result in the unexpected eviction of pods that have exhausted their storage quotas.\nRefer to [Logging Architecture](/docs/concepts/cluster-administration/logging/) for details.", "zh": "如果所使用的是 CRI 容器运行时，容器日志会被计入临时存储配额，\n这可能会导致存储配额耗尽的 Pod 被意外地驱逐出节点。\n参考[日志架构](/zh-cn/docs/concepts/cluster-administration/logging/)了解详细信息。\n{{< /note >}}"}
{"en": "## Object Count Quota\n\nYou can set quota for *the total number of one particular resource kind* in the Kubernetes API,\nusing the following syntax:\n\n* `count/<resource>.<group>` for resources from non-core groups\n* `count/<resource>` for resources from the core group", "zh": "## 对象数量配额  {#object-count-quota}\n\n你可以使用以下语法为 Kubernetes API 中“一种特定资源类型的总数”设置配额：\n\n* `count/<resource>.<group>`：用于非核心（core）组的资源\n* `count/<resource>`：用于核心组的资源"}
{"en": "Here is an example set of resources users may want to put under object count quota:", "zh": "这是用户可能希望利用对象计数配额来管理的一组资源示例：\n\n* `count/persistentvolumeclaims`\n* `count/services`\n* `count/secrets`\n* `count/configmaps`\n* `count/replicationcontrollers`\n* `count/deployments.apps`\n* `count/replicasets.apps`\n* `count/statefulsets.apps`\n* `count/jobs.batch`\n* `count/cronjobs.batch`"}
{"en": "If you define a quota this way, it applies to Kubernetes' APIs that are part of the API server, and\nto any custom resources backed by a CustomResourceDefinition. If you use [API aggregation](/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/) to\nadd additional, custom APIs that are not defined as CustomResourceDefinitions, the core Kubernetes\ncontrol plane does not enforce quota for the aggregated API. The extension API  server is expected to\nprovide quota enforcement if that's appropriate for the custom API.\nFor example, to create a quota on a `widgets` custom resource in the `example.com` API group, use `count/widgets.example.com`.", "zh": "如果你以这种方式定义配额，它将应用于属于 API 服务器一部分的 Kubernetes API，以及 CustomResourceDefinition\n支持的任何自定义资源。\n如果你使用[聚合 API](/zh-cn/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)\n添加未定义为 CustomResourceDefinitions 的其他自定义 API，则核心 Kubernetes 控制平面不会对聚合 API 实施配额管理。\n如果合适，扩展 API 服务器需要为自定义 API 提供配额管理。\n例如，要对 `example.com` API 组中的自定义资源 `widgets` 设置配额，请使用\n`count/widgets.example.com`。"}
{"en": "When using such a resource quota (nearly for all object kinds), an object is charged\nagainst the quota if the object kind exists (is defined) in the control plane.\nThese types of quotas are useful to protect against exhaustion of storage resources.  For example, you may\nwant to limit the number of Secrets in a server given their large size. Too many Secrets in a cluster can\nactually prevent servers and controllers from starting. You can set a quota for Jobs to protect against\na poorly configured CronJob. CronJobs that create too many Jobs in a namespace can lead to a denial of service.", "zh": "当使用这样的资源配额（几乎涵盖所有对象类别）时，如果对象类别在控制平面中已存在（已定义），\n则该对象管理会参考配额设置。\n这些类型的配额有助于防止存储资源耗尽。例如，用户可能想根据服务器的存储能力来对服务器中\nSecret 的数量进行配额限制。\n集群中存在过多的 Secret 实际上会导致服务器和控制器无法启动。\n用户可以选择对 Job 进行配额管理，以防止配置不当的 CronJob 在某命名空间中创建太多\nJob 而导致集群拒绝服务。"}
{"en": "There is another syntax only to set the same type of quota for certain resources.\nThe following types are supported:", "zh": "还有另一种语法仅用于为某些资源设置相同类型的配额。\n\n支持以下类型："}
{"en": "| Resource Name | Description |\n| ----------------------------|--------------------------------------------- |\n| `configmaps` | The total number of ConfigMaps that can exist in the namespace. |\n| `persistentvolumeclaims` | The total number of [PersistentVolumeClaims](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) that can exist in the namespace. |\n| `pods` | The total number of Pods in a non-terminal state that can exist in the namespace.  A pod is in a terminal state if `.status.phase in (Failed, Succeeded)` is true.  |\n| `replicationcontrollers` | The total number of ReplicationControllers that can exist in the namespace. |\n| `resourcequotas` | The total number of ResourceQuotas that can exist in the namespace. |\n| `services` | The total number of Services that can exist in the namespace. |\n| `services.loadbalancers` | The total number of Services of type `LoadBalancer` that can exist in the namespace. |\n| `services.nodeports` | The total number of `NodePorts` allocated to Services of type `NodePort` or `LoadBalancer` that can exist in the namespace.                                                      |\n| `secrets` | The total number of Secrets that can exist in the namespace. |", "zh": "| 资源名称 | 描述 |\n| ------------------------------- | ------------------------------------------------- |\n| `configmaps` | 在该命名空间中允许存在的 ConfigMap 总数上限。 |\n| `persistentvolumeclaims` | 在该命名空间中允许存在的 [PVC](/zh-cn/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims) 的总数上限。 |\n| `pods` | 在该命名空间中允许存在的非终止状态的 Pod 总数上限。Pod 终止状态等价于 Pod 的 `.status.phase in (Failed, Succeeded)` 为真。 |\n| `replicationcontrollers` | 在该命名空间中允许存在的 ReplicationController 总数上限。 |\n| `resourcequotas` | 在该命名空间中允许存在的 ResourceQuota 总数上限。 |\n| `services` | 在该命名空间中允许存在的 Service 总数上限。 |\n| `services.loadbalancers` | 在该命名空间中允许存在的 LoadBalancer 类型的 Service 总数上限。 |\n| `services.nodeports` | 在该命名空间中允许存在的 NodePort 或 LoadBalancer 类型的 Service 的 NodePort 总数上限。 |\n| `secrets` | 在该命名空间中允许存在的 Secret 总数上限。 |"}
{"en": "For example, `pods` quota counts and enforces a maximum on the number of `pods`\ncreated in a single namespace that are not terminal. You might want to set a `pods`\nquota on a namespace to avoid the case where a user creates many small pods and\nexhausts the cluster's supply of Pod IPs.", "zh": "例如，`pods` 配额统计某个命名空间中所创建的、非终止状态的 `pods` 个数并确保其不超过某上限值。\n用户可能希望在某命名空间中设置 `pods` 配额，以避免有用户创建很多小的 Pod，\n从而耗尽集群所能提供的 Pod IP 地址。"}
{"en": "您可以在[查看和设置配额](#viewing-and-setting-quotas) 上找到更多示例。", "zh": "你可以在[查看和设置配额](#viewing-and-setting-quotas)节查看更多示例。"}
{"en": "## Quota Scopes\n\nEach quota can have an associated set of `scopes`. A quota will only measure usage for a resource if it matches\nthe intersection of enumerated scopes.", "zh": "## 配额作用域   {#quota-scopes}\n\n每个配额都有一组相关的 `scope`（作用域），配额只会对作用域内的资源生效。\n配额机制仅统计所列举的作用域的交集中的资源用量。"}
{"en": "When a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope.\nResources specified on the quota outside of the allowed set results in a validation error.", "zh": "当一个作用域被添加到配额中后，它会对作用域相关的资源数量作限制。\n如配额中指定了允许（作用域）集合之外的资源，会导致验证错误。"}
{"en": "| Scope | Description |\n| ----- | ------------ |\n| `Terminating` | Match pods where `.spec.activeDeadlineSeconds >= 0` |\n| `NotTerminating` | Match pods where `.spec.activeDeadlineSeconds is nil` |\n| `BestEffort` | Match pods that have best effort quality of service. |\n| `NotBestEffort` | Match pods that do not have best effort quality of service. |\n| `PriorityClass` | Match pods that references the specified [priority class](/docs/concepts/scheduling-eviction/pod-priority-preemption). |\n| `CrossNamespacePodAffinity` | Match pods that have cross-namespace pod [(anti)affinity terms](/docs/concepts/scheduling-eviction/assign-pod-node). |", "zh": "| 作用域 | 描述 |\n| ----- | ----------- |\n| `Terminating` | 匹配所有 `spec.activeDeadlineSeconds` 不小于 0 的 Pod。 |\n| `NotTerminating` | 匹配所有 `spec.activeDeadlineSeconds` 是 nil 的 Pod。 |\n| `BestEffort` | 匹配所有 Qos 是 BestEffort 的 Pod。 |\n| `NotBestEffort` | 匹配所有 Qos 不是 BestEffort 的 Pod。 |\n| `PriorityClass` | 匹配所有引用了所指定的[优先级类](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption)的 Pod。 |\n| `CrossNamespacePodAffinity` | 匹配那些设置了跨名字空间[（反）亲和性条件](/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node)的 Pod。 |"}
{"en": "The `BestEffort` scope restricts a quota to tracking the following resource:\n\n* `pods`\n\nThe `Terminating`, `NotTerminating`, `NotBestEffort` and `PriorityClass`\nscopes restrict a quota to tracking the following resources:", "zh": "`BestEffort` 作用域限制配额跟踪以下资源：\n\n* `pods`\n\n`Terminating`、`NotTerminating`、`NotBestEffort` 和 `PriorityClass` 这些作用域限制配额跟踪以下资源：\n\n* `pods`\n* `cpu`\n* `memory`\n* `requests.cpu`\n* `requests.memory`\n* `limits.cpu`\n* `limits.memory`"}
{"en": "Note that you cannot specify both the `Terminating` and the `NotTerminating`\nscopes in the same quota, and you cannot specify both the `BestEffort` and\n`NotBestEffort` scopes in the same quota either.\n\nThe `scopeSelector` supports the following values in the `operator` field:", "zh": "需要注意的是，你不可以在同一个配额对象中同时设置 `Terminating` 和 `NotTerminating`\n作用域，你也不可以在同一个配额中同时设置 `BestEffort` 和 `NotBestEffort`\n作用域。\n\n`scopeSelector` 支持在 `operator` 字段中使用以下值：\n\n* `In`\n* `NotIn`\n* `Exists`\n* `DoesNotExist`"}
{"en": "When using one of the following values as the `scopeName` when defining the\n`scopeSelector`, the `operator` must be `Exists`.", "zh": "定义 `scopeSelector` 时，如果使用以下值之一作为 `scopeName` 的值，则对应的\n`operator` 只能是 `Exists`。\n\n* `Terminating`\n* `NotTerminating`\n* `BestEffort`\n* `NotBestEffort`"}
{"en": "If the `operator` is `In` or `NotIn`, the `values` field must have at least\none value. For example:", "zh": "如果 `operator` 是 `In` 或 `NotIn` 之一，则 `values` 字段必须至少包含一个值。\n例如：\n\n```yaml\n  scopeSelector:\n    matchExpressions:\n      - scopeName: PriorityClass\n        operator: In\n        values:\n          - middle\n```"}
{"en": "If the `operator` is `Exists` or `DoesNotExist`, the `values` field must *NOT* be\nspecified.", "zh": "如果 `operator` 为 `Exists` 或 `DoesNotExist`，则**不**可以设置 `values` 字段。"}
{"en": "### Resource Quota Per PriorityClass", "zh": "### 基于优先级类（PriorityClass）来设置资源配额  {#resource-quota-per-priorityclass}\n\n{{< feature-state for_k8s_version=\"v1.17\" state=\"stable\" >}}"}
{"en": "Pods can be created at a specific [priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority).\nYou can control a pod's consumption of system resources based on a pod's priority, by using the `scopeSelector`\nfield in the quota spec.", "zh": "Pod 可以创建为特定的[优先级](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority)。\n通过使用配额规约中的 `scopeSelector` 字段，用户可以根据 Pod 的优先级控制其系统资源消耗。"}
{"en": "A quota is matched and consumed only if `scopeSelector` in the quota spec selects the pod.", "zh": "仅当配额规范中的 `scopeSelector` 字段选择到某 Pod 时，配额机制才会匹配和计量 Pod 的资源消耗。"}
{"en": "When quota is scoped for priority class using `scopeSelector` field, quota object\nis restricted to track only following resources:", "zh": "如果配额对象通过 `scopeSelector` 字段设置其作用域为优先级类，\n则配额对象只能跟踪以下资源：\n\n* `pods`\n* `cpu`\n* `memory`\n* `ephemeral-storage`\n* `limits.cpu`\n* `limits.memory`\n* `limits.ephemeral-storage`\n* `requests.cpu`\n* `requests.memory`\n* `requests.ephemeral-storage`"}
{"en": "This example creates a quota object and matches it with pods at specific priorities. The example\nworks as follows:", "zh": "本示例创建一个配额对象，并将其与具有特定优先级的 Pod 进行匹配，其工作方式如下："}
{"en": "- Pods in the cluster have one of the three priority classes, \"low\", \"medium\", \"high\".\n- One quota object is created for each priority.", "zh": "- 集群中的 Pod 可取三个优先级类之一，即 \"low\"、\"medium\"、\"high\"。\n- 为每个优先级创建一个配额对象。"}
{"en": "Save the following YAML to a file `quota.yml`.", "zh": "将以下 YAML 保存到文件 `quota.yml` 中。\n\n```yaml\napiVersion: v1\nkind: List\nitems:\n- apiVersion: v1\n  kind: ResourceQuota\n  metadata:\n    name: pods-high\n  spec:\n    hard:\n      cpu: \"1000\"\n      memory: 200Gi\n      pods: \"10\"\n    scopeSelector:\n      matchExpressions:\n      - operator: In\n        scopeName: PriorityClass\n        values: [\"high\"]\n- apiVersion: v1\n  kind: ResourceQuota\n  metadata:\n    name: pods-medium\n  spec:\n    hard:\n      cpu: \"10\"\n      memory: 20Gi\n      pods: \"10\"\n    scopeSelector:\n      matchExpressions:\n      - operator: In\n        scopeName: PriorityClass\n        values: [\"medium\"]\n- apiVersion: v1\n  kind: ResourceQuota\n  metadata:\n    name: pods-low\n  spec:\n    hard:\n      cpu: \"5\"\n      memory: 10Gi\n      pods: \"10\"\n    scopeSelector:\n      matchExpressions:\n      - operator: In\n        scopeName: PriorityClass\n        values: [\"low\"]\n```"}
{"en": "Apply the YAML using `kubectl create`.", "zh": "使用 `kubectl create` 命令运行以下操作。\n\n```shell\nkubectl create -f ./quota.yml\n```\n\n```\nresourcequota/pods-high created\nresourcequota/pods-medium created\nresourcequota/pods-low created\n```"}
{"en": "Verify that `Used` quota is `0` using `kubectl describe quota`.", "zh": "使用 `kubectl describe quota` 操作验证配额的 `Used` 值为 `0`。\n\n```shell\nkubectl describe quota\n```\n\n```\nName:       pods-high\nNamespace:  default\nResource    Used  Hard\n--------    ----  ----\ncpu         0     1k\nmemory      0     200Gi\npods        0     10\n\n\nName:       pods-low\nNamespace:  default\nResource    Used  Hard\n--------    ----  ----\ncpu         0     5\nmemory      0     10Gi\npods        0     10\n\n\nName:       pods-medium\nNamespace:  default\nResource    Used  Hard\n--------    ----  ----\ncpu         0     10\nmemory      0     20Gi\npods        0     10\n```"}
{"en": "Create a pod with priority \"high\". Save the following YAML to a\nfile `high-priority-pod.yml`.", "zh": "创建优先级为 \"high\" 的 Pod。\n将以下 YAML 保存到文件 `high-priority-pod.yml` 中。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: high-priority\nspec:\n  containers:\n  - name: high-priority\n    image: ubuntu\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"while true; do echo hello; sleep 10;done\"]\n    resources:\n      requests:\n        memory: \"10Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"10Gi\"\n        cpu: \"500m\"\n  priorityClassName: high\n```"}
{"en": "Apply it with `kubectl create`.", "zh": "使用 `kubectl create` 运行以下操作。\n\n```shell\nkubectl create -f ./high-priority-pod.yml\n```"}
{"en": "Verify that \"Used\" stats for \"high\" priority quota, `pods-high`, has changed and that\nthe other two quotas are unchanged.", "zh": "确认 \"high\" 优先级配额 `pods-high` 的 \"Used\" 统计信息已更改，并且其他两个配额未更改。\n\n```shell\nkubectl describe quota\n```\n\n```\nName:       pods-high\nNamespace:  default\nResource    Used  Hard\n--------    ----  ----\ncpu         500m  1k\nmemory      10Gi  200Gi\npods        1     10\n\n\nName:       pods-low\nNamespace:  default\nResource    Used  Hard\n--------    ----  ----\ncpu         0     5\nmemory      0     10Gi\npods        0     10\n\n\nName:       pods-medium\nNamespace:  default\nResource    Used  Hard\n--------    ----  ----\ncpu         0     10\nmemory      0     20Gi\npods        0     10\n```"}
{"en": "### Cross-namespace Pod Affinity Quota", "zh": "### 跨名字空间的 Pod 亲和性配额   {#cross-namespace-pod-affinity-quota}\n\n{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}"}
{"en": "Operators can use `CrossNamespacePodAffinity` quota scope to limit which namespaces are allowed to\nhave pods with affinity terms that cross namespaces. Specifically, it controls which pods are allowed\nto set `namespaces` or `namespaceSelector` fields in pod affinity terms.", "zh": "集群运维人员可以使用 `CrossNamespacePodAffinity`\n配额作用域来限制哪个名字空间中可以存在包含跨名字空间亲和性规则的 Pod。\n更为具体一点，此作用域用来配置哪些 Pod 可以在其 Pod 亲和性规则中设置\n`namespaces` 或 `namespaceSelector` 字段。"}
{"en": "Preventing users from using cross-namespace affinity terms might be desired since a pod\nwith anti-affinity constraints can block pods from all other namespaces \nfrom getting scheduled in a failure domain.", "zh": "禁止用户使用跨名字空间的亲和性规则可能是一种被需要的能力，\n因为带有反亲和性约束的 Pod 可能会阻止所有其他名字空间的 Pod 被调度到某失效域中。"}
{"en": "Using this scope operators can prevent certain namespaces (`foo-ns` in the example below)\nfrom having pods that use cross-namespace pod affinity by creating a resource quota object in\nthat namespace with `CrossNamespacePodAffinity` scope and hard limit of 0:", "zh": "使用此作用域操作符可以避免某些名字空间（例如下面例子中的 `foo-ns`）运行特别的 Pod，\n这类 Pod 使用跨名字空间的 Pod 亲和性约束，在该名字空间中创建了作用域为\n`CrossNamespacePodAffinity` 的、硬性约束为 0 的资源配额对象。\n\n```yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: disable-cross-namespace-affinity\n  namespace: foo-ns\nspec:\n  hard:\n    pods: \"0\"\n  scopeSelector:\n    matchExpressions:\n    - scopeName: CrossNamespacePodAffinity\n      operator: Exists\n```"}
{"en": "If operators want to disallow using `namespaces` and `namespaceSelector` by default, and\nonly allow it for specific namespaces, they could configure `CrossNamespacePodAffinity`\nas a limited resource by setting the kube-apiserver flag --admission-control-config-file\nto the path of the following configuration file:", "zh": "如果集群运维人员希望默认禁止使用 `namespaces` 和 `namespaceSelector`，\n而仅仅允许在特定命名空间中这样做，他们可以将 `CrossNamespacePodAffinity`\n作为一个被约束的资源。方法是为 `kube-apiserver` 设置标志\n`--admission-control-config-file`，使之指向如下的配置文件：\n\n```yaml\napiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: \"ResourceQuota\"\n  configuration:\n    apiVersion: apiserver.config.k8s.io/v1\n    kind: ResourceQuotaConfiguration\n    limitedResources:\n    - resource: pods\n      matchScopes:\n      - scopeName: CrossNamespacePodAffinity\n        operator: Exists\n```"}
{"en": "With the above configuration, pods can use `namespaces` and `namespaceSelector` in pod affinity only\nif the namespace where they are created have a resource quota object with\n`CrossNamespacePodAffinity` scope and a hard limit greater than or equal to the number of pods using those fields.", "zh": "基于上面的配置，只有名字空间中包含作用域为 `CrossNamespacePodAffinity`\n且硬性约束大于或等于使用 `namespaces` 和 `namespaceSelector` 字段的 Pod\n个数时，才可以在该名字空间中继续创建在其 Pod 亲和性规则中设置 `namespaces`\n或 `namespaceSelector` 的新 Pod。"}
{"en": "## Requests compared to Limits {#requests-vs-limits}\n\nWhen allocating compute resources, each container may specify a request and a limit value for either CPU or memory.\nThe quota can be configured to quota either value.", "zh": "## 请求与限制的比较   {#requests-vs-limits}\n\n分配计算资源时，每个容器可以为 CPU 或内存指定请求和约束。\n配额可以针对二者之一进行设置。"}
{"en": "If the quota has a value specified for `requests.cpu` or `requests.memory`, then it requires that every incoming\ncontainer makes an explicit request for those resources.  If the quota has a value specified for `limits.cpu` or `limits.memory`,\nthen it requires that every incoming container specifies an explicit limit for those resources.", "zh": "如果配额中指定了 `requests.cpu` 或 `requests.memory` 的值，则它要求每个容器都显式给出对这些资源的请求。\n同理，如果配额中指定了 `limits.cpu` 或 `limits.memory` 的值，那么它要求每个容器都显式设定对应资源的限制。"}
{"en": "## Viewing and Setting Quotas\n\nKubectl supports creating, updating, and viewing quotas:", "zh": "## 查看和设置配额   {#viewing-and-setting-quotas}\n\nkubectl 支持创建、更新和查看配额：\n\n```shell\nkubectl create namespace myspace\n```\n\n```shell\ncat <<EOF > compute-resources.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-resources\nspec:\n  hard:\n    requests.cpu: \"1\"\n    requests.memory: 1Gi\n    limits.cpu: \"2\"\n    limits.memory: 2Gi\n    requests.nvidia.com/gpu: 4\nEOF\n```\n\n```shell\nkubectl create -f ./compute-resources.yaml --namespace=myspace\n```\n\n```shell\ncat <<EOF > object-counts.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: object-counts\nspec:\n  hard:\n    configmaps: \"10\"\n    persistentvolumeclaims: \"4\"\n    pods: \"4\"\n    replicationcontrollers: \"20\"\n    secrets: \"10\"\n    services: \"10\"\n    services.loadbalancers: \"2\"\nEOF\n```\n\n```shell\nkubectl create -f ./object-counts.yaml --namespace=myspace\n```\n\n```shell\nkubectl get quota --namespace=myspace\n```\n\n```none\nNAME                    AGE\ncompute-resources       30s\nobject-counts           32s\n```\n\n```shell\nkubectl describe quota compute-resources --namespace=myspace\n```\n\n```none\nName:                    compute-resources\nNamespace:               myspace\nResource                 Used  Hard\n--------                 ----  ----\nlimits.cpu               0     2\nlimits.memory            0     2Gi\nrequests.cpu             0     1\nrequests.memory          0     1Gi\nrequests.nvidia.com/gpu  0     4\n```\n\n```shell\nkubectl describe quota object-counts --namespace=myspace\n```\n\n```none\nName:                   object-counts\nNamespace:              myspace\nResource                Used    Hard\n--------                ----    ----\nconfigmaps              0       10\npersistentvolumeclaims  0       4\npods                    0       4\nreplicationcontrollers  0       20\nsecrets                 1       10\nservices                0       10\nservices.loadbalancers  0       2\n```"}
{"en": "Kubectl also supports object count quota for all standard namespaced resources\nusing the syntax `count/<resource>.<group>`:", "zh": "kubectl 还使用语法 `count/<resource>.<group>` 支持所有标准的、命名空间域的资源的对象计数配额：\n\n```shell\nkubectl create namespace myspace\n```\n\n```shell\nkubectl create quota test --hard=count/deployments.apps=2,count/replicasets.apps=4,count/pods=3,count/secrets=4 --namespace=myspace\n```\n\n```shell\nkubectl create deployment nginx --image=nginx --namespace=myspace --replicas=2\n```\n\n```shell\nkubectl describe quota --namespace=myspace\n```\n\n```\nName:                         test\nNamespace:                    myspace\nResource                      Used  Hard\n--------                      ----  ----\ncount/deployments.apps        1     2\ncount/pods                    2     3\ncount/replicasets.apps        1     4\ncount/secrets                 1     4\n```"}
{"en": "## Quota and Cluster Capacity\n\nResourceQuotas are independent of the cluster capacity. They are\nexpressed in absolute units.  So, if you add nodes to your cluster, this does *not*\nautomatically give each namespace the ability to consume more resources.", "zh": "## 配额和集群容量   {#quota-and-cluster-capacity}\n\nResourceQuota 与集群资源总量是完全独立的。它们通过绝对的单位来配置。\n所以，为集群添加节点时，资源配额**不会**自动赋予每个命名空间消耗更多资源的能力。"}
{"en": "Sometimes more complex policies may be desired, such as:\n\n- Proportionally divide total cluster resources among several teams.\n- Allow each tenant to grow resource usage as needed, but have a generous\n  limit to prevent accidental resource exhaustion.\n- Detect demand from one namespace, add nodes, and increase quota.", "zh": "有时可能需要资源配额支持更复杂的策略，比如：\n\n- 在几个团队中按比例划分总的集群资源。\n- 允许每个租户根据需要增加资源使用量，但要有足够的限制以防止资源意外耗尽。\n- 探测某个命名空间的需求，添加物理节点并扩大资源配额值。"}
{"en": "Such policies could be implemented using `ResourceQuotas` as building blocks, by\nwriting a \"controller\" that watches the quota usage and adjusts the quota\nhard limits of each namespace according to other signals.", "zh": "这些策略可以通过将资源配额作为一个组成模块、手动编写一个控制器来监控资源使用情况，\n并结合其他信号调整命名空间上的硬性资源配额来实现。"}
{"en": "Note that resource quota divides up aggregate cluster resources, but it creates no\nrestrictions around nodes: pods from several namespaces may run on the same node.", "zh": "注意：资源配额对集群资源总体进行划分，但它对节点没有限制：来自不同命名空间的 Pod 可能在同一节点上运行。"}
{"en": "## Limit Priority Class consumption by default\n\nIt may be desired that pods at a particular priority, eg. \"cluster-services\",\nshould be allowed in a namespace, if and only if, a matching quota object exists.", "zh": "## 默认情况下限制特定优先级的资源消耗  {#limit-priority-class-consumption-by-default}\n\n有时候可能希望当且仅当某名字空间中存在匹配的配额对象时，才可以创建特定优先级\n（例如 \"cluster-services\"）的 Pod。"}
{"en": "With this mechanism, operators are able to restrict usage of certain high\npriority classes to a limited number of namespaces and not every namespace\nwill be able to consume these priority classes by default.", "zh": "通过这种机制，操作人员能够限制某些高优先级类仅出现在有限数量的命名空间中，\n而并非每个命名空间默认情况下都能够使用这些优先级类。"}
{"en": "To enforce this, `kube-apiserver` flag `--admission-control-config-file` should be\nused to pass path to the following configuration file:", "zh": "要实现此目的，应设置 `kube-apiserver` 的标志 `--admission-control-config-file` \n指向如下配置文件：\n\n```yaml\napiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: \"ResourceQuota\"\n  configuration:\n    apiVersion: apiserver.config.k8s.io/v1\n    kind: ResourceQuotaConfiguration\n    limitedResources:\n    - resource: pods\n      matchScopes:\n      - scopeName: PriorityClass\n        operator: In\n        values: [\"cluster-services\"]\n```"}
{"en": "Then, create a resource quota object in the `kube-system` namespace:", "zh": "现在在 `kube-system` 名字空间中创建一个资源配额对象：\n\n{{% code_sample file=\"policy/priority-class-resourcequota.yaml\" %}}\n\n```shell\nkubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-system\n```\n\n```none\nresourcequota/pods-cluster-services created\n```"}
{"en": "In this case, a pod creation will be allowed if:\n\n1.  the Pod's `priorityClassName` is not specified.\n1.  the Pod's `priorityClassName` is specified to a value other than `cluster-services`.\n1.  the Pod's `priorityClassName` is set to `cluster-services`, it is to be created\n   in the `kube-system` namespace, and it has passed the resource quota check.", "zh": "在这里，当以下条件满足时可以创建 Pod：\n\n1. Pod 未设置 `priorityClassName`\n1. Pod 的 `priorityClassName` 设置值不是 `cluster-services`\n1. Pod 的 `priorityClassName` 设置值为 `cluster-services`，它将被创建于\n   `kube-system` 名字空间中，并且它已经通过了资源配额检查。"}
{"en": "A Pod creation request is rejected if its `priorityClassName` is set to `cluster-services`\nand it is to be created in a namespace other than `kube-system`.", "zh": "如果 Pod 的 `priorityClassName` 设置为 `cluster-services`，但要被创建到\n`kube-system` 之外的别的名字空间，则 Pod 创建请求也被拒绝。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- See [ResourceQuota design doc](https://git.k8s.io/design-proposals-archive/resource-management/admission_control_resource_quota.md) for more information.\n- See a [detailed example for how to use resource quota](/docs/tasks/administer-cluster/quota-api-object/).\n- Read [Quota support for priority class design doc](https://git.k8s.io/design-proposals-archive/scheduling/pod-priority-resourcequota.md).\n- See [LimitedResources](https://github.com/kubernetes/kubernetes/pull/36765)", "zh": "- 参阅[资源配额设计文档](https://git.k8s.io/design-proposals-archive/resource-management/admission_control_resource_quota.md)。\n- 参阅[如何使用资源配额的详细示例](/zh-cn/docs/tasks/administer-cluster/quota-api-object/)。\n- 参阅[优先级类配额支持的设计文档](https://git.k8s.io/design-proposals-archive/scheduling/pod-priority-resourcequota.md)了解更多信息。\n- 参阅 [LimitedResources](https://github.com/kubernetes/kubernetes/pull/36765)。"}
{"en": "In order to support latency-critical and high-throughput workloads, Kubernetes offers a suite of Resource Managers. The managers aim to co-ordinate and optimise node's resources alignment for pods configured with a specific requirement for CPUs, devices, and memory (hugepages) resources.", "zh": "Kubernetes 提供了一组资源管理器，用于支持延迟敏感的、高吞吐量的工作负载。\n资源管理器的目标是协调和优化节点资源，以支持对 CPU、设备和内存（巨页）等资源有特殊需求的 Pod。"}
{"en": "The main manager, the Topology Manager, is a Kubelet component that co-ordinates the overall resource management process through its [policy](/docs/tasks/administer-cluster/topology-manager/).\n\nThe configuration of individual managers is elaborated in dedicated documents:", "zh": "主管理器，也叫拓扑管理器（Topology Manager），是一个 Kubelet 组件，\n它通过[策略](/zh-cn/docs/tasks/administer-cluster/topology-manager/)，\n协调全局的资源管理过程。\n\n各个管理器的配置方式会在专项文档中详细阐述："}
{"en": "- [CPU Manager Policies](/docs/tasks/administer-cluster/cpu-management-policies/)\n- [Device Manager](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager)\n- [Memory Manager Policies](/docs/tasks/administer-cluster/memory-manager/)", "zh": "- [CPU 管理器策略](/zh-cn/docs/tasks/administer-cluster/cpu-management-policies/)\n- [设备管理器](/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager)\n- [内存管理器策略](/zh-cn/docs/tasks/administer-cluster/memory-manager/)"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.20\" state=\"stable\" >}}"}
{"en": "Kubernetes allow you to limit the number of process IDs (PIDs) that a\n{{< glossary_tooltip term_id=\"Pod\" text=\"Pod\" >}} can use.\nYou can also reserve a number of allocatable PIDs for each {{< glossary_tooltip term_id=\"node\" text=\"node\" >}}\nfor use by the operating system and daemons (rather than by Pods).", "zh": "Kubernetes 允许你限制一个 {{< glossary_tooltip term_id=\"Pod\" text=\"Pod\" >}}\n中可以使用的进程 ID（PID）数目。\n你也可以为每个{{< glossary_tooltip term_id=\"node\" text=\"节点\" >}}预留一定数量的可分配的 PID，\n供操作系统和守护进程（而非 Pod）使用。"}
{"en": "Process IDs (PIDs) are a fundamental resource on nodes. It is trivial to hit the\ntask limit without hitting any other resource limits, which can then cause\ninstability to a host machine.", "zh": "进程 ID（PID）是节点上的一种基础资源。很容易就会在尚未超出其它资源约束的时候就已经触及任务个数上限，\n进而导致宿主机器不稳定。"}
{"en": "Cluster administrators require mechanisms to ensure that Pods running in the\ncluster cannot induce PID exhaustion that prevents host daemons (such as the\n{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} or\n{{< glossary_tooltip text=\"kube-proxy\" term_id=\"kube-proxy\" >}},\nand potentially also the container runtime) from running.\nIn addition, it is important to ensure that PIDs are limited among Pods in order\nto ensure they have limited impact on other workloads on the same node.", "zh": "集群管理员需要一定的机制来确保集群中运行的 Pod 不会导致 PID 资源枯竭，\n甚而造成宿主机上的守护进程（例如\n{{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} 或者\n{{< glossary_tooltip text=\"kube-proxy\" term_id=\"kube-proxy\" >}}\n乃至包括容器运行时本身）无法正常运行。\n此外，确保 Pod 中 PID 的个数受限对于保证其不会影响到同一节点上其它负载也很重要。\n\n{{< note >}}"}
{"en": "On certain Linux installations, the operating system sets the PIDs limit to a low default,\nsuch as `32768`. Consider raising the value of `/proc/sys/kernel/pid_max`.", "zh": "在某些 Linux 安装环境中，操作系统会将 PID 约束设置为一个较低的默认值，例如\n`32768`。这时可以考虑提升 `/proc/sys/kernel/pid_max` 的设置值。\n{{< /note >}}"}
{"en": "You can configure a kubelet to limit the number of PIDs a given Pod can consume.\nFor example, if your node's host OS is set to use a maximum of `262144` PIDs and\nexpect to host less than `250` Pods, one can give each Pod a budget of `1000`\nPIDs to prevent using up that node's overall number of available PIDs. If the\nadmin wants to overcommit PIDs similar to CPU or memory, they may do so as well\nwith some additional risks. Either way, a single Pod will not be able to bring\nthe whole machine down. This kind of resource limiting helps to prevent simple\nfork bombs from affecting operation of an entire cluster.", "zh": "你可以配置 kubelet 限制给定 Pod 能够使用的 PID 个数。\n例如，如果你的节点上的宿主操作系统被设置为最多可使用 `262144` 个 PID，\n同时预期节点上会运行的 Pod 个数不会超过 `250`，那么你可以为每个 Pod 设置 `1000` 个 PID\n的预算，避免耗尽该节点上可用 PID 的总量。\n如果管理员系统像 CPU 或内存那样允许对 PID 进行过量分配（Overcommit），他们也可以这样做，\n只是会有一些额外的风险。不管怎样，任何一个 Pod 都不可以将整个机器的运行状态破坏。\n这类资源限制有助于避免简单的派生炸弹（Fork Bomb）影响到整个集群的运行。"}
{"en": "Per-Pod PID limiting allows administrators to protect one Pod from another, but\ndoes not ensure that all Pods scheduled onto that host are unable to impact the node overall.\nPer-Pod limiting also does not protect the node agents themselves from PID exhaustion.\n\nYou can also reserve an amount of PIDs for node overhead, separate from the\nallocation to Pods. This is similar to how you can reserve CPU, memory, or other\nresources for use by the operating system and other facilities outside of Pods\nand their containers.", "zh": "在 Pod 级别设置 PID 限制使得管理员能够保护 Pod 之间不会互相伤害，\n不过无法确保所有调度到该宿主机器上的所有 Pod 都不会影响到节点整体。\nPod 级别的限制也无法保护节点代理任务自身不会受到 PID 耗尽的影响。\n\n你也可以预留一定量的 PID，作为节点的额外开销，与分配给 Pod 的 PID 集合独立。\n这有点类似于在给操作系统和其它设施预留 CPU、内存或其它资源时所做的操作，\n这些任务都在 Pod 及其所包含的容器之外运行。"}
{"en": "PID limiting is a an important sibling to [compute\nresource](/docs/concepts/configuration/manage-resources-containers/) requests\nand limits. However, you specify it in a different way: rather than defining a\nPod's resource limit in the `.spec` for a Pod, you configure the limit as a\nsetting on the kubelet. Pod-defined PID limits are not currently supported.", "zh": "PID 限制是与[计算资源](/zh-cn/docs/concepts/configuration/manage-resources-containers/)\n请求和限制相辅相成的一种机制。不过，你需要用一种不同的方式来设置这一限制：\n你需要将其设置到 kubelet 上而不是在 Pod 的 `.spec` 中为 Pod 设置资源限制。\n目前还不支持在 Pod 级别设置 PID 限制。\n\n{{< caution >}}"}
{"en": "This means that the limit that applies to a Pod may be different depending on\nwhere the Pod is scheduled. To make things simple, it's easiest if all Nodes use\nthe same PID resource limits and reservations.", "zh": "这意味着，施加在 Pod 之上的限制值可能因为 Pod 运行所在的节点不同而有差别。\n为了简化系统，最简单的方法是为所有节点设置相同的 PID 资源限制和预留值。\n{{< /caution >}}"}
{"en": "## Node PID limits\n\nKubernetes allows you to reserve a number of process IDs for the system use. To\nconfigure the reservation, use the parameter `pid=<number>` in the\n`--system-reserved` and `--kube-reserved` command line options to the kubelet.\nThe value you specified declares that the specified number of process IDs will\nbe reserved for the system as a whole and for Kubernetes system daemons\nrespectively.", "zh": "## 节点级别 PID 限制   {#node-pid-limits}\n\nKubernetes 允许你为系统预留一定量的进程 ID。为了配置预留数量，你可以使用\nkubelet 的 `--system-reserved` 和 `--kube-reserved` 命令行选项中的参数\n`pid=<number>`。你所设置的参数值分别用来声明为整个系统和 Kubernetes\n系统守护进程所保留的进程 ID 数目。"}
{"en": "## Pod PID limits\n\nKubernetes allows you to limit the number of processes running in a Pod. You\nspecify this limit at the node level, rather than configuring it as a resource\nlimit for a particular Pod. Each Node can have a different PID limit.  \nTo configure the limit, you can specify the command line parameter `--pod-max-pids`\nto the kubelet, or set `PodPidsLimit` in the kubelet\n[configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).", "zh": "## Pod 级别 PID 限制   {#pod-pid-limits}\n\nKubernetes 允许你限制 Pod 中运行的进程个数。你可以在节点级别设置这一限制，\n而不是为特定的 Pod 来将其设置为资源限制。每个节点都可以有不同的 PID 限制设置。\n要设置限制值，你可以设置 kubelet 的命令行参数 `--pod-max-pids`，或者在 kubelet\n的[配置文件](/zh-cn/docs/tasks/administer-cluster/kubelet-config-file/)中设置\n`PodPidsLimit`。"}
{"en": "## PID based eviction\n\nYou can configure kubelet to start terminating a Pod when it is misbehaving and consuming abnormal amount of resources.\nThis feature is called eviction. You can\n[Configure Out of Resource Handling](/docs/concepts/scheduling-eviction/node-pressure-eviction/)\nfor various eviction signals.\nUse `pid.available` eviction signal to configure the threshold for number of PIDs used by Pod.\nYou can set soft and hard eviction policies.\nHowever, even with the hard eviction policy, if the number of PIDs growing very fast,\nnode can still get into unstable state by hitting the node PIDs limit.\nEviction signal value is calculated periodically and does NOT enforce the limit.", "zh": "## 基于 PID 的驱逐    {#pid-based-eviction}\n\n你可以配置 kubelet 使之在 Pod 行为不正常或者消耗不正常数量资源的时候将其终止。这一特性称作驱逐。\n你可以针对不同的驱逐信号[配置资源不足的处理](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)。\n使用 `pid.available` 驱逐信号来配置 Pod 使用的 PID 个数的阈值。\n你可以设置硬性的和软性的驱逐策略。不过，即使使用硬性的驱逐策略，\n如果 PID 个数增长过快，节点仍然可能因为触及节点 PID 限制而进入一种不稳定状态。\n驱逐信号的取值是周期性计算的，而不是一直能够强制实施约束。"}
{"en": "PID limiting - per Pod and per Node sets the hard limit.\nOnce the limit is hit, workload will start experiencing failures when trying to get a new PID.\nIt may or may not lead to rescheduling of a Pod,\ndepending on how workload reacts on these failures and how liveness and readiness\nprobes are configured for the Pod. However, if limits were set correctly,\nyou can guarantee that other Pods workload and system processes will not run out of PIDs\nwhen one Pod is misbehaving.", "zh": "Pod 级别和节点级别的 PID 限制会设置硬性限制。\n一旦触及限制值，工作负载会在尝试获得新的 PID 时开始遇到问题。\n这可能会也可能不会导致 Pod 被重新调度，取决于工作负载如何应对这类失败以及\nPod 的存活性和就绪态探测是如何配置的。\n可是，如果限制值被正确设置，你可以确保其它 Pod 负载和系统进程不会因为某个\nPod 行为不正常而没有 PID 可用。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Refer to the [PID Limiting enhancement document](https://github.com/kubernetes/enhancements/blob/097b4d8276bc9564e56adf72505d43ce9bc5e9e8/keps/sig-node/20190129-pid-limiting.md) for more information.\n- For historical context, read\n  [Process ID Limiting for Stability Improvements in Kubernetes 1.14](/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/).\n- Read [Managing Resources for Containers](/docs/concepts/configuration/manage-resources-containers/).\n- Learn how to [Configure Out of Resource Handling](/docs/concepts/scheduling-eviction/node-pressure-eviction/).", "zh": "- 参阅 [PID 约束改进文档](https://github.com/kubernetes/enhancements/blob/097b4d8276bc9564e56adf72505d43ce9bc5e9e8/keps/sig-node/20190129-pid-limiting.md)\n  以了解更多信息。\n- 关于历史背景，请阅读\n  [Kubernetes 1.14 中限制进程 ID 以提升稳定性](/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/)\n  的博文。\n- 请阅读[为容器管理资源](/zh-cn/docs/concepts/configuration/manage-resources-containers/)。\n- 学习如何[配置资源不足情况的处理](/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/)。"}
{"en": "Kubernetes policies are configurations that manage other configurations or runtime behaviors. Kubernetes offers various forms of policies, described below:", "zh": "Kubernetes 策略是管理其他配置或运行时行为的一些配置。\nKubernetes 提供了各种形式的策略，具体如下所述："}
{"en": "## Apply policies using API objects\n\n Some API objects act as policies. Here are some examples:\n* [NetworkPolicies](/docs/concepts/services-networking/network-policies/) can be used to restrict ingress and egress traffic for a workload.\n* [LimitRanges](/docs/concepts/policy/limit-range/) manage resource allocation constraints across different object kinds.\n* [ResourceQuotas](/docs/concepts/policy/resource-quotas/) limit resource consumption for a {{< glossary_tooltip text=\"namespace\" term_id=\"namespace\" >}}.", "zh": "## 使用 API 对象应用策略   {#apply-policies-using-api-objects}\n\n一些 API 对象可用作策略。以下是一些示例：\n\n* [NetworkPolicy](/zh-cn/docs/concepts/services-networking/network-policies/) 用于限制工作负载的出入站流量。\n* [LimitRange](/zh-cn/docs/concepts/policy/limit-range/) 管理多个不同对象类别的资源分配约束。\n* [ResourceQuota](/zh-cn/docs/concepts/policy/resource-quotas/)\n  限制{{< glossary_tooltip text=\"名字空间\" term_id=\"namespace\" >}}的资源消耗。"}
{"en": "## Apply policies using admission controllers\n\nAn {{< glossary_tooltip text=\"admission controller\" term_id=\"admission-controller\" >}}\nruns in the API server\nand can validate or mutate API requests. Some admission controllers act to apply policies.\nFor example, the [AlwaysPullImages](/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages) admission controller modifies a new Pod to set the image pull policy to `Always`.", "zh": "## 使用准入控制器应用策略   {#apply-policies-using-admission-controllers}\n\n{{< glossary_tooltip text=\"准入控制器\" term_id=\"admission-controller\" >}}运行在 API 服务器上，\n可以验证或变更 API 请求。某些准入控制器用于应用策略。\n例如，[AlwaysPullImages](/zh-cn/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages)\n准入控制器会修改新 Pod，将镜像拉取策略设置为 `Always`。"}
{"en": "Kubernetes has several built-in admission controllers that are configurable via the API server `--enable-admission-plugins` flag.\n\nDetails on admission controllers, with the complete list of available admission controllers, are documented in a dedicated section:\n\n* [Admission Controllers](/docs/reference/access-authn-authz/admission-controllers/)", "zh": "Kubernetes 具有多个内置的准入控制器，可通过 API 服务器的 `--enable-admission-plugins` 标志进行配置。\n\n关于准入控制器的详细信息（包括可用准入控制器的完整列表），请查阅专门的章节：\n\n* [准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers/)"}
{"en": "## Apply policies using ValidatingAdmissionPolicy\n\nValidating admission policies allow configurable validation checks to be executed in the API server using the Common Expression Language (CEL). For example, a `ValidatingAdmissionPolicy` can be used to disallow use of the `latest` image tag.", "zh": "## 使用 ValidatingAdmissionPolicy 应用策略   {#apply-policies-using-validatingadmissionpolicy}\n\n验证性的准入策略允许使用通用表达式语言 (CEL) 在 API 服务器中执行可配置的验证检查。\n例如，`ValidatingAdmissionPolicy` 可用于禁止使用 `latest` 镜像标签。"}
{"en": "A `ValidatingAdmissionPolicy` operates on an API request and can be used to block, audit, and warn users about non-compliant configurations.\n\nDetails on the `ValidatingAdmissionPolicy` API, with examples, are documented in a dedicated section:\n* [Validating Admission Policy](/docs/reference/access-authn-authz/validating-admission-policy/)", "zh": "`ValidatingAdmissionPolicy` 对请求 API 进行操作，可就不合规的配置执行阻止、审计和警告用户等操作。\n有关 `ValidatingAdmissionPolicy` API 的详细信息及示例，请查阅专门的章节：\n\n* [验证准入策略](/zh-cn/docs/reference/access-authn-authz/validating-admission-policy/)"}
{"en": "## Apply policies using dynamic admission control\n\nDynamic admission controllers (or admission webhooks) run outside the API server as separate applications that register to receive webhooks requests to perform validation or mutation of API requests.", "zh": "## 使用动态准入控制应用策略   {#apply-policies-using-dynamic-admission-control}\n\n动态准入控制器（或准入 Webhook）作为单独的应用在 API 服务器之外运行，\n这些应用注册自身后可以接收 Webhook 请求以便对 API 请求进行验证或变更。"}
{"en": "Dynamic admission controllers can be used to apply policies on API requests and trigger other policy-based workflows. A dynamic admission controller can perform complex checks including those that require retrieval of other cluster resources and external data. For example, an image verification check can lookup data from OCI registries to validate the container image signatures and attestations.\n\nDetails on dynamic admission control are documented in a dedicated section:\n* [Dynamic Admission Control](/docs/reference/access-authn-authz/extensible-admission-controllers/)", "zh": "动态准入控制器可用于在 API 请求上应用策略并触发其他基于策略的工作流。\n动态准入控制器可以执行一些复杂的检查，包括需要读取其他集群资源和外部数据的复杂检查。\n例如，镜像验证检查可以从 OCI 镜像仓库中查找数据，以验证容器镜像签名和证明信息。\n\n有关动态准入控制的详细信息，请查阅专门的章节：\n\n* [动态准入控制](/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/)"}
{"en": "### Implementations {#implementations-admission-control}", "zh": "### 实现 {#implementations-admission-control}\n\n{{% thirdparty-content %}}"}
{"en": "Dynamic Admission Controllers that act as flexible policy engines are being developed in the Kubernetes ecosystem, such as:\n- [Kubewarden](https://github.com/kubewarden)\n- [Kyverno](https://kyverno.io)\n- [OPA Gatekeeper](https://github.com/open-policy-agent/gatekeeper)\n- [Polaris](https://polaris.docs.fairwinds.com/admission-controller/)", "zh": "Kubernetes 生态系统中正在开发作为灵活策略引擎的动态准入控制器，例如：\n\n- [Kubewarden](https://github.com/kubewarden)\n- [Kyverno](https://kyverno.io)\n- [OPA Gatekeeper](https://github.com/open-policy-agent/gatekeeper)\n- [Polaris](https://polaris.docs.fairwinds.com/admission-controller/)"}
{"en": "## Apply policies using Kubelet configurations\n\nKubernetes allows configuring the Kubelet on each worker node.  Some Kubelet configurations act as policies:\n* [Process ID limits and reservations](/docs/concepts/policy/pid-limiting/) are used to limit and reserve allocatable PIDs.\n* [Node Resource Managers](/docs/concepts/policy/node-resource-managers/) can manage compute, memory, and device resources for latency-critical and high-throughput workloads.", "zh": "## 使用 Kubelet 配置应用策略   {#apply-policies-using-kubelet-configurations}\n\nKubernetes 允许在每个工作节点上配置 Kubelet。一些 Kubelet 配置可以视为策略：\n\n* [进程 ID 限制和保留](/zh-cn/docs/concepts/policy/pid-limiting/)用于限制和保留可分配的 PID。\n* [节点资源管理器](/zh-cn/docs/concepts/policy/node-resource-managers/)可以为低延迟和高吞吐量工作负载管理计算、内存和设备资源。"}
{"en": "This page explains proxies used with Kubernetes.", "zh": "本文讲述了 Kubernetes 中所使用的代理。"}
{"en": "## Proxies\n\nThere are several different proxies you may encounter when using Kubernetes:", "zh": "## 代理  {#proxies}\n\n用户在使用 Kubernetes 的过程中可能遇到几种不同的代理（proxy）："}
{"en": "1.  The [kubectl proxy](/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api):\n\n    - runs on a user's desktop or in a pod\n    - proxies from a localhost address to the Kubernetes apiserver\n    - client to proxy uses HTTP\n    - proxy to apiserver uses HTTPS\n    - locates apiserver\n    - adds authentication headers", "zh": "1. [kubectl proxy](/zh-cn/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api)：\n\n    - 运行在用户的桌面或 pod 中\n    - 从本机地址到 Kubernetes apiserver 的代理\n    - 客户端到代理使用 HTTP 协议\n    - 代理到 apiserver 使用 HTTPS 协议\n    - 指向 apiserver\n    - 添加认证头信息"}
{"en": "1.  The [apiserver proxy](/docs/tasks/access-application-cluster/access-cluster-services/#discovering-builtin-services):\n\n    - is a bastion built into the apiserver\n    - connects a user outside of the cluster to cluster IPs which otherwise might not be reachable\n    - runs in the apiserver processes\n    - client to proxy uses HTTPS (or http if apiserver so configured)\n    - proxy to target may use HTTP or HTTPS as chosen by proxy using available information\n    - can be used to reach a Node, Pod, or Service\n    - does load balancing when used to reach a Service", "zh": "2. [apiserver proxy](/zh-cn/docs/tasks/access-application-cluster/access-cluster-services/#discovering-builtin-services)：\n\n    - 是一个建立在 apiserver 内部的“堡垒”\n    - 将集群外部的用户与集群 IP 相连接，这些 IP 是无法通过其他方式访问的\n    - 运行在 apiserver 进程内\n    - 客户端到代理使用 HTTPS 协议 (如果配置 apiserver 使用 HTTP 协议，则使用 HTTP 协议)\n    - 通过可用信息进行选择，代理到目的地可能使用 HTTP 或 HTTPS 协议\n    - 可以用来访问 Node、 Pod 或 Service\n    - 当用来访问 Service 时，会进行负载均衡"}
{"en": "1.  The [kube proxy](/docs/concepts/services-networking/service/#ips-and-vips):\n\n    - runs on each node\n    - proxies UDP, TCP and SCTP\n    - does not understand HTTP\n    - provides load balancing\n    - is only used to reach services", "zh": "3. [kube proxy](/zh-cn/docs/concepts/services-networking/service/#ips-and-vips)：\n\n    - 在每个节点上运行\n    - 代理 UDP、TCP 和 SCTP\n    - 不支持 HTTP\n    - 提供负载均衡能力\n    - 只用来访问 Service"}
{"en": "1.  A Proxy/Load-balancer in front of apiserver(s):\n\n    - existence and implementation varies from cluster to cluster (e.g. nginx)\n    - sits between all clients and one or more apiservers\n    - acts as load balancer if there are several apiservers.", "zh": "4. apiserver 之前的代理/负载均衡器：\n\n    - 在不同集群中的存在形式和实现不同 (如 nginx)\n    - 位于所有客户端和一个或多个 API 服务器之间\n    - 存在多个 API 服务器时，扮演负载均衡器的角色"}
{"en": "1.  Cloud Load Balancers on external services:\n\n    - are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)\n    - are created automatically when the Kubernetes service has type `LoadBalancer`\n    - usually supports UDP/TCP only\n    - SCTP support is up to the load balancer implementation of the cloud provider\n    - implementation varies by cloud provider.", "zh": "5. 外部服务的云负载均衡器：\n\n    - 由一些云供应商提供 (如 AWS ELB、Google Cloud Load Balancer)\n    - Kubernetes 服务类型为 `LoadBalancer` 时自动创建\n    - 通常仅支持 UDP/TCP 协议\n    - SCTP 支持取决于云供应商的负载均衡器实现\n    - 不同云供应商的云负载均衡器实现不同"}
{"en": "Kubernetes users will typically not need to worry about anything other than the first two types.  The cluster admin\nwill typically ensure that the latter types are set up correctly.", "zh": "Kubernetes 用户通常只需要关心前两种类型的代理，集群管理员通常需要确保后面几种类型的代理设置正确。"}
{"en": "## Requesting redirects\n\nProxies have replaced redirect capabilities.  Redirects have been deprecated.", "zh": "## 请求重定向\n\n代理已经取代重定向功能，重定向功能已被弃用。"}
{"en": "overview", "zh": "{{< feature-state feature_gate_name=\"CoordinatedLeaderElection\" >}}"}
{"en": "Kubernetes {{< skew currentVersion >}} includes an alpha feature that allows {{<\nglossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} components to\ndeterministically select a leader via _coordinated leader election_.\nThis is useful to satisfy Kubernetes version skew constraints during cluster upgrades.\nCurrently, the only builtin selection strategy is `OldestEmulationVersion`,\npreferring the leader with the lowest emulation version, followed by binary\nversion, followed by creation timestamp.", "zh": "Kubernetes {{< skew currentVersion >}} 包含一个 Alpha 特性，\n允许{{< glossary_tooltip text=\"控制平面\" term_id=\"control-plane\" >}}组件通过**协调领导者选举**确定性地选择一个领导者。\n这对于在集群升级期间满足 Kubernetes 版本偏差约束非常有用。\n目前，唯一内置的选择策略是 `OldestEmulationVersion`，\n此策略会优先选择最低仿真版本作为领导者，其次按二进制版本选择领导者，最后会按创建时间戳选择领导者。"}
{"en": "## Enabling coordinated leader election\n\nEnsure that `CoordinatedLeaderElection` [feature\ngate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled\nwhen you start the {{< glossary_tooltip text=\"API Server\"\nterm_id=\"kube-apiserver\" >}}: and that the `coordination.k8s.io/v1alpha1` API group is\nenabled.", "zh": "## 启用协调领导者选举   {#enabling-coordinated-leader-election}\n\n确保你在启动 {{< glossary_tooltip text=\"API 服务器\" term_id=\"kube-apiserver\" >}}时\n`CoordinatedLeaderElection` [特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)被启用，\n并且 `coordination.k8s.io/v1alpha1` API 组被启用。"}
{"en": "This can be done by setting flags `--feature-gates=\"CoordinatedLeaderElection=true\"` and\n`--runtime-config=\"coordination.k8s.io/v1alpha1=true\"`.", "zh": "此操作可以通过设置 `--feature-gates=\"CoordinatedLeaderElection=true\"`\n和 `--runtime-config=\"coordination.k8s.io/v1alpha1=true\"` 标志来完成。"}
{"en": "## Component configuration\n\nProvided that you have enabled the `CoordinatedLeaderElection` feature gate _and_  \nhave the `coordination.k8s.io/v1alpha1` API group enabled, compatible control plane  \ncomponents automatically use the LeaseCandidate and Lease APIs to elect a leader  \nas needed.", "zh": "## 组件配置   {#component-configuration}\n\n前提是你已启用 `CoordinatedLeaderElection` 特性门控**并且**\n启用了 `coordination.k8s.io/v1alpha1` API 组，\n兼容的控制平面组件会自动使用 LeaseCandidate 和 Lease API 根据需要选举领导者。"}
{"en": "For Kubernetes {{< skew currentVersion >}}, two control plane components  \n(kube-controller-manager and kube-scheduler) automatically use coordinated  \nleader election when the feature gate and API group are enabled.", "zh": "对于 Kubernetes {{< skew currentVersion >}}，\n当特性门控和 API 组被启用时，\n两个控制平面组件（kube-controller-manager 和 kube-scheduler）会自动使用协调领导者选举。"}
{"en": "Networking is a central part of Kubernetes, but it can be challenging to\nunderstand exactly how it is expected to work.  There are 4 distinct networking\nproblems to address:\n\n1. Highly-coupled container-to-container communications: this is solved by\n   {{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} and `localhost` communications.\n2. Pod-to-Pod communications: this is the primary focus of this document.\n3. Pod-to-Service communications: this is covered by [Services](/docs/concepts/services-networking/service/).\n4. External-to-Service communications: this is also covered by Services.", "zh": "集群网络系统是 Kubernetes 的核心部分，但是想要准确了解它的工作原理可是个不小的挑战。\n下面列出的是网络系统的的四个主要问题：\n\n1. 高度耦合的容器间通信：这个已经被 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}\n   和 `localhost` 通信解决了。\n2. Pod 间通信：这是本文档讲述的重点。\n3. Pod 与 Service 间通信：涵盖在 [Service](/zh-cn/docs/concepts/services-networking/service/) 中。\n4. 外部与 Service 间通信：也涵盖在 Service 中。"}
{"en": "Kubernetes is all about sharing machines among applications.  Typically,\nsharing machines requires ensuring that two applications do not try to use the\nsame ports.  Coordinating ports across multiple developers is very difficult to\ndo at scale and exposes users to cluster-level issues outside of their control.", "zh": "Kubernetes 的宗旨就是在应用之间共享机器。\n通常来说，共享机器需要两个应用之间不能使用相同的端口，但是在多个应用开发者之间\n去大规模地协调端口是件很困难的事情，尤其是还要让用户暴露在他们控制范围之外的集群级别的问题上。"}
{"en": "Dynamic port allocation brings a lot of complications to the system - every\napplication has to take ports as flags, the API servers have to know how to\ninsert dynamic port numbers into configuration blocks, services have to know\nhow to find each other, etc.  Rather than deal with this, Kubernetes takes a\ndifferent approach.\n\nTo learn about the Kubernetes networking model, see [here](/docs/concepts/services-networking/).", "zh": "动态分配端口也会给系统带来很多复杂度 - 每个应用都需要设置一个端口的参数，\n而 API 服务器还需要知道如何将动态端口数值插入到配置模块中，服务也需要知道如何找到对方等等。\n与其去解决这些问题，Kubernetes 选择了其他不同的方法。\n\n要了解 Kubernetes 网络模型，请参阅[此处](/zh-cn/docs/concepts/services-networking/)。"}
{"en": "## Kubernetes IP address ranges\n\nKubernetes clusters require to allocate non-overlapping IP addresses for Pods, Services and Nodes,\nfrom a range of available addresses configured in the following components:", "zh": "## Kubernetes IP 地址范围   {#kubernetest-ip-address-ranges}\n\nKubernetes 集群需要从以下组件中配置的可用地址范围中为 Pod、Service 和 Node 分配不重叠的 IP 地址："}
{"en": "- The network plugin is configured to assign IP addresses to Pods.\n- The kube-apiserver is configured to assign IP addresses to Services.\n- The kubelet or the cloud-controller-manager is configured to assign IP addresses to Nodes.", "zh": "- 网络插件配置为向 Pod 分配 IP 地址。\n- kube-apiserver 配置为向 Service 分配 IP 地址。\n- kubelet 或 cloud-controller-manager 配置为向 Node 分配 IP 地址。"}
{"en": "{{< figure src=\"/docs/images/kubernetes-cluster-network.svg\" alt=\"A figure illustrating the different network ranges in a kubernetes cluster\" class=\"diagram-medium\" >}}", "zh": "{{< figure src=\"/zh-cn/docs/images/kubernetes-cluster-network.svg\" alt=\"此图展示了 Kubernetes 集群中不同的网络范围\" class=\"diagram-medium\" >}}"}
{"en": "## Cluster networking types {#cluster-network-ipfamilies}\n\nKubernetes clusters, attending to the IP families configured, can be categorized into:", "zh": "## 集群网络类型   {#cluster-network-ipfamilies}\n\n根据配置的 IP 协议族，Kubernetes 集群可以分为以下几类："}
{"en": "- IPv4 only: The network plugin, kube-apiserver and kubelet/cloud-controller-manager are configured to assign only IPv4 addresses.\n- IPv6 only: The network plugin, kube-apiserver and kubelet/cloud-controller-manager are configured to assign only IPv6 addresses.\n- IPv4/IPv6 or IPv6/IPv4 [dual-stack](/docs/concepts/services-networking/dual-stack/):\n  - The network plugin is configured to assign IPv4 and IPv6 addresses.\n  - The kube-apiserver is configured to assign IPv4 and IPv6 addresses.\n  - The kubelet or cloud-controller-manager is configured to assign IPv4 and IPv6 address.\n  - All components must agree on the configured primary IP family.", "zh": "- 仅 IPv4：网络插件、kube-apiserver 和 kubelet/cloud-controller-manager 配置为仅分配 IPv4 地址。\n- 仅 IPv6：网络插件、kube-apiserver 和 kubelet/cloud-controller-manager 配置为仅分配 IPv6 地址。\n- IPv4/IPv6 或 IPv6/IPv4 [双协议栈](/zh-cn/docs/concepts/services-networking/dual-stack/)：\n  - 网络插件配置为分配 IPv4 和 IPv6 地址。\n  - kube-apiserver 配置为分配 IPv4 和 IPv6 地址。\n  - kubelet 或 cloud-controller-manager 配置为分配 IPv4 和 IPv6 地址。\n  - 所有组件必须就配置的主要 IP 协议族达成一致。"}
{"en": "Kubernetes clusters only consider the IP families present on the Pods, Services and Nodes objects,\nindependently of the existing IPs of the represented objects. Per example, a server or a pod can have multiple\nIP addresses on its interfaces, but only the IP addresses in `node.status.addresses` or `pod.status.ips` are\nconsidered for implementing the Kubernetes network model and defining the type of the cluster.", "zh": "Kubernetes 集群只考虑 Pod、Service 和 Node 对象中存在的 IP 协议族，而不考虑所表示对象的现有 IP。\n例如，服务器或 Pod 的接口上可以有多个 IP 地址，但只有 `node.status.addresses` 或 `pod.status.ips`\n中的 IP 地址被认为是实现 Kubernetes 网络模型和定义集群类型的。"}
{"en": "## How to implement the Kubernetes network model\n\nThe network model is implemented by the container runtime on each node. The most common container\nruntimes use [Container Network Interface](https://github.com/containernetworking/cni) (CNI)\nplugins to manage their network and security capabilities. Many different CNI plugins exist from\nmany different vendors. Some of these provide only basic features of adding and removing network\ninterfaces, while others provide more sophisticated solutions, such as integration with other\ncontainer orchestration systems, running multiple CNI plugins, advanced IPAM features etc.", "zh": "## 如何实现 Kubernetes 的网络模型    {#how-to-implement-the-kubernetes-network-model}\n\n网络模型由各节点上的容器运行时来实现。最常见的容器运行时使用\n[Container Network Interface](https://github.com/containernetworking/cni) (CNI) 插件来管理其网络和安全能力。\n来自不同供应商 CNI 插件有很多。其中一些仅提供添加和删除网络接口的基本功能，\n而另一些则提供更复杂的解决方案，例如与其他容器编排系统集成、运行多个 CNI 插件、高级 IPAM 功能等。"}
{"en": "See [this page](/docs/concepts/cluster-administration/addons/#networking-and-network-policy)\nfor a non-exhaustive list of networking addons supported by Kubernetes.", "zh": "请参阅[此页面](/zh-cn/docs/concepts/cluster-administration/addons/#networking-and-network-policy)了解\nKubernetes 支持的网络插件的非详尽列表。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "The early design of the networking model and its rationale are described in more detail in the\n[networking design document](https://git.k8s.io/design-proposals-archive/network/networking.md).\nFor future plans and some on-going efforts that aim to improve Kubernetes networking, please\nrefer to the SIG-Network\n[KEPs](https://github.com/kubernetes/enhancements/tree/master/keps/sig-network).", "zh": "网络模型的早期设计、运行原理都在[联网设计文档](https://git.k8s.io/design-proposals-archive/network/networking.md)里有详细描述。\n关于未来的计划，以及旨在改进 Kubernetes 联网能力的一些正在进行的工作，可以参考 SIG Network\n的 [KEPs](https://github.com/kubernetes/enhancements/tree/master/keps/sig-network)。"}
{"en": "The state of Kubernetes objects in the Kubernetes API can be exposed as metrics.\nAn add-on agent called [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) can connect to the Kubernetes API server and expose a HTTP endpoint with metrics generated from the state of individual objects in the cluster.\nIt exposes various information about the state of objects like labels and annotations, startup and termination times, status or the phase the object currently is in.\nFor example, containers running in pods create a `kube_pod_container_info` metric.\nThis includes the name of the container, the name of the pod it is part of, the {{< glossary_tooltip text=\"namespace\" term_id=\"namespace\" >}} the pod is running in, the name of the container image, the ID of the image, the image name from the spec of the container, the ID of the running container and the ID of the pod as labels.", "zh": "Kubernetes API 中 Kubernetes 对象的状态可以被公开为指标。\n一个名为 [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)\n的插件代理可以连接到 Kubernetes API 服务器并公开一个 HTTP 端点，提供集群中各个对象的状态所生成的指标。\n此代理公开了关于对象状态的各种信息，如标签和注解、启动和终止时间、对象当前所处的状态或阶段。\n例如，针对运行在 Pod 中的容器会创建一个 `kube_pod_container_info` 指标。\n其中包括容器的名称、所属的 Pod 的名称、Pod 所在的{{< glossary_tooltip text=\"命名空间\" term_id=\"namespace\" >}}、\n容器镜像的名称、镜像的 ID、容器规约中的镜像名称、运行中容器的 ID 和用作标签的 Pod ID。\n\n{{% thirdparty-content single=\"true\" %}}"}
{"en": "An external component that is able and capable to scrape the endpoint of kube-state-metrics (for example via Prometheus) can now be used to enable the following use cases.", "zh": "有能力（例如通过 Prometheus）抓取 kube-state-metrics 端点的外部组件现在可用于实现以下使用场景。"}
{"en": "## Example: using metrics from kube-state-metrics to query the cluster state {#example-kube-state-metrics-query-1}\n\nMetric series generated by kube-state-metrics are helpful to gather further insights into the cluster, as they can be used for querying.\n\nIf you use Prometheus or another tool that uses the same query language, the following PromQL query returns the number of pods that are not ready:", "zh": "## 示例：使用来自 kube-state-metrics 的指标查询集群状态   {#example-kube-state-metrics-query-1}\n\n通过 kube-state-metrics 生成的系列指标有助于进一步洞察集群，因为这些指标可用于查询。\n\n如果你使用 Prometheus 或其他采用相同查询语言的工具，则以下 PromQL 查询将返回未就绪的 Pod 数：\n\n```\ncount(kube_pod_status_ready{condition=\"false\"}) by (namespace, pod)\n```"}
{"en": "## Example: alerting based on from kube-state-metrics {#example-kube-state-metrics-alert-1}\n\nMetrics generated from kube-state-metrics also allow for alerting on issues in the cluster.\n\nIf you use Prometheus or a similar tool that uses the same alert rule language, the following alert will fire if there are pods that have been in a `Terminating` state for more than 5 minutes:", "zh": "## 示例：基于 kube-state-metrics 发出警报   {#example-kube-state-metrics-alert-1}\n\nkube-state-metrics 生成的指标还允许在集群中出现问题时发出警报。\n\n如果你使用 Prometheus 或类似采用相同警报规则语言的工具，若有某些 Pod 处于 `Terminating` 状态超过 5 分钟，将触发以下警报：\n\n```yaml\ngroups:\n- name: Pod state\n  rules:\n  - alert: PodsBlockedInTerminatingState\n    expr: count(kube_pod_deletion_timestamp) by (namespace, pod) * count(kube_pod_status_reason{reason=\"NodeLost\"} == 0) by (namespace, pod) > 0\n    for: 5m\n    labels:\n      severity: page\n    annotations:\n      summary: Pod {{$labels.namespace}}/{{$labels.pod}} blocked in Terminating state.\n```"}
{"en": "Kubernetes requires {{< glossary_tooltip text=\"nodes\" term_id=\"node\" >}} in your cluster to\nrun {{< glossary_tooltip text=\"pods\" term_id=\"pod\" >}}. This means providing capacity for\nthe workload Pods and for Kubernetes itself.\n\nYou can adjust the amount of resources available in your cluster automatically:\n_node autoscaling_. You can either change the number of nodes, or change the capacity\nthat nodes provide. The first approach is referred to as _horizontal scaling_, while the\nsecond is referred to as _vertical scaling_.\n\nKubernetes can even provide multidimensional automatic scaling for nodes.", "zh": "Kubernetes 需要集群中的{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}来运行\n{{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}。\n这意味着需要为工作负载 Pod 以及 Kubernetes 本身提供容量。\n\n你可以自动调整集群中可用的资源量：**节点自动扩缩容**。\n你可以更改节点的数量，或者更改节点提供的容量。\n第一种方法称为**水平扩缩容**，而第二种方法称为**垂直扩缩容**。\n\nKubernetes 甚至可以为节点提供多维度的自动扩缩容。"}
{"en": "## Manual node management\n\nYou can manually manage node-level capacity, where you configure a fixed amount of nodes;\nyou can use this approach even if the provisioning (the process to set up, manage, and\ndecommission) for these nodes is automated.\n\nThis page is about taking the next step, and automating management of the amount of\nnode capacity (CPU, memory, and other node resources) available in your cluster.", "zh": "## 手动节点管理   {#manual-node-management}\n\n你可以手动管理节点级别的容量，例如你可以配置固定数量的节点；\n即使这些节点的制备（搭建、管理和停用过程）是自动化的，你也可以使用这种方法。\n\n本文介绍的是下一步操作，即自动化管理集群中可用的节点容量（CPU、内存和其他节点资源）。"}
{"en": "## Automatic horizontal scaling {#autoscaling-horizontal}\n\n### Cluster Autoscaler\n\nYou can use the [Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler) to manage the scale of your nodes automatically.\nThe cluster autoscaler can integrate with a cloud provider, or with Kubernetes'\n[cluster API](https://github.com/kubernetes/autoscaler/blob/c6b754c359a8563050933a590f9a5dece823c836/cluster-autoscaler/cloudprovider/clusterapi/README.md),\nto achieve the actual node management that's needed.", "zh": "## 自动水平扩缩容   {#autoscaling-horizontal}\n\n### Cluster Autoscaler\n\n你可以使用 [Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)\n自动管理节点的数目规模。Cluster Autoscaler 可以与云驱动或 Kubernetes 的\n[Cluster API](https://github.com/kubernetes/autoscaler/blob/c6b754c359a8563050933a590f9a5dece823c836/cluster-autoscaler/cloudprovider/clusterapi/README.md)\n集成，以完成实际所需的节点管理。"}
{"en": "The cluster autoscaler adds nodes when there are unschedulable Pods, and\nremoves nodes when those nodes are empty.\n\n#### Cloud provider integrations {#cluster-autoscaler-providers}\n\nThe [README](https://github.com/kubernetes/autoscaler/tree/c6b754c359a8563050933a590f9a5dece823c836/cluster-autoscaler#readme)\nfor the cluster autoscaler lists some of the cloud provider integrations\nthat are available.", "zh": "当存在不可调度的 Pod 时，Cluster Autoscaler 会添加节点；\n当这些节点为空时，Cluster Autoscaler 会移除节点。\n\n#### 云驱动集成组件   {#cluster-autoscaler-providers}\n\nCluster Autoscaler 的\n[README](https://github.com/kubernetes/autoscaler/tree/c6b754c359a8563050933a590f9a5dece823c836/cluster-autoscaler#readme)\n列举了一些可用的云驱动集成组件。"}
{"en": "## Cost-aware multidimensional scaling {#autoscaling-multi-dimension}\n\n### Karpenter {#autoscaler-karpenter}\n\n[Karpenter](https://karpenter.sh/) supports direct node management, via\nplugins that integrate with specific cloud providers, and can manage nodes\nfor you whilst optimizing for overall cost.", "zh": "## 成本感知多维度扩缩容   {#autoscaling-multi-dimension}\n\n### Karpenter   {#autoscaler-karpenter}\n\n[Karpenter](https://karpenter.sh/) 支持通过继承了特定云驱动的插件来直接管理节点，\n还可以在优化总体成本的同时为你管理节点。"}
{"en": "> Karpenter automatically launches just the right compute resources to\n> handle your cluster's applications. It is designed to let you take\n> full advantage of the cloud with fast and simple compute provisioning\n> for Kubernetes clusters.", "zh": "> Karpenter 自动启动适合你的集群应用的计算资源。\n> Karpenter 设计为让你充分利用云资源，快速简单地为 Kubernetes 集群制备计算资源。"}
{"en": "The Karpenter tool is designed to integrate with a cloud provider that\nprovides API-driven server management, and where the price information for\navailable servers is also available via a web API.\n\nFor example, if you start some more Pods in your cluster, the Karpenter\ntool might buy a new node that is larger than one of the nodes you are\nalready using, and then shut down an existing node once the new node\nis in service.", "zh": "Karpenter 工具设计为与云驱动集成，提供 API 驱动的服务器管理，\n此工具可以通过 Web API 获取可用服务器的价格信息。\n\n例如，如果你在集群中启动更多 Pod，Karpenter 工具可能会购买一个比你当前使用的节点更大的新节点，\n然后在这个新节点投入使用后关闭现有的节点。"}
{"en": "#### Cloud provider integrations {#karpenter-providers}", "zh": "#### 云驱动集成组件 {#karpenter-providers}\n\n{{% thirdparty-content vendor=\"true\" %}}"}
{"en": "There are integrations available between Karpenter's core and the following\ncloud providers:\n\n- [Amazon Web Services](https://github.com/aws/karpenter-provider-aws)\n- [Azure](https://github.com/Azure/karpenter-provider-azure)", "zh": "在 Karpenter 的核心与以下云驱动之间，存在可用的集成组件：\n\n- [Amazon Web Services](https://github.com/aws/karpenter-provider-aws)\n- [Azure](https://github.com/Azure/karpenter-provider-azure)"}
{"en": "## Related components\n\n### Descheduler\n\nThe [descheduler](https://github.com/kubernetes-sigs/descheduler) can help you\nconsolidate Pods onto a smaller number of nodes, to help with automatic scale down\nwhen the cluster has space capacity.", "zh": "## 相关组件   {#related-components}\n\n### Descheduler\n\n[Descheduler](https://github.com/kubernetes-sigs/descheduler)\n可以帮助你将 Pod 集中到少量节点上，以便在集群有空闲容量时帮助自动缩容。"}
{"en": "### Sizing a workload based on cluster size\n\n#### Cluster proportional autoscaler\n\nFor workloads that need to be scaled based on the size of the cluster (for example\n`cluster-dns` or other system components), you can use the\n[_Cluster Proportional Autoscaler_](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler).<br />\n\nThe Cluster Proportional Autoscaler watches the number of schedulable nodes\nand cores, and scales the number of replicas of the target workload accordingly.", "zh": "### 基于集群大小调整工作负载    {#sizing-a-workload-based-on-cluster-size}\n\n#### Cluster Proportional Autoscaler\n\n对于需要基于集群大小进行扩缩容的工作负载（例如 `cluster-dns` 或其他系统组件），\n你可以使用 [Cluster Proportional Autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-autoscaler)。\n\nCluster Proportional Autoscaler 监视可调度节点和核心的数量，并相应地调整目标工作负载的副本数量。"}
{"en": "#### Cluster proportional vertical autoscaler\n\nIf the number of replicas should stay the same, you can scale your workloads vertically according to the cluster size using\nthe [_Cluster Proportional Vertical Autoscaler_](https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler).\nThis project is in **beta** and can be found on GitHub.\n\nWhile the Cluster Proportional Autoscaler scales the number of replicas of a workload, the Cluster Proportional Vertical Autoscaler\nadjusts the resource requests for a workload (for example a Deployment or DaemonSet) based on the number of nodes and/or cores\nin the cluster.", "zh": "#### Cluster Proportional Vertical Autoscaler\n\n如果副本数量应该保持不变，你可以使用\n[Cluster Proportional Vertical Autoscaler](https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler)\n基于集群大小垂直扩缩你的工作负载。此项目处于 **Beta** 阶段，托管在 GitHub 上。\n\nCluster Proportional Autoscaler 扩缩工作负载的副本数量，而 Cluster Proportional Vertical Autoscaler\n基于集群中的节点和/或核心数量调整工作负载（例如 Deployment 或 DaemonSet）的资源请求。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- Read about [workload-level autoscaling](/docs/concepts/workloads/autoscaling/)", "zh": "- 参阅[工作负载级别自动扩缩容](/zh-cn/docs/concepts/workloads/autoscaling/)"}
{"en": "System component metrics can give a better look into what is happening inside them. Metrics are\nparticularly useful for building dashboards and alerts.\n\nKubernetes components emit metrics in [Prometheus format](https://prometheus.io/docs/instrumenting/exposition_formats/).\nThis format is structured plain text, designed so that people and machines can both read it.", "zh": "通过系统组件指标可以更好地了解系统组个内部发生的情况。系统组件指标对于构建仪表板和告警特别有用。\n\nKubernetes 组件以\n[Prometheus 格式](https://prometheus.io/docs/instrumenting/exposition_formats/)生成度量值。\n这种格式是结构化的纯文本，旨在使人和机器都可以阅读。"}
{"en": "## Metrics in Kubernetes\n\nIn most cases metrics are available on `/metrics` endpoint of the HTTP server. For components that\ndon't expose endpoint by default, it can be enabled using `--bind-address` flag.\n\nExamples of those components:", "zh": "## Kubernetes 中组件的指标  {#metrics-in-kubernetes}\n\n在大多数情况下，可以通过 HTTP 访问组件的 `/metrics` 端点来获取组件的度量值。\n对于那些默认情况下不暴露端点的组件，可以使用 `--bind-address` 标志启用。\n\n这些组件的示例：\n\n* {{< glossary_tooltip term_id=\"kube-controller-manager\" text=\"kube-controller-manager\" >}}\n* {{< glossary_tooltip term_id=\"kube-proxy\" text=\"kube-proxy\" >}}\n* {{< glossary_tooltip term_id=\"kube-apiserver\" text=\"kube-apiserver\" >}}\n* {{< glossary_tooltip term_id=\"kube-scheduler\" text=\"kube-scheduler\" >}}\n* {{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}}"}
{"en": "In a production environment you may want to configure [Prometheus Server](https://prometheus.io/)\nor some other metrics scraper to periodically gather these metrics and make them available in some\nkind of time series database.\n\nNote that {{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}} also exposes metrics in\n`/metrics/cadvisor`, `/metrics/resource` and `/metrics/probes` endpoints. Those metrics do not\nhave the same lifecycle.\n\nIf your cluster uses {{< glossary_tooltip term_id=\"rbac\" text=\"RBAC\" >}}, reading metrics requires\nauthorization via a user, group or ServiceAccount with a ClusterRole that allows accessing\n`/metrics`. For example:", "zh": "在生产环境中，你可能需要配置 [Prometheus 服务器](https://prometheus.io/)或\n某些其他指标搜集器以定期收集这些指标，并使它们在某种时间序列数据库中可用。\n\n请注意，{{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}} 还会在 `/metrics/cadvisor`，\n`/metrics/resource` 和 `/metrics/probes` 端点中公开度量值。这些度量值的生命周期各不相同。\n\n如果你的集群使用了 {{< glossary_tooltip term_id=\"rbac\" text=\"RBAC\" >}}，\n则读取指标需要通过基于用户、组或 ServiceAccount 的鉴权，要求具有允许访问\n`/metrics` 的 ClusterRole。\n例如：\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n  - nonResourceURLs:\n      - \"/metrics\"\n    verbs:\n      - get\n```"}
{"en": "## Metric lifecycle\n\nAlpha metric →  Stable metric →  Deprecated metric →  Hidden metric → Deleted metric\n\nAlpha metrics have no stability guarantees. These metrics can be modified or deleted at any time.\n\nStable metrics are guaranteed to not change. This means:\n\n* A stable metric without a deprecated signature will not be deleted or renamed\n* A stable metric's type will not be modified\n\nDeprecated metrics are slated for deletion, but are still available for use.\nThese metrics include an annotation about the version in which they became deprecated.", "zh": "## 指标生命周期  {#metric-lifecycle}\n\nAlpha 指标 →  稳定的指标 →  弃用的指标 →  隐藏的指标 → 删除的指标\n\nAlpha 指标没有稳定性保证。这些指标可以随时被修改或者删除。\n\n稳定的指标可以保证不会改变。这意味着：\n\n* 稳定的、不包含已弃用（deprecated）签名的指标不会被删除（或重命名）\n* 稳定的指标的类型不会被更改\n\n已弃用的指标最终将被删除，不过仍然可用。\n这类指标包含注解，标明其被废弃的版本。"}
{"en": "For example:\n\n* Before deprecation", "zh": "例如：\n\n* 被弃用之前：\n\n  ```\n  # HELP some_counter this counts things\n  # TYPE some_counter counter\n  some_counter 0\n  ```"}
{"en": "* After deprecation", "zh": "* 被弃用之后：\n\n  ```\n  # HELP some_counter (Deprecated since 1.15.0) this counts things\n  # TYPE some_counter counter\n  some_counter 0\n  ```"}
{"en": "Hidden metrics are no longer published for scraping, but are still available for use. To use a\nhidden metric, please refer to the [Show hidden metrics](#show-hidden-metrics) section.\n\nDeleted metrics are no longer published and cannot be used.", "zh": "隐藏的指标不会再被发布以供抓取，但仍然可用。\n要使用隐藏指标，请参阅[显式隐藏指标](#show-hidden-metrics)节。\n\n删除的指标不再被发布，亦无法使用。"}
{"en": "## Show hidden metrics\n\nAs described above, admins can enable hidden metrics through a command-line flag on a specific\nbinary. This intends to be used as an escape hatch for admins if they missed the migration of the\nmetrics deprecated in the last release.\n\nThe flag `show-hidden-metrics-for-version` takes a version for which you want to show metrics\ndeprecated in that release. The version is expressed as x.y, where x is the major version, y is\nthe minor version. The patch version is not needed even though a metrics can be deprecated in a\npatch release, the reason for that is the metrics deprecation policy runs against the minor release.\n\nThe flag can only take the previous minor version as it's value. All metrics hidden in previous\nwill be emitted if admins set the previous version to `show-hidden-metrics-for-version`. The too\nold version is not allowed because this violates the metrics deprecated policy.\n\nTake metric `A` as an example, here assumed that `A` is deprecated in 1.n. According to metrics\ndeprecated policy, we can reach the following conclusion:", "zh": "## 显示隐藏指标   {#show-hidden-metrics}\n\n如上所述，管理员可以通过设置可执行文件的命令行参数来启用隐藏指标，\n如果管理员错过了上一版本中已经弃用的指标的迁移，则可以把这个用作管理员的逃生门。\n\n`show-hidden-metrics-for-version` 标志接受版本号作为取值，版本号给出\n你希望显示该发行版本中已弃用的指标。\n版本表示为 x.y，其中 x 是主要版本，y 是次要版本。补丁程序版本不是必须的，\n即使指标可能会在补丁程序发行版中弃用，原因是指标弃用策略规定仅针对次要版本。\n\n该参数只能使用前一个次要版本。如果管理员将先前版本设置为 `show-hidden-metrics-for-version`，\n则先前版本中隐藏的度量值会再度生成。不允许使用过旧的版本，因为那样会违反指标弃用策略。\n\n以指标 `A` 为例，此处假设 `A` 在 1.n 中已弃用。根据指标弃用策略，我们可以得出以下结论："}
{"en": "* In release `1.n`, the metric is deprecated, and it can be emitted by default.\n* In release `1.n+1`, the metric is hidden by default and it can be emitted by command line\n  `show-hidden-metrics-for-version=1.n`.\n* In release `1.n+2`, the metric should be removed from the codebase. No escape hatch anymore.\n\nIf you're upgrading from release `1.12` to `1.13`, but still depend on a metric `A` deprecated in\n`1.12`, you should set hidden metrics via command line: `--show-hidden-metrics=1.12` and remember\nto remove this metric dependency before upgrading to `1.14`", "zh": "* 在版本 `1.n` 中，这个指标已经弃用，且默认情况下可以生成。\n* 在版本 `1.n+1` 中，这个指标默认隐藏，可以通过命令行参数 `show-hidden-metrics-for-version=1.n` 来再度生成。\n* 在版本 `1.n+2` 中，这个指标就将被从代码中移除，不会再有任何逃生窗口。\n\n如果你要从版本 `1.12` 升级到 `1.13`，但仍依赖于 `1.12` 中弃用的指标 `A`，则应通过命令行设置隐藏指标：\n`--show-hidden-metrics=1.12`，并记住在升级到 `1.14` 版本之前删除此指标依赖项。"}
{"en": "## Component metrics\n\n### kube-controller-manager metrics\n\nController manager metrics provide important insight into the performance and health of the\ncontroller manager. These metrics include common Go language runtime metrics such as go_routine\ncount and controller specific metrics such as etcd request latencies or Cloudprovider (AWS, GCE,\nOpenStack) API latencies that can be used to gauge the health of a cluster.\n\nStarting from Kubernetes 1.7, detailed Cloudprovider metrics are available for storage operations\nfor GCE, AWS, Vsphere and OpenStack.\nThese metrics can be used to monitor health of persistent volume operations.\n\nFor example, for GCE these metrics are called:", "zh": "## 组件指标  {#component-metrics}\n\n### kube-controller-manager 指标  {#kube-controller-manager-metrics}\n\n控制器管理器指标可提供有关控制器管理器性能和运行状况的重要洞察。\n这些指标包括通用的 Go 语言运行时指标（例如 go_routine 数量）和控制器特定的度量指标，\n例如可用于评估集群运行状况的 etcd 请求延迟或云提供商（AWS、GCE、OpenStack）的 API 延迟等。\n\n从 Kubernetes 1.7 版本开始，详细的云提供商指标可用于 GCE、AWS、Vsphere 和 OpenStack 的存储操作。\n这些指标可用于监控持久卷操作的运行状况。\n\n比如，对于 GCE，这些指标称为：\n\n```\ncloudprovider_gce_api_request_duration_seconds { request = \"instance_list\"}\ncloudprovider_gce_api_request_duration_seconds { request = \"disk_insert\"}\ncloudprovider_gce_api_request_duration_seconds { request = \"disk_delete\"}\ncloudprovider_gce_api_request_duration_seconds { request = \"attach_disk\"}\ncloudprovider_gce_api_request_duration_seconds { request = \"detach_disk\"}\ncloudprovider_gce_api_request_duration_seconds { request = \"list_disk\"}\n```"}
{"en": "### kube-scheduler metrics", "zh": "### kube-scheduler 指标   {#kube-scheduler-metrics}\n\n{{< feature-state for_k8s_version=\"v1.21\" state=\"beta\" >}}"}
{"en": "The scheduler exposes optional metrics that reports the requested resources and the desired limits\nof all running pods. These metrics can be used to build capacity planning dashboards, assess\ncurrent or historical scheduling limits, quickly identify workloads that cannot schedule due to\nlack of resources, and compare actual usage to the pod's request.", "zh": "调度器会暴露一些可选的指标，报告所有运行中 Pod 所请求的资源和期望的约束值。\n这些指标可用来构造容量规划监控面板、访问调度约束的当前或历史数据、\n快速发现因为缺少资源而无法被调度的负载，或者将 Pod 的实际资源用量与其请求值进行比较。"}
{"en": "The kube-scheduler identifies the resource [requests and limits](/docs/concepts/configuration/manage-resources-containers/)\nconfigured for each Pod; when either a request or limit is non-zero, the kube-scheduler reports a\nmetrics timeseries. The time series is labelled by:\n\n- namespace\n- pod name\n- the node where the pod is scheduled or an empty string if not yet scheduled\n- priority\n- the assigned scheduler for that pod\n- the name of the resource (for example, `cpu`)\n- the unit of the resource if known (for example, `cores`)", "zh": "kube-scheduler 组件能够辩识各个 Pod 所配置的资源\n[请求和约束](/zh-cn/docs/concepts/configuration/manage-resources-containers/)。\n在 Pod 的资源请求值或者约束值非零时，kube-scheduler 会以度量值时间序列的形式\n生成报告。该时间序列值包含以下标签：\n\n- 名字空间\n- Pod 名称\n- Pod 调度所处节点，或者当 Pod 未被调度时用空字符串表示\n- 优先级\n- 为 Pod 所指派的调度器\n- 资源的名称（例如，`cpu`）\n- 资源的单位，如果知道的话（例如，`cores`）"}
{"en": "Once a pod reaches completion (has a `restartPolicy` of `Never` or `OnFailure` and is in the\n`Succeeded` or `Failed` pod phase, or has been deleted and all containers have a terminated state)\nthe series is no longer reported since the scheduler is now free to schedule other pods to run.\nThe two metrics are called `kube_pod_resource_request` and `kube_pod_resource_limit`.\n\nThe metrics are exposed at the HTTP endpoint `/metrics/resources` and require the same\nauthorization as the `/metrics` endpoint on the scheduler. You must use the\n`--show-hidden-metrics-for-version=1.20` flag to expose these alpha stability metrics.", "zh": "一旦 Pod 进入完成状态（其 `restartPolicy` 为 `Never` 或 `OnFailure`，且\n其处于 `Succeeded` 或 `Failed` Pod 阶段，或者已经被删除且所有容器都具有\n终止状态），该时间序列停止报告，因为调度器现在可以调度其它 Pod 来执行。\n这两个指标称作 `kube_pod_resource_request` 和 `kube_pod_resource_limit`。\n\n指标暴露在 HTTP 端点 `/metrics/resources`，与调度器上的 `/metrics` 端点\n一样要求相同的访问授权。你必须使用\n`--show-hidden-metrics-for-version=1.20` 标志才能暴露那些稳定性为 Alpha\n的指标。"}
{"en": "## Disabling metrics\n\nYou can explicitly turn off metrics via command line flag `--disabled-metrics`. This may be\ndesired if, for example, a metric is causing a performance problem. The input is a list of\ndisabled metrics (i.e. `--disabled-metrics=metric1,metric2`).", "zh": "## 禁用指标 {#disabling-metrics}\n\n你可以通过命令行标志 `--disabled-metrics` 来关闭某指标。\n在例如某指标会带来性能问题的情况下，这一操作可能是有用的。\n标志的参数值是一组被禁止的指标（例如：`--disabled-metrics=metric1,metric2`）。"}
{"en": "## Metric cardinality enforcement\n\nMetrics with unbounded dimensions could cause memory issues in the components they instrument. To\nlimit resource use, you can use the `--allow-metric-labels` command line option to dynamically\nconfigure an allow-list of label values for a metric.\n\nIn alpha stage, the flag can only take in a series of mappings as metric label allow-list.\nEach mapping is of the format `<metric_name>,<label_name>=<allowed_labels>` where \n`<allowed_labels>` is a comma-separated list of acceptable label names.", "zh": "## 指标顺序性保证    {#metric-cardinality-enforcement}\n\n具有无限维度的指标可能会在其监控的组件中引起内存问题。\n为了限制资源使用，你可以使用 `--allow-metric-labels` 命令行选项来为指标动态配置允许的标签值列表。\n\n在 Alpha 阶段，此选项只能接受一组映射值作为可以使用的指标标签。\n每个映射值的格式为`<指标名称>,<标签名称>=<可用标签列表>`，其中\n`<可用标签列表>` 是一个用逗号分隔的、可接受的标签名的列表。"}
{"en": "The overall format looks like:\n\n```\n--allow-metric-labels <metric_name>,<label_name>='<allow_value1>, <allow_value2>...', <metric_name2>,<label_name>='<allow_value1>, <allow_value2>...', ...\n```", "zh": "最终的格式看起来会是这样：\n\n```\n--allow-metric-labels <指标名称>,<标签名称>='<可用值1>,<可用值2>...', <指标名称2>,<标签名称>='<可用值1>, <可用值2>...', ...\n```"}
{"en": "Here is an example:", "zh": "下面是一个例子：\n\n```none\n--allow-metric-labels number_count_metric,odd_number='1,3,5', number_count_metric,even_number='2,4,6', date_gauge_metric,weekend='Saturday,Sunday'\n```"}
{"en": "In addition to specifying this from the CLI, this can also be done within a configuration file. You\ncan specify the path to that configuration file using the `--allow-metric-labels-manifest` command\nline argument to a component. Here's an example of the contents of that configuration file:", "zh": "除了从 CLI 中指定之外，还可以在配置文件中完成此操作。\n你可以使用组件的 `--allow-metric-labels-manifest` 命令行参数指定该配置文件的路径。\n以下是该配置文件的内容示例：\n\n```yaml\n\"metric1,label2\": \"v1,v2,v3\"\n\"metric2,label1\": \"v1,v2,v3\"\n```"}
{"en": "Additionally, the `cardinality_enforcement_unexpected_categorizations_total` meta-metric records the\ncount of unexpected categorizations during cardinality enforcement, that is, whenever a label value\nis encountered that is not allowed with respect to the allow-list constraints.", "zh": "此外，`cardinality_enforcement_unexpected_categorizations_total`\n元指标记录基数执行期间意外分类的计数，即每当遇到允许列表约束不允许的标签值时。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about the [Prometheus text format](https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format)\n  for metrics\n* See the list of [stable Kubernetes metrics](https://github.com/kubernetes/kubernetes/blob/master/test/instrumentation/testdata/stable-metrics-list.yaml)\n* Read about the [Kubernetes deprecation policy](/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior)", "zh": "* 阅读有关指标的 [Prometheus 文本格式](https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format)\n* 参阅[稳定的 Kubernetes 指标](https://github.com/kubernetes/kubernetes/blob/master/test/instrumentation/testdata/stable-metrics-list.yaml)的列表\n* 阅读有关 [Kubernetes 弃用策略](/zh-cn/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior)"}
{"en": "Application logs can help you understand what is happening inside your application. The\nlogs are particularly useful for debugging problems and monitoring cluster activity. Most\nmodern applications have some kind of logging mechanism. Likewise, container engines\nare designed to support logging. The easiest and most adopted logging method for\ncontainerized applications is writing to standard output and standard error streams.", "zh": "应用日志可以让你了解应用内部的运行状况。日志对调试问题和监控集群活动非常有用。\n大部分现代化应用都有某种日志记录机制。同样地，容器引擎也被设计成支持日志记录。\n针对容器化应用，最简单且最广泛采用的日志记录方式就是写入标准输出和标准错误流。"}
{"en": "However, the native functionality provided by a container engine or runtime is usually\nnot enough for a complete logging solution.\n\nFor example, you may want to access your application's logs if a container crashes,\na pod gets evicted, or a node dies.\n\nIn a cluster, logs should have a separate storage and lifecycle independent of nodes,\npods, or containers. This concept is called\n[cluster-level logging](#cluster-level-logging-architectures).", "zh": "但是，由容器引擎或运行时提供的原生功能通常不足以构成完整的日志记录方案。\n\n例如，如果发生容器崩溃、Pod 被逐出或节点宕机等情况，你可能想访问应用日志。\n\n在集群中，日志应该具有独立的存储，并且其生命周期与节点、Pod 或容器的生命周期相独立。\n这个概念叫[集群级的日志](#cluster-level-logging-architectures)。"}
{"en": "Cluster-level logging architectures require a separate backend to store, analyze, and\nquery logs. Kubernetes does not provide a native storage solution for log data. Instead,\nthere are many logging solutions that integrate with Kubernetes. The following sections\ndescribe how to handle and store logs on nodes.", "zh": "集群级日志架构需要一个独立的后端用来存储、分析和查询日志。\nKubernetes 并不为日志数据提供原生的存储解决方案。\n相反，有很多现成的日志方案可以集成到 Kubernetes 中。\n下面各节描述如何在节点上处理和存储日志。"}
{"en": "## Pod and container logs {#basic-logging-in-kubernetes}\n\nKubernetes captures logs from each container in a running Pod.\n\nThis example uses a manifest for a `Pod` with a container\nthat writes text to the standard output stream, once per second.", "zh": "## Pod 和容器日志   {#basic-logging-in-kubernetes}\n\nKubernetes 从正在运行的 Pod 中捕捉每个容器的日志。\n\n此示例使用带有一个容器的 `Pod` 的清单，该容器每秒将文本写入标准输出一次。\n\n{{% code_sample file=\"debug/counter-pod.yaml\" %}}"}
{"en": "To run this pod, use the following command:", "zh": "要运行此 Pod，请执行以下命令：\n\n```shell\nkubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml\n```"}
{"en": "The output is:", "zh": "输出为：\n\n```console\npod/counter created\n```"}
{"en": "To fetch the logs, use the `kubectl logs` command, as follows:", "zh": "要获取这些日志，请执行以下 `kubectl logs` 命令：\n\n```shell\nkubectl logs counter\n```"}
{"en": "The output is similar to:", "zh": "输出类似于：\n\n```console\n0: Fri Apr  1 11:42:23 UTC 2022\n1: Fri Apr  1 11:42:24 UTC 2022\n2: Fri Apr  1 11:42:25 UTC 2022\n```"}
{"en": "You can use `kubectl logs --previous` to retrieve logs from a previous instantiation of a container.\nIf your pod has multiple containers, specify which container's logs you want to access by\nappending a container name to the command, with a `-c` flag, like so:", "zh": "你可以使用 `kubectl logs --previous` 从容器的先前实例中检索日志。\n如果你的 Pod 有多个容器，请如下通过将容器名称追加到该命令并使用 `-c`\n标志来指定要访问哪个容器的日志：\n\n```shell\nkubectl logs counter -c count\n```"}
{"en": "See the [`kubectl logs` documentation](/docs/reference/generated/kubectl/kubectl-commands#logs)\nfor more details.", "zh": "详见 [`kubectl logs` 文档](/docs/reference/generated/kubectl/kubectl-commands#logs)。"}
{"en": "### How nodes handle container logs\n\n![Node level logging](/images/docs/user-guide/logging/logging-node-level.png)\n\nA container runtime handles and redirects any output generated to a containerized\napplication's `stdout` and `stderr` streams.\nDifferent container runtimes implement this in different ways; however, the integration\nwith the kubelet is standardized as the _CRI logging format_.", "zh": "### 节点的容器日志处理方式   {#how-nodes-handle-container-logs}\n\n![节点级别的日志记录](/images/docs/user-guide/logging/logging-node-level.png)\n\n容器运行时对写入到容器化应用程序的 `stdout` 和 `stderr` 流的所有输出进行处理和转发。\n不同的容器运行时以不同的方式实现这一点；不过它们与 kubelet 的集成都被标准化为 **CRI 日志格式**。"}
{"en": "By default, if a container restarts, the kubelet keeps one terminated container with its logs.\nIf a pod is evicted from the node, all corresponding containers are also evicted, along with their logs.\n\nThe kubelet makes logs available to clients via a special feature of the Kubernetes API.\nThe usual way to access this is by running `kubectl logs`.", "zh": "默认情况下，如果容器重新启动，kubelet 会保留一个终止的容器及其日志。\n如果一个 Pod 被逐出节点，所对应的所有容器及其日志也会被逐出。\n\nkubelet 通过 Kubernetes API 的特殊功能将日志提供给客户端访问。\n访问这个日志的常用方法是运行 `kubectl logs`。"}
{"en": "### Log rotation", "zh": "### 日志轮转   {#log-rotation}\n\n{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}"}
{"en": "The kubelet is responsible for rotating container logs and managing the\nlogging directory structure.\nThe kubelet sends this information to the container runtime (using CRI),\nand the runtime writes the container logs to the given location.", "zh": "kubelet 负责轮换容器日志并管理日志目录结构。\nkubelet（使用 CRI）将此信息发送到容器运行时，而运行时则将容器日志写到给定位置。"}
{"en": "You can configure two kubelet [configuration settings](/docs/reference/config-api/kubelet-config.v1beta1/),\n`containerLogMaxSize` (default 10Mi) and `containerLogMaxFiles` (default 5),\nusing the [kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).\nThese settings let you configure the maximum size for each log file and the maximum number of\nfiles allowed for each container respectively.", "zh": "你可以使用 [kubelet 配置文件](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/)配置两个\nkubelet [配置选项](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration)、\n`containerLogMaxSize` （默认 10Mi）和 `containerLogMaxFiles`（默认 5）。\n这些设置分别允许你分别配置每个日志文件大小的最大值和每个容器允许的最大文件数。"}
{"en": "In order to perform an efficient log rotation in clusters where the volume of the logs generated by\nthe workload is large, kubelet also provides a mechanism to tune how the logs are rotated in\nterms of how many concurrent log rotations can be performed and the interval at which the logs are\nmonitored and rotated as required.\nYou can configure two kubelet [configuration settings](/docs/reference/config-api/kubelet-config.v1beta1/),\n`containerLogMaxWorkers` and `containerLogMonitorInterval` using the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).", "zh": "为了在工作负载生成的日志量较大的集群中执行高效的日志轮换，kubelet\n还提供了一种机制，基于可以执行多少并发日志轮换以及监控和轮换日志所需要的间隔来调整日志的轮换方式。\n你可以使用 [kubelet 配置文件](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/)\n配置两个 kubelet [配置选项](/zh-cn/docs/tasks/administer-cluster/kubelet-config-file/)：\n`containerLogMaxWorkers` 和 `containerLogMonitorInterval`。"}
{"en": "When you run [`kubectl logs`](/docs/reference/generated/kubectl/kubectl-commands#logs) as in\nthe basic logging example, the kubelet on the node handles the request and\nreads directly from the log file. The kubelet returns the content of the log file.", "zh": "当类似于基本日志示例一样运行 [`kubectl logs`](/docs/reference/generated/kubectl/kubectl-commands#logs) 时，\n节点上的 kubelet 会处理请求并直接从日志文件读取。kubelet 将返回该日志文件的内容。\n\n{{< note >}}"}
{"en": "Only the contents of the latest log file are available through `kubectl logs`.\n\nFor example, if a Pod writes 40 MiB of logs and the kubelet rotates logs\nafter 10 MiB, running `kubectl logs` returns at most 10MiB of data.", "zh": "只有最新的日志文件的内容可以通过 `kubectl logs` 获得。\n\n例如，如果 Pod 写入 40 MiB 的日志，并且 kubelet 在 10 MiB 之后轮转日志，\n则运行 `kubectl logs` 将最多返回 10 MiB 的数据。\n{{< /note >}}"}
{"en": "## System component logs\n\nThere are two types of system components: those that typically run in a container,\nand those components directly involved in running containers. For example:", "zh": "### 系统组件日志   {#system-component-logs}\n\n系统组件有两种类型：通常在容器中运行的组件和直接参与容器运行的组件。例如："}
{"en": "* The kubelet and container runtime do not run in containers. The kubelet runs\n  your containers (grouped together in {{< glossary_tooltip text=\"pods\" term_id=\"pod\" >}})\n* The Kubernetes scheduler, controller manager, and API server run within pods\n  (usually {{< glossary_tooltip text=\"static Pods\" term_id=\"static-pod\" >}}).\n  The etcd component runs in the control plane, and most commonly also as a static pod.\n  If your cluster uses kube-proxy, you typically run this as a `DaemonSet`.", "zh": "* kubelet 和容器运行时不在容器中运行。kubelet 运行你的容器\n  （一起按 {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} 分组）\n* Kubernetes 调度器、控制器管理器和 API 服务器在 Pod 中运行\n  （通常是{{< glossary_tooltip text=\"静态 Pod\" term_id=\"static-pod\" >}}。\n  etcd 组件在控制平面中运行，最常见的也是作为静态 Pod。\n  如果你的集群使用 kube-proxy，则通常将其作为 `DaemonSet` 运行。"}
{"en": "### Log locations {#log-location-node}\n\nThe way that the kubelet and container runtime write logs depends on the operating\nsystem that the node uses:", "zh": "### 日志位置   {#log-location-node}\n\nkubelet 和容器运行时写入日志的方式取决于节点使用的操作系统：\n\n{{< tabs name=\"log_location_node_tabs\" >}}\n{{% tab name=\"Linux\" %}}"}
{"en": "On Linux nodes that use systemd, the kubelet and container runtime write to journald\nby default. You use `journalctl` to read the systemd journal; for example:\n`journalctl -u kubelet`.\n\nIf systemd is not present, the kubelet and container runtime write to `.log` files in the\n`/var/log` directory. If you want to have logs written elsewhere, you can indirectly\nrun the kubelet via a helper tool, `kube-log-runner`, and use that tool to redirect\nkubelet logs to a directory that you choose.", "zh": "在使用 systemd 的 Linux 节点上，kubelet 和容器运行时默认写入 journald。\n你要使用 `journalctl` 来阅读 systemd 日志；例如：`journalctl -u kubelet`。\n\n如果 systemd 不存在，kubelet 和容器运行时将写入到 `/var/log` 目录中的 `.log` 文件。\n如果你想将日志写入其他地方，你可以通过辅助工具 `kube-log-runner` 间接运行 kubelet，\n并使用该工具将 kubelet 日志重定向到你所选择的目录。"}
{"en": "By default, kubelet directs your container runtime to write logs into directories within\n`/var/log/pods`.\n\nFor more information on `kube-log-runner`, read [System Logs](/docs/concepts/cluster-administration/system-logs/#klog).", "zh": "默认情况下，kubelet 指示你的容器运行时将日志写入 `/var/log/pods` 中的目录。\n\n有关 `kube-log-runner` 的更多信息，请阅读[系统日志](/zh-cn/docs/concepts/cluster-administration/system-logs/#klog)。\n\n{{% /tab %}}\n{{% tab name=\"Windows\" %}}"}
{"en": "By default, the kubelet writes logs to files within the directory `C:\\var\\logs`\n(notice that this is not `C:\\var\\log`).\n\nAlthough `C:\\var\\log` is the Kubernetes default location for these logs, several\ncluster deployment tools set up Windows nodes to log to `C:\\var\\log\\kubelet` instead.", "zh": "默认情况下，kubelet 将日志写入目录 `C:\\var\\logs` 中的文件（注意这不是 `C:\\var\\log`）。\n\n尽管 `C:\\var\\log` 是这些日志的 Kubernetes 默认位置，\n但一些集群部署工具会将 Windows 节点设置为将日志放到 `C:\\var\\log\\kubelet`。"}
{"en": "If you want to have logs written elsewhere, you can indirectly\nrun the kubelet via a helper tool, `kube-log-runner`, and use that tool to redirect\nkubelet logs to a directory that you choose.\n\nHowever, by default, kubelet directs your container runtime to write logs within the\ndirectory `C:\\var\\log\\pods`.\n\nFor more information on `kube-log-runner`, read [System Logs](/docs/concepts/cluster-administration/system-logs/#klog).", "zh": "如果你想将日志写入其他地方，你可以通过辅助工具 `kube-log-runner` 间接运行 kubelet，\n并使用该工具将 kubelet 日志重定向到你所选择的目录。\n\n但是，kubelet 默认指示你的容器运行时在目录 `C:\\var\\log\\pods` 中写入日志。\n\n有关 `kube-log-runner` 的更多信息，请阅读[系统日志](/zh-cn/docs/concepts/cluster-administration/system-logs/#klog)。\n{{% /tab %}}\n{{< /tabs >}}\n\n<br />"}
{"en": "For Kubernetes cluster components that run in pods, these write to files inside\nthe `/var/log` directory, bypassing the default logging mechanism (the components\ndo not write to the systemd journal). You can use Kubernetes' storage mechanisms\nto map persistent storage into the container that runs the component.", "zh": "对于在 Pod 中运行的 Kubernetes 集群组件，其日志会写入 `/var/log` 目录中的文件，\n相当于绕过默认的日志机制（组件不会写入 systemd 日志）。\n你可以使用 Kubernetes 的存储机制将持久存储映射到运行该组件的容器中。"}
{"en": "Kubelet allows changing the pod logs directory from default `/var/log/pods`\nto a custom path. This adjustment can be made by configuring the `podLogsDir`\nparameter in the kubelet's configuration file.", "zh": "kubelet 允许将 Pod 日志目录从默认的 `/var/log/pods` 更改为自定义路径。\n可以通过在 kubelet 的配置文件中配置 `podLogsDir` 参数来进行此调整。\n\n{{< caution >}}"}
{"en": "It's important to note that the default location `/var/log/pods` has been in use for\nan extended period and certain processes might implicitly assume this path.\nTherefore, altering this parameter must be approached with caution and at your own risk.", "zh": "需要注意的是，默认位置 `/var/log/pods` 已使用很长一段时间，并且某些进程可能会隐式使用此路径。\n因此，更改此参数必须谨慎，并自行承担风险。"}
{"en": "Another caveat to keep in mind is that the kubelet supports the location being on the same\ndisk as `/var`. Otherwise, if the logs are on a separate filesystem from `/var`,\nthen the kubelet will not track that filesystem's usage, potentially leading to issues if\nit fills up.", "zh": "另一个需要留意的问题是 kubelet 支持日志写入位置与 `/var` 位于同一磁盘上。\n否则，如果日志位于与 `/var` 不同的文件系统上，kubelet\n将不会跟踪该文件系统的使用情况。如果文件系统已满，则可能会出现问题。\n{{< /caution >}}"}
{"en": "For details about etcd and its logs, view the [etcd documentation](https://etcd.io/docs/).\nAgain, you can use Kubernetes' storage mechanisms to map persistent storage into\nthe container that runs the component.", "zh": "有关 etcd 及其日志的详细信息，请查阅 [etcd 文档](https://etcd.io/docs/)。\n同样，你可以使用 Kubernetes 的存储机制将持久存储映射到运行该组件的容器中。\n\n{{< note >}}"}
{"en": "If you deploy Kubernetes cluster components (such as the scheduler) to log to\na volume shared from the parent node, you need to consider and ensure that those\nlogs are rotated. **Kubernetes does not manage that log rotation**.\n\nYour operating system may automatically implement some log rotation - for example,\nif you share the directory `/var/log` into a static Pod for a component, node-level\nlog rotation treats a file in that directory the same as a file written by any component\noutside Kubernetes.\n\nSome deploy tools account for that log rotation and automate it; others leave this\nas your responsibility.", "zh": "如果你部署 Kubernetes 集群组件（例如调度器）以将日志记录到从父节点共享的卷中，\n则需要考虑并确保这些日志被轮转。**Kubernetes 不管理这种日志轮转**。\n\n你的操作系统可能会自动实现一些日志轮转。例如，如果你将目录 `/var/log` 共享到一个组件的静态 Pod 中，\n则节点级日志轮转会将该目录中的文件视同为 Kubernetes 之外的组件所写入的文件。\n\n一些部署工具会考虑日志轮转并将其自动化；而其他一些工具会将此留给你来处理。\n{{< /note >}}"}
{"en": "## Cluster-level logging architectures\n\nWhile Kubernetes does not provide a native solution for cluster-level logging, there are\nseveral common approaches you can consider. Here are some options:\n\n* Use a node-level logging agent that runs on every node.\n* Include a dedicated sidecar container for logging in an application pod.\n* Push logs directly to a backend from within an application.", "zh": "## 集群级日志架构   {#cluster-level-logging-architectures}\n\n虽然 Kubernetes 没有为集群级日志记录提供原生的解决方案，但你可以考虑几种常见的方法。\n以下是一些选项：\n\n* 使用在每个节点上运行的节点级日志记录代理。\n* 在应用程序的 Pod 中，包含专门记录日志的边车（Sidecar）容器。\n* 将日志直接从应用程序中推送到日志记录后端。"}
{"en": "### Using a node logging agent\n\n![Using a node level logging agent](/images/docs/user-guide/logging/logging-with-node-agent.png)", "zh": "### 使用节点级日志代理   {#using-a-node-logging-agent}\n\n![使用节点级日志代理](/images/docs/user-guide/logging/logging-with-node-agent.png)"}
{"en": "You can implement cluster-level logging by including a _node-level logging agent_ on each node.\nThe logging agent is a dedicated tool that exposes logs or pushes logs to a backend.\nCommonly, the logging agent is a container that has access to a directory with log files from all of the\napplication containers on that node.", "zh": "你可以通过在每个节点上使用**节点级的日志记录代理**来实现集群级日志记录。\n日志记录代理是一种用于暴露日志或将日志推送到后端的专用工具。\n通常，日志记录代理程序是一个容器，它可以访问包含该节点上所有应用程序容器的日志文件的目录。"}
{"en": "Because the logging agent must run on every node, it is recommended to run the agent\nas a `DaemonSet`.\n\nNode-level logging creates only one agent per node and doesn't require any changes to the\napplications running on the node.", "zh": "由于日志记录代理必须在每个节点上运行，推荐以 `DaemonSet` 的形式运行该代理。\n\n节点级日志在每个节点上仅创建一个代理，不需要对节点上的应用做修改。"}
{"en": "Containers write to stdout and stderr, but with no agreed format. A node-level agent collects\nthese logs and forwards them for aggregation.", "zh": "容器向标准输出和标准错误输出写出数据，但在格式上并不统一。\n节点级代理收集这些日志并将其进行转发以完成汇总。"}
{"en": "### Using a sidecar container with the logging agent {#sidecar-container-with-logging-agent}\n\nYou can use a sidecar container in one of the following ways:", "zh": "### 使用边车容器运行日志代理   {#sidecar-container-with-logging-agent}\n\n你可以通过以下方式之一使用边车（Sidecar）容器："}
{"en": "* The sidecar container streams application logs to its own `stdout`.\n* The sidecar container runs a logging agent, which is configured to pick up logs\n  from an application container.", "zh": "* 边车容器将应用程序日志传送到自己的标准输出。\n* 边车容器运行一个日志代理，配置该日志代理以便从应用容器收集日志。"}
{"en": "#### Streaming sidecar container\n\n![Sidecar container with a streaming container](/images/docs/user-guide/logging/logging-with-streaming-sidecar.png)\n\nBy having your sidecar containers write to their own `stdout` and `stderr`\nstreams, you can take advantage of the kubelet and the logging agent that\nalready run on each node. The sidecar containers read logs from a file, a socket,\nor journald. Each sidecar container prints a log to its own `stdout` or `stderr` stream.", "zh": "#### 传输数据流的边车容器\n\n![带数据流容器的边车容器](/images/docs/user-guide/logging/logging-with-streaming-sidecar.png)\n\n利用边车容器，写入到自己的 `stdout` 和 `stderr` 传输流，\n你就可以利用每个节点上的 kubelet 和日志代理来处理日志。\n边车容器从文件、套接字或 journald 读取日志。\n每个边车容器向自己的 `stdout` 和 `stderr` 流中输出日志。"}
{"en": "This approach allows you to separate several log streams from different\nparts of your application, some of which can lack support\nfor writing to `stdout` or `stderr`. The logic behind redirecting logs\nis minimal, so it's not a significant overhead. Additionally, because\n`stdout` and `stderr` are handled by the kubelet, you can use built-in tools\nlike `kubectl logs`.", "zh": "这种方法允许你将日志流从应用程序的不同部分分离开，其中一些可能缺乏对写入\n`stdout` 或 `stderr` 的支持。重定向日志背后的逻辑是最小的，因此它的开销不大。\n另外，因为 `stdout` 和 `stderr` 由 kubelet 处理，所以你可以使用内置的工具 `kubectl logs`。"}
{"en": "For example, a pod runs a single container, and the container\nwrites to two different log files using two different formats. Here's a\nmanifest for the Pod:", "zh": "例如，某 Pod 中运行一个容器，且该容器使用两个不同的格式写入到两个不同的日志文件。\n下面是这个 Pod 的清单：\n\n{{% code_sample file=\"admin/logging/two-files-counter-pod.yaml\" %}}"}
{"en": "It is not recommended to write log entries with different formats to the same log\nstream, even if you managed to redirect both components to the `stdout` stream of\nthe container. Instead, you can create two sidecar containers. Each sidecar\ncontainer could tail a particular log file from a shared volume and then redirect\nthe logs to its own `stdout` stream.", "zh": "不建议在同一个日志流中写入不同格式的日志条目，即使你成功地将其重定向到容器的 `stdout` 流。\n相反，你可以创建两个边车容器。每个边车容器可以从共享卷跟踪特定的日志文件，\n并将文件内容重定向到各自的 `stdout` 流。"}
{"en": "Here's a manifest for a pod that has two sidecar containers:", "zh": "下面是运行两个边车容器的 Pod 的清单：\n\n{{% code_sample file=\"admin/logging/two-files-counter-pod-streaming-sidecar.yaml\" %}}"}
{"en": "Now when you run this pod, you can access each log stream separately by\nrunning the following commands:", "zh": "现在当你运行这个 Pod 时，你可以运行如下命令分别访问每个日志流：\n\n```shell\nkubectl logs counter count-log-1\n```"}
{"en": "The output is similar to:", "zh": "输出类似于：\n\n```console\n0: Fri Apr  1 11:42:26 UTC 2022\n1: Fri Apr  1 11:42:27 UTC 2022\n2: Fri Apr  1 11:42:28 UTC 2022\n...\n```\n\n```shell\nkubectl logs counter count-log-2\n```"}
{"en": "The output is similar to:", "zh": "输出类似于：\n\n```console\nFri Apr  1 11:42:29 UTC 2022 INFO 0\nFri Apr  1 11:42:30 UTC 2022 INFO 0\nFri Apr  1 11:42:31 UTC 2022 INFO 0\n...\n```"}
{"en": "If you installed a node-level agent in your cluster, that agent picks up those log\nstreams automatically without any further configuration. If you like, you can configure\nthe agent to parse log lines depending on the source container.\n\nEven for Pods that only have low CPU and memory usage (order of a couple of millicores\nfor cpu and order of several megabytes for memory), writing logs to a file and\nthen streaming them to `stdout` can double how much storage you need on the node.\nIf you have an application that writes to a single file, it's recommended to set\n`/dev/stdout` as the destination rather than implement the streaming sidecar\ncontainer approach.", "zh": "如果你在集群中安装了节点级代理，由代理自动获取上述日志流，而无需任何进一步的配置。\n如果你愿意，你可以将代理配置为根据源容器解析日志行。\n\n即使对于 CPU 和内存使用率较低的 Pod（CPU 为几毫核，内存为几兆字节），将日志写入一个文件，\n将这些日志流写到 `stdout` 也有可能使节点所需的存储量翻倍。\n如果你有一个写入特定文件的应用程序，则建议将 `/dev/stdout` 设置为目标文件，而不是采用流式边车容器方法。"}
{"en": "Sidecar containers can also be used to rotate log files that cannot be rotated by\nthe application itself. An example of this approach is a small container running\n`logrotate` periodically.\nHowever, it's more straightforward to use `stdout` and `stderr` directly, and\nleave rotation and retention policies to the kubelet.", "zh": "边车容器还可用于轮转应用程序本身无法轮转的日志文件。\n这种方法的一个例子是定期运行 `logrotate` 的小容器。\n但是，直接使用 `stdout` 和 `stderr` 更直接，而将轮转和保留策略留给 kubelet。"}
{"en": "The node-level agent installed in your cluster picks up those log streams\nautomatically without any further configuration. If you like, you can configure\nthe agent to parse log lines depending on the source container.", "zh": "集群中安装的节点级代理会自动获取这些日志流，而无需进一步配置。\n如果你愿意，你也可以配置代理程序来解析源容器的日志行。"}
{"en": "Note, that despite low CPU and memory usage (order of couple of millicores\nfor cpu and order of several megabytes for memory), writing logs to a file and\nthen streaming them to `stdout` can double disk usage. If you have\nan application that writes to a single file, it's generally better to set\n`/dev/stdout` as destination rather than implementing the streaming sidecar\ncontainer approach.", "zh": "注意，尽管 CPU 和内存使用率都很低（以多个 CPU 毫核指标排序或者按内存的兆字节排序），\n向文件写日志然后输出到 `stdout` 流仍然会成倍地增加磁盘使用率。\n如果你的应用向单一文件写日志，通常最好设置 `/dev/stdout` 作为目标路径，\n而不是使用流式的边车容器方式。"}
{"en": "Sidecar containers can also be used to rotate log files that cannot be\nrotated by the application itself. An example of this approach is a small container running logrotate periodically.\nHowever, it's recommended to use `stdout` and `stderr` directly and leave rotation\nand retention policies to the kubelet.", "zh": "如果应用程序本身不能轮转日志文件，则可以通过边车容器实现。\n这种方式的一个例子是运行一个小的、定期轮转日志的容器。\n然而，还是推荐直接使用 `stdout` 和 `stderr`，将日志的轮转和保留策略交给 kubelet。"}
{"en": "#### Sidecar container with a logging agent\n\n![Sidecar container with a logging agent](/images/docs/user-guide/logging/logging-with-sidecar-agent.png)", "zh": "### 具有日志代理功能的边车容器\n\n![含日志代理的边车容器](/images/docs/user-guide/logging/logging-with-sidecar-agent.png)"}
{"en": "If the node-level logging agent is not flexible enough for your situation, you\ncan create a sidecar container with a separate logging agent that you have\nconfigured specifically to run with your application.", "zh": "如果节点级日志记录代理程序对于你的场景来说不够灵活，\n你可以创建一个带有单独日志记录代理的边车容器，将代理程序专门配置为与你的应用程序一起运行。\n\n{{< note >}}"}
{"en": "Using a logging agent in a sidecar container can lead\nto significant resource consumption. Moreover, you won't be able to access\nthose logs using `kubectl logs` because they are not controlled\nby the kubelet.", "zh": "在边车容器中使用日志代理会带来严重的资源损耗。\n此外，你不能使用 `kubectl logs` 访问日志，因为日志并没有被 kubelet 管理。\n{{< /note >}}"}
{"en": "Here are two example manifests that you can use to implement a sidecar container with a logging agent.\nThe first manifest contains a [`ConfigMap`](/docs/tasks/configure-pod-container/configure-pod-configmap/)\nto configure fluentd.", "zh": "下面是两个配置文件，可以用来实现一个带日志代理的边车容器。\n第一个文件包含用来配置 fluentd 的\n[ConfigMap](/zh-cn/docs/tasks/configure-pod-container/configure-pod-configmap/)。\n\n{{% code_sample file=\"admin/logging/fluentd-sidecar-config.yaml\" %}}\n\n{{< note >}}"}
{"en": "In the sample configurations, you can replace fluentd with any logging agent, reading\nfrom any source inside an application container.", "zh": "你可以将此示例配置中的 fluentd 替换为其他日志代理，从应用容器内的其他来源读取数据。\n{{< /note >}}"}
{"en": "The second manifest describes a pod that has a sidecar container running fluentd.\nThe pod mounts a volume where fluentd can pick up its configuration data.", "zh": "第二个清单描述了一个运行 fluentd 边车容器的 Pod。\n该 Pod 挂载一个卷，flutend 可以从这个卷上拣选其配置数据。\n\n{{% code_sample file=\"admin/logging/two-files-counter-pod-agent-sidecar.yaml\" %}}"}
{"en": "### Exposing logs directly from the application\n\n![Exposing logs directly from the application](/images/docs/user-guide/logging/logging-from-application.png)", "zh": "### 从应用中直接暴露日志目录   {#exposing-logs-directly-from-the-application}\n\n![直接从应用程序暴露日志](/images/docs/user-guide/logging/logging-from-application.png)"}
{"en": "Cluster-logging that exposes or pushes logs directly from every application is outside the scope\nof Kubernetes.", "zh": "从各个应用中直接暴露和推送日志数据的集群日志机制已超出 Kubernetes 的范围。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about [Kubernetes system logs](/docs/concepts/cluster-administration/system-logs/)\n* Learn about [Traces For Kubernetes System Components](/docs/concepts/cluster-administration/system-traces/)\n* Learn how to [customise the termination message](/docs/tasks/debug/debug-application/determine-reason-pod-failure/#customizing-the-termination-message)\n  that Kubernetes records when a Pod fails", "zh": "* 阅读有关 [Kubernetes 系统日志](/zh-cn/docs/concepts/cluster-administration/system-logs/)的信息\n* 进一步了解[追踪 Kubernetes 系统组件](/zh-cn/docs/concepts/cluster-administration/system-traces/)\n* 了解当 Pod 失效时如何[定制 Kubernetes 记录的终止消息](/zh-cn/docs/tasks/debug/debug-application/determine-reason-pod-failure/#customizing-the-termination-message)"}
{"en": "In a Kubernetes cluster, a {{< glossary_tooltip text=\"node\" term_id=\"node\" >}}\ncan be shutdown in a planned graceful way or unexpectedly because of reasons such\nas a power outage or something else external. A node shutdown could lead to workload\nfailure if the node is not drained before the shutdown. A node shutdown can be\neither **graceful** or **non-graceful**.", "zh": "在 Kubernetes 集群中，{{< glossary_tooltip text=\"节点\" term_id=\"node\" >}}可以按计划的体面方式关闭，\n也可能因断电或其他某些外部原因被意外关闭。如果节点在关闭之前未被排空，则节点关闭可能会导致工作负载失败。\n节点可以**体面关闭**或**非体面关闭**。"}
{"en": "## Graceful node shutdown {#graceful-node-shutdown}", "zh": "## 节点体面关闭 {#graceful-node-shutdown}\n\n{{< feature-state feature_gate_name=\"GracefulNodeShutdown\" >}}"}
{"en": "The kubelet attempts to detect node system shutdown and terminates pods running on the node.\n\nKubelet ensures that pods follow the normal\n[pod termination process](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)\nduring the node shutdown. During node shutdown, the kubelet does not accept new\nPods (even if those Pods are already bound to the node).", "zh": "kubelet 会尝试检测节点系统关闭事件并终止在节点上运行的所有 Pod。\n\n在节点终止期间，kubelet 保证 Pod 遵从常规的\n[Pod 终止流程](/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)，\n且不接受新的 Pod（即使这些 Pod 已经绑定到该节点）。"}
{"en": "The Graceful node shutdown feature depends on systemd since it takes advantage of\n[systemd inhibitor locks](https://www.freedesktop.org/wiki/Software/systemd/inhibit/) to\ndelay the node shutdown with a given duration.", "zh": "节点体面关闭特性依赖于 systemd，因为它要利用\n[systemd 抑制器锁](https://www.freedesktop.org/wiki/Software/systemd/inhibit/)机制，\n在给定的期限内延迟节点关闭。"}
{"en": "Graceful node shutdown is controlled with the `GracefulNodeShutdown`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) which is\nenabled by default in 1.21.", "zh": "节点体面关闭特性受 `GracefulNodeShutdown`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)控制，\n在 1.21 版本中是默认启用的。"}
{"en": "Note that by default, both configuration options described below,\n`shutdownGracePeriod` and `shutdownGracePeriodCriticalPods` are set to zero,\nthus not activating the graceful node shutdown functionality.\nTo activate the feature, the two kubelet config settings should be configured appropriately and\nset to non-zero values.", "zh": "注意，默认情况下，下面描述的两个配置选项，`shutdownGracePeriod` 和\n`shutdownGracePeriodCriticalPods` 都是被设置为 0 的，因此不会激活节点体面关闭功能。\n要激活此功能特性，这两个 kubelet 配置选项要适当配置，并设置为非零值。"}
{"en": "Once systemd detects or notifies node shutdown, the kubelet sets a `NotReady` condition on\nthe Node, with the `reason` set to `\"node is shutting down\"`. The kube-scheduler honors this condition\nand does not schedule any Pods onto the affected node; other third-party schedulers are\nexpected to follow the same logic. This means that new Pods won't be scheduled onto that node\nand therefore none will start.", "zh": "一旦 systemd 检测到或通知节点关闭，kubelet 就会在节点上设置一个\n`NotReady` 状况，并将 `reason` 设置为 `\"node is shutting down\"`。\nkube-scheduler 会重视此状况，不将 Pod 调度到受影响的节点上；\n其他第三方调度程序也应当遵循相同的逻辑。这意味着新的 Pod 不会被调度到该节点上，\n因此不会有新 Pod 启动。"}
{"en": "The kubelet **also** rejects Pods during the `PodAdmission` phase if an ongoing\nnode shutdown has been detected, so that even Pods with a\n{{< glossary_tooltip text=\"toleration\" term_id=\"toleration\" >}} for\n`node.kubernetes.io/not-ready:NoSchedule` do not start there.", "zh": "如果检测到节点关闭正在进行中，kubelet **也会**在 `PodAdmission`\n阶段拒绝 Pod，即使是该 Pod 带有 `node.kubernetes.io/not-ready:NoSchedule`\n的{{< glossary_tooltip text=\"容忍度\" term_id=\"toleration\" >}}，也不会在此节点上启动。"}
{"en": "At the same time when kubelet is setting that condition on its Node via the API, the kubelet also begins\nterminating any Pods that are running locally.", "zh": "同时，当 kubelet 通过 API 在其 Node 上设置该状况时，kubelet\n也开始终止在本地运行的所有 Pod。"}
{"en": "During a graceful shutdown, kubelet terminates pods in two phases:\n\n1. Terminate regular pods running on the node.\n2. Terminate [critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical)\n   running on the node.", "zh": "在体面关闭节点过程中，kubelet 分两个阶段来终止 Pod：\n\n1. 终止在节点上运行的常规 Pod。\n2. 终止在节点上运行的[关键 Pod](/zh-cn/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical)。"}
{"en": "Graceful node shutdown feature is configured with two\n[`KubeletConfiguration`](/docs/tasks/administer-cluster/kubelet-config-file/) options:\n* `shutdownGracePeriod`:\n  * Specifies the total duration that the node should delay the shutdown by. This is the total\n    grace period for pod termination for both regular and\n    [critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical).\n* `shutdownGracePeriodCriticalPods`:\n  * Specifies the duration used to terminate\n    [critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical)\n    during a node shutdown. This value should be less than `shutdownGracePeriod`.", "zh": "节点体面关闭的特性对应两个\n[`KubeletConfiguration`](/zh-cn/docs/tasks/administer-cluster/kubelet-config-file/) 选项：\n\n* `shutdownGracePeriod`：\n  * 指定节点应延迟关闭的总持续时间。这是 Pod 体面终止的时间总和，不区分常规 Pod\n    还是[关键 Pod](/zh-cn/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical)。\n* `shutdownGracePeriodCriticalPods`：\n  * 在节点关闭期间指定用于终止[关键 Pod](/zh-cn/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical)\n    的持续时间。该值应小于 `shutdownGracePeriod`。\n\n{{< note >}}"}
{"en": "There are cases when Node termination was cancelled by the system (or perhaps manually\nby an administrator). In either of those situations the\nNode will return to the `Ready` state. However Pods which already started the process\nof termination\nwill not be restored by kubelet and will need to be re-scheduled.", "zh": "在某些情况下，节点终止过程会被系统取消（或者可能由管理员手动取消）。\n无论哪种情况下，节点都将返回到 `Ready` 状态。然而，已经开始终止进程的\nPod 将不会被 kubelet 恢复，需要被重新调度。\n{{< /note >}}"}
{"en": "For example, if `shutdownGracePeriod=30s`, and\n`shutdownGracePeriodCriticalPods=10s`, kubelet will delay the node shutdown by\n30 seconds. During the shutdown, the first 20 (30-10) seconds would be reserved\nfor gracefully terminating normal pods, and the last 10 seconds would be\nreserved for terminating [critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical).", "zh": "例如，如果设置了 `shutdownGracePeriod=30s` 和 `shutdownGracePeriodCriticalPods=10s`，\n则 kubelet 将延迟 30 秒关闭节点。\n在关闭期间，将保留前 20（30 - 10）秒用于体面终止常规 Pod，\n而保留最后 10 秒用于终止[关键 Pod](/zh-cn/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical)。\n\n{{< note >}}"}
{"en": "When pods were evicted during the graceful node shutdown, they are marked as shutdown.\nRunning `kubectl get pods` shows the status of the evicted pods as `Terminated`.\nAnd `kubectl describe pod` indicates that the pod was evicted because of node shutdown:", "zh": "当 Pod 在正常节点关闭期间被驱逐时，它们会被标记为关闭。\n运行 `kubectl get pods` 时，被驱逐的 Pod 的状态显示为 `Terminated`。\n并且 `kubectl describe pod` 表示 Pod 因节点关闭而被驱逐：\n\n```\nReason:         Terminated\nMessage:        Pod was terminated in response to imminent node shutdown.\n```\n{{< /note >}}"}
{"en": "### Pod Priority based graceful node shutdown {#pod-priority-graceful-node-shutdown}", "zh": "### 基于 Pod 优先级的节点体面关闭    {#pod-priority-graceful-node-shutdown}\n\n{{< feature-state feature_gate_name=\"GracefulNodeShutdownBasedOnPodPriority\" >}}"}
{"en": "To provide more flexibility during graceful node shutdown around the ordering\nof pods during shutdown, graceful node shutdown honors the PriorityClass for\nPods, provided that you enabled this feature in your cluster. The feature\nallows cluster administers to explicitly define the ordering of pods\nduring graceful node shutdown based on\n[priority classes](/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass).", "zh": "为了在节点体面关闭期间提供更多的灵活性，尤其是处理关闭期间的 Pod 排序问题，\n节点体面关闭机制能够关注 Pod 的 PriorityClass 设置，前提是你已经在集群中启用了此功能特性。\n此特性允许集群管理员基于 Pod\n的[优先级类（Priority Class）](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass)\n显式地定义节点体面关闭期间 Pod 的处理顺序。"}
{"en": "The [Graceful Node Shutdown](#graceful-node-shutdown) feature, as described\nabove, shuts down pods in two phases, non-critical pods, followed by critical\npods. If additional flexibility is needed to explicitly define the ordering of\npods during shutdown in a more granular way, pod priority based graceful\nshutdown can be used.", "zh": "前文所述的[节点体面关闭](#graceful-node-shutdown)特性能够分两个阶段关闭 Pod，\n首先关闭的是非关键的 Pod，之后再处理关键 Pod。\n如果需要显式地以更细粒度定义关闭期间 Pod 的处理顺序，需要一定的灵活度，\n这时可以使用基于 Pod 优先级的体面关闭机制。"}
{"en": "When graceful node shutdown honors pod priorities, this makes it possible to do\ngraceful node shutdown in multiple phases, each phase shutting down a\nparticular priority class of pods. The kubelet can be configured with the exact\nphases and shutdown time per phase.", "zh": "当节点体面关闭能够处理 Pod 优先级时，节点体面关闭的处理可以分为多个阶段，\n每个阶段关闭特定优先级类的 Pod。可以配置 kubelet 按确切的阶段处理 Pod，\n且每个阶段可以独立设置关闭时间。"}
{"en": "Assuming the following custom pod\n[priority classes](/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass)\nin a cluster,", "zh": "假设集群中存在以下自定义的 Pod\n[优先级类](/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass)。"}
{"en": "|Pod priority class name|Pod priority class value|\n|-------------------------|------------------------|\n|`custom-class-a`         | 100000                 |\n|`custom-class-b`         | 10000                  |\n|`custom-class-c`         | 1000                   |\n|`regular/unset`          | 0                      |", "zh": "| Pod 优先级类名称        | Pod 优先级类数值       |\n|-------------------------|------------------------|\n|`custom-class-a`         | 100000                 |\n|`custom-class-b`         | 10000                  |\n|`custom-class-c`         | 1000                   |\n|`regular/unset`          | 0                      |"}
{"en": "Within the [kubelet configuration](/docs/reference/config-api/kubelet-config.v1beta1/)\nthe settings for `shutdownGracePeriodByPodPriority` could look like:", "zh": "在 [kubelet 配置](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/)中，\n`shutdownGracePeriodByPodPriority` 看起来可能是这样："}
{"en": "|Pod priority class value|Shutdown period|\n|------------------------|---------------|\n| 100000                 |10 seconds     |\n| 10000                  |180 seconds    |\n| 1000                   |120 seconds    |\n| 0                      |60 seconds     |", "zh": "| Pod 优先级类数值       | 关闭期限  |\n|------------------------|-----------|\n| 100000                 | 10 秒     |\n| 10000                  | 180 秒    |\n| 1000                   | 120 秒    |\n| 0                      | 60 秒     |"}
{"en": "The corresponding kubelet config YAML configuration would be:", "zh": "对应的 kubelet 配置 YAML 将会是：\n\n```yaml\nshutdownGracePeriodByPodPriority:\n  - priority: 100000\n    shutdownGracePeriodSeconds: 10\n  - priority: 10000\n    shutdownGracePeriodSeconds: 180\n  - priority: 1000\n    shutdownGracePeriodSeconds: 120\n  - priority: 0\n    shutdownGracePeriodSeconds: 60\n```"}
{"en": "The above table implies that any pod with `priority` value >= 100000 will get\njust 10 seconds to stop, any pod with value >= 10000 and < 100000 will get 180\nseconds to stop, any pod with value >= 1000 and < 10000 will get 120 seconds to stop.\nFinally, all other pods will get 60 seconds to stop.\n\nOne doesn't have to specify values corresponding to all of the classes. For\nexample, you could instead use these settings:", "zh": "上面的表格表明，所有 `priority` 值大于等于 100000 的 Pod 停止期限只有 10 秒，\n所有 `priority` 值介于 10000 和 100000 之间的 Pod 停止期限是 180 秒，\n所有 `priority` 值介于 1000 和 10000 之间的 Pod 停止期限是 120 秒，\n其他所有 Pod  停止期限是 60 秒。\n\n用户不需要为所有的优先级类都设置数值。例如，你也可以使用下面这种配置："}
{"en": "|Pod priority class value|Shutdown period|\n|------------------------|---------------|\n| 100000                 |300 seconds    |\n| 1000                   |120 seconds    |\n| 0                      |60 seconds     |", "zh": "| Pod 优先级类数值       | 关闭期限  |\n|------------------------|-----------|\n| 100000                 | 300 秒    |\n| 1000                   | 120 秒    |\n| 0                      | 60 秒     |"}
{"en": "In the above case, the pods with `custom-class-b` will go into the same bucket\nas `custom-class-c` for shutdown.\n\nIf there are no pods in a particular range, then the kubelet does not wait\nfor pods in that priority range. Instead, the kubelet immediately skips to the\nnext priority class value range.", "zh": "在上面这个场景中，优先级类为 `custom-class-b` 的 Pod 会与优先级类为 `custom-class-c`\n的 Pod 在关闭时按相同期限处理。\n\n如果在特定的范围内不存在 Pod，则 kubelet 不会等待对应优先级范围的 Pod。\nkubelet 会直接跳到下一个优先级数值范围进行处理。"}
{"en": "If this feature is enabled and no configuration is provided, then no ordering\naction will be taken.\n\nUsing this feature requires enabling the `GracefulNodeShutdownBasedOnPodPriority`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n, and setting `ShutdownGracePeriodByPodPriority` in the\n[kubelet config](/docs/reference/config-api/kubelet-config.v1beta1/)\nto the desired configuration containing the pod priority class values and\ntheir respective shutdown periods.", "zh": "如果此功能特性被启用，但没有提供配置数据，则不会出现排序操作。\n\n使用此功能特性需要启用 `GracefulNodeShutdownBasedOnPodPriority`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)，\n并将 [kubelet 配置](/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/)\n中的 `shutdownGracePeriodByPodPriority` 设置为期望的配置，\n其中包含 Pod 的优先级类数值以及对应的关闭期限。\n\n{{< note >}}"}
{"en": "The ability to take Pod priority into account during graceful node shutdown was introduced\nas an Alpha feature in Kubernetes v1.23. In Kubernetes {{< skew currentVersion >}}\nthe feature is Beta and is enabled by default.", "zh": "在节点体面关闭期间考虑 Pod 优先级的能力是作为 Kubernetes v1.23 中的 Alpha 功能引入的。\n在 Kubernetes {{< skew currentVersion >}} 中该功能是 Beta 版，默认启用。\n{{< /note >}}"}
{"en": "Metrics `graceful_shutdown_start_time_seconds` and `graceful_shutdown_end_time_seconds`\nare emitted under the kubelet subsystem to monitor node shutdowns.", "zh": "kubelet 子系统中会生成 `graceful_shutdown_start_time_seconds` 和\n`graceful_shutdown_end_time_seconds` 度量指标以便监视节点关闭行为。"}
{"en": "## Non-graceful node shutdown handling {#non-graceful-node-shutdown}", "zh": "## 处理节点非体面关闭 {#non-graceful-node-shutdown}\n\n{{< feature-state feature_gate_name=\"NodeOutOfServiceVolumeDetach\" >}}"}
{"en": "A node shutdown action may not be detected by kubelet's Node Shutdown Manager,\neither because the command does not trigger the inhibitor locks mechanism used by\nkubelet or because of a user error, i.e., the ShutdownGracePeriod and\nShutdownGracePeriodCriticalPods are not configured properly. Please refer to above\nsection [Graceful Node Shutdown](#graceful-node-shutdown) for more details.", "zh": "节点关闭的操作可能无法被 kubelet 的节点关闭管理器检测到，\n或是因为该命令没有触发 kubelet 所使用的抑制器锁机制，或是因为用户错误，\n即 ShutdownGracePeriod 和 ShutdownGracePeriodCriticalPod 配置不正确。\n请参考以上[节点体面关闭](#graceful-node-shutdown)部分了解更多详细信息。"}
{"en": "When a node is shutdown but not detected by kubelet's Node Shutdown Manager, the pods\nthat are part of a {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}} will be stuck in terminating status on\nthe shutdown node and cannot move to a new running node. This is because kubelet on\nthe shutdown node is not available to delete the pods so the StatefulSet cannot\ncreate a new pod with the same name. If there are volumes used by the pods, the\nVolumeAttachments will not be deleted from the original shutdown node so the volumes\nused by these pods cannot be attached to a new running node. As a result, the\napplication running on the StatefulSet cannot function properly. If the original\nshutdown node comes up, the pods will be deleted by kubelet and new pods will be\ncreated on a different running node. If the original shutdown node does not come up,\nthese pods will be stuck in terminating status on the shutdown node forever.", "zh": "当某节点关闭但 kubelet 的节点关闭管理器未检测到这一事件时，\n在那个已关闭节点上、属于 {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}\n的 Pod 将停滞于终止状态，并且不能移动到新的运行节点上。\n这是因为已关闭节点上的 kubelet 已不存在，亦无法删除 Pod，\n因此 StatefulSet 无法创建同名的新 Pod。\n如果 Pod 使用了卷，则 VolumeAttachments 无法从原来的已关闭节点上删除，\n因此这些 Pod 所使用的卷也无法挂接到新的运行节点上。\n最终，那些以 StatefulSet 形式运行的应用无法正常工作。\n如果原来的已关闭节点被恢复，kubelet 将删除 Pod，新的 Pod 将被在不同的运行节点上创建。\n如果原来的已关闭节点没有被恢复，那些在已关闭节点上的 Pod 将永远滞留在终止状态。"}
{"en": "To mitigate the above situation, a user can manually add the taint `node.kubernetes.io/out-of-service` with either `NoExecute`\nor `NoSchedule` effect to a Node marking it out-of-service.\nIf the `NodeOutOfServiceVolumeDetach`[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nis enabled on {{< glossary_tooltip text=\"kube-controller-manager\" term_id=\"kube-controller-manager\" >}}, and a Node is marked out-of-service with this taint, the\npods on the node will be forcefully deleted if there are no matching tolerations on it and volume\ndetach operations for the pods terminating on the node will happen immediately. This allows the\nPods on the out-of-service node to recover quickly on a different node.", "zh": "为了缓解上述情况，用户可以手动将具有 `NoExecute` 或 `NoSchedule` 效果的\n`node.kubernetes.io/out-of-service` 污点添加到节点上，标记其无法提供服务。\n如果在 {{< glossary_tooltip text=\"kube-controller-manager\" term_id=\"kube-controller-manager\" >}}\n上启用了 `NodeOutOfServiceVolumeDetach`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)，\n并且节点被污点标记为无法提供服务，如果节点 Pod 上没有设置对应的容忍度，\n那么这样的 Pod 将被强制删除，并且该在节点上被终止的 Pod 将立即进行卷分离操作。\n这样就允许那些在无法提供服务节点上的 Pod 能在其他节点上快速恢复。"}
{"en": "During a non-graceful shutdown, Pods are terminated in the two phases:\n\n1. Force delete the Pods that do not have matching `out-of-service` tolerations.\n2. Immediately perform detach volume operation for such pods.", "zh": "在非体面关闭期间，Pod 分两个阶段终止：\n\n1. 强制删除没有匹配的 `out-of-service` 容忍度的 Pod。\n2. 立即对此类 Pod 执行分离卷操作。\n\n{{< note >}}"}
{"en": "- Before adding the taint `node.kubernetes.io/out-of-service` , it should be verified\n  that the node is already in shutdown or power off state (not in the middle of\n  restarting).\n- The user is required to manually remove the out-of-service taint after the pods are\n  moved to a new node and the user has checked that the shutdown node has been\n  recovered since the user was the one who originally added the taint.", "zh": "- 在添加 `node.kubernetes.io/out-of-service` 污点之前，\n  应该验证节点已经处于关闭或断电状态（而不是在重新启动中）。\n- 将 Pod 移动到新节点后，用户需要手动移除停止服务的污点，\n  并且用户要检查关闭节点是否已恢复，因为该用户是最初添加污点的用户。\n{{< /note >}}"}
{"en": "### Forced storage detach on timeout {#storage-force-detach-on-timeout}\n\nIn any situation where a pod deletion has not succeeded for 6 minutes, kubernetes will\nforce detach volumes being unmounted if the node is unhealthy at that instant. Any\nworkload still running on the node that uses a force-detached volume will cause a\nviolation of the\n[CSI specification](https://github.com/container-storage-interface/spec/blob/master/spec.md#controllerunpublishvolume),\nwhich states that `ControllerUnpublishVolume` \"**must** be called after all\n`NodeUnstageVolume` and `NodeUnpublishVolume` on the volume are called and succeed\".\nIn such circumstances, volumes on the node in question might encounter data corruption.", "zh": "### 存储超时强制解除挂接  {#storage-force-detach-on-timeout}\n\n在任何情况下，当 Pod 未能在 6 分钟内删除成功，如果节点当时不健康，\nKubernetes 将强制解除挂接正在被卸载的卷。\n任何运行在使用了强制解除挂接卷的节点之上的工作负载，\n都将违反 [CSI 规范](https://github.com/container-storage-interface/spec/blob/master/spec.md#controllerunpublishvolume)，\n该规范指出 `ControllerUnpublishVolume`\n\"**必须**在调用卷上的所有 `NodeUnstageVolume` 和 `NodeUnpublishVolume` 执行且成功后被调用\"。\n在这种情况下，相关节点上的卷可能会遇到数据损坏。"}
{"en": "The forced storage detach behaviour is optional; users might opt to use the \"Non-graceful\nnode shutdown\" feature instead.", "zh": "强制存储解除挂接行为是可选的；用户可以选择使用\"非体面节点关闭\"特性。"}
{"en": "Force storage detach on timeout can be disabled by setting the `disable-force-detach-on-timeout`\nconfig field in `kube-controller-manager`. Disabling the force detach on timeout feature means\nthat a volume that is hosted on a node that is unhealthy for more than 6 minutes will not have\nits associated\n[VolumeAttachment](/docs/reference/kubernetes-api/config-and-storage-resources/volume-attachment-v1/)\ndeleted.", "zh": "可以通过在 `kube-controller-manager` 中设置 `disable-force-detach-on-timeout`\n配置字段来禁用超时时存储强制解除挂接。\n禁用超时强制解除挂接特性意味着，托管在异常超过 6 分钟的节点上的卷将不会保留其关联的\n[VolumeAttachment](/zh-cn/docs/reference/kubernetes-api/config-and-storage-resources/volume-attachment-v1/)。"}
{"en": "After this setting has been applied, unhealthy pods still attached to a volumes must be recovered\nvia the [Non-Graceful Node Shutdown](#non-graceful-node-shutdown) procedure mentioned above.", "zh": "应用此设置后，仍然关联卷到不健康 Pod 必须通过上述[非体面节点关闭](#non-graceful-node-shutdown)过程进行恢复。\n\n{{< note >}}"}
{"en": "- Caution must be taken while using the [Non-Graceful Node Shutdown](#non-graceful-node-shutdown) procedure.\n- Deviation from the steps documented above can result in data corruption.", "zh": "- 使用[非体面节点关闭](#non-graceful-node-shutdown)过程时必须小心。\n- 偏离上述步骤可能会导致数据损坏。\n{{< /note >}}\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "Learn more about the following:\n* Blog: [Non-Graceful Node Shutdown](/blog/2023/08/16/kubernetes-1-28-non-graceful-node-shutdown-ga/).\n* Cluster Architecture: [Nodes](/docs/concepts/architecture/nodes/).", "zh": "了解更多以下信息：\n\n- 博客：[非体面节点关闭](/zh-cn/blog/2023/08/16/kubernetes-1-28-non-graceful-node-shutdown-ga/)。\n- 集群架构：[节点](/zh-cn/docs/concepts/architecture/nodes/)。"}
{"en": "To learn how to generate certificates for your cluster, see [Certificates](/docs/tasks/administer-cluster/certificates/).", "zh": "要了解如何为集群生成证书，参阅[证书](/zh-cn/docs/tasks/administer-cluster/certificates/)。"}
{"en": "overview", "zh": "{{< feature-state for_k8s_version=\"v1.27\" state=\"beta\" >}}"}
{"en": "System component traces record the latency of and relationships between operations in the cluster.", "zh": "系统组件追踪功能记录各个集群操作的时延信息和这些操作之间的关系。"}
{"en": "Kubernetes components emit traces using the\n[OpenTelemetry Protocol](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/otlp.md#opentelemetry-protocol-specification)\nwith the gRPC exporter and can be collected and routed to tracing backends using an\n[OpenTelemetry Collector](https://github.com/open-telemetry/opentelemetry-collector#-opentelemetry-collector).", "zh": "Kubernetes 组件基于 gRPC 导出器的\n[OpenTelemetry 协议](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/otlp.md#opentelemetry-protocol-specification)\n发送追踪信息，并用\n[OpenTelemetry Collector](https://github.com/open-telemetry/opentelemetry-collector#-opentelemetry-collector)\n收集追踪信息，再将其转交给追踪系统的后台。"}
{"en": "## Trace Collection\n\nKubernetes components have built-in gRPC exporters for OTLP to export traces, either with an OpenTelemetry Collector, \nor without an OpenTelemetry Collector.", "zh": "## 追踪信息的收集 {#trace-collection}\n\nKubernetes 组件具有内置的 gRPC 导出器，供 OTLP 导出追踪信息，可以使用 OpenTelemetry Collector，\n也可以不使用 OpenTelemetry Collector。"}
{"en": "For a complete guide to collecting traces and using the collector, see\n[Getting Started with the OpenTelemetry Collector](https://opentelemetry.io/docs/collector/getting-started/).\nHowever, there are a few things to note that are specific to Kubernetes components.", "zh": "关于收集追踪信息、以及使用收集器的完整指南，可参见\n[Getting Started with the OpenTelemetry Collector](https://opentelemetry.io/docs/collector/getting-started/)。\n不过，还有一些特定于 Kubernetes 组件的事项值得注意。"}
{"en": "By default, Kubernetes components export traces using the grpc exporter for OTLP on the\n[IANA OpenTelemetry port](https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=opentelemetry), 4317.\nAs an example, if the collector is running as a sidecar to a Kubernetes component,\nthe following receiver configuration will collect spans and log them to standard output:", "zh": "默认情况下，Kubernetes 组件使用 gRPC 的 OTLP 导出器来导出追踪信息，将信息写到\n[IANA OpenTelemetry 端口](https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=opentelemetry)。\n举例来说，如果收集器以 Kubernetes 组件的边车模式运行，\n以下接收器配置会收集 span 信息，并将它们写入到标准输出。"}
{"en": "```yaml\nreceivers:\n  otlp:\n    protocols:\n      grpc:\nexporters:\n  # Replace this exporter with the exporter for your backend\n  logging:\n    logLevel: debug\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [logging]\n```", "zh": "```yaml\nreceivers:\n  otlp:\n    protocols:\n      grpc:\nexporters:\n  # 用适合你后端环境的导出器替换此处的导出器\n  logging:\n    logLevel: debug\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [logging]\n```"}
{"en": "To directly emit traces to a backend without utilizing a collector, \nspecify the endpoint field in the Kubernetes tracing configuration file with the desired trace backend address. \nThis method negates the need for a collector and simplifies the overall structure.", "zh": "要在不使用收集器的情况下直接将追踪信息发送到后端，请在 Kubernetes\n追踪配置文件中指定端点字段以及所需的追踪后端地址。\n该方法不需要收集器并简化了整体结构。"}
{"en": "For trace backend header configuration, including authentication details, environment variables can be used with `OTEL_EXPORTER_OTLP_HEADERS`, \nsee [OTLP Exporter Configuration](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/).", "zh": "对于追踪后端标头配置，包括身份验证详细信息，环境变量可以与 `OTEL_EXPORTER_OTLP_HEADERS`\n一起使用，请参阅 [OTLP 导出器配置](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/)。"}
{"en": "Additionally, for trace resource attribute configuration such as Kubernetes cluster name, namespace, Pod name, etc., \nenvironment variables can also be used with `OTEL_RESOURCE_ATTRIBUTES`, see [OTLP Kubernetes Resource](https://opentelemetry.io/docs/specs/semconv/resource/k8s/).", "zh": "另外，对于 Kubernetes 集群名称、命名空间、Pod 名称等追踪资源属性配置，\n环境变量也可以与 `OTEL_RESOURCE_ATTRIBUTES` 配合使用，请参见\n[OTLP Kubernetes 资源](https://opentelemetry.io/docs/specs/semconv/resource/k8s/)。"}
{"en": "## Component traces\n\n### kube-apiserver traces", "zh": "## 组件追踪 {#component-traces}\n\n### kube-apiserver 追踪 {#kube-apiserver-traces}"}
{"en": "The kube-apiserver generates spans for incoming HTTP requests, and for outgoing requests\nto webhooks, etcd, and re-entrant requests. It propagates the\n[W3C Trace Context](https://www.w3.org/TR/trace-context/) with outgoing requests\nbut does not make use of the trace context attached to incoming requests,\nas the kube-apiserver is often a public endpoint.", "zh": "kube-apiserver 为传入的 HTTP 请求、传出到 webhook 和 etcd 的请求以及重入的请求生成 span。\n由于 kube-apiserver 通常是一个公开的端点，所以它通过出站的请求传播\n[W3C 追踪上下文](https://www.w3.org/TR/trace-context/)，\n但不使用入站请求的追踪上下文。"}
{"en": "#### Enabling tracing in the kube-apiserver", "zh": "#### 在 kube-apiserver 中启用追踪 {#enabling-tracing-in-the-kube-apiserver}"}
{"en": "To enable tracing, provide the kube-apiserver with a tracing configuration file\nwith `--tracing-config-file=<path-to-config>`. This is an example config that records\nspans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:\n\n```yaml\napiVersion: apiserver.config.k8s.io/v1beta1\nkind: TracingConfiguration\n# default value\n#endpoint: localhost:4317\nsamplingRatePerMillion: 100\n```", "zh": "要启用追踪特性，需要使用 `--tracing-config-file=<<配置文件路径>` 为\nkube-apiserver 提供追踪配置文件。下面是一个示例配置，它为万分之一的请求记录\nspan，并使用了默认的 OpenTelemetry 端点。\n\n```yaml\napiVersion: apiserver.config.k8s.io/v1beta1\nkind: TracingConfiguration\n# 默认值\n#endpoint: localhost:4317\nsamplingRatePerMillion: 100\n```"}
{"en": "For more information about the `TracingConfiguration` struct, see\n[API server config API (v1beta1)](/docs/reference/config-api/apiserver-config.v1beta1/#apiserver-k8s-io-v1beta1-TracingConfiguration).", "zh": "有关 TracingConfiguration 结构体的更多信息，请参阅\n[API 服务器配置 API (v1beta1)](/zh-cn/docs/reference/config-api/apiserver-config.v1beta1/#apiserver-k8s-io-v1beta1-TracingConfiguration)。"}
{"en": "### kubelet traces", "zh": "### kubelet 追踪   {#kubelet-traces}\n\n{{< feature-state feature_gate_name=\"KubeletTracing\" >}}"}
{"en": "The kubelet CRI interface and authenticated http servers are instrumented to generate\ntrace spans. As with the apiserver, the endpoint and sampling rate are configurable.\nTrace context propagation is also configured. A parent span's sampling decision is always respected.\nA provided tracing configuration sampling rate will apply to spans without a parent.\nEnabled without a configured endpoint, the default OpenTelemetry Collector receiver address of \"localhost:4317\" is set.", "zh": "kubelet CRI 接口和实施身份验证的 HTTP 服务器被插桩以生成追踪 span。\n与 API 服务器一样，端点和采样率是可配置的。\n追踪上下文传播也是可以配置的。始终优先采用父 span 的采样决策。\n用户所提供的追踪配置采样率将被应用到不带父级的 span。\n如果在没有配置端点的情况下启用，将使用默认的 OpenTelemetry Collector\n接收器地址 “localhost:4317”。"}
{"en": "#### Enabling tracing in the kubelet\n\nTo enable tracing, apply the [tracing configuration](https://github.com/kubernetes/component-base/blob/release-1.27/tracing/api/v1/types.go).\nThis is an example snippet of a kubelet config that records spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:\n\n```yaml\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nfeatureGates:\n  KubeletTracing: true\ntracing:\n  # default value\n  #endpoint: localhost:4317\n  samplingRatePerMillion: 100\n```", "zh": "#### 在 kubelet 中启用追踪 {#enabling-tracing-in-the-kubelet}\n\n要启用追踪，需应用[追踪配置](https://github.com/kubernetes/component-base/blob/release-1.27/tracing/api/v1/types.go)。\n以下是 kubelet 配置的示例代码片段，每 10000 个请求中记录一个请求的\nspan，并使用默认的 OpenTelemetry 端点：\n\n```yaml\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nfeatureGates:\n  KubeletTracing: true\ntracing:\n  # 默认值\n  #endpoint: localhost:4317\n  samplingRatePerMillion: 100\n```"}
{"en": "If the `samplingRatePerMillion` is set to one million (`1000000`), then every\nspan will be sent to the exporter.", "zh": "如果 `samplingRatePerMillion` 被设置为一百万 (`1000000`)，则所有 span 都将被发送到导出器。"}
{"en": "The kubelet in Kubernetes v{{< skew currentVersion >}} collects spans from\nthe garbage collection, pod synchronization routine as well as every gRPC\nmethod. The kubelet propagates trace context with gRPC requests so that\ncontainer runtimes with trace instrumentation, such as CRI-O and containerd,\ncan associate their exported spans with the trace context from the kubelet.\nThe resulting traces will have parent-child links between kubelet and\ncontainer runtime spans, providing helpful context when debugging node\nissues.", "zh": "Kubernetes v{{< skew currentVersion >}} 中的 kubelet 收集与垃圾回收、Pod\n同步例程以及每个 gRPC 方法相关的 Span。\nkubelet 借助 gRPC 来传播跟踪上下文，以便 CRI-O 和 containerd\n这类带有跟踪插桩的容器运行时可以在其导出的 Span 与 kubelet\n所提供的跟踪上下文之间建立关联。所得到的跟踪数据会包含 kubelet\n与容器运行时 Span 之间的父子链接关系，从而为调试节点问题提供有用的上下文信息。"}
{"en": "Please note that exporting spans always comes with a small performance overhead\non the networking and CPU side, depending on the overall configuration of the\nsystem. If there is any issue like that in a cluster which is running with\ntracing enabled, then mitigate the problem by either reducing the\n`samplingRatePerMillion` or disabling tracing completely by removing the\nconfiguration.", "zh": "请注意导出 span 始终会对网络和 CPU 产生少量性能开销，具体取决于系统的总体配置。\n如果在启用追踪的集群中出现类似性能问题，可以通过降低 `samplingRatePerMillion`\n或通过移除此配置来彻底禁用追踪来缓解问题。"}
{"en": "## Stability", "zh": "## 稳定性 {#stability}"}
{"en": "Tracing instrumentation is still under active development, and may change\nin a variety of ways. This includes span names, attached attributes,\ninstrumented endpoints, etc. Until this feature graduates to stable,\nthere are no guarantees of backwards compatibility for tracing instrumentation.", "zh": "追踪工具仍在积极开发中，未来它会以多种方式发生变化。\n这些变化包括：span 名称、附加属性、检测端点等等。\n此类特性在达到稳定版本之前，不能保证追踪工具的向后兼容性。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about [Getting Started with the OpenTelemetry Collector](https://opentelemetry.io/docs/collector/getting-started/)\n* Read about [OTLP Exporter Configuration](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/)", "zh": "* 阅读 [Getting Started with the OpenTelemetry Collector](https://opentelemetry.io/docs/collector/getting-started/)\n* 了解 [OTLP 导出器配置](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/)"}
{"en": "overview", "zh": "{{< feature-state state=\"stable\"  for_k8s_version=\"v1.29\" >}}"}
{"en": "Controlling the behavior of the Kubernetes API server in an overload situation\nis a key task for cluster administrators. The {{< glossary_tooltip\nterm_id=\"kube-apiserver\" text=\"kube-apiserver\" >}} has some controls available\n(i.e. the `--max-requests-inflight` and `--max-mutating-requests-inflight`\ncommand-line flags) to limit the amount of outstanding work that will be\naccepted, preventing a flood of inbound requests from overloading and\npotentially crashing the API server, but these flags are not enough to ensure\nthat the most important requests get through in a period of high traffic.", "zh": "对于集群管理员来说，控制 Kubernetes API 服务器在过载情况下的行为是一项关键任务。\n{{< glossary_tooltip term_id=\"kube-apiserver\" text=\"kube-apiserver\" >}}\n有一些控件（例如：命令行标志 `--max-requests-inflight` 和 `--max-mutating-requests-inflight`），\n可以限制将要接受的未处理的请求，从而防止过量请求入站，潜在导致 API 服务器崩溃。\n但是这些标志不足以保证在高流量期间，最重要的请求仍能被服务器接受。"}
{"en": "The API Priority and Fairness feature (APF) is an alternative that improves upon\naforementioned max-inflight limitations. APF classifies\nand isolates requests in a more fine-grained way. It also introduces\na limited amount of queuing, so that no requests are rejected in cases\nof very brief bursts. Requests are dispatched from queues using a\nfair queuing technique so that, for example, a poorly-behaved\n{{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}} need not\nstarve others (even at the same priority level).", "zh": "API 优先级和公平性（APF）是一种替代方案，可提升上述最大并发限制。\nAPF 以更细粒度的方式对请求进行分类和隔离。\n它还引入了空间有限的排队机制，因此在非常短暂的突发情况下，API 服务器不会拒绝任何请求。\n通过使用公平排队技术从队列中分发请求，这样，\n一个行为不佳的{{< glossary_tooltip text=\"控制器\" term_id=\"controller\" >}}就不会饿死其他控制器\n（即使优先级相同）。"}
{"en": "This feature is designed to work well with standard controllers, which\nuse informers and react to failures of API requests with exponential\nback-off, and other clients that also work this way.", "zh": "本功能特性在设计上期望其能与标准控制器一起工作得很好；\n这类控制器使用通知组件（Informers）获得信息并对 API 请求的失效作出反应，\n在处理失效时能够执行指数型回退。其他客户端也以类似方式工作。\n\n{{< caution >}}"}
{"en": "Some requests classified as \"long-running\"&mdash;such as remote\ncommand execution or log tailing&mdash;are not subject to the API\nPriority and Fairness filter. This is also true for the\n`--max-requests-inflight` flag without the API Priority and Fairness\nfeature enabled. API Priority and Fairness _does_ apply to **watch**\nrequests. When API Priority and Fairness is disabled, **watch** requests\nare not subject to the `--max-requests-inflight` limit.", "zh": "属于 “长时间运行” 类型的某些请求（例如远程命令执行或日志拖尾）不受 API 优先级和公平性过滤器的约束。\n如果未启用 APF 特性，即便设置 `--max-requests-inflight` 标志，该类请求也不受约束。\nAPF 适用于 **watch** 请求。当 APF 被禁用时，**watch** 请求不受 `--max-requests-inflight` 限制。\n{{< /caution >}}"}
{"en": "## Enabling/Disabling API Priority and Fairness", "zh": "## 启用/禁用 API 优先级和公平性    {#enabling-api-priority-and-fairness}"}
{"en": "The API Priority and Fairness feature is controlled by a command-line flag\nand is enabled by default. See \n[Options](/docs/reference/command-line-tools-reference/kube-apiserver/#options)\nfor a general explanation of the available kube-apiserver command-line \noptions and how to enable and disable them. The name of the \ncommand-line option for APF is \"--enable-priority-and-fairness\". This feature\nalso involves an {{<glossary_tooltip term_id=\"api-group\" text=\"API Group\" >}} \nwith: (a) a stable `v1` version, introduced in 1.29, and \nenabled by default (b) a `v1beta3` version, enabled by default, and\ndeprecated in v1.29. You can\ndisable the API group beta version `v1beta3` by adding the\nfollowing command-line flags to your `kube-apiserver` invocation:", "zh": "API 优先级与公平性（APF）特性由命令行标志控制，默认情况下启用。\n有关可用 kube-apiserver 命令行参数以及如何启用和禁用的说明，\n请参见[参数](/zh-cn/docs/reference/command-line-tools-reference/kube-apiserver/#options)。\nAPF 的命令行参数是 \"--enable-priority-and-fairness\"。\n此特性也与某个 {{< glossary_tooltip term_id=\"api-group\" text=\"API 组\" >}}相关：\n(a) 稳定的 `v1` 版本，在 1.29 中引入，默认启用；\n(b) `v1beta3` 版本，默认被启用，在 1.29 中被弃用。\n你可以通过添加以下内容来禁用 Beta 版的 `v1beta3` API 组："}
{"en": "```shell\nkube-apiserver \\\n--runtime-config=flowcontrol.apiserver.k8s.io/v1beta3=false \\\n # …and other flags as usual\n```", "zh": "```shell\nkube-apiserver \\\n--runtime-config=flowcontrol.apiserver.k8s.io/v1beta3=false \\\n  # ...其他配置不变\n```"}
{"en": "The command-line flag `--enable-priority-and-fairness=false` will disable the\nAPI Priority and Fairness feature.", "zh": "命令行标志 `--enable-priority-fairness=false` 将彻底禁用 APF 特性。"}
{"en": "## Recursive server scenarios\n\nAPI Priority and Fairness must be used carefully in recursive server\nscenarios. These are scenarios in which some server A, while serving\na request, issues a subsidiary request to some server B. Perhaps\nserver B might even make a further subsidiary call back to server\nA. In situations where Priority and Fairness control is applied to\nboth the original request and some subsidiary ones(s), no matter how\ndeep in the recursion, there is a danger of priority inversions and/or\ndeadlocks.", "zh": "## 递归服务器场景     {#Recursive server scenarios}\n\n在递归服务器场景中，必须谨慎使用 API 优先级和公平性。这些场景指的是服务器 A 在处理一个请求时，\n会向服务器 B 发出一个辅助请求。服务器 B 可能会进一步向服务器 A 发出辅助请求。\n当优先级和公平性控制同时应用于原始请求及某些辅助请求（无论递归多深）时，存在优先级反转和/或死锁的风险。"}
{"en": "One example of recursion is when the `kube-apiserver` issues an\nadmission webhook call to server B, and while serving that call,\nserver B makes a further subsidiary request back to the\n`kube-apiserver`. Another example of recursion is when an `APIService`\nobject directs the `kube-apiserver` to delegate requests about a\ncertain API group to a custom external server B (this is one of the\nthings called \"aggregation\").", "zh": "递归的一个例子是 `kube-apiserver` 向服务器 B 发出一个准入 Webhook 调用，\n而在处理该调用时，服务器 B 进一步向 `kube-apiserver` 发出一个辅助请求。\n另一个递归的例子是，某个 `APIService` 对象指示 `kube-apiserver`\n将某个 API 组的请求委托给自定义的外部服务器 B（这被称为\"聚合\"）。"}
{"en": "When the original request is known to belong to a certain priority\nlevel, and the subsidiary controlled requests are classified to higher\npriority levels, this is one possible solution. When the original\nrequests can belong to any priority level, the subsidiary controlled\nrequests have to be exempt from Priority and Fairness limitation. One\nway to do that is with the objects that configure classification and\nhandling, discussed below. Another way is to disable Priority and\nFairness on server B entirely, using the techniques discussed above. A\nthird way, which is the simplest to use when server B is not\n`kube-apisever`, is to build server B with Priority and Fairness\ndisabled in the code.", "zh": "当原始请求被确定为属于某个特定优先级别时，将辅助请求分类为更高的优先级别是一个可行的解决方案。\n当原始请求可能属于任何优先级时，辅助受控请求必须免受优先级和公平性限制。\n一种实现方法是使用下文中讨论的配置分类和处理的对象。\n另一种方法是采用前面提到的技术，在服务器 B 上完全禁用优先级和公平性。第三种方法是，\n当服务器 B 不是 `kube-apiserver` 时，最简单的做法是在服务器 B 的代码中禁用优先级和公平性。"}
{"en": "## Concepts\n\nThere are several distinct features involved in the API Priority and Fairness\nfeature. Incoming requests are classified by attributes of the request using\n_FlowSchemas_, and assigned to priority levels. Priority levels add a degree of\nisolation by maintaining separate concurrency limits, so that requests assigned\nto different priority levels cannot starve each other. Within a priority level,\na fair-queuing algorithm prevents requests from different _flows_ from starving\neach other, and allows for requests to be queued to prevent bursty traffic from\ncausing failed requests when the average load is acceptably low.", "zh": "## 概念    {#concepts}\n\nAPF 特性包含几个不同的功能。\n传入的请求通过 **FlowSchema** 按照其属性分类，并分配优先级。\n每个优先级维护自定义的并发限制，加强了隔离度，这样不同优先级的请求，就不会相互饿死。\n在同一个优先级内，公平排队算法可以防止来自不同 **流（Flow）** 的请求相互饿死。\n该算法将请求排队，通过排队机制，防止在平均负载较低时，通信量突增而导致请求失败。"}
{"en": "### Priority Levels\n\nWithout APF enabled, overall concurrency in the API server is limited by the\n`kube-apiserver` flags `--max-requests-inflight` and\n`--max-mutating-requests-inflight`. With APF enabled, the concurrency limits\ndefined by these flags are summed and then the sum is divided up among a\nconfigurable set of _priority levels_. Each incoming request is assigned to a\nsingle priority level, and each priority level will only dispatch as many\nconcurrent requests as its particular limit allows.", "zh": "### 优先级    {#priority-levels}\n\n如果未启用 APF，API 服务器中的整体并发量将受到 `kube-apiserver` 的参数\n`--max-requests-inflight` 和 `--max-mutating-requests-inflight` 的限制。\n启用 APF 后，将对这些参数定义的并发限制进行求和，然后将总和分配到一组可配置的 **优先级** 中。\n每个传入的请求都会分配一个优先级；每个优先级都有各自的限制，设定特定限制允许分发的并发请求数。"}
{"en": "The default configuration, for example, includes separate priority levels for\nleader-election requests, requests from built-in controllers, and requests from\nPods. This means that an ill-behaved Pod that floods the API server with\nrequests cannot prevent leader election or actions by the built-in controllers\nfrom succeeding.", "zh": "例如，默认配置包括针对领导者选举请求、内置控制器请求和 Pod 请求都单独设置优先级。\n这表示即使异常的 Pod 向 API 服务器发送大量请求，也无法阻止领导者选举或内置控制器的操作执行成功。"}
{"en": "The concurrency limits of the priority levels are periodically\nadjusted, allowing under-utilized priority levels to temporarily lend\nconcurrency to heavily-utilized levels. These limits are based on\nnominal limits and bounds on how much concurrency a priority level may\nlend and how much it may borrow, all derived from the configuration\nobjects mentioned below.", "zh": "优先级的并发限制会被定期调整，允许利用率较低的优先级将并发度临时借给利用率很高的优先级。\n这些限制基于一个优先级可以借出多少个并发度以及可以借用多少个并发度的额定限制和界限，\n所有这些均源自下述配置对象。"}
{"en": "### Seats Occupied by a Request\n\nThe above description of concurrency management is the baseline story.\nRequests have different durations but are counted equally at any given\nmoment when comparing against a priority level's concurrency limit. In\nthe baseline story, each request occupies one unit of concurrency. The\nword \"seat\" is used to mean one unit of concurrency, inspired by the\nway each passenger on a train or aircraft takes up one of the fixed\nsupply of seats.\n\nBut some requests take up more than one seat. Some of these are **list**\nrequests that the server estimates will return a large number of\nobjects. These have been found to put an exceptionally heavy burden\non the server. For this reason, the server estimates the number of objects\nthat will be returned and considers the request to take a number of seats\nthat is proportional to that estimated number.", "zh": "### 请求占用的席位  {#seats-occupied-by-a-request}\n\n上述并发管理的描述是基线情况。各个请求具有不同的持续时间，\n但在与一个优先级的并发限制进行比较时，这些请求在任何给定时刻都以同等方式进行计数。\n在这个基线场景中，每个请求占用一个并发单位。\n我们用 “席位（Seat）” 一词来表示一个并发单位，其灵感来自火车或飞机上每位乘客占用一个固定座位的供应方式。\n\n但有些请求所占用的席位不止一个。有些请求是服务器预估将返回大量对象的 **list** 请求。\n我们发现这类请求会给服务器带来异常沉重的负担。\n出于这个原因，服务器估算将返回的对象数量，并认为请求所占用的席位数与估算得到的数量成正比。"}
{"en": "### Execution time tweaks for watch requests\n\nAPI Priority and Fairness manages **watch** requests, but this involves a\ncouple more excursions from the baseline behavior. The first concerns\nhow long a **watch** request is considered to occupy its seat. Depending\non request parameters, the response to a **watch** request may or may not\nbegin with **create** notifications for all the relevant pre-existing\nobjects. API Priority and Fairness considers a **watch** request to be\ndone with its seat once that initial burst of notifications, if any,\nis over.\n\nThe normal notifications are sent in a concurrent burst to all\nrelevant **watch** response streams whenever the server is notified of an\nobject create/update/delete. To account for this work, API Priority\nand Fairness considers every write request to spend some additional\ntime occupying seats after the actual writing is done. The server\nestimates the number of notifications to be sent and adjusts the write\nrequest's number of seats and seat occupancy time to include this\nextra work.", "zh": "### watch 请求的执行时间调整  {#execution-time-tweak-for-watch-requests}\n\nAPF 管理 **watch** 请求，但这需要考量基线行为之外的一些情况。\n第一个关注点是如何判定 **watch** 请求的席位占用时长。\n取决于请求参数不同，对 **watch** 请求的响应可能以针对所有预先存在的对象 **create** 通知开头，也可能不这样。\n一旦最初的突发通知（如果有）结束，APF 将认为 **watch** 请求已经用完其席位。\n\n每当向服务器通知创建/更新/删除一个对象时，正常通知都会以并发突发的方式发送到所有相关的 **watch** 响应流。\n为此，APF 认为每个写入请求都会在实际写入完成后花费一些额外的时间来占用席位。\n服务器估算要发送的通知数量，并调整写入请求的席位数以及包含这些额外工作后的席位占用时间。"}
{"en": "### Queuing\n\nEven within a priority level there may be a large number of distinct sources of\ntraffic. In an overload situation, it is valuable to prevent one stream of\nrequests from starving others (in particular, in the relatively common case of a\nsingle buggy client flooding the kube-apiserver with requests, that buggy client\nwould ideally not have much measurable impact on other clients at all). This is\nhandled by use of a fair-queuing algorithm to process requests that are assigned\nthe same priority level. Each request is assigned to a _flow_, identified by the\nname of the matching FlowSchema plus a _flow distinguisher_ — which\nis either the requesting user, the target resource's namespace, or nothing — and the\nsystem attempts to give approximately equal weight to requests in different\nflows of the same priority level.\nTo enable distinct handling of distinct instances, controllers that have\nmany instances should authenticate with distinct usernames", "zh": "### 排队    {#queuing}\n\n即使在同一优先级内，也可能存在大量不同的流量源。\n在过载情况下，防止一个请求流饿死其他流是非常有价值的\n（尤其是在一个较为常见的场景中，一个有故障的客户端会疯狂地向 kube-apiserver 发送请求，\n理想情况下，这个有故障的客户端不应对其他客户端产生太大的影响）。\n公平排队算法在处理具有相同优先级的请求时，实现了上述场景。\n每个请求都被分配到某个 **流（Flow）** 中，该 **流** 由对应的 FlowSchema 的名字加上一个\n**流区分项（Flow Distinguisher）** 来标识。\n这里的流区分项可以是发出请求的用户、目标资源的名字空间或什么都不是。\n系统尝试为不同流中具有相同优先级的请求赋予近似相等的权重。\n要启用对不同实例的不同处理方式，多实例的控制器要分别用不同的用户名来执行身份认证。"}
{"en": "After classifying a request into a flow, the API Priority and Fairness\nfeature then may assign the request to a queue. This assignment uses\na technique known as {{< glossary_tooltip term_id=\"shuffle-sharding\"\ntext=\"shuffle sharding\" >}}, which makes relatively efficient use of\nqueues to insulate low-intensity flows from high-intensity flows.", "zh": "将请求划分到流中之后，APF 功能将请求分配到队列中。\n分配时使用一种称为{{< glossary_tooltip term_id=\"shuffle-sharding\" text=\"混洗分片（Shuffle-Sharding）\" >}}的技术。\n该技术可以相对有效地利用队列隔离低强度流与高强度流。"}
{"en": "The details of the queuing algorithm are tunable for each priority level, and\nallow administrators to trade off memory use, fairness (the property that\nindependent flows will all make progress when total traffic exceeds capacity),\ntolerance for bursty traffic, and the added latency induced by queuing.", "zh": "排队算法的细节可针对每个优先等级进行调整，并允许管理员在内存占用、\n公平性（当总流量超标时，各个独立的流将都会取得进展）、\n突发流量的容忍度以及排队引发的额外延迟之间进行权衡。"}
{"en": "### Exempt requests\n\nSome requests are considered sufficiently important that they are not subject to\nany of the limitations imposed by this feature. These exemptions prevent an\nimproperly-configured flow control configuration from totally disabling an API\nserver.", "zh": "### 豁免请求    {#exempt-requests}\n\n某些特别重要的请求不受制于此特性施加的任何限制。\n这些豁免可防止不当的流控配置完全禁用 API 服务器。"}
{"en": "## Resources\n\nThe flow control API involves two kinds of resources.\n[PriorityLevelConfigurations](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#prioritylevelconfiguration-v1-flowcontrol-apiserver-k8s-io)\ndefine the available priority levels, the share of the available concurrency\nbudget that each can handle, and allow for fine-tuning queuing behavior.\n[FlowSchemas](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#flowschema-v1-flowcontrol-apiserver-k8s-io)\nare used to classify individual inbound requests, matching each to a\nsingle PriorityLevelConfiguration.", "zh": "## 资源    {#resources}\n\n流控 API 涉及两种资源。\n[PriorityLevelConfiguration](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#prioritylevelconfiguration-v1-flowcontrol-apiserver-k8s-io)\n定义可用的优先级和可处理的并发预算量，还可以微调排队行为。\n[FlowSchema](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#flowschema-v1-flowcontrol-apiserver-k8s-io)\n用于对每个入站请求进行分类，并与一个 PriorityLevelConfiguration 相匹配。"}
{"en": "### PriorityLevelConfiguration\n\nA PriorityLevelConfiguration represents a single priority level. Each\nPriorityLevelConfiguration has an independent limit on the number of outstanding\nrequests, and limitations on the number of queued requests.", "zh": "### PriorityLevelConfiguration\n\n一个 PriorityLevelConfiguration 表示单个优先级。每个 PriorityLevelConfiguration\n对未完成的请求数有各自的限制，对排队中的请求数也有限制。"}
{"en": "The nominal concurrency limit for a PriorityLevelConfiguration is not\nspecified in an absolute number of seats, but rather in \"nominal\nconcurrency shares.\" The total concurrency limit for the API Server is\ndistributed among the existing PriorityLevelConfigurations in\nproportion to these shares, to give each level its nominal limit in\nterms of seats. This allows a cluster administrator to scale up or\ndown the total amount of traffic to a server by restarting\n`kube-apiserver` with a different value for `--max-requests-inflight`\n(or `--max-mutating-requests-inflight`), and all\nPriorityLevelConfigurations will see their maximum allowed concurrency\ngo up (or down) by the same fraction.", "zh": "PriorityLevelConfiguration 的额定并发限制不是指定请求绝对数量，而是以“额定并发份额”的形式指定。\nAPI 服务器的总并发量限制通过这些份额按例分配到现有 PriorityLevelConfiguration 中，\n为每个级别按照数量赋予其额定限制。\n集群管理员可以更改 `--max-requests-inflight` （或 `--max-mutating-requests-inflight`）的值，\n再重新启动 `kube-apiserver` 来增加或减小服务器的总流量，\n然后所有的 PriorityLevelConfiguration 将看到其最大并发增加（或减少）了相同的比例。\n\n{{< caution >}}"}
{"en": "In the versions before `v1beta3` the relevant\nPriorityLevelConfiguration field is named \"assured concurrency shares\"\nrather than \"nominal concurrency shares\". Also, in Kubernetes release\n1.25 and earlier there were no periodic adjustments: the\nnominal/assured limits were always applied without adjustment.", "zh": "在 `v1beta3` 之前的版本中，相关的 PriorityLevelConfiguration\n字段被命名为“保证并发份额”而不是“额定并发份额”。此外在 Kubernetes v1.25\n及更早的版本中，不存在定期的调整：所实施的始终是额定/保证的限制，不存在调整。\n{{< /caution >}}"}
{"en": "The bounds on how much concurrency a priority level may lend and how\nmuch it may borrow are expressed in the PriorityLevelConfiguration as\npercentages of the level's nominal limit. These are resolved to\nabsolute numbers of seats by multiplying with the nominal limit /\n100.0 and rounding. The dynamically adjusted concurrency limit of a\npriority level is constrained to lie between (a) a lower bound of its\nnominal limit minus its lendable seats and (b) an upper bound of its\nnominal limit plus the seats it may borrow. At each adjustment the\ndynamic limits are derived by each priority level reclaiming any lent\nseats for which demand recently appeared and then jointly fairly\nresponding to the recent seat demand on the priority levels, within\nthe bounds just described.", "zh": "一个优先级可以借出的并发数界限以及可以借用的并发数界限在\nPriorityLevelConfiguration 表现该优先级的额定限制。\n这些界限值乘以额定限制/100.0 并取整，被解析为绝对席位数量。\n某优先级的动态调整并发限制范围被约束在\n(a) 其额定限制的下限值减去其可借出的席位和\n(b) 其额定限制的上限值加上它可以借用的席位之间。\n在每次调整时，通过每个优先级推导得出动态限制，具体过程为回收最近出现需求的所有借出的席位，\n然后在刚刚描述的界限内共同公平地响应有关这些优先级最近的席位需求。\n\n{{< caution >}}"}
{"en": "With the Priority and Fairness feature enabled, the total concurrency limit for\nthe server is set to the sum of `--max-requests-inflight` and\n`--max-mutating-requests-inflight`. There is no longer any distinction made\nbetween mutating and non-mutating requests; if you want to treat them\nseparately for a given resource, make separate FlowSchemas that match the\nmutating and non-mutating verbs respectively.", "zh": "启用 APF 特性时，服务器的总并发限制被设置为 `--max-requests-inflight` 及\n`--max-mutating-requests-inflight` 之和。变更性和非变更性请求之间不再有任何不同；\n如果你想针对某给定资源分别进行处理，请制作单独的 FlowSchema，分别匹配变更性和非变更性的动作。\n{{< /caution >}}"}
{"en": "When the volume of inbound requests assigned to a single\nPriorityLevelConfiguration is more than its permitted concurrency level, the\n`type` field of its specification determines what will happen to extra requests.\nA type of `Reject` means that excess traffic will immediately be rejected with\nan HTTP 429 (Too Many Requests) error. A type of `Queue` means that requests\nabove the threshold will be queued, with the shuffle sharding and fair queuing techniques used\nto balance progress between request flows.", "zh": "当入站请求的数量大于分配的 PriorityLevelConfiguration 中允许的并发级别时，\n`type` 字段将确定对额外请求的处理方式。\n`Reject` 类型，表示多余的流量将立即被 HTTP 429（请求过多）错误所拒绝。\n`Queue` 类型，表示对超过阈值的请求进行排队，将使用阈值分片和公平排队技术来平衡请求流之间的进度。"}
{"en": "The queuing configuration allows tuning the fair queuing algorithm for a\npriority level. Details of the algorithm can be read in the\n[enhancement proposal](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness), but in short:", "zh": "公平排队算法支持通过排队配置对优先级微调。\n可以在[增强建议](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness)中阅读算法的详细信息，\n但总之："}
{"en": "* Increasing `queues` reduces the rate of collisions between different flows, at\n  the cost of increased memory usage. A value of 1 here effectively disables the\n  fair-queuing logic, but still allows requests to be queued.", "zh": "* `queues` 递增能减少不同流之间的冲突概率，但代价是增加了内存使用量。\n  值为 1 时，会禁用公平排队逻辑，但仍允许请求排队。"}
{"en": "* Increasing `queueLengthLimit` allows larger bursts of traffic to be\n  sustained without dropping any requests, at the cost of increased\n  latency and memory usage.", "zh": "* `queueLengthLimit` 递增可以在不丢弃任何请求的情况下支撑更大的突发流量，\n  但代价是增加了等待时间和内存使用量。"}
{"en": "* Changing `handSize` allows you to adjust the probability of collisions between\n  different flows and the overall concurrency available to a single flow in an\n  overload situation.", "zh": "* 修改 `handSize` 允许你调整过载情况下不同流之间的冲突概率以及单个流可用的整体并发性。\n\n  {{< note >}}"}
{"en": "A larger `handSize` makes it less likely for two individual flows to collide\n  (and therefore for one to be able to starve the other), but more likely that\n  a small number of flows can dominate the apiserver. A larger `handSize` also\n  potentially increases the amount of latency that a single high-traffic flow\n  can cause. The maximum number of queued requests possible from a\n  single flow is `handSize * queueLengthLimit`.", "zh": "较大的 `handSize` 使两个单独的流程发生碰撞的可能性较小（因此，一个流可以饿死另一个流），\n  但是更有可能的是少数流可以控制 apiserver。\n  较大的 `handSize` 还可能增加单个高并发流的延迟量。\n  单个流中可能排队的请求的最大数量为 `handSize * queueLengthLimit`。\n  {{< /note >}}"}
{"en": "Following is a table showing an interesting collection of shuffle\nsharding configurations, showing for each the probability that a\ngiven mouse (low-intensity flow) is squished by the elephants (high-intensity flows) for\nan illustrative collection of numbers of elephants. See\nhttps://play.golang.org/p/Gi0PLgVHiUg , which computes this table.", "zh": "下表显示了有趣的随机分片配置集合，每行显示给定的老鼠（低强度流）\n被不同数量的大象挤压（高强度流）的概率。\n表来源请参阅： https://play.golang.org/p/Gi0PLgVHiUg\n\n{{< table caption = \"混分切片配置示例\" >}}"}
{"en": "HandSize | Queues | 1 elephant | 4 elephants | 16 elephants", "zh": "随机分片 | 队列数 | 1 头大象 | 4 头大象 | 16 头大象\n|----------|-----------|------------|----------------|--------------------|\n| 12 | 32 | 4.428838398950118e-09 | 0.11431348830099144 | 0.9935089607656024 |\n| 10 | 32 | 1.550093439632541e-08 | 0.0626479840223545 | 0.9753101519027554 |\n| 10 | 64 | 6.601827268370426e-12 | 0.00045571320990370776 | 0.49999929150089345 |\n| 9 | 64 | 3.6310049976037345e-11 | 0.00045501212304112273 | 0.4282314876454858 |\n| 8 | 64 | 2.25929199850899e-10 | 0.0004886697053040446 | 0.35935114681123076 |\n| 8 | 128 | 6.994461389026097e-13 | 3.4055790161620863e-06 | 0.02746173137155063 |\n| 7 | 128 | 1.0579122850901972e-11 | 6.960839379258192e-06 | 0.02406157386340147 |\n| 7 | 256 | 7.597695465552631e-14 | 6.728547142019406e-08 | 0.0006709661542533682 |\n| 6 | 256 | 2.7134626662687968e-12 | 2.9516464018476436e-07 | 0.0008895654642000348 |\n| 6 | 512 | 4.116062922897309e-14 | 4.982983350480894e-09 | 2.26025764343413e-05 |\n| 6 | 1024 | 6.337324016514285e-16 | 8.09060164312957e-11 | 4.517408062903668e-07 |\n{{< /table >}}"}
{"en": "### FlowSchema\n\nA FlowSchema matches some inbound requests and assigns them to a\npriority level. Every inbound request is tested against FlowSchemas,\nstarting with those with the numerically lowest `matchingPrecedence` and\nworking upward. The first match wins.", "zh": "### FlowSchema\n\nFlowSchema 匹配一些入站请求，并将它们分配给优先级。\n每个入站请求都会对 FlowSchema 测试是否匹配，\n首先从 `matchingPrecedence` 数值最低的匹配开始，\n然后依次进行，直到首个匹配出现。\n\n{{< caution >}}"}
{"en": "Only the first matching FlowSchema for a given request matters. If multiple\nFlowSchemas match a single inbound request, it will be assigned based on the one\nwith the highest `matchingPrecedence`. If multiple FlowSchemas with equal\n`matchingPrecedence` match the same request, the one with lexicographically\nsmaller `name` will win, but it's better not to rely on this, and instead to\nensure that no two FlowSchemas have the same `matchingPrecedence`.", "zh": "对一个请求来说，只有首个匹配的 FlowSchema 才有意义。\n如果一个入站请求与多个 FlowSchema 匹配，则将基于逻辑上最高优先级 `matchingPrecedence` 的请求进行筛选。\n如果一个请求匹配多个 FlowSchema 且 `matchingPrecedence` 的值相同，则按 `name` 的字典序选择最小，\n但是最好不要依赖它，而是确保不存在两个 FlowSchema 具有相同的 `matchingPrecedence` 值。\n{{< /caution >}}"}
{"en": "A FlowSchema matches a given request if at least one of its `rules`\nmatches. A rule matches if at least one of its `subjects` *and* at least\none of its `resourceRules` or `nonResourceRules` (depending on whether the\nincoming request is for a resource or non-resource URL) match the request.", "zh": "当给定的请求与某个 FlowSchema 的 `rules` 的其中一条匹配，那么就认为该请求与该 FlowSchema 匹配。\n判断规则与该请求是否匹配，**不仅**要求该条规则的 `subjects` 字段至少存在一个与该请求相匹配，\n**而且**要求该条规则的 `resourceRules` 或 `nonResourceRules`\n（取决于传入请求是针对资源 URL 还是非资源 URL）字段至少存在一个与该请求相匹配。"}
{"en": "For the `name` field in subjects, and the `verbs`, `apiGroups`, `resources`,\n`namespaces`, and `nonResourceURLs` fields of resource and non-resource rules,\nthe wildcard `*` may be specified to match all values for the given field,\neffectively removing it from consideration.", "zh": "对于 `subjects` 中的 `name` 字段和资源和非资源规则的\n`verbs`、`apiGroups`、`resources`、`namespaces` 和 `nonResourceURLs` 字段，\n可以指定通配符 `*` 来匹配任意值，从而有效地忽略该字段。"}
{"en": "A FlowSchema's `distinguisherMethod.type` determines how requests matching that\nschema will be separated into flows. It may be `ByUser`, in which one requesting\nuser will not be able to starve other users of capacity; `ByNamespace`, in which\nrequests for resources in one namespace will not be able to starve requests for\nresources in other namespaces of capacity; or blank (or `distinguisherMethod` may be\nomitted entirely), in which all requests matched by this FlowSchema will be\nconsidered part of a single flow. The correct choice for a given FlowSchema\ndepends on the resource and your particular environment.", "zh": "FlowSchema 的 `distinguisherMethod.type` 字段决定了如何把与该模式匹配的请求分散到各个流中。\n可能是 `ByUser`，在这种情况下，一个请求用户将无法饿死其他容量的用户；\n或者是 `ByNamespace`，在这种情况下，一个名字空间中的资源请求将无法饿死其它名字空间的资源请求；\n或者为空（或者可以完全省略 `distinguisherMethod`），\n在这种情况下，与此 FlowSchema 匹配的请求将被视为单个流的一部分。\n资源和你的特定环境决定了如何选择正确一个 FlowSchema。"}
{"en": "## Defaults\n\nEach kube-apiserver maintains two sorts of APF configuration objects:\nmandatory and suggested.", "zh": "## 默认值    {#defaults}\n\n每个 kube-apiserver 会维护两种类型的 APF 配置对象：强制的（Mandatory）和建议的（Suggested）。"}
{"en": "### Mandatory Configuration Objects\n\nThe four mandatory configuration objects reflect fixed built-in\nguardrail behavior. This is behavior that the servers have before\nthose objects exist, and when those objects exist their specs reflect\nthis behavior. The four mandatory objects are as follows.", "zh": "### 强制的配置对象   {#mandatory-configuration-objects}\n\n有四种强制的配置对象对应内置的守护行为。这里的行为是服务器在还未创建对象之前就具备的行为，\n而当这些对象存在时，其规约反映了这类行为。四种强制的对象如下："}
{"en": "* The mandatory `exempt` priority level is used for requests that are\n  not subject to flow control at all: they will always be dispatched\n  immediately. The mandatory `exempt` FlowSchema classifies all\n  requests from the `system:masters` group into this priority\n  level. You may define other FlowSchemas that direct other requests\n  to this priority level, if appropriate.", "zh": "* 强制的 `exempt` 优先级用于完全不受流控限制的请求：它们总是立刻被分发。\n  强制的 `exempt` FlowSchema 把 `system:masters` 组的所有请求都归入该优先级。\n  如果合适，你可以定义新的 FlowSchema，将其他请求定向到该优先级。"}
{"en": "* The mandatory `catch-all` priority level is used in combination with\n  the mandatory `catch-all` FlowSchema to make sure that every request\n  gets some kind of classification. Typically you should not rely on\n  this catch-all configuration, and should create your own catch-all\n  FlowSchema and PriorityLevelConfiguration (or use the suggested\n  `global-default` priority level that is installed by default) as\n  appropriate. Because it is not expected to be used normally, the\n  mandatory `catch-all` priority level has a very small concurrency\n  share and does not queue requests.", "zh": "* 强制的 `catch-all` 优先级与强制的 `catch-all` FlowSchema 结合使用，\n  以确保每个请求都分类。一般而言，你不应该依赖于 `catch-all` 的配置，\n  而应适当地创建自己的 `catch-all` FlowSchema 和 PriorityLevelConfiguration\n  （或使用默认安装的 `global-default` 配置）。\n  因为这一优先级不是正常场景下要使用的，`catch-all` 优先级的并发度份额很小，\n  并且不会对请求进行排队。"}
{"en": "### Suggested Configuration Objects\n\nThe suggested FlowSchemas and PriorityLevelConfigurations constitute a\nreasonable default configuration. You can modify these and/or create\nadditional configuration objects if you want. If your cluster is\nlikely to experience heavy load then you should consider what\nconfiguration will work best.\n\nThe suggested configuration groups requests into six priority levels:", "zh": "### 建议的配置对象   {#suggested-configuration-objects}\n\n建议的 FlowSchema 和 PriorityLevelConfiguration 包含合理的默认配置。\n你可以修改这些对象或者根据需要创建新的配置对象。如果你的集群可能承受较重负载，\n那么你就要考虑哪种配置最合适。\n\n建议的配置把请求分为六个优先级："}
{"en": "* The `node-high` priority level is for health updates from nodes.", "zh": "* `node-high` 优先级用于来自节点的健康状态更新。"}
{"en": "* The `system` priority level is for non-health requests from the\n  `system:nodes` group, i.e. Kubelets, which must be able to contact\n  the API server in order for workloads to be able to schedule on\n  them.", "zh": "* `system` 优先级用于 `system:nodes` 组（即 kubelet）的与健康状态更新无关的请求；\n  kubelet 必须能连上 API 服务器，以便工作负载能够调度到其上。"}
{"en": "* The `leader-election` priority level is for leader election requests from\n  built-in controllers (in particular, requests for `endpoints`, `configmaps`,\n  or `leases` coming from the `system:kube-controller-manager` or\n  `system:kube-scheduler` users and service accounts in the `kube-system`\n  namespace). These are important to isolate from other traffic because failures\n  in leader election cause their controllers to fail and restart, which in turn\n  causes more expensive traffic as the new controllers sync their informers.", "zh": "* `leader-election` 优先级用于内置控制器的领导选举的请求\n  （特别是来自 `kube-system` 名字空间中 `system:kube-controller-manager` 和\n  `system:kube-scheduler` 用户和服务账号，针对 `endpoints`、`configmaps` 或 `leases` 的请求）。\n  将这些请求与其他流量相隔离非常重要，因为领导者选举失败会导致控制器发生故障并重新启动，\n  这反过来会导致新启动的控制器在同步信息时，流量开销更大。"}
{"en": "* The `workload-high` priority level is for other requests from built-in\n  controllers.\n\n* The `workload-low` priority level is for requests from any other service\n  account, which will typically include all requests from controllers running in\n  Pods.\n\n* The `global-default` priority level handles all other traffic, e.g.\n  interactive `kubectl` commands run by nonprivileged users.", "zh": "* `workload-high` 优先级用于内置控制器的其他请求。\n* `workload-low` 优先级用于来自所有其他服务帐户的请求，通常包括来自 Pod\n  中运行的控制器的所有请求。\n* `global-default` 优先级可处理所有其他流量，例如：非特权用户运行的交互式\n  `kubectl` 命令。"}
{"en": "The suggested FlowSchemas serve to steer requests into the above\npriority levels, and are not enumerated here.", "zh": "建议的 FlowSchema 用来将请求导向上述的优先级内，这里不再一一列举。"}
{"en": "### Maintenance of the Mandatory and Suggested Configuration Objects\n\nEach `kube-apiserver` independently maintains the mandatory and\nsuggested configuration objects, using initial and periodic behavior.\nThus, in a situation with a mixture of servers of different versions\nthere may be thrashing as long as different servers have different\nopinions of the proper content of these objects.", "zh": "### 强制的与建议的配置对象的维护   {#maintenance-of-the-mandatory-and-suggested-configuration-objects}\n\n每个 `kube-apiserver` 都独立地维护其强制的与建议的配置对象，\n这一维护操作既是服务器的初始行为，也是其周期性操作的一部分。\n因此，当存在不同版本的服务器时，如果各个服务器对于配置对象中的合适内容有不同意见，\n就可能出现抖动。"}
{"en": "Each `kube-apiserver` makes an initial maintenance pass over the\nmandatory and suggested configuration objects, and after that does\nperiodic maintenance (once per minute) of those objects.\n\nFor the mandatory configuration objects, maintenance consists of\nensuring that the object exists and, if it does, has the proper spec.\nThe server refuses to allow a creation or update with a spec that is\ninconsistent with the server's guardrail behavior.", "zh": "每个 `kube-apiserver` 都会对强制的与建议的配置对象执行初始的维护操作，\n之后（每分钟）对这些对象执行周期性的维护。\n\n对于强制的配置对象，维护操作包括确保对象存在并且包含合适的规约（如果存在的话）。\n服务器会拒绝创建或更新与其守护行为不一致的规约。"}
{"en": "Maintenance of suggested configuration objects is designed to allow\ntheir specs to be overridden. Deletion, on the other hand, is not\nrespected: maintenance will restore the object. If you do not want a\nsuggested configuration object then you need to keep it around but set\nits spec to have minimal consequences. Maintenance of suggested\nobjects is also designed to support automatic migration when a new\nversion of the `kube-apiserver` is rolled out, albeit potentially with\nthrashing while there is a mixed population of servers.", "zh": "对建议的配置对象的维护操作被设计为允许其规约被重载。删除操作是不允许的，\n维护操作期间会重建这类配置对象。如果你不需要某个建议的配置对象，\n你需要将它放在一边，并让其规约所产生的影响最小化。\n对建议的配置对象而言，其维护方面的设计也支持在上线新的 `kube-apiserver`\n时完成自动的迁移动作，即便可能因为当前的服务器集合存在不同的版本而可能造成抖动仍是如此。"}
{"en": "Maintenance of a suggested configuration object consists of creating\nit --- with the server's suggested spec --- if the object does not\nexist. OTOH, if the object already exists, maintenance behavior\ndepends on whether the `kube-apiservers` or the users control the\nobject. In the former case, the server ensures that the object's spec\nis what the server suggests; in the latter case, the spec is left\nalone.", "zh": "对建议的配置对象的维护操作包括基于服务器建议的规约创建对象\n（如果对象不存在的话）。反之，如果对象已经存在，维护操作的行为取决于是否\n`kube-apiserver` 或者用户在控制对象。如果 `kube-apiserver` 在控制对象，\n则服务器确保对象的规约与服务器所给的建议匹配，如果用户在控制对象，\n对象的规约保持不变。"}
{"en": "The question of who controls the object is answered by first looking\nfor an annotation with key `apf.kubernetes.io/autoupdate-spec`. If\nthere is such an annotation and its value is `true` then the\nkube-apiservers control the object. If there is such an annotation\nand its value is `false` then the users control the object. If\nneither of those conditions holds then the `metadata.generation` of the\nobject is consulted. If that is 1 then the kube-apiservers control\nthe object. Otherwise the users control the object. These rules were\nintroduced in release 1.22 and their consideration of\n`metadata.generation` is for the sake of migration from the simpler\nearlier behavior. Users who wish to control a suggested configuration\nobject should set its `apf.kubernetes.io/autoupdate-spec` annotation\nto `false`.", "zh": "关于谁在控制对象这个问题，首先要看对象上的 `apf.kubernetes.io/autoupdate-spec`\n注解。如果对象上存在这个注解，并且其取值为`true`，则 kube-apiserver\n在控制该对象。如果存在这个注解，并且其取值为`false`，则用户在控制对象。\n如果这两个条件都不满足，则需要进一步查看对象的 `metadata.generation`。\n如果该值为 1，则 kube-apiserver 控制对象，否则用户控制对象。\n这些规则是在 1.22 发行版中引入的，而对 `metadata.generation`\n的考量是为了便于从之前较简单的行为迁移过来。希望控制建议的配置对象的用户应该将对象的\n`apf.kubernetes.io/autoupdate-spec` 注解设置为 `false`。"}
{"en": "Maintenance of a mandatory or suggested configuration object also\nincludes ensuring that it has an `apf.kubernetes.io/autoupdate-spec`\nannotation that accurately reflects whether the kube-apiservers\ncontrol the object.\n\nMaintenance also includes deleting objects that are neither mandatory\nnor suggested but are annotated\n`apf.kubernetes.io/autoupdate-spec=true`.", "zh": "对强制的或建议的配置对象的维护操作也包括确保对象上存在 `apf.kubernetes.io/autoupdate-spec`\n这一注解，并且其取值准确地反映了是否 kube-apiserver 在控制着对象。\n\n维护操作还包括删除那些既非强制又非建议的配置，同时注解配置为\n`apf.kubernetes.io/autoupdate-spec=true` 的对象。"}
{"en": "## Health check concurrency exemption\n\nThe suggested configuration gives no special treatment to the health\ncheck requests on kube-apiservers from their local kubelets --- which\ntend to use the secured port but supply no credentials. With the\nsuggested config, these requests get assigned to the `global-default`\nFlowSchema and the corresponding `global-default` priority level,\nwhere other traffic can crowd them out.", "zh": "## 健康检查并发豁免    {#health-check-concurrency-exemption}\n\n推荐配置没有为本地 kubelet 对 kube-apiserver 执行健康检查的请求进行任何特殊处理\n——它们倾向于使用安全端口，但不提供凭据。\n在推荐配置中，这些请求将分配 `global-default` FlowSchema 和 `global-default` 优先级，\n这样其他流量可以排除健康检查。"}
{"en": "If you add the following additional FlowSchema, this exempts those\nrequests from rate limiting.", "zh": "如果添加以下 FlowSchema，健康检查请求不受速率限制。\n\n{{< caution >}}"}
{"en": "Making this change also allows any hostile party to then send\nhealth-check requests that match this FlowSchema, at any volume they\nlike. If you have a web traffic filter or similar external security\nmechanism to protect your cluster's API server from general internet\ntraffic, you can configure rules to block any health check requests\nthat originate from outside your cluster.", "zh": "进行此更改后，任何敌对方都可以发送与此 FlowSchema 匹配的任意数量的健康检查请求。\n如果你有 Web 流量过滤器或类似的外部安全机制保护集群的 API 服务器免受常规网络流量的侵扰，\n则可以配置规则，阻止所有来自集群外部的健康检查请求。\n{{< /caution >}}\n\n{{% code_sample file=\"priority-and-fairness/health-for-strangers.yaml\" %}}"}
{"en": "## Observability\n\n### Metrics", "zh": "## 可观察性    {#observability}\n\n### 指标    {#metrics}\n\n{{< note >}}"}
{"en": "In versions of Kubernetes before v1.20, the labels `flow_schema` and\n`priority_level` were inconsistently named `flowSchema` and `priorityLevel`,\nrespectively. If you're running Kubernetes versions v1.19 and earlier, you\nshould refer to the documentation for your version.", "zh": "在 Kubernetes v1.20 之前的版本中，标签 `flow_schema` 和 `priority_level`\n的名称有时被写作 `flowSchema` 和 `priorityLevel`，即存在不一致的情况。\n如果你在运行 Kubernetes v1.19 或者更早版本，你需要参考你所使用的集群版本对应的文档。\n{{< /note >}}"}
{"en": "When you enable the API Priority and Fairness feature, the kube-apiserver\nexports additional metrics. Monitoring these can help you determine whether your\nconfiguration is inappropriately throttling important traffic, or find\npoorly-behaved workloads that may be harming system health.", "zh": "当你开启了 APF 后，kube-apiserver 会暴露额外指标。\n监视这些指标有助于判断你的配置是否不当地限制了重要流量，\n或者发现可能会损害系统健康的，行为不良的工作负载。"}
{"en": "#### Maturity level BETA", "zh": "#### 成熟度水平 BETA"}
{"en": "* `apiserver_flowcontrol_rejected_requests_total` is a counter vector\n  (cumulative since server start) of requests that were rejected,\n  broken down by the labels `flow_schema` (indicating the one that\n  matched the request), `priority_level` (indicating the one to which\n  the request was assigned), and `reason`. The `reason` label will be\n  one of the following values:", "zh": "* `apiserver_flowcontrol_rejected_requests_total` 是一个计数器向量，\n  记录被拒绝的请求数量（自服务器启动以来累计值），\n  可按标签 `flow_chema`（表示与请求匹配的 FlowSchema）、`priority_level`\n  （表示分配给请该求的优先级）和 `reason` 分解。\n  `reason` 标签将是以下值之一："}
{"en": "* `queue-full`, indicating that too many requests were already\n    queued.\n  * `concurrency-limit`, indicating that the\n    PriorityLevelConfiguration is configured to reject rather than\n    queue excess requests.\n  * `time-out`, indicating that the request was still in the queue\n    when its queuing time limit expired.\n  * `cancelled`, indicating that the request is not purge locked\n    and has been ejected from the queue.", "zh": "* `queue-full`，表明已经有太多请求排队\n  * `concurrency-limit`，表示将 PriorityLevelConfiguration 配置为\n    `Reject` 而不是 `Queue`，或者\n  * `time-out`，表示在其排队时间超期的请求仍在队列中。\n  * `cancelled`，表示该请求未被清除锁定，已从队列中移除。"}
{"en": "* `apiserver_flowcontrol_dispatched_requests_total` is a counter\n  vector (cumulative since server start) of requests that began\n  executing, broken down by `flow_schema` and `priority_level`.", "zh": "* `apiserver_flowcontrol_dispatched_requests_total` 是一个计数器向量，\n  记录开始执行的请求数量（自服务器启动以来的累积值），\n  可按 `flow_schema` 和 `priority_level` 分解。"}
{"en": "* `apiserver_flowcontrol_current_inqueue_requests` is a gauge vector\n  holding the instantaneous number of queued (not executing) requests,\n  broken down by `priority_level` and `flow_schema`.\n\n* `apiserver_flowcontrol_current_executing_requests` is a gauge vector\n  holding the instantaneous number of executing (not waiting in a\n  queue) requests, broken down by `priority_level` and `flow_schema`.", "zh": "* `apiserver_flowcontrol_current_inqueue_requests` 是一个测量向量，\n  记录排队中的（未执行）请求的瞬时数量，可按 `priority_level` 和 `flow_schema` 分解。\n\n* `apiserver_flowcontrol_current_executing_requests` 是一个测量向量，\n  记录执行中（不在队列中等待）请求的瞬时数量，可按 `priority_level` 和 `flow_schema` 分解。"}
{"en": "* `apiserver_flowcontrol_current_executing_seats` is a gauge vector\n  holding the instantaneous number of occupied seats, broken down by\n  `priority_level` and `flow_schema`.\n\n* `apiserver_flowcontrol_request_wait_duration_seconds` is a histogram\n  vector of how long requests spent queued, broken down by the labels\n  `flow_schema`, `priority_level`, and `execute`. The `execute` label\n  indicates whether the request has started executing.", "zh": "* `apiserver_flowcontrol_current_executing_seats` 是一个测量向量，\n  记录了按 `priority_level` 和 `flow_schema` 细分的瞬时占用席位数量。\n\n* `apiserver_flowcontrol_request_wait_duration_seconds` 是一个直方图向量，\n  记录了按 `flow_schema`、`priority_level` 和 `execute` 标签细分的请求在队列中等待的时长。\n  `execute` 标签表示请求是否已开始执行。\n\n  {{< note >}}"}
{"en": "Since each FlowSchema always assigns requests to a single\n  PriorityLevelConfiguration, you can add the histograms for all the\n  FlowSchemas for one priority level to get the effective histogram for\n  requests assigned to that priority level.", "zh": "由于每个 FlowSchema 总会给请求分配 PriorityLevelConfiguration，\n  因此你可以将一个优先级的所有 FlowSchema 的直方图相加，以得到分配给该优先级的请求的有效直方图。\n  {{< /note >}}"}
{"en": "* `apiserver_flowcontrol_nominal_limit_seats` is a gauge vector\n  holding each priority level's nominal concurrency limit, computed\n  from the API server's total concurrency limit and the priority\n  level's configured nominal concurrency shares.", "zh": "* `apiserver_flowcontrol_nominal_limit_seats` 是一个测量向量，\n  记录了每个优先级的额定并发限制。\n  此值是根据 API 服务器的总并发限制和优先级的配置额定并发份额计算得出的。"}
{"en": "#### Maturity level ALPHA", "zh": "#### 成熟度水平 ALPHA"}
{"en": "* `apiserver_current_inqueue_requests` is a gauge vector of recent\n  high water marks of the number of queued requests, grouped by a\n  label named `request_kind` whose value is `mutating` or `readOnly`.\n  These high water marks describe the largest number seen in the one\n  second window most recently completed. These complement the older\n  `apiserver_current_inflight_requests` gauge vector that holds the\n  last window's high water mark of number of requests actively being\n  served.", "zh": "* `apiserver_current_inqueue_requests` 是一个测量向量，\n  记录最近排队请求数量的高水位线，\n  由标签 `request_kind` 分组，标签的值为 `mutating` 或 `readOnly`。\n  这些高水位线表示在最近一秒钟内看到的最大数字。\n  它们补充说明了老的测量向量 `apiserver_current_inflight_requests`\n  （该量保存了最后一个窗口中，正在处理的请求数量的高水位线）。"}
{"en": "* `apiserver_current_inqueue_seats` is a gauge vector of the sum over\n  queued requests of the largest number of seats each will occupy,\n  grouped by labels named `flow_schema` and `priority_level`.", "zh": "* `apiserver_current_inqueue_seats` 是一个测量向量，\n  记录了排队请求中每个请求将占用的最大席位数的总和，\n  按 `flow_schema` 和 `priority_level` 两个标签进行分组。"}
{"en": "* `apiserver_flowcontrol_read_vs_write_current_requests` is a\n  histogram vector of observations, made at the end of every\n  nanosecond, of the number of requests broken down by the labels\n  `phase` (which takes on the values `waiting` and `executing`) and\n  `request_kind` (which takes on the values `mutating` and\n  `readOnly`). Each observed value is a ratio, between 0 and 1, of\n  the number of requests divided by the corresponding limit on the\n  number of requests (queue volume limit for waiting and concurrency\n  limit for executing).", "zh": "* `apiserver_flowcontrol_read_vs_write_current_requests` 是一个直方图向量，\n  在每个纳秒结束时记录请求数量的观察值，可按标签 `phase`（取值为 `waiting` 及 `executing`）\n  和 `request_kind`（取值为 `mutating` 及 `readOnly`）分解。\n  每个观察到的值是一个介于 0 和 1 之间的比值，计算方式为请求数除以该请求数的对应限制\n  （等待的队列长度限制和执行所用的并发限制）。"}
{"en": "* `apiserver_flowcontrol_request_concurrency_in_use` is a gauge vector\n  holding the instantaneous number of occupied seats, broken down by\n  `priority_level` and `flow_schema`.", "zh": "* `apiserver_flowcontrol_request_concurrency_in_use` 是一个规范向量，\n  包含占用席位的瞬时数量，可按 `priority_level` 和 `flow_schema` 分解。"}
{"en": "* `apiserver_flowcontrol_priority_level_request_utilization` is a\n  histogram vector of observations, made at the end of each\n  nanosecond, of the number of requests broken down by the labels\n  `phase` (which takes on the values `waiting` and `executing`) and\n  `priority_level`. Each observed value is a ratio, between 0 and 1,\n  of a number of requests divided by the corresponding limit on the\n  number of requests (queue volume limit for waiting and concurrency\n  limit for executing).", "zh": "* `apiserver_flowcontrol_priority_level_request_utilization` 是一个直方图向量，\n  在每个纳秒结束时记录请求数量的观察值，\n  可按标签 `phase`（取值为 `waiting` 及 `executing`）和 `priority_level` 分解。\n  每个观察到的值是一个介于 0 和 1 之间的比值，计算方式为请求数除以该请求数的对应限制\n  （等待的队列长度限制和执行所用的并发限制）。"}
{"en": "* `apiserver_flowcontrol_priority_level_seat_utilization` is a\n  histogram vector of observations, made at the end of each\n  nanosecond, of the utilization of a priority level's concurrency\n  limit, broken down by `priority_level`. This utilization is the\n  fraction (number of seats occupied) / (concurrency limit). This\n  metric considers all stages of execution (both normal and the extra\n  delay at the end of a write to cover for the corresponding\n  notification work) of all requests except WATCHes; for those it\n  considers only the initial stage that delivers notifications of\n  pre-existing objects. Each histogram in the vector is also labeled\n  with `phase: executing` (there is no seat limit for the waiting\n  phase).", "zh": "* `apiserver_flowcontrol_priority_level_seat_utilization` 是一个直方图向量，\n  在每个纳秒结束时记录某个优先级并发度限制利用率的观察值，可按标签 `priority_level` 分解。\n  此利用率是一个分数：（占用的席位数）/（并发限制）。\n  此指标考虑了除 WATCH 之外的所有请求的所有执行阶段（包括写入结束时的正常延迟和额外延迟，\n  以覆盖相应的通知操作）；对于 WATCH 请求，只考虑传递预先存在对象通知的初始阶段。\n  该向量中的每个直方图也带有 `phase: executing`（等待阶段没有席位限制）的标签。"}
{"en": "* `apiserver_flowcontrol_request_queue_length_after_enqueue` is a\n  histogram vector of queue lengths for the queues, broken down by\n  `priority_level` and `flow_schema`, as sampled by the enqueued requests.\n  Each request that gets queued contributes one sample to its histogram,\n  reporting the length of the queue immediately after the request was added.\n  Note that this produces different statistics than an unbiased survey would.", "zh": "* `apiserver_flowcontrol_request_queue_length_after_enqueue` 是一个直方图向量，\n  记录请求队列的长度，可按 `priority_level` 和 `flow_schema` 分解。\n  每个排队中的请求都会为其直方图贡献一个样本，并在添加请求后立即上报队列的长度。\n  请注意，这样产生的统计数据与无偏调查不同。\n\n  {{< note >}}"}
{"en": "An outlier value in a histogram here means it is likely that a single flow\n  (i.e., requests by one user or for one namespace, depending on\n  configuration) is flooding the API server, and being throttled. By contrast,\n  if one priority level's histogram shows that all queues for that priority\n  level are longer than those for other priority levels, it may be appropriate\n  to increase that PriorityLevelConfiguration's concurrency shares.", "zh": "直方图中的离群值在这里表示单个流（即，一个用户或一个名字空间的请求，\n  具体取决于配置）正在疯狂地向 API 服务器发请求，并受到限制。\n  相反，如果一个优先级的直方图显示该优先级的所有队列都比其他优先级的队列长，\n  则增加 PriorityLevelConfiguration 的并发份额是比较合适的。\n  {{< /note >}}"}
{"en": "* `apiserver_flowcontrol_request_concurrency_limit` is the same as\n  `apiserver_flowcontrol_nominal_limit_seats`. Before the\n  introduction of concurrency borrowing between priority levels,\n  this was always equal to `apiserver_flowcontrol_current_limit_seats`\n  (which did not exist as a distinct metric).", "zh": "* `apiserver_flowcontrol_request_concurrency_limit` 与\n  `apiserver_flowcontrol_nominal_limit_seats` 相同。在优先级之间引入并发度借用之前，\n  此字段始终等于 `apiserver_flowcontrol_current_limit_seats`\n  （它过去不作为一个独立的指标存在）。"}
{"en": "* `apiserver_flowcontrol_lower_limit_seats` is a gauge vector holding\n  the lower bound on each priority level's dynamic concurrency limit.", "zh": "* `apiserver_flowcontrol_lower_limit_seats` 是一个测量向量，包含每个优先级的动态并发度限制的下限。"}
{"en": "* `apiserver_flowcontrol_upper_limit_seats` is a gauge vector holding\n  the upper bound on each priority level's dynamic concurrency limit.", "zh": "* `apiserver_flowcontrol_upper_limit_seats` 是一个测量向量，包含每个优先级的动态并发度限制的上限。"}
{"en": "* `apiserver_flowcontrol_demand_seats` is a histogram vector counting\n  observations, at the end of every nanosecond, of each priority\n  level's ratio of (seat demand) / (nominal concurrency limit). \n  A priority level's seat demand is the sum, over both queued requests\n  and those in the initial phase of execution, of the maximum of the\n  number of seats occupied in the request's initial and final\n  execution phases.", "zh": "* `apiserver_flowcontrol_demand_seats` 是一个直方图向量，\n  统计每纳秒结束时每个优先级的（席位需求）/（额定并发限制）比率的观察值。\n  某优先级的席位需求是针对排队的请求和初始执行阶段的请求，在请求的初始和最终执行阶段占用的最大席位数之和。"}
{"en": "* `apiserver_flowcontrol_demand_seats_high_watermark` is a gauge vector\n  holding, for each priority level, the maximum seat demand seen\n  during the last concurrency borrowing adjustment period.", "zh": "* `apiserver_flowcontrol_demand_seats_high_watermark` 是一个测量向量，\n  为每个优先级包含了上一个并发度借用调整期间所观察到的最大席位需求。"}
{"en": "* `apiserver_flowcontrol_demand_seats_average` is a gauge vector\n  holding, for each priority level, the time-weighted average seat\n  demand seen during the last concurrency borrowing adjustment period.", "zh": "* `apiserver_flowcontrol_demand_seats_average` 是一个测量向量，\n  为每个优先级包含了上一个并发度借用调整期间所观察到的时间加权平均席位需求。"}
{"en": "* `apiserver_flowcontrol_demand_seats_stdev` is a gauge vector\n  holding, for each priority level, the time-weighted population\n  standard deviation of seat demand seen during the last concurrency\n  borrowing adjustment period.", "zh": "* `apiserver_flowcontrol_demand_seats_stdev` 是一个测量向量，\n  为每个优先级包含了上一个并发度借用调整期间所观察到的席位需求的时间加权总标准偏差。"}
{"en": "* `apiserver_flowcontrol_demand_seats_smoothed` is a gauge vector\n  holding, for each priority level, the smoothed enveloped seat demand\n  determined at the last concurrency adjustment.", "zh": "* `apiserver_flowcontrol_demand_seats_smoothed` 是一个测量向量，\n  为每个优先级包含了上一个并发度调整期间确定的平滑包络席位需求。"}
{"en": "* `apiserver_flowcontrol_target_seats` is a gauge vector holding, for\n  each priority level, the concurrency target going into the borrowing\n  allocation problem.", "zh": "* `apiserver_flowcontrol_target_seats` 是一个测量向量，\n  包含每个优先级触发借用分配问题的并发度目标值。"}
{"en": "* `apiserver_flowcontrol_seat_fair_frac` is a gauge holding the fair\n  allocation fraction determined in the last borrowing adjustment.", "zh": "* `apiserver_flowcontrol_seat_fair_frac` 是一个测量向量，\n  包含了上一个借用调整期间确定的公平分配比例。"}
{"en": "* `apiserver_flowcontrol_current_limit_seats` is a gauge vector\n  holding, for each priority level, the dynamic concurrency limit\n  derived in the last adjustment.", "zh": "* `apiserver_flowcontrol_current_limit_seats` 是一个测量向量，\n  包含每个优先级的上一次调整期间得出的动态并发限制。"}
{"en": "* `apiserver_flowcontrol_request_execution_seconds` is a histogram\n  vector of how long requests took to actually execute, broken down by\n  `flow_schema` and `priority_level`.", "zh": "* `apiserver_flowcontrol_request_execution_seconds` 是一个直方图向量，\n  记录请求实际执行需要花费的时间，\n  可按标签 `flow_schema` 和 `priority_level` 分解。"}
{"en": "* `apiserver_flowcontrol_watch_count_samples` is a histogram vector of\n  the number of active WATCH requests relevant to a given write,\n  broken down by `flow_schema` and `priority_level`.", "zh": "* `apiserver_flowcontrol_watch_count_samples` 是一个直方图向量，\n  记录给定写的相关活动 WATCH 请求数量，\n  可按标签 `flow_schema` 和 `priority_level` 分解。"}
{"en": "* `apiserver_flowcontrol_work_estimated_seats` is a histogram vector\n  of the number of estimated seats (maximum of initial and final stage\n  of execution) associated with requests, broken down by `flow_schema`\n  and `priority_level`.", "zh": "* `apiserver_flowcontrol_work_estimated_seats` 是一个直方图向量，\n  记录与估计席位（最初阶段和最后阶段的最多人数）相关联的请求数量，\n  可按标签 `flow_schema` 和 `priority_level` 分解。"}
{"en": "* `apiserver_flowcontrol_request_dispatch_no_accommodation_total` is a\n  counter vector of the number of events that in principle could have led\n  to a request being dispatched but did not, due to lack of available\n  concurrency, broken down by `flow_schema` and `priority_level`.", "zh": "* `apiserver_flowcontrol_request_dispatch_no_accommodation_total`\n  是一个事件数量的计数器，这些事件在原则上可能导致请求被分派，\n  但由于并发度不足而没有被分派，\n  可按标签 `flow_schema` 和 `priority_level` 分解。"}
{"en": "* `apiserver_flowcontrol_epoch_advance_total` is a counter vector of\n  the number of attempts to jump a priority level's progress meter\n  backward to avoid numeric overflow, grouped by `priority_level` and\n  `success`.", "zh": "* `apiserver_flowcontrol_epoch_advance_total` 是一个计数器向量，\n  记录了将优先级进度计向后跳跃以避免数值溢出的尝试次数，\n  按 `priority_level` 和 `success` 两个标签进行分组。"}
{"en": "## Good practices for using API Priority and Fairness\n\nWhen a given priority level exceeds its permitted concurrency, requests can\nexperience increased latency or be dropped with an HTTP 429 (Too Many Requests)\nerror. To prevent these side effects of APF, you can modify your workload or\ntweak your APF settings to ensure there are sufficient seats available to serve\nyour requests.", "zh": "## 使用 API 优先级和公平性的最佳实践   {#good-practices-for-using-api-priority-and-fairness}\n\n当某个给定的优先级级别超过其所被允许的并发数时，请求可能会遇到延迟增加，\n或以错误 HTTP 429 (Too Many Requests) 的形式被拒绝。\n为了避免这些 APF 的副作用，你可以修改你的工作负载或调整你的 APF 设置，确保有足够的席位来处理请求。"}
{"en": "To detect whether requests are being rejected due to APF, check the following\nmetrics:\n\n- apiserver_flowcontrol_rejected_requests_total: the total number of requests\n  rejected per FlowSchema and PriorityLevelConfiguration.\n- apiserver_flowcontrol_current_inqueue_requests: the current number of requests\n  queued per FlowSchema and PriorityLevelConfiguration.\n- apiserver_flowcontrol_request_wait_duration_seconds: the latency added to\n  requests waiting in queues.\n- apiserver_flowcontrol_priority_level_seat_utilization: the seat utilization\n  per PriorityLevelConfiguration.", "zh": "要检测请求是否由于 APF 而被拒绝，可以检查以下指标：\n\n- apiserver_flowcontrol_rejected_requests_total：\n  每个 FlowSchema 和 PriorityLevelConfiguration 拒绝的请求总数。\n- apiserver_flowcontrol_current_inqueue_requests：\n  每个 FlowSchema 和 PriorityLevelConfiguration 中排队的当前请求数。\n- apiserver_flowcontrol_request_wait_duration_seconds：请求在队列中等待的延迟时间。\n- apiserver_flowcontrol_priority_level_seat_utilization：\n  每个 PriorityLevelConfiguration 的席位利用率。"}
{"en": "### Workload modifications {#good-practice-workload-modifications}\n\nTo prevent requests from queuing and adding latency or being dropped due to APF,\nyou can optimize your requests by:", "zh": "### 工作负载修改 {#good-practice-workload-modifications}\n\n为了避免由于 APF 导致请求排队、延迟增加或被拒绝，你可以通过以下方式优化请求："}
{"en": "- Reducing the rate at which requests are executed. A fewer number of requests\n  over a fixed period will result in a fewer number of seats being needed at a\n  given time.", "zh": "- 减少请求执行的速率。在固定时间段内减少请求数量将导致在某一给定时间点需要的席位数更少。"}
{"en": "- Avoid issuing a large number of expensive requests concurrently. Requests can\n  be optimized to use fewer seats or have lower latency so that these requests\n  hold those seats for a shorter duration. List requests can occupy more than 1\n  seat depending on the number of objects fetched during the request. Restricting\n  the number of objects retrieved in a list request, for example by using\n  pagination, will use less total seats over a shorter period. Furthermore,\n  replacing list requests with watch requests will require lower total concurrency\n  shares as watch requests only occupy 1 seat during its initial burst of\n  notifications. If using streaming lists in versions 1.27 and later, watch\n  requests will occupy the same number of seats as a list request for its initial\n  burst of notifications because the entire state of the collection has to be\n  streamed. Note that in both cases, a watch request will not hold any seats after\n  this initial phase.", "zh": "- 避免同时发出大量消耗较多席位的请求。请求可以被优化为使用更少的席位或降低延迟，\n  使这些请求占用席位的时间变短。列表请求根据请求期间获取的对象数量可能会占用多个席位。\n  例如通过使用分页等方式限制列表请求中取回的对象数量，可以在更短时间内使用更少的总席位数。\n  此外，将列表请求替换为监视请求将需要更低的总并发份额，因为监视请求仅在初始的通知突发阶段占用 1 个席位。\n  如果在 1.27 及更高版本中使用流式列表，因为集合的整个状态必须以流式传输，\n  所以监视请求在其初始的通知突发阶段将占用与列表请求相同数量的席位。\n  请注意，在这两种情况下，监视请求在此初始阶段之后将不再保留任何席位。"}
{"en": "Keep in mind that queuing or rejected requests from APF could be induced by\neither an increase in the number of requests or an increase in latency for\nexisting requests. For example, if requests that normally take 1s to execute\nstart taking 60s, it is possible that APF will start rejecting requests because\nrequests are occupying seats for a longer duration than normal due to this\nincrease in latency. If APF starts rejecting requests across multiple priority\nlevels without a significant change in workload, it is possible there is an\nunderlying issue with control plane performance rather than the workload or APF\nsettings.", "zh": "请注意，由于请求数量增加或现有请求的延迟增加，APF 可能会导致请求排队或被拒绝。\n例如，如果通常需要 1 秒执行的请求开始需要 60 秒，由于延迟增加，\n请求所占用的席位时间可能超过了正常情况下的时长，APF 将开始拒绝请求。\n如果在没有工作负载显著变化的情况下，APF 开始在多个优先级级别上拒绝请求，\n则可能存在控制平面性能的潜在问题，而不是工作负载或 APF 设置的问题。"}
{"en": "### Priority and fairness settings {#good-practice-apf-settings}\n\nYou can also modify the default FlowSchema and PriorityLevelConfiguration\nobjects or create new objects of these types to better accommodate your\nworkload.\n\nAPF settings can be modified to:\n\n- Give more seats to high priority requests.\n- Isolate non-essential or expensive requests that would starve a concurrency\n  level if it was shared with other flows.", "zh": "### 优先级和公平性设置   {#good-practice-apf-settings}\n\n你还可以修改默认的 FlowSchema 和 PriorityLevelConfiguration 对象，\n或创建新的对象来更好地容纳你的工作负载。\n\nAPF 设置可以被修改以实现下述目标：\n\n- 给予高优先级请求更多的席位。\n- 隔离那些非必要或开销大的请求，因为如果与其他流共享，这些请求可能会耗尽所有并发级别。"}
{"en": "#### Give more seats to high priority requests", "zh": "#### 给予高优先级请求更多的席位"}
{"en": "1. If possible, the number of seats available across all priority levels for a\n   particular `kube-apiserver` can be increased by increasing the values for the\n   `max-requests-inflight` and `max-mutating-requests-inflight` flags. Alternatively,\n   horizontally scaling the number of `kube-apiserver` instances will increase the\n   total concurrency per priority level across the cluster assuming there is\n   sufficient load balancing of requests.", "zh": "1. 如果有可能，你可以通过提高 `max-requests-inflight` 和 `max-mutating-requests-inflight`\n   参数的值为特定 `kube-apiserver` 提高所有优先级级别均可用的席位数量。另外，\n   如果在请求的负载均衡足够好的情况下，水平扩缩 `kube-apiserver` 实例的数量将提高集群中每个优先级级别的总并发数。"}
{"en": "1. You can create a new FlowSchema which references a PriorityLevelConfiguration\n   with a larger concurrency level. This new PriorityLevelConfiguration could be an\n   existing level or a new level with its own set of nominal concurrency shares.\n   For example, a new FlowSchema could be introduced to change the\n   PriorityLevelConfiguration for your requests from global-default to workload-low\n   to increase the number of seats available to your user. Creating a new\n   PriorityLevelConfiguration will reduce the number of seats designated for\n   existing levels. Recall that editing a default FlowSchema or\n   PriorityLevelConfiguration will require setting the\n   `apf.kubernetes.io/autoupdate-spec` annotation to false.", "zh": "2. 你可以创建一个新的 FlowSchema，在其中引用并发级别更高的 PriorityLevelConfiguration。\n   这个新的 PriorityLevelConfiguration 可以是现有的级别，也可以是具有自己一组额定并发份额的新级别。\n   例如，你可以引入一个新的 FlowSchema 来将请求的 PriorityLevelConfiguration\n   从全局默认值更改为工作负载较低的级别，以增加用户可用的席位数。\n   创建一个新的 PriorityLevelConfiguration 将减少为现有级别指定的席位数。\n   请注意，编辑默认的 FlowSchema 或 PriorityLevelConfiguration 需要将\n   `apf.kubernetes.io/autoupdate-spec` 注解设置为 false。"}
{"en": "1. You can also increase the NominalConcurrencyShares for the\n   PriorityLevelConfiguration which is serving your high priority requests.\n   Alternatively, for versions 1.26 and later, you can increase the LendablePercent\n   for competing priority levels so that the given priority level has a higher pool\n   of seats it can borrow.", "zh": "3. 你还可以为服务于高优先级请求的 PriorityLevelConfiguration 提高 NominalConcurrencyShares。\n   此外在 1.26 及更高版本中，你可以为有竞争的优先级级别提高 LendablePercent，以便给定优先级级别可以借用更多的席位。"}
{"en": "#### Isolate non-essential requests from starving other flows\n\nFor request isolation, you can create a FlowSchema whose subject matches the\nuser making these requests or create a FlowSchema that matches what the request\nis (corresponding to the resourceRules). Next, you can map this FlowSchema to a\nPriorityLevelConfiguration with a low share of seats.", "zh": "#### 隔离非关键请求以免饿死其他流\n\n为了进行请求隔离，你可以创建一个 FlowSchema，使其主体与发起这些请求的用户匹配，\n或者创建一个与请求内容匹配（对应 resourceRules）的 FlowSchema。\n接下来，你可以将该 FlowSchema 映射到一个具有较低席位份额的 PriorityLevelConfiguration。"}
{"en": "For example, suppose list event requests from Pods running in the default namespace\nare using 10 seats each and execute for 1 minute. To prevent these expensive\nrequests from impacting requests from other Pods using the existing service-accounts\nFlowSchema, you can apply the following FlowSchema to isolate these list calls\nfrom other requests.\n\nExample FlowSchema object to isolate list event requests:", "zh": "例如，假设来自 default 名字空间中运行的 Pod 的每个事件列表请求使用 10 个席位，并且执行时间为 1 分钟。\n为了防止这些开销大的请求影响使用现有服务账号 FlowSchema 的其他 Pod 的请求，你可以应用以下\nFlowSchema 将这些列表调用与其他请求隔离开来。\n\n用于隔离列表事件请求的 FlowSchema 对象示例：\n\n{{% code_sample file=\"priority-and-fairness/list-events-default-service-account.yaml\" %}}"}
{"en": "- This FlowSchema captures all list event calls made by the default service\n  account in the default namespace. The matching precedence 8000 is lower than the\n  value of 9000 used by the existing service-accounts FlowSchema so these list\n  event calls will match list-events-default-service-account rather than\n  service-accounts.\n- The catch-all PriorityLevelConfiguration is used to isolate these requests.\n  The catch-all priority level has a very small concurrency share and does not\n  queue requests.", "zh": "- 这个 FlowSchema 用于抓取 default 名字空间中默认服务账号所发起的所有事件列表调用。\n  匹配优先级为 8000，低于现有服务账号 FlowSchema 所用的 9000，因此这些列表事件调用将匹配到\n  list-events-default-service-account 而不是服务账号。\n- 通用 PriorityLevelConfiguration 用于隔离这些请求。通用优先级级别具有非常小的并发份额，并且不对请求进行排队。\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "- You can visit flow control [reference doc](/docs/reference/debug-cluster/flow-control/) to learn more about troubleshooting.\n- For background information on design details for API priority and fairness, see\n  the [enhancement proposal](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness).\n- You can make suggestions and feature requests via [SIG API Machinery](https://github.com/kubernetes/community/tree/master/sig-api-machinery)\n  or the feature's [slack channel](https://kubernetes.slack.com/messages/api-priority-and-fairness).", "zh": "- 你可以查阅流控[参考文档](/zh-cn/docs/reference/debug-cluster/flow-control/)了解有关故障排查的更多信息。\n- 有关 API 优先级和公平性的设计细节的背景信息，\n  请参阅[增强提案](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness)。\n- 你可以通过 [SIG API Machinery](https://github.com/kubernetes/community/tree/master/sig-api-machinery/)\n  或特性的 [Slack 频道](https://kubernetes.slack.com/messages/api-priority-and-fairness/)提出建议和特性请求。"}
{"en": "System component logs record events happening in cluster, which can be very useful for debugging.\nYou can configure log verbosity to see more or less detail.\nLogs can be as coarse-grained as showing errors within a component, or as fine-grained as showing\nstep-by-step traces of events (like HTTP access logs, pod state changes, controller actions, or\nscheduler decisions).", "zh": "系统组件的日志记录集群中发生的事件，这对于调试非常有用。\n你可以配置日志的精细度，以展示更多或更少的细节。\n日志可以是粗粒度的，如只显示组件内的错误，\n也可以是细粒度的，如显示事件的每一个跟踪步骤（比如 HTTP 访问日志、pod 状态更新、控制器动作或调度器决策）。"}
{"en": "body", "zh": "{{< warning >}}"}
{"en": "In contrast to the command line flags described here, the *log\noutput* itself does *not* fall under the Kubernetes API stability guarantees:\nindividual log entries and their formatting may change from one release\nto the next!", "zh": "与此处描述的命令行标志不同，日志输出本身不属于 Kubernetes API 的稳定性保证范围：\n单个日志条目及其格式可能会在不同版本之间发生变化！\n{{< /warning >}}\n\n## Klog"}
{"en": "klog is the Kubernetes logging library. [klog](https://github.com/kubernetes/klog)\ngenerates log messages for the Kubernetes system components.", "zh": "klog 是 Kubernetes 的日志库。\n[klog](https://github.com/kubernetes/klog)\n为 Kubernetes 系统组件生成日志消息。"}
{"en": "Kubernetes is in the process of simplifying logging in its components.\nThe following klog command line flags\n[are deprecated](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components)\nstarting with Kubernetes v1.23 and removed in Kubernetes v1.26:", "zh": "Kubernetes 正在进行简化其组件日志的努力。下面的 klog 命令行参数从 Kubernetes v1.23\n开始[已被废弃](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components)，\n在 Kubernetes v1.26 中被移除：\n\n- `--add-dir-header`\n- `--alsologtostderr`\n- `--log-backtrace-at`\n- `--log-dir`\n- `--log-file`\n- `--log-file-max-size`\n- `--logtostderr`\n- `--one-output`\n- `--skip-headers`\n- `--skip-log-headers`\n- `--stderrthreshold`"}
{"en": "Output will always be written to stderr, regardless of the output format. Output redirection is\nexpected to be handled by the component which invokes a Kubernetes component. This can be a POSIX\nshell or a tool like systemd.", "zh": "输出总会被写到标准错误输出（stderr）之上，无论输出格式如何。\n对输出的重定向将由调用 Kubernetes 组件的软件来处理。\n这一软件可以是 POSIX Shell 或者类似 systemd 这样的工具。"}
{"en": "In some cases, for example a distroless container or a Windows system service, those options are\nnot available. Then the\n[`kube-log-runner`](https://github.com/kubernetes/kubernetes/blob/d2a8a81639fcff8d1221b900f66d28361a170654/staging/src/k8s.io/component-base/logs/kube-log-runner/README.md)\nbinary can be used as wrapper around a Kubernetes component to redirect\noutput. A prebuilt binary is included in several Kubernetes base images under\nits traditional name as `/go-runner` and as `kube-log-runner` in server and\nnode release archives.", "zh": "在某些场合下，例如对于无发行主体的（distroless）容器或者 Windows 系统服务，\n这些替代方案都是不存在的。那么你可以使用\n[`kube-log-runner`](https://github.com/kubernetes/kubernetes/blob/d2a8a81639fcff8d1221b900f66d28361a170654/staging/src/k8s.io/component-base/logs/kube-log-runner/README.md)\n可执行文件来作为 Kubernetes 的封装层，完成对输出的重定向。\n在很多 Kubernetes 基础镜像中，都包含一个预先构建的可执行程序。\n这个程序原来称作 `/go-runner`，而在服务器和节点的发行版本库中，称作 `kube-log-runner`。"}
{"en": "This table shows how `kube-log-runner` invocations correspond to shell redirection:", "zh": "下表展示的是 `kube-log-runner` 调用与 Shell 重定向之间的对应关系："}
{"en": "| Usage                                    | POSIX shell (such as bash) | `kube-log-runner <options> <cmd>`                           |\n| -----------------------------------------|----------------------------|-------------------------------------------------------------|\n| Merge stderr and stdout, write to stdout | `2>&1`                     | `kube-log-runner` (default behavior)                        |\n| Redirect both into log file              | `1>>/tmp/log 2>&1`         | `kube-log-runner -log-file=/tmp/log`                        |\n| Copy into log file and to stdout         | `2>&1 \\| tee -a /tmp/log`  | `kube-log-runner -log-file=/tmp/log -also-stdout`           |\n| Redirect only stdout into log file       | `>/tmp/log`                | `kube-log-runner -log-file=/tmp/log -redirect-stderr=false` |", "zh": "| 用法                            | POSIX Shell（例如 Bash） | `kube-log-runner <options> <cmd>`  |\n| --------------------------------|--------------------------|------------------------------------|\n| 合并 stderr 与 stdout，写出到 stdout | `2>&1`             | `kube-log-runner`（默认行为 ）|\n| 将 stderr 与 stdout 重定向到日志文件 | `1>>/tmp/log 2>&1` | `kube-log-runner -log-file=/tmp/log` |\n| 输出到 stdout 并复制到日志文件中     | `2>&1 \\| tee -a /tmp/log`  | `kube-log-runner -log-file=/tmp/log -also-stdout` |\n| 仅将 stdout 重定向到日志 | `>/tmp/log` | `kube-log-runner -log-file=/tmp/log -redirect-stderr=false` |"}
{"en": "### Klog output\n\nAn example of the traditional klog native format:", "zh": "### klog 输出   {#klog-output}\n\n传统的 klog 原生格式示例：\n\n```\nI1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756]\n```"}
{"en": "The message string may contain line breaks:", "zh": "消息字符串可能包含换行符：\n\n```\nI1025 00:15:15.525108       1 example.go:79] This is a message\nwhich has a line break.\n```"}
{"en": "### Structured Logging", "zh": "### 结构化日志   {#structured-logging}\n\n{{< feature-state for_k8s_version=\"v1.23\" state=\"beta\" >}}\n\n{{< warning >}}"}
{"en": "Migration to structured log messages is an ongoing process. Not all log messages are structured in\nthis version. When parsing log files, you must also handle unstructured log messages.\n\nLog formatting and value serialization are subject to change.", "zh": "迁移到结构化日志消息是一个正在进行的过程。在此版本中，并非所有日志消息都是结构化的。\n解析日志文件时，你也必须要处理非结构化日志消息。\n\n日志格式和值的序列化可能会发生变化。\n{{< /warning>}}"}
{"en": "Structured logging introduces a uniform structure in log messages allowing for programmatic\nextraction of information. You can store and process structured logs with less effort and cost.\nThe code which generates a log message determines whether it uses the traditional unstructured\nklog output or structured logging.", "zh": "结构化日志记录旨在日志消息中引入统一结构，以便以编程方式提取信息。\n你可以方便地用更小的开销来处理结构化日志。\n生成日志消息的代码决定其使用传统的非结构化的 klog 还是结构化的日志。"}
{"en": "The default formatting of structured log messages is as text, with a format that is backward\ncompatible with traditional klog:", "zh": "默认的结构化日志消息是以文本形式呈现的，其格式与传统的 klog 保持向后兼容：\n\n```\n<klog header> \"<message>\" <key1>=\"<value1>\" <key2>=\"<value2>\" ...\n```"}
{"en": "Example:", "zh": "示例：\n\n```\nI1025 00:15:15.525108       1 controller_utils.go:116] \"Pod status updated\" pod=\"kube-system/kubedns\" status=\"ready\"\n```"}
{"en": "Strings are quoted. Other values are formatted with\n[`%+v`](https://pkg.go.dev/fmt#hdr-Printing), which may cause log messages to\ncontinue on the next line [depending on the data](https://github.com/kubernetes/kubernetes/issues/106428).", "zh": "字符串在输出时会被添加引号。其他数值类型都使用 [`%+v`](https://pkg.go.dev/fmt#hdr-Printing)\n来格式化，因此可能导致日志消息会延续到下一行，\n[具体取决于数据本身](https://github.com/kubernetes/kubernetes/issues/106428)。\n\n```\nI1025 00:15:15.525108       1 example.go:116] \"Example\" data=\"This is text with a line break\\nand \\\"quotation marks\\\".\" someInt=1 someFloat=0.1 someStruct={StringField: First line,\nsecond line.}\n```"}
{"en": "### Contextual Logging", "zh": "### 上下文日志   {#contextual-logging}\n\n{{< feature-state for_k8s_version=\"v1.30\" state=\"beta\" >}}"}
{"en": "Contextual logging builds on top of structured logging. It is primarily about\nhow developers use logging calls: code based on that concept is more flexible\nand supports additional use cases as described in the [Contextual Logging\nKEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging).", "zh": "上下文日志建立在结构化日志之上。\n它主要是关于开发人员如何使用日志记录调用：基于该概念的代码将更加灵活，\n并且支持在[结构化日志 KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging)\n中描述的额外用例。"}
{"en": "If developers use additional functions like `WithValues` or `WithName` in\ntheir components, then log entries contain additional information that gets\npassed into functions by their caller.", "zh": "如果开发人员在他们的组件中使用额外的函数，比如 `WithValues` 或 `WithName`，\n那么日志条目将会包含额外的信息，这些信息会被调用者传递给函数。"}
{"en": "For Kubernetes {{< skew currentVersion >}}, this is gated behind the `ContextualLogging`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) and is\nenabled by default. The infrastructure for this was added in 1.24 without\nmodifying components. The\n[`component-base/logs/example`](https://github.com/kubernetes/kubernetes/blob/v1.24.0-beta.0/staging/src/k8s.io/component-base/logs/example/cmd/logger.go)\ncommand demonstrates how to use the new logging calls and how a component\nbehaves that supports contextual logging.", "zh": "对于 Kubernetes {{< skew currentVersion >}}，这一特性是由 `StructuredLogging`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)所控制的，默认启用。\n这个基础设施是在 1.24 中被添加的，并不需要修改组件。\n该 [`component-base/logs/example`](https://github.com/kubernetes/kubernetes/blob/v1.24.0-beta.0/staging/src/k8s.io/component-base/logs/example/cmd/logger.go)\n命令演示了如何使用新的日志记录调用以及组件如何支持上下文日志记录。\n\n```console\n$ cd $GOPATH/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/\n$ go run . --help\n...\n      --feature-gates mapStringBool  A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:\n                                     AllAlpha=true|false (ALPHA - default=false)\n                                     AllBeta=true|false (BETA - default=false)\n                                     ContextualLogging=true|false (BETA - default=true)\n$ go run . --feature-gates ContextualLogging=true\n...\nI0222 15:13:31.645988  197901 example.go:54] \"runtime\" logger=\"example.myname\" foo=\"bar\" duration=\"1m0s\"\nI0222 15:13:31.646007  197901 example.go:55] \"another runtime\" logger=\"example\" foo=\"bar\" duration=\"1h0m0s\" duration=\"1m0s\"\n```"}
{"en": "The `logger` key and `foo=\"bar\"` were added by the caller of the function\nwhich logs the `runtime` message and `duration=\"1m0s\"` value, without having to\nmodify that function.\n\nWith contextual logging disable, `WithValues` and `WithName` do nothing and log\ncalls go through the global klog logger. Therefore this additional information\nis not in the log output anymore:", "zh": "`logger` 键和 `foo=\"bar\"` 会被函数的调用者添加上，\n不需修改该函数，它就会记录 `runtime` 消息和 `duration=\"1m0s\"` 值。\n\n禁用上下文日志后，`WithValues` 和 `WithName` 什么都不会做，\n并且会通过调用全局的 klog 日志记录器记录日志。\n因此，这些附加信息不再出现在日志输出中：\n\n```console\n$ go run . --feature-gates ContextualLogging=false\n...\nI0222 15:14:40.497333  198174 example.go:54] \"runtime\" duration=\"1m0s\"\nI0222 15:14:40.497346  198174 example.go:55] \"another runtime\" duration=\"1h0m0s\" duration=\"1m0s\"\n```"}
{"en": "### JSON log format", "zh": "### JSON 日志格式   {#json-log-format}\n\n{{< feature-state for_k8s_version=\"v1.19\" state=\"alpha\" >}}\n\n{{< warning >}}"}
{"en": "JSON output does not support many standard klog flags. For list of unsupported klog flags, see the\n[Command line tool reference](/docs/reference/command-line-tools-reference/).\n\nNot all logs are guaranteed to be written in JSON format (for example, during process start).\nIf you intend to parse logs, make sure you can handle log lines that are not JSON as well.\n\nField names and JSON serialization are subject to change.", "zh": "JSON 输出并不支持太多标准 klog 参数。对于不受支持的 klog 参数的列表，\n请参见[命令行工具参考](/zh-cn/docs/reference/command-line-tools-reference/)。\n\n并不是所有日志都保证写成 JSON 格式（例如，在进程启动期间）。\n如果你打算解析日志，请确保可以处理非 JSON 格式的日志行。\n\n字段名和 JSON 序列化可能会发生变化。\n{{< /warning >}}"}
{"en": "The `--logging-format=json` flag changes the format of logs from klog native format to JSON format.\nExample of JSON log format (pretty printed):", "zh": "`--logging-format=json` 参数将日志格式从 klog 原生格式改为 JSON 格式。\nJSON 日志格式示例（美化输出）：\n\n```json\n{\n   \"ts\": 1580306777.04728,\n   \"v\": 4,\n   \"msg\": \"Pod status updated\",\n   \"pod\":{\n      \"name\": \"nginx-1\",\n      \"namespace\": \"default\"\n   },\n   \"status\": \"ready\"\n}\n```"}
{"en": "Keys with special meaning:\n\n* `ts` - timestamp as Unix time (required, float)\n* `v` - verbosity (only for info and not for error messages, int)\n* `err` - error string (optional, string)\n* `msg` - message (required, string)\n\nList of components currently supporting JSON format:", "zh": "具有特殊意义的 key：\n\n* `ts` - Unix 时间风格的时间戳（必选项，浮点值）\n* `v` - 精细度（仅用于 info 级别，不能用于错误信息，整数）\n* `err` - 错误字符串（可选项，字符串）\n* `msg` - 消息（必选项，字符串）\n\n当前支持 JSON 格式的组件列表：\n\n* {{< glossary_tooltip term_id=\"kube-controller-manager\" text=\"kube-controller-manager\" >}}\n* {{< glossary_tooltip term_id=\"kube-apiserver\" text=\"kube-apiserver\" >}}\n* {{< glossary_tooltip term_id=\"kube-scheduler\" text=\"kube-scheduler\" >}}\n* {{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}}"}
{"en": "### Log verbosity level\n\nThe `-v` flag controls log verbosity. Increasing the value increases the number of logged events.\nDecreasing the value decreases the number of logged events.  Increasing verbosity settings logs\nincreasingly less severe events. A verbosity setting of 0 logs only critical events.", "zh": "### 日志精细度级别   {#log-verbosity-level}\n\n参数 `-v` 控制日志的精细度。增大该值会增大日志事件的数量。\n减小该值可以减小日志事件的数量。增大精细度会记录更多的不太严重的事件。\n精细度设置为 0 时只记录关键（critical）事件。"}
{"en": "### Log location\n\nThere are two types of system components: those that run in a container and those\nthat do not run in a container. For example:\n\n* The Kubernetes scheduler and kube-proxy run in a container.\n* The kubelet and {{<glossary_tooltip term_id=\"container-runtime\" text=\"container runtime\">}}\n  do not run in containers.", "zh": "### 日志位置   {#log-location}\n\n有两种类型的系统组件：运行在容器中的组件和不运行在容器中的组件。例如：\n\n* Kubernetes 调度器和 kube-proxy 在容器中运行。\n* kubelet 和{{<glossary_tooltip term_id=\"container-runtime\" text=\"容器运行时\">}}不在容器中运行。"}
{"en": "On machines with systemd, the kubelet and container runtime write to journald.\nOtherwise, they write to `.log` files in the `/var/log` directory.\nSystem components inside containers always write to `.log` files in the `/var/log` directory,\nbypassing the default logging mechanism.\nSimilar to the container logs, you should rotate system component logs in the `/var/log` directory.\nIn Kubernetes clusters created by the `kube-up.sh` script, log rotation is configured by the `logrotate` tool.\nThe `logrotate` tool rotates logs daily, or once the log size is greater than 100MB.", "zh": "在使用 systemd 的系统中，kubelet 和容器运行时写入 journald。\n在别的系统中，日志写入 `/var/log` 目录下的 `.log` 文件中。\n容器中的系统组件总是绕过默认的日志记录机制，写入 `/var/log` 目录下的 `.log` 文件。\n与容器日志类似，你应该轮转 `/var/log` 目录下系统组件日志。\n在 `kube-up.sh` 脚本创建的 Kubernetes 集群中，日志轮转由 `logrotate` 工具配置。\n`logrotate` 工具，每天或者当日志大于 100MB 时，轮转日志。"}
{"en": "## Log query", "zh": "## 日志查询   {#log-query}\n\n{{< feature-state feature_gate_name=\"NodeLogQuery\" >}}"}
{"en": "To help with debugging issues on nodes, Kubernetes v1.27 introduced a feature that allows viewing logs of services\nrunning on the node. To use the feature, ensure that the `NodeLogQuery`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled for that node, and that the\nkubelet configuration options `enableSystemLogHandler` and `enableSystemLogQuery` are both set to true. On Linux\nthe assumption is that service logs are available via journald. On Windows the assumption is that service logs are\navailable in the application log provider. On both operating systems, logs are also available by reading files within\n`/var/log/`.", "zh": "为了帮助在节点上调试问题，Kubernetes v1.27 引入了一个特性来查看节点上当前运行服务的日志。\n要使用此特性，请确保已为节点启用了 `NodeLogQuery`\n[特性门控](/zh-cn/docs/reference/command-line-tools-reference/feature-gates/)，\n且 kubelet 配置选项 `enableSystemLogHandler` 和 `enableSystemLogQuery` 均被设置为 true。\n在 Linux 上，我们假设可以通过 journald 查看服务日志。\n在 Windows 上，我们假设可以在应用日志提供程序中查看服务日志。\n在两种操作系统上，都可以通过读取 `/var/log/` 内的文件查看日志。"}
{"en": "Provided you are authorized to interact with node objects, you can try out this feature on all your nodes or\njust a subset. Here is an example to retrieve the kubelet service logs from a node:\n\n```shell\n# Fetch kubelet logs from a node named node-1.example\nkubectl get --raw \"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet\"\n```", "zh": "假如你被授权与节点对象交互，你可以在所有节点或只是某个子集上试用此特性。\n这里有一个从节点中检索 kubelet 服务日志的例子：\n\n```shell\n# 从名为 node-1.example 的节点中获取 kubelet 日志\nkubectl get --raw \"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet\"\n```"}
{"en": "You can also fetch files, provided that the files are in a directory that the kubelet allows for log\nfetches. For example, you can fetch a log from `/var/log` on a Linux node:", "zh": "你也可以获取文件，前提是日志文件位于 kubelet 允许进行日志获取的目录中。\n例如你可以从 Linux 节点上的 `/var/log` 中获取日志。\n\n```shell\nkubectl get --raw \"/api/v1/nodes/<insert-node-name-here>/proxy/logs/?query=/<insert-log-file-name-here>\"\n```"}
{"en": "The kubelet uses heuristics to retrieve logs. This helps if you are not aware whether a given system service is\nwriting logs to the operating system's native logger like journald or to a log file in `/var/log/`. The heuristics\nfirst checks the native logger and if that is not available attempts to retrieve the first logs from\n`/var/log/<servicename>` or `/var/log/<servicename>.log` or `/var/log/<servicename>/<servicename>.log`.\n\nThe complete list of options that can be used are:", "zh": "kubelet 使用启发方式来检索日志。\n如果你还未意识到给定的系统服务正将日志写入到操作系统的原生日志记录程序（例如 journald）\n或 `/var/log/` 中的日志文件，这会很有帮助。这种启发方式先检查原生的日志记录程序，\n如果不可用，则尝试从 `/var/log/<servicename>`、`/var/log/<servicename>.log`\n或 `/var/log/<servicename>/<servicename>.log` 中检索第一批日志。\n\n可用选项的完整列表如下："}
{"en": "Option | Description\n------ | -----------\n`boot` | boot show messages from a specific system boot\n`pattern` | pattern filters log entries by the provided PERL-compatible regular expression\n`query` | query specifies services(s) or files from which to return logs (required)\n`sinceTime` | an [RFC3339](https://www.rfc-editor.org/rfc/rfc3339) timestamp from which to show logs (inclusive)\n`untilTime` | an [RFC3339](https://www.rfc-editor.org/rfc/rfc3339) timestamp until which to show logs (inclusive)\n`tailLines` | specify how many lines from the end of the log to retrieve; the default is to fetch the whole log", "zh": "选项 | 描述\n------ | -----------\n`boot` | `boot` 显示来自特定系统引导的消息\n`pattern` | `pattern` 通过提供的兼容 PERL 的正则表达式来过滤日志条目\n`query` | `query` 是必需的，指定返回日志的服务或文件\n`sinceTime` | 显示日志的 [RFC3339](https://www.rfc-editor.org/rfc/rfc3339) 起始时间戳（包含）\n`untilTime` | 显示日志的 [RFC3339](https://www.rfc-editor.org/rfc/rfc3339) 结束时间戳（包含）\n`tailLines` | 指定要从日志的末尾检索的行数；默认为获取全部日志"}
{"en": "Example of a more complex query:\n\n```shell\n# Fetch kubelet logs from a node named node-1.example that have the word \"error\"\nkubectl get --raw \"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet&pattern=error\"\n```", "zh": "更复杂的查询示例：\n\n```shell\n# 从名为 node-1.example 且带有单词 \"error\" 的节点中获取 kubelet 日志\nkubectl get --raw \"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet&pattern=error\"\n```\n\n## {{% heading \"whatsnext\" %}}"}
{"en": "* Read about the [Kubernetes Logging Architecture](/docs/concepts/cluster-administration/logging/)\n* Read about [Structured Logging](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging)\n* Read about [Contextual Logging](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging)\n* Read about [deprecation of klog flags](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components)\n* Read about the [Conventions for logging severity](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md)\n* Read about [Log Query](https://kep.k8s.io/2258)", "zh": "* 阅读 [Kubernetes 日志架构](/zh-cn/docs/concepts/cluster-administration/logging/)\n* 阅读[结构化日志提案（英文）](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging)\n* 阅读[上下文日志提案（英文）](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging)\n* 阅读 [klog 参数的废弃（英文）](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components)\n* 阅读[日志严重级别约定（英文）](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md)\n* 阅读[日志查询](https://kep.k8s.io/2258)"}
{"en": "overview", "zh": "{{% thirdparty-content %}}"}
{"en": "Add-ons extend the functionality of Kubernetes.\n\nThis page lists some of the available add-ons and links to their respective\ninstallation instructions. The list does not try to be exhaustive.", "zh": "Add-on 扩展了 Kubernetes 的功能。\n\n本文列举了一些可用的 add-on 以及到它们各自安装说明的链接。该列表并不试图详尽无遗。"}
{"en": "## Networking and Network Policy\n\n* [ACI](https://www.github.com/noironetworks/aci-containers) provides integrated\n  container networking and network security with Cisco ACI.\n* [Antrea](https://antrea.io/) operates at Layer 3/4 to provide networking and\n  security services for Kubernetes, leveraging Open vSwitch as the networking\n  data plane. Antrea is a [CNCF project at the Sandbox level](https://www.cncf.io/projects/antrea/).\n* [Calico](https://www.tigera.io/project-calico/) is a networking and network\n  policy provider. Calico supports a flexible set of networking options so you\n  can choose the most efficient option for your situation, including non-overlay\n  and overlay networks, with or without BGP. Calico uses the same engine to\n  enforce network policy for hosts, pods, and (if using Istio & Envoy)\n  applications at the service mesh layer.\n* [Canal](https://projectcalico.docs.tigera.io/getting-started/kubernetes/flannel/flannel)\n  unites Flannel and Calico, providing networking and network policy.\n* [Cilium](https://github.com/cilium/cilium) is a networking, observability,\n  and security solution with an eBPF-based data plane. Cilium provides a\n  simple flat Layer 3 network with the ability to span multiple clusters\n  in either a native routing or overlay/encapsulation mode, and can enforce\n  network policies on L3-L7 using an identity-based security model that is\n  decoupled from network addressing. Cilium can act as a replacement for\n  kube-proxy; it also offers additional, opt-in observability and security features.\n  Cilium is a [CNCF project at the Graduated level](https://www.cncf.io/projects/cilium/).", "zh": "## 联网和网络策略   {#networking-and-network-policy}\n\n* [ACI](https://www.github.com/noironetworks/aci-containers) 通过 Cisco ACI 提供集成的容器网络和安全网络。\n* [Antrea](https://antrea.io/) 在第 3/4 层执行操作，为 Kubernetes\n  提供网络连接和安全服务。Antrea 利用 Open vSwitch 作为网络的数据面。\n  Antrea 是一个[沙箱级的 CNCF 项目](https://www.cncf.io/projects/antrea/)。\n* [Calico](https://www.tigera.io/project-calico/) 是一个联网和网络策略供应商。\n  Calico 支持一套灵活的网络选项，因此你可以根据自己的情况选择最有效的选项，包括非覆盖和覆盖网络，带或不带 BGP。\n  Calico 使用相同的引擎为主机、Pod 和（如果使用 Istio 和 Envoy）应用程序在服务网格层执行网络策略。\n* [Canal](https://projectcalico.docs.tigera.io/getting-started/kubernetes/flannel/flannel)\n  结合 Flannel 和 Calico，提供联网和网络策略。\n* [Cilium](https://github.com/cilium/cilium) 是一种网络、可观察性和安全解决方案，具有基于 eBPF 的数据平面。\n  Cilium 提供了简单的 3 层扁平网络，\n  能够以原生路由（routing）和覆盖/封装（overlay/encapsulation）模式跨越多个集群，\n  并且可以使用与网络寻址分离的基于身份的安全模型在 L3 至 L7 上实施网络策略。\n  Cilium 可以作为 kube-proxy 的替代品；它还提供额外的、可选的可观察性和安全功能。\n  Cilium 是一个[毕业级别的 CNCF 项目](https://www.cncf.io/projects/cilium/)。"}
{"en": "* [CNI-Genie](https://github.com/cni-genie/CNI-Genie) enables Kubernetes to seamlessly\n  connect to a choice of CNI plugins, such as Calico, Canal, Flannel, or Weave.\n  CNI-Genie is a [CNCF project at the Sandbox level](https://www.cncf.io/projects/cni-genie/).\n* [Contiv](https://contivpp.io/) provides configurable networking (native L3 using BGP,\n  overlay using vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich\n  policy framework. Contiv project is fully [open sourced](https://github.com/contiv).\n  The [installer](https://github.com/contiv/install) provides both kubeadm and\n  non-kubeadm based installation options.\n* [Contrail](https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/),\n  based on [Tungsten Fabric](https://tungsten.io), is an open source, multi-cloud\n  network virtualization and policy management platform. Contrail and Tungsten\n  Fabric are integrated with orchestration systems such as Kubernetes, OpenShift,\n  OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods\n  and bare metal workloads.", "zh": "* [CNI-Genie](https://github.com/cni-genie/CNI-Genie) 使 Kubernetes 无缝连接到\n  Calico、Canal、Flannel 或 Weave 等其中一种 CNI 插件。\n  CNI-Genie 是一个[沙箱级的 CNCF 项目](https://www.cncf.io/projects/cni-genie/)。\n* [Contiv](https://contivpp.io/) 为各种用例和丰富的策略框架提供可配置的网络\n  （带 BGP 的原生 L3、带 vxlan 的覆盖、标准 L2 和 Cisco-SDN/ACI）。\n  Contiv 项目完全[开源](https://github.com/contiv)。\n  其[安装程序](https://github.com/contiv/install) 提供了基于 kubeadm 和非 kubeadm 的安装选项。\n* [Contrail](https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/) 基于\n  [Tungsten Fabric](https://tungsten.io)，是一个开源的多云网络虚拟化和策略管理平台。\n  Contrail 和 Tungsten Fabric 与业务流程系统（例如 Kubernetes、OpenShift、OpenStack 和 Mesos）集成在一起，\n  为虚拟机、容器或 Pod 以及裸机工作负载提供了隔离模式。"}
{"en": "* [Flannel](https://github.com/flannel-io/flannel#deploying-flannel-manually) is\n  an overlay network provider that can be used with Kubernetes.\n* [Gateway API](/docs/concepts/services-networking/gateway/) is an open source project managed by\n  the [SIG Network](https://github.com/kubernetes/community/tree/master/sig-network) community and\n  provides an expressive, extensible, and role-oriented API for modeling service networking.\n* [Knitter](https://github.com/ZTE/Knitter/) is a plugin to support multiple network\n  interfaces in a Kubernetes pod.\n* [Multus](https://github.com/k8snetworkplumbingwg/multus-cni) is a Multi plugin for\n  multiple network support in Kubernetes to support all CNI plugins\n  (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK and\n  VPP based workloads in Kubernetes.", "zh": "* [Flannel](https://github.com/flannel-io/flannel#deploying-flannel-manually)\n  是一个可以用于 Kubernetes 的 overlay 网络提供者。\n* [Gateway API](/zh-cn/docs/concepts/services-networking/gateway/) 是一个由\n  [SIG Network](https://github.com/kubernetes/community/tree/master/sig-network) 社区管理的开源项目，\n  为服务网络建模提供一种富有表达力、可扩展和面向角色的 API。\n* [Knitter](https://github.com/ZTE/Knitter/) 是在一个 Kubernetes Pod 中支持多个网络接口的插件。\n* [Multus](https://github.com/k8snetworkplumbingwg/multus-cni) 是一个多插件，\n  可在 Kubernetes 中提供多种网络支持，以支持所有 CNI 插件（例如 Calico、Cilium、Contiv、Flannel），\n  而且包含了在 Kubernetes 中基于 SRIOV、DPDK、OVS-DPDK 和 VPP 的工作负载。"}
{"en": "* [OVN-Kubernetes](https://github.com/ovn-org/ovn-kubernetes/) is a networking\n  provider for Kubernetes based on [OVN (Open Virtual Network)](https://github.com/ovn-org/ovn/),\n  a virtual networking implementation that came out of the Open vSwitch (OVS) project.\n  OVN-Kubernetes provides an overlay based networking implementation for Kubernetes,\n  including an OVS based implementation of load balancing and network policy.\n* [Nodus](https://github.com/akraino-edge-stack/icn-nodus) is an OVN based CNI\n  controller plugin to provide cloud native based Service function chaining(SFC).", "zh": "* [OVN-Kubernetes](https://github.com/ovn-org/ovn-kubernetes/) 是一个 Kubernetes 网络驱动，\n  基于 [OVN（Open Virtual Network）](https://github.com/ovn-org/ovn/)实现，是从 Open vSwitch (OVS)\n  项目衍生出来的虚拟网络实现。OVN-Kubernetes 为 Kubernetes 提供基于覆盖网络的网络实现，\n  包括一个基于 OVS 实现的负载均衡器和网络策略。\n* [Nodus](https://github.com/akraino-edge-stack/icn-nodus) 是一个基于 OVN 的 CNI 控制器插件，\n  提供基于云原生的服务功能链 (SFC)。"}
{"en": "* [NSX-T](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html) Container Plug-in (NCP)\n  provides integration between VMware NSX-T and container orchestrators such as\n  Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS\n  platforms such as Pivotal Container Service (PKS) and OpenShift.\n* [Nuage](https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst)\n  is an SDN platform that provides policy-based networking between Kubernetes\n  Pods and non-Kubernetes environments with visibility and security monitoring.\n* [Romana](https://github.com/romana) is a Layer 3 networking solution for pod\n  networks that also supports the [NetworkPolicy](/docs/concepts/services-networking/network-policies/) API.\n* [Spiderpool](https://github.com/spidernet-io/spiderpool) is an underlay and RDMA\n  networking solution for Kubernetes. Spiderpool is supported on bare metal, virtual machines,\n  and public cloud environments.\n* [Weave Net](https://github.com/rajch/weave#using-weave-on-kubernetes)\n  provides networking and network policy, will carry on working on both sides\n  of a network partition, and does not require an external database.", "zh": "* [NSX-T](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html) 容器插件（NCP）\n  提供了 VMware NSX-T 与容器协调器（例如 Kubernetes）之间的集成，以及 NSX-T 与基于容器的\n  CaaS / PaaS 平台（例如关键容器服务（PKS）和 OpenShift）之间的集成。\n* [Nuage](https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst)\n  是一个 SDN 平台，可在 Kubernetes Pods 和非 Kubernetes 环境之间提供基于策略的联网，并具有可视化和安全监控。\n* [Romana](https://github.com/romana) 是一个 Pod 网络的第三层解决方案，并支持\n  [NetworkPolicy](/zh-cn/docs/concepts/services-networking/network-policies/) API。\n* [Spiderpool](https://github.com/spidernet-io/spiderpool) 为 Kubernetes\n  提供了下层网络和 RDMA 高速网络解决方案，兼容裸金属、虚拟机和公有云等运行环境。\n* [Weave Net](https://github.com/rajch/weave#using-weave-on-kubernetes)\n  提供在网络分组两端参与工作的联网和网络策略，并且不需要额外的数据库。"}
{"en": "## Service Discovery\n\n* [CoreDNS](https://coredns.io) is a flexible, extensible DNS server which can\n  be [installed](https://github.com/coredns/deployment/tree/master/kubernetes)\n  as the in-cluster DNS for pods.", "zh": "## 服务发现   {#service-discovery}\n\n* [CoreDNS](https://coredns.io) 是一种灵活的，可扩展的 DNS 服务器，可以\n  [安装](https://github.com/coredns/deployment/tree/master/kubernetes)为集群内的 Pod 提供 DNS 服务。"}
{"en": "## Visualization &amp; Control\n\n* [Dashboard](https://github.com/kubernetes/dashboard#kubernetes-dashboard)\n  is a dashboard web interface for Kubernetes.", "zh": "## 可视化管理   {#visualization-and-control}\n\n* [Dashboard](https://github.com/kubernetes/dashboard#kubernetes-dashboard) 是一个 Kubernetes 的 Web 控制台界面。"}
{"en": "## Infrastructure\n\n* [KubeVirt](https://kubevirt.io/user-guide/#/installation/installation) is an add-on\n  to run virtual machines on Kubernetes. Usually run on bare-metal clusters.\n* The\n  [node problem detector](https://github.com/kubernetes/node-problem-detector)\n  runs on Linux nodes and reports system issues as either\n  [Events](/docs/reference/kubernetes-api/cluster-resources/event-v1/) or\n  [Node conditions](/docs/concepts/architecture/nodes/#condition).", "zh": "## 基础设施   {#infrastructure}\n\n* [KubeVirt](https://kubevirt.io/user-guide/#/installation/installation) 是可以让 Kubernetes\n  运行虚拟机的 add-on。通常运行在裸机集群上。\n* [节点问题检测器](https://github.com/kubernetes/node-problem-detector) 在 Linux 节点上运行，\n  并将系统问题报告为[事件](/zh-cn/docs/reference/kubernetes-api/cluster-resources/event-v1/)\n  或[节点状况](/zh-cn/docs/concepts/architecture/nodes/#condition)。"}
{"en": "## Instrumentation\n\n* [kube-state-metrics](/docs/concepts/cluster-administration/kube-state-metrics)", "zh": "## 插桩  {#instrumentation}\n\n* [kube-state-metrics](/zh-cn/docs/concepts/cluster-administration/kube-state-metrics)"}
{"en": "## Legacy Add-ons\n\nThere are several other add-ons documented in the deprecated\n[cluster/addons](https://git.k8s.io/kubernetes/cluster/addons) directory.\n\nWell-maintained ones should be linked to here. PRs welcome!", "zh": "## 遗留 Add-on   {#legacy-addons}\n\n还有一些其它 add-on 归档在已废弃的 [cluster/addons](https://git.k8s.io/kubernetes/cluster/addons) 路径中。\n\n维护完善的 add-on 应该被链接到这里。欢迎提出 PR！"}
{"en": "The cluster administration overview is for anyone creating or administering a Kubernetes cluster.\nIt assumes some familiarity with core Kubernetes [concepts](/docs/concepts/).", "zh": "集群管理概述面向任何创建和管理 Kubernetes 集群的读者人群。\n我们假设你大概了解一些核心的 Kubernetes [概念](/zh-cn/docs/concepts/)。"}
{"en": "## Planning a cluster\n\nSee the guides in [Setup](/docs/setup/) for examples of how to plan, set up, and configure\nKubernetes clusters. The solutions listed in this article are called *distros*.", "zh": "## 规划集群   {#planning-a-cluster}\n\n查阅[安装](/zh-cn/docs/setup/)中的指导，获取如何规划、建立以及配置 Kubernetes\n集群的示例。本文所列的文章称为**发行版**。\n\n{{< note >}}"}
{"en": "Not all distros are actively maintained. Choose distros which have been tested with a recent\nversion of Kubernetes.", "zh": "并非所有发行版都是被积极维护的。\n请选择使用最近 Kubernetes 版本测试过的发行版。\n{{< /note >}}"}
{"en": "Before choosing a guide, here are some considerations:", "zh": "在选择一个指南前，有一些因素需要考虑："}
{"en": "- Do you want to try out Kubernetes on your computer, or do you want to build a high-availability,\n  multi-node cluster? Choose distros best suited for your needs.\n- Will you be using **a hosted Kubernetes cluster**, such as\n  [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine/), or **hosting your own cluster**?\n- Will your cluster be **on-premises**, or **in the cloud (IaaS)**? Kubernetes does not directly\n  support hybrid clusters. Instead, you can set up multiple clusters.\n- **If you are configuring Kubernetes on-premises**, consider which\n  [networking model](/docs/concepts/cluster-administration/networking/) fits best.\n- Will you be running Kubernetes on **\"bare metal\" hardware** or on **virtual machines (VMs)**?\n- Do you **want to run a cluster**, or do you expect to do **active development of Kubernetes project code**?\n  If the latter, choose an actively-developed distro. Some distros only use binary releases, but\n  offer a greater variety of choices.\n- Familiarize yourself with the [components](/docs/concepts/overview/components/) needed to run a cluster.", "zh": "- 你是打算在你的计算机上尝试 Kubernetes，还是要构建一个高可用的多节点集群？\n  请选择最适合你需求的发行版。\n- 你正在使用类似 [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine/)\n  这样的**被托管的 Kubernetes 集群**, 还是**管理你自己的集群**？\n- 你的集群是在**本地**还是**云（IaaS）** 上？Kubernetes 不能直接支持混合集群。\n  作为代替，你可以建立多个集群。\n- **如果你在本地配置 Kubernetes**，\n  需要考虑哪种[网络模型](/zh-cn/docs/concepts/cluster-administration/networking/)最适合。\n- 你的 Kubernetes 在**裸机**上还是**虚拟机（VM）** 上运行？\n- 你是想**运行一个集群**，还是打算**参与开发 Kubernetes 项目代码**？\n  如果是后者，请选择一个处于开发状态的发行版。\n  某些发行版只提供二进制发布版，但提供更多的选择。\n- 让你自己熟悉运行一个集群所需的[组件](/zh-cn/docs/concepts/overview/components/)。"}
{"en": "## Managing a cluster\n\n* Learn how to [manage nodes](/docs/concepts/architecture/nodes/).\n  * Read about [cluster autoscaling](/docs/concepts/cluster-administration/cluster-autoscaling/).\n\n* Learn how to set up and manage the [resource quota](/docs/concepts/policy/resource-quotas/) for shared clusters.", "zh": "## 管理集群   {#managing-a-cluster}\n\n* 学习如何[管理节点](/zh-cn/docs/concepts/architecture/nodes/)。\n  * 阅读[集群自动扩缩](/zh-cn/docs/concepts/cluster-administration/cluster-autoscaling/)。\n\n* 学习如何设定和管理集群共享的[资源配额](/zh-cn/docs/concepts/policy/resource-quotas/)。"}
{"en": "## Securing a cluster\n* [Generate Certificates](/docs/tasks/administer-cluster/certificates/) describes the steps to\n  generate certificates using different tool chains.\n* [Kubernetes Container Environment](/docs/concepts/containers/container-environment/) describes\n  the environment for Kubelet managed containers on a Kubernetes node.\n* [Controlling Access to the Kubernetes API](/docs/concepts/security/controlling-access) describes\n  how Kubernetes implements access control for its own API.\n* [Authenticating](/docs/reference/access-authn-authz/authentication/) explains authentication in\n  Kubernetes, including the various authentication options.\n* [Authorization](/docs/reference/access-authn-authz/authorization/) is separate from\n  authentication, and controls how HTTP calls are handled.\n* [Using Admission Controllers](/docs/reference/access-authn-authz/admission-controllers/)\n  explains plug-ins which intercepts requests to the Kubernetes API server after authentication\n  and authorization.\n* [Using Sysctls in a Kubernetes Cluster](/docs/tasks/administer-cluster/sysctl-cluster/)\n  describes to an administrator how to use the `sysctl` command-line tool to set kernel parameters.\n* [Auditing](/docs/tasks/debug/debug-cluster/audit/) describes how to interact with Kubernetes'\n  audit logs.", "zh": "## 保护集群  {#securing-a-cluster}\n\n* [生成证书](/zh-cn/docs/tasks/administer-cluster/certificates/)描述了使用不同的工具链生成证书的步骤。\n* [Kubernetes 容器环境](/zh-cn/docs/concepts/containers/container-environment/)描述了\n  Kubernetes 节点上由 Kubelet 管理的容器的环境。\n* [控制对 Kubernetes API 的访问](/zh-cn/docs/concepts/security/controlling-access/)描述了 Kubernetes\n  如何为自己的 API 实现访问控制。\n* [身份认证](/zh-cn/docs/reference/access-authn-authz/authentication/)阐述了 Kubernetes\n  中的身份认证功能，包括许多认证选项。\n* [鉴权](/zh-cn/docs/reference/access-authn-authz/authorization/)与身份认证不同，用于控制如何处理 HTTP 请求。\n* [使用准入控制器](/zh-cn/docs/reference/access-authn-authz/admission-controllers)阐述了在认证和授权之后拦截到\n  Kubernetes API 服务的请求的插件。\n* [在 Kubernetes 集群中使用 sysctl](/zh-cn/docs/tasks/administer-cluster/sysctl-cluster/)\n  描述了管理员如何使用 `sysctl` 命令行工具来设置内核参数。\n* [审计](/zh-cn/docs/tasks/debug/debug-cluster/audit/)描述了如何与 Kubernetes 的审计日志交互。"}
{"en": "### Securing the kubelet\n\n* [Control Plane-Node communication](/docs/concepts/architecture/control-plane-node-communication/)\n* [TLS bootstrapping](/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/)\n* [Kubelet authentication/authorization](/docs/reference/access-authn-authz/kubelet-authn-authz/)", "zh": "### 保护 kubelet   {#securing-the-kubelet}\n\n* [节点与控制面之间的通信](/zh-cn/docs/concepts/architecture/control-plane-node-communication/)\n* [TLS 启动引导](/zh-cn/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/)\n* [Kubelet 认证/鉴权](/zh-cn/docs/reference/access-authn-authz/kubelet-authn-authz/)"}
{"en": "## Optional Cluster Services\n\n* [DNS Integration](/docs/concepts/services-networking/dns-pod-service/) describes how to resolve\n  a DNS name directly to a Kubernetes service.\n* [Logging and Monitoring Cluster Activity](/docs/concepts/cluster-administration/logging/)\n  explains how logging in Kubernetes works and how to implement it.", "zh": "## 可选集群服务   {#optional-cluster-services}\n\n* [DNS 集成](/zh-cn/docs/concepts/services-networking/dns-pod-service/)描述了如何将一个 DNS\n  名解析到一个 Kubernetes service。\n* [记录和监控集群活动](/zh-cn/docs/concepts/cluster-administration/logging/)阐述了 Kubernetes\n  的日志如何工作以及怎样实现。"}
